% vim:syntax=tex


\begin{figure}[t]
\footnotesize

{\bf bookkeeper-server.src.main.java.}org.apache.bookkeeper.bookie.Bookie

{\bf bookkeeper-server.src.main.java.}org.apache.bookkeeper.bookie.EntryLogger

{\bf bookkeeper-server.src.test.java.}org.apache.bookkeeper.bookie.LedgerCacheTest

\caption{Corrected BookKeeper goldset for issue \#10. Bold text denotes the portion removed.}
\label{fig:goldsetfix}
\vspace{-15pt}
\end{figure}


Our study has limitations that impact the validity of our findings,
as well as our ability to generalize them.
We describe some of these limitations and their impacts.

Threats to construct validity concern measurements accurately reflecting the features of interest.
A possible threat to construct validity is our benchmarks.
Errors in the datasets could result in inaccurate effectiveness measures.
The datasets were produced by other researchers, are publicly available,
and have been used in previous research~\cite{Dit-etal:2013, Revelle-etal:2010, Moreno-etal:2014}.
While both datasets extracted source code entities automatically from changesets and patches,
previous work shows this approach is on par with manual extraction~\cite{Corley-etal:2011}.

Additionally, we found errors within the datasets themselves that would be a threat to construct validity.
In particular, the Moreno et al. dataset included classes that had package names that were not valid.
Figure~\ref{fig:goldsetfix} describes the classes reported in the dataset and the corrected classes used in our dataset.
We make the assumption that the authors of this dataset used the directory structure of the project to build the package names.
Manual correction was required as our tool parses the files and uses the package name given in the source file, not the directory structure.

Threats to internal validity include possible defects in our tool chain and possible errors
in our execution of the study procedure,
the presence of which might affect the accuracy of our results and the conclusions we draw from them.
We controlled for these threats by testing our tool chain and by assessing the quality of our data.
Because we applied the same tool chain to all subject systems, any errors are systematic and are unlikely
to affect our results substantially.

Another threat to internal validity pertains to the value of parameters such as $K$ that we selected for all models trained.
We decided that the changeset and snapshot models should have the same parameters to help facilitate evaluation and comparison.
We argue that our study is not about selecting the best parameters,
but to show that our changeset TM-based FLT approach is reasonable.

Threats to external validity concern the extent to which we can generalize our results.
The subjects of our study comprise fourteen open source projects in Java,
so we cannot generalize our results to systems implemented in other languages.
However, the systems are of different sizes, are from different domains, and
have characteristics in common with those of systems developed in industry.

