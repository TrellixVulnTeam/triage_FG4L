<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 05:22:17 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/SOLR-1423/SOLR-1423.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[SOLR-1423] Lucene 2.9 RC4 may need some changes in Solr Analyzers using CharStream &amp; others</title>
                <link>https://issues.apache.org/jira/browse/SOLR-1423</link>
                <project id="12310230" key="SOLR">Solr</project>
                    <description>&lt;p&gt;Because of some backwards compatibility problems (&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1906&quot; title=&quot;Backwards problems with CharStream and Tokenizers with custom reset(Reader) method&quot; class=&quot;issue-link&quot; data-issue-key=&quot;LUCENE-1906&quot;&gt;&lt;del&gt;LUCENE-1906&lt;/del&gt;&lt;/a&gt;) we changed the CharStream/CharFilter API a little bit. Tokenizer now only has a input field of type java.io.Reader (as before the CharStream code). To correct offsets, it is now needed to call the Tokenizer.correctOffset(int) method, which delegates to the CharStream (if input is subclass of CharStream), else returns an uncorrected offset. Normally it is enough to change all occurences of input.correctOffset() to this.correctOffset() in Tokenizers. It should also be checked, if custom Tokenizers in Solr do correct their offsets.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12435422">SOLR-1423</key>
            <summary>Lucene 2.9 RC4 may need some changes in Solr Analyzers using CharStream &amp; others</summary>
                <type id="3" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/task.png">Task</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="koji">Koji Sekiguchi</assignee>
                                    <reporter username="thetaphi">Uwe Schindler</reporter>
                        <labels>
                    </labels>
                <created>Fri, 11 Sep 2009 13:08:48 +0100</created>
                <updated>Tue, 10 Nov 2009 15:52:14 +0000</updated>
                            <resolved>Fri, 18 Sep 2009 08:38:22 +0100</resolved>
                                    <version>1.4</version>
                                    <fixVersion>1.4</fixVersion>
                                    <component>Schema and Analysis</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>1</watches>
                                                                <comments>
                            <comment id="12754152" author="koji" created="Fri, 11 Sep 2009 15:33:46 +0100"  >&lt;p&gt;I&apos;d like to check it before 1.4 release. I&apos;ll look into it once RC4 is checked in Solr.&lt;/p&gt;</comment>
                            <comment id="12754810" author="koji" created="Mon, 14 Sep 2009 03:25:23 +0100"  >&lt;p&gt;I thought I call tokenizer.correctOffset() in newToken() method, but I couldn&apos;t because the method is protected. In this patch, I converted the anonymous Tokenizer class to PatternTokenizer, and PatternTokenizer has the following:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+    &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; correct( &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; currentOffset ){                                   
+      &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; correctOffset( currentOffset );                                   
+    }                                                                          
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12754836" author="thetaphi" created="Mon, 14 Sep 2009 07:02:22 +0100"  >&lt;p&gt;I have seen this PatternTokenizer, too. The method is protected, as the corectOffset should only be called from inside Tokenizer (e.g. in incrementToken), never from the outside. Why does the PatternTokenizer does not have the methods newToken and so on in its own class (by the way: This should be updated to new TokenStream API)?&lt;/p&gt;</comment>
                            <comment id="12754844" author="thetaphi" created="Mon, 14 Sep 2009 07:41:22 +0100"  >&lt;p&gt;I searched for setOffset() in Solr source code and found one additional occurence of it without offset correcting in FieldType.java. This patch fixes this.&lt;/p&gt;

&lt;p&gt;I will provide a completely streaming PatternTokenizer not using ArrayLists soon. It will move the while(matcher.find()) loop into the incrementToken() enumeration and will also use the new TokenStream API.&lt;/p&gt;</comment>
                            <comment id="12754901" author="thetaphi" created="Mon, 14 Sep 2009 10:53:25 +0100"  >&lt;p&gt;This is a complete more effective rewrite of the whole Tokenizer (I would like to put this into, Lucene contrib, too!) using the new TokenStream API.&lt;/p&gt;

&lt;p&gt;When going through the code, I realized the following: This Tokenizer can return empty tokens, it only filters enpty tokens in split() mode. Is this exspected? If empty tokens should be omitted, the if (matcher.find()) should be replaced by while (match.find()) with if match.length==0 continue; - The logic behind the strange omit empty token at the end  will get very simple after this change.&lt;/p&gt;

&lt;p&gt;This patch removes the whole split()/group() methods from the factory as not needed anymore. If this is a backwards break, replace them by not used dummies (e.g. initialize a Tokenizer and return the token&apos;s TermText).&lt;/p&gt;

&lt;p&gt;In my opinion, one should never index empty tokens...&lt;/p&gt;

&lt;p&gt;A second thing: Lucene has a new BaseTokenStreamTest class for checking tokens without Token instances (which would no loger work, when Lucene 3.0 switches to Attributes only). Maybe you should update these test and use assertAnalyzesTo from the new base class instead.&lt;/p&gt;</comment>
                            <comment id="12755245" author="koji" created="Tue, 15 Sep 2009 00:32:53 +0100"  >&lt;p&gt;The patch that is Uwe&apos;s one with replacing split()/group() methods.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Why does the PatternTokenizer does not have the methods newToken and so on in its own class&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yeah, I&apos;d realized it immediately after posting the patch, but I was going to be out.&lt;/p&gt;

&lt;p&gt;And thank you for adapting it for new TokenStream API.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I searched for setOffset() in Solr source code and found one additional occurence of it without offset correcting in FieldType.java. This patch fixes this.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Good catch, Uwe! I slipped over it.&lt;/p&gt;

&lt;p&gt;I think the empty tokens is a bug and should be omitted in this patch.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;A second thing: Lucene has a new BaseTokenStreamTest class for checking tokens without Token instances (which would no loger work, when Lucene 3.0 switches to Attributes only). Maybe you should update these test and use assertAnalyzesTo from the new base class instead.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Very nice! Can you open a separate ticket?&lt;/p&gt;</comment>
                            <comment id="12755409" author="thetaphi" created="Tue, 15 Sep 2009 08:57:23 +0100"  >&lt;blockquote&gt;&lt;p&gt;I think the empty tokens is a bug and should be omitted in this patch.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The Javadocs say, that it works like String.split() which return empty tokens, but strips empty tokens at the end of the string. This functionality is provided by Solr before and with this patch.&lt;br/&gt;
The code would get simplier, if the Tokenizer would generally strip empty tokens, but it is a backwards break. I would tend to just commit and then open another issue.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Very nice! Can you open a separate ticket?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Will open one about Lucene&apos;s BaseTokenStreamTestCase &lt;/p&gt;</comment>
                            <comment id="12755410" author="thetaphi" created="Tue, 15 Sep 2009 09:00:23 +0100"  >&lt;p&gt;I foget: I would deprecate the unneeded methods in the factory!&lt;/p&gt;</comment>
                            <comment id="12755425" author="thetaphi" created="Tue, 15 Sep 2009 09:58:05 +0100"  >&lt;p&gt;Some refactoring (I moved the PatternTokenizer to its own class, like PatternReplaceFilter). This patch is functionally identical to current trunk, but more effective and uses new TokenStream API and implements end() (which sets the offset to the end of the string).&lt;/p&gt;

&lt;p&gt;I will soon post a patch, which removes empty tokens.&lt;/p&gt;</comment>
                            <comment id="12755444" author="thetaphi" created="Tue, 15 Sep 2009 11:21:54 +0100"  >&lt;p&gt;This is a patch that fixes the empty tokens:&lt;br/&gt;
This Tokenizer is not backwards compatible, as it only return non-zero length tokens. Maybe we should have a switch somewhere to change this behaviour. It is currently for discussion only.&lt;/p&gt;</comment>
                            <comment id="12755503" author="yseeley@gmail.com" created="Tue, 15 Sep 2009 15:25:20 +0100"  >&lt;blockquote&gt;&lt;p&gt;This Tokenizer is not backwards compatible, as it only return non-zero length tokens. Maybe we should have a switch somewhere to change this behaviour. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Passing through only non-zero length tokens was probably always the intent, and the old behavior is a bug and isn&apos;t useful, so I don&apos;t think  we need a switch.&lt;/p&gt;</comment>
                            <comment id="12755564" author="thetaphi" created="Tue, 15 Sep 2009 17:41:55 +0100"  >&lt;p&gt;Then you could use &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-1423&quot; title=&quot;Lucene 2.9 RC4 may need some changes in Solr Analyzers using CharStream &amp;amp; others&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-1423&quot;&gt;&lt;del&gt;SOLR-1423&lt;/del&gt;&lt;/a&gt;-fix-empty-tokens.patch it should work. The comparison with String.split() in one of the tests was commented out, because it does not work with the tokenizer (as empty tokens are not returned).&lt;/p&gt;

&lt;p&gt;I only wanted to check, that the offsets are calculated correctly. The second tests does this, but I want to be sure, that they are correct for both group=-1 and group&amp;gt;=0.&lt;/p&gt;</comment>
                            <comment id="12755577" author="thetaphi" created="Tue, 15 Sep 2009 18:04:22 +0100"  >&lt;p&gt;Attached a new patch with the empty token fix.&lt;/p&gt;

&lt;p&gt;It has an additional test for the offsets, if group!=-1. It also is more optimized, as it uses setTermBuffer( string, offset, len) to copy the chars into the termbuffer, which is faster than allocating a new string with substring().&lt;/p&gt;</comment>
                            <comment id="12756923" author="koji" created="Fri, 18 Sep 2009 03:58:32 +0100"  >&lt;p&gt;The patch looks good! Will commit shortly.&lt;/p&gt;</comment>
                            <comment id="12757028" author="koji" created="Fri, 18 Sep 2009 08:38:22 +0100"  >&lt;p&gt;Committed revision 816502. Thanks, Uwe!&lt;/p&gt;</comment>
                            <comment id="12775869" author="gsingers" created="Tue, 10 Nov 2009 15:52:14 +0000"  >&lt;p&gt;Bulk close for Solr 1.4&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12435329">LUCENE-1906</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12434647">SOLR-1404</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12419493" name="SOLR-1423-FieldType.patch" size="566" author="thetaphi" created="Mon, 14 Sep 2009 07:41:22 +0100"/>
                            <attachment id="12419653" name="SOLR-1423-fix-empty-tokens.patch" size="14334" author="thetaphi" created="Tue, 15 Sep 2009 18:04:22 +0100"/>
                            <attachment id="12419620" name="SOLR-1423-fix-empty-tokens.patch" size="13725" author="thetaphi" created="Tue, 15 Sep 2009 11:21:54 +0100"/>
                            <attachment id="12419613" name="SOLR-1423-with-empty-tokens.patch" size="10869" author="thetaphi" created="Tue, 15 Sep 2009 09:58:05 +0100"/>
                            <attachment id="12419583" name="SOLR-1423.patch" size="8134" author="koji" created="Tue, 15 Sep 2009 00:32:53 +0100"/>
                            <attachment id="12419502" name="SOLR-1423.patch" size="7820" author="thetaphi" created="Mon, 14 Sep 2009 10:53:25 +0100"/>
                            <attachment id="12419445" name="SOLR-1423.patch" size="6686" author="koji" created="Mon, 14 Sep 2009 03:25:23 +0100"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>7.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Fri, 11 Sep 2009 14:33:46 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>6230</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hxxlcn:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>19778</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                </customfields>
    </item>
</channel>
</rss>