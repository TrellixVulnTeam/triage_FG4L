<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 05:16:03 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/SOLR-906/SOLR-906.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[SOLR-906] Buffered / Streaming SolrServer implementaion</title>
                <link>https://issues.apache.org/jira/browse/SOLR-906</link>
                <project id="12310230" key="SOLR">Solr</project>
                    <description>&lt;p&gt;While indexing lots of documents, the CommonsHttpSolrServer add( SolrInputDocument ) is less then optimal.  This makes a new request for each document.&lt;/p&gt;

&lt;p&gt;With a &quot;StreamingHttpSolrServer&quot;, documents are buffered and then written to a single open Http connection.&lt;/p&gt;

&lt;p&gt;For related discussion see:&lt;br/&gt;
&lt;a href=&quot;http://www.nabble.com/solr-performance-tt9055437.html#a20833680&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.nabble.com/solr-performance-tt9055437.html#a20833680&lt;/a&gt;&lt;/p&gt;</description>
                <environment></environment>
        <key id="12410387">SOLR-906</key>
            <summary>Buffered / Streaming SolrServer implementaion</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="ryantxu">Ryan McKinley</assignee>
                                    <reporter username="ryantxu">Ryan McKinley</reporter>
                        <labels>
                    </labels>
                <created>Wed, 10 Dec 2008 18:07:49 +0000</created>
                <updated>Tue, 10 Nov 2009 15:51:52 +0000</updated>
                            <resolved>Tue, 6 Jan 2009 19:57:25 +0000</resolved>
                                                    <fixVersion>1.4</fixVersion>
                                    <component>clients - java</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>4</watches>
                                                                <comments>
                            <comment id="12655298" author="ryantxu" created="Wed, 10 Dec 2008 18:14:15 +0000"  >&lt;p&gt;One basic problem with calling add( SolrInputDocument) with the CommonsHttpSolrServer is that it logs a request for each document.  This can be a substantial impact.  For example while indexing 40K docs on my machine, it takes ~3 1/2 mins.  If I turn logging off the time drops to ! 2 1/2 mins.  With the streaming approach, the time drops to 20sec!   Some of that is obviously because it limits the logging:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
INFO: {add=[id1,id2,id3,id4, ...(38293 more)]} 0 20714
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12655299" author="ryantxu" created="Wed, 10 Dec 2008 18:17:53 +0000"  >&lt;p&gt;This implementation buffers documents in a BlockingQueue&amp;lt;SolrInputDocument&amp;gt;&lt;br/&gt;
when a document is added, it makes sure a Thread is working on sending the queue to solr.&lt;/p&gt;

&lt;p&gt;While this implementation only starts one thread, it would be easy to extent this so that multiple threads are writing to solr simultaneously and draining the same Queue.&lt;/p&gt;</comment>
                            <comment id="12655302" author="ryantxu" created="Wed, 10 Dec 2008 18:22:58 +0000"  >&lt;p&gt;The error handling behavior is less then ideal....  (as it is for &amp;lt;add&amp;gt; with multiple documents already)&lt;/p&gt;

&lt;p&gt;As is, it will break the connection when an error occures, but will not know which one.  However, it will continue indexing the Queue when an error occures.&lt;/p&gt;</comment>
                            <comment id="12655357" author="ryantxu" created="Wed, 10 Dec 2008 21:08:23 +0000"  >&lt;p&gt;Here is an updated version that lets you specify how many threads should work on emptying the Queue.  It also adds tests to make sure it passes all the same tests that CommonsHttpSolrServer and EmbeddedSolrServer already pass.   That is, it is a drop in replacement and passes all existing tests.&lt;/p&gt;

&lt;p&gt;One big change is that calling commit or optimize with waitSearcher=true:&lt;br/&gt;
1. blocks adding new docs to the Queue&lt;br/&gt;
2. waits for the Queue to empty (send all docs)&lt;br/&gt;
3. waits for &amp;lt;commit waitSearcher=true /&amp;gt; to return&lt;br/&gt;
4. unblocks everything&lt;br/&gt;
5. finally continues execution.&lt;/p&gt;

&lt;p&gt;My threading chops are not great, so I may be doing something really strange.  It would be good to get some more eyes on this!&lt;/p&gt;</comment>
                            <comment id="12655363" author="ryantxu" created="Wed, 10 Dec 2008 21:13:11 +0000"  >&lt;p&gt;also, using the recent patch with a Queue size = 20 and thread count=3 (on a dual core machine), the indexing time dropped from 30 secs -&amp;gt; 20 secs.  In sum:  with the data I am working with, switch from CommonsHttpSolrServer =&amp;gt; StreamingHttpSolrServer changes the index time from 3.5 min =&amp;gt; 20 sec, or ~10x faster  &lt;/p&gt;</comment>
                            <comment id="12655427" author="iholsman" created="Wed, 10 Dec 2008 23:33:17 +0000"  >&lt;p&gt;how much of the 3.5minutes -&amp;gt; 30seconds is due to the logging?&lt;br/&gt;
would it be simpler to change the log message to &apos;DEBUG&apos; instead of &apos;INFO&apos; and see how the performance of the regular commons server behaves then?&lt;/p&gt;</comment>
                            <comment id="12655431" author="iholsman" created="Wed, 10 Dec 2008 23:39:02 +0000"  >
&lt;p&gt;also..&lt;/p&gt;

&lt;p&gt;The way I&apos;ve always thought commonsHttpServer was designed to work was to batch up the documents in groups and use the add(List&amp;lt;SolrInputDocument&amp;gt;) function instead of the individual add(SolrInputDocument) function. &lt;/p&gt;

&lt;p&gt;is adding documents in batches (say 100 or 1,000 at a time) also as slow as 3.5m ? or wont this work as you won&apos;t get the feedback on which document failed to add if there was an error.&lt;/p&gt;</comment>
                            <comment id="12655438" author="ryantxu" created="Thu, 11 Dec 2008 00:25:42 +0000"  >&lt;p&gt;&amp;gt; how much of the 3.5minutes -&amp;gt; 30seconds is due to the logging?&lt;/p&gt;

&lt;p&gt;~1 min.  When I turn off logging completely, the time is ~2.5 mins  (also, note that with 3 threads, it is down to 20sec)&lt;/p&gt;

&lt;p&gt;RE: calling add( doc ) vs add( List&amp;lt;doc&amp;gt; )...  &lt;br/&gt;
yes, things are much better if you call add( List&amp;lt;doc&amp;gt; ) however, it is not the most convenient api if you are running though tons of things.&lt;/p&gt;

&lt;p&gt;I would expect (but have not tried) adding 40K docs in one call to add( List&amp;lt;doc&amp;gt; ) would have the same time as this StreamingHttpSolrServer.  It is probably also similar if you buffer 100? 1,000? at a time, but I have not tried.&lt;/p&gt;

&lt;p&gt;The StreamingHttpSolrServer essentially handles the buffering for you.  It keeps an http connection open as long as the Queue has docs to send.  It can start multiple threads and drain the same Queue simultaneously.&lt;/p&gt;

&lt;p&gt;Essentially, this just offers an easier interface to get the best possible performance.  The trade off (for now) is that there is no good error reporting.&lt;/p&gt;
</comment>
                            <comment id="12655442" author="ryantxu" created="Thu, 11 Dec 2008 00:37:05 +0000"  >&lt;p&gt;The other aspect no note is that StreamingHttpSolrServer sends the request in the background so after you call: add( doc ) or add( list ) that thread is free to keep working.  With the off the shelf CommonsHttpSolrServer the client needs to wait for the server to parse the request and index the data before it can continue.&lt;/p&gt;

&lt;p&gt;This switches where the client gets blocked.&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;with CommonsHttpSolrServer it blocks while waiting for solr to &lt;b&gt;write the response&lt;/b&gt;&lt;/li&gt;
	&lt;li&gt;with StreamingHttpSolrServer it blocks while waiting for solr to &lt;b&gt;read the request&lt;/b&gt;&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12655917" author="shalinmangar" created="Fri, 12 Dec 2008 08:19:28 +0000"  >&lt;p&gt;Ryan, I&apos;m seeing compile errors related to @Override with interface methods (that&apos;s a Java 6 feature). Also, new IOException( e ) is not defined (also Java 6, I guess).&lt;/p&gt;</comment>
                            <comment id="12656063" author="ryantxu" created="Fri, 12 Dec 2008 16:57:41 +0000"  >&lt;p&gt;removes @Override from interfaces&lt;/p&gt;

&lt;p&gt;I guess:&lt;br/&gt;
  &amp;lt;property name=&quot;java.compat.version&quot; value=&quot;1.5&quot; /&amp;gt;&lt;/p&gt;

&lt;p&gt;does not take check everything!&lt;/p&gt;</comment>
                            <comment id="12657102" author="shalinmangar" created="Tue, 16 Dec 2008 18:37:54 +0000"  >&lt;p&gt;I&apos;ve started taking a look at this. A couple of points:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Instantiating the lock in blockUntilFinished and nulling it can cause a race condition. A thread in the &apos;add&apos; method can find that the lock is not null, another thread can null it and the first thread proceeds to lock on it leading to NPE. In the same way, creation of multiple locks is possible in the blockUntilFinished method.&lt;/li&gt;
	&lt;li&gt;The run method calling itself recursively looks suspicious. We may be in danger of overflowing the stack.&lt;/li&gt;
	&lt;li&gt;The SolrExampleTest cannot be used directly because it depends on the order of the commands being executed. We must clearly document that clients should not depend on the order of commands being executed in the same order as they are given.&lt;/li&gt;
	&lt;li&gt;The if (req.getCommitWithin() &amp;lt; 0) should be &amp;gt; 0, right?&lt;/li&gt;
	&lt;li&gt;The add calls do not come to the process method. Due to this some add calls may still get in before the commit acquires the lock (assuming multiple producers). Is this class strictly meant for a single document producer use-case?&lt;/li&gt;
	&lt;li&gt;The wait loop in blockUntilFinished is very CPU intensive. It can probably be optimized.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I&apos;m experimenting with a slightly different implementation. Still trying to tie the loose ends. I hope to have a patch soon.&lt;/p&gt;</comment>
                            <comment id="12657107" author="ryantxu" created="Tue, 16 Dec 2008 18:46:24 +0000"  >&lt;p&gt;Thanks for looking at this!&lt;/p&gt;

&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; The if (req.getCommitWithin() &amp;lt; 0) should be &amp;gt; 0, right?&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;no &amp;#8211; if a commit within time is specified, we can not use the open request.  It needs to start a new request so that a new &amp;lt;add ...&amp;gt; command could be sent.  I experimeted with sending everything over the open connection, but we would need to add a new parent tag to the xml format.   That might not be a bad idea.  Then we could send:&lt;br/&gt;
&amp;lt;stream&amp;gt;&lt;br/&gt;
  &amp;lt;add&amp;gt;&lt;br/&gt;
     &amp;lt;doc...&amp;gt;&lt;br/&gt;
  &amp;lt;/add&amp;gt;&lt;br/&gt;
  &amp;lt;add commitWithin=&quot;&quot;&amp;gt;&lt;br/&gt;
  ...&lt;br/&gt;
 &amp;lt;commit /&amp;gt;&lt;br/&gt;
 &amp;lt;add ...&amp;gt;&lt;/p&gt;

&lt;p&gt;and finally&lt;br/&gt;
&amp;lt;/stream&amp;gt;&lt;/p&gt;</comment>
                            <comment id="12657108" author="ryantxu" created="Tue, 16 Dec 2008 18:48:54 +0000"  >&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; The SolrExampleTest cannot be used directly&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;why not?  All the tests pass for me... &lt;/p&gt;

&lt;p&gt;sending a commit() (with waitSearcher=true) should wait for all the docs to get added, then issue the commit, then return.&lt;/p&gt;
</comment>
                            <comment id="12657110" author="ryantxu" created="Tue, 16 Dec 2008 18:53:38 +0000"  >&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; The add calls do not come to the process method. Due to this some add calls may still get in before the commit acquires the lock (assuming multiple producers). Is this class strictly meant for a single document producer use-case?&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;I don&apos;t totally follow... but if possible, it would be good if multiple threads could fill the same queue.  This would let the StreamingHttpSolrServer  manage all solr communication&lt;/p&gt;</comment>
                            <comment id="12657115" author="shalinmangar" created="Tue, 16 Dec 2008 19:02:36 +0000"  >&lt;blockquote&gt;&lt;p&gt;why not? All the tests pass for me... &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;There are multiple places where SolrExampleTest calls commit without waitSearcher=true and proceeds to query and assert on results. The failure happens intermittently. Try varying the number of threads and you may be able to reproduce the failure.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The add calls do not come to the process method.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I meant the request method. Sorry about that. The SolrServer.add() calls the request method but this implementation does not. If there are multiple threads using this class, new documents may get added to the queue before we acquire the lock inside blockUntilFinished due to the call to commit.&lt;/p&gt;</comment>
                            <comment id="12657278" author="noble.paul" created="Wed, 17 Dec 2008 04:00:50 +0000"  >&lt;p&gt;another observation :&lt;br/&gt;
why do we need a ScheduledExecutorService we only need a ThreadPoolExecutorService&lt;br/&gt;
The name of the class is somewhat misleading. We must document that this may be exclusively used for updates&lt;/p&gt;

&lt;p&gt;How about renaming this to StreamingUpdateSolrServer&lt;/p&gt;</comment>
                            <comment id="12657849" author="ryantxu" created="Thu, 18 Dec 2008 18:42:46 +0000"  >&lt;p&gt;Here is an updated patch that bufferes UpdateRequests rather then SolrInputDocuments &amp;#8211; this is good because then everything is handled in request( final SolrRequest request ) so blocking can be easier.&lt;/p&gt;

&lt;p&gt;Also this lets up submit submit &amp;lt;add commands with the commitWithin syntax.&lt;/p&gt;

&lt;p&gt;All /update commands are streamed to server unless waitSearcher==true&lt;/p&gt;

&lt;p&gt;Shalin &amp;#8211; can you check this over for threading issues or general improvements?&lt;/p&gt;</comment>
                            <comment id="12657868" author="yseeley@gmail.com" created="Thu, 18 Dec 2008 19:41:21 +0000"  >&lt;p&gt;I haven&apos;t looked at the code, but can someone outline what the API looks like (i.e. what would typical pseudo code that used the API look like).&lt;/p&gt;</comment>
                            <comment id="12657871" author="ryantxu" created="Thu, 18 Dec 2008 19:47:15 +0000"  >&lt;p&gt;API is identical to SolrServer&lt;/p&gt;

&lt;p&gt;rather then instantiating with CommonsHttpSolrServer, you just use StreamingSolrServer.&lt;/p&gt;

&lt;p&gt;The constructor args are:&lt;br/&gt;
  StreamingHttpSolrServer(String solrServerUrl, int queueSize, int threadCount ) &lt;/p&gt;

&lt;p&gt;queueSize is how big you want the buffering queue to be&lt;br/&gt;
threadCount is the maximum number of threads that will get used to empty the queue&lt;/p&gt;</comment>
                            <comment id="12657877" author="yseeley@gmail.com" created="Thu, 18 Dec 2008 20:06:00 +0000"  >&lt;p&gt;Isn&apos;t there a need for a polling API to check for errors?&lt;br/&gt;
I think something like the following client code would be easiest for people to deal with:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; not done:
   myDoc = [...] &lt;span class=&quot;code-comment&quot;&gt;// make the doc
&lt;/span&gt;   solr.addDoc(myDoc)
   List&amp;lt;Error&amp;gt; errors = solr.getErrors();
   &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (errors != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) handleErrors(errors)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12657882" author="ryantxu" created="Thu, 18 Dec 2008 20:21:15 +0000"  >&lt;p&gt;I suppose...  though I don&apos;t see it as a strict requirement &amp;#8211; if you need full error handling, use a different SolrServer implementation.&lt;/p&gt;

&lt;p&gt;I think a more reasonable error API would be a callback function rather then polling &amp;#8211; the error could occur outside you loop (assuming you break at some point).  That callback could easily be converted to a polling api if desired.&lt;/p&gt;

&lt;p&gt;The big thing to note with this API is that calling:&lt;br/&gt;
 solr.add( doc )&lt;br/&gt;
just adds it to the queue processes it in the background.  It is a BlockingQueue, so after it hits the max size the client will block before it can add &amp;#8211; but that should be transparent to the client.&lt;/p&gt;

&lt;p&gt;the error caused by adding that doc may happen much later in time.&lt;/p&gt;

&lt;p&gt;I&apos;ll go ahead and add that callback...&lt;/p&gt;</comment>
                            <comment id="12657888" author="ryantxu" created="Thu, 18 Dec 2008 20:36:15 +0000"  >&lt;p&gt;updated version that includes the callback function:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void handleError( Throwable ex )
  {
    log.error( &lt;span class=&quot;code-quote&quot;&gt;&quot;error&quot;&lt;/span&gt;, ex );
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;this gets called whenever an error occurs.  We could have that keep a list of any errors or whatever.  I&apos;m not sure what the default error behavior should be.  Collecting everything in a list?  just logging?&lt;/p&gt;</comment>
                            <comment id="12657889" author="ryantxu" created="Thu, 18 Dec 2008 20:39:06 +0000"  >&lt;p&gt;This can be use like so:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
SolrServer solr = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; StreamingHttpSolrServer( url, 2, 5 ) {
  @Override
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void handleError(Throwable ex) {
     &lt;span class=&quot;code-comment&quot;&gt;// &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; somethign...
&lt;/span&gt;  }
};
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12657893" author="yseeley@gmail.com" created="Thu, 18 Dec 2008 20:44:41 +0000"  >&lt;p&gt;IMO callbacks can be tougher to deal with and require client code to be multi-threaded (like signal handling in C that caused tons of trouble for people because they weren&apos;t thinking about the fact that it could be called at any time concurrently with other code, esp back in the days when C library calls were not re-entrant).&lt;/p&gt;

&lt;p&gt;But I guess as you say, polling could be added on top of a callback API if needed later.&lt;/p&gt;</comment>
                            <comment id="12657896" author="yseeley@gmail.com" created="Thu, 18 Dec 2008 20:49:58 +0000"  >&lt;blockquote&gt;&lt;p&gt;public void handleError(Throwable ex)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ideally, we could give the user back&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;the actual error response (http error code, and the body?)&lt;/li&gt;
	&lt;li&gt;the InputDocument that caused the failure, if there was one&lt;/li&gt;
	&lt;li&gt;what else?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;So it seems like we should add these parameters to handleError, and just document that they currently return null if we can&apos;t get that info yet.  Or perhaps more extensible, a SolrError class that has these fields?&lt;/p&gt;</comment>
                            <comment id="12657911" author="ryantxu" created="Thu, 18 Dec 2008 21:23:44 +0000"  >&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; the actual error response (http error code, and the body?)&lt;br/&gt;
That is / can be encoded in the Throwable implementation no?  As is, it adds the same Exception that you get when running the standard one.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; the InputDocument that caused the failure, if there was one&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;I don&apos;t know if there is a good way to do this.  The scope that catches exceptions is way outside of the context where we knew what was written.  This is just like hitting an error somewhere in the add( List&amp;lt;Doc&amp;gt; ) &amp;#8211; the response has no way to know where it broke.  Ideally the error that Solr returns is enough information, but the current exception behavior is to barf or not.&lt;/p&gt;

&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; what else?&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;My thought was we could add any of the specialized parameters in finer grained Throwable implementations.  When we have a real &apos;error&apos; response, this could be parsed and passed as an exception.&lt;/p&gt;</comment>
                            <comment id="12659750" author="ryantxu" created="Tue, 30 Dec 2008 06:53:45 +0000"  >&lt;p&gt;Shalin, did you get a change to look at this version?&lt;/p&gt;</comment>
                            <comment id="12660457" author="noble.paul" created="Sat, 3 Jan 2009 10:06:20 +0000"  >&lt;ul&gt;
	&lt;li&gt;One problem with the current implementation is that it writes everything to a local buffer and then uploads the whole content in one go. So essentially we are wasting time till your 40K docs are written into this huge XML. Another issue is that this XML has to fit in memory. We need to fix the comonsHttpSolrServer first. It must stream the docs .&lt;/li&gt;
	&lt;li&gt;We can enhance the SolrServer API by adding a method SolrServer#add(Iterator&amp;lt;SolrInputDocs&amp;gt; docs) . So CommonsHttpSolrServer can start writing the documents as and when you are producing your documents . We also have the advantage of not storing the huge list of docs in memory.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Another enhancement is using a different format (&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-865&quot; title=&quot;Support document updates in binary format&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-865&quot;&gt;&lt;del&gt;SOLR-865&lt;/del&gt;&lt;/a&gt;). It uses javabin format and it can be extremely fast compared to XML and the payload can be reduced substantially.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Probably we can overcome the perf problems to a certain extent with these two fixes. &lt;/p&gt;


</comment>
                            <comment id="12660496" author="ryantxu" created="Sat, 3 Jan 2009 18:11:56 +0000"  >&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; One problem with the current implementation is that it writes everything to a local buffer and then uploads the whole content in one go. So essentially we are wasting time till your 40K docs are written into this huge XML. Another issue is that this XML has to fit in memory. We need to fix the comonsHttpSolrServer first. It must stream the docs .&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;Really?!&lt;/p&gt;

&lt;p&gt;Are you saying that the &lt;a href=&quot;http://hc.apache.org/httpclient-3.x/apidocs/org/apache/commons/httpclient/methods/RequestEntity.html#getContentLength()&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;RequestEntity.html#getContentLength()&lt;/a&gt; does not behave as advertised?&lt;/p&gt;

&lt;p&gt;This implementation returns -1 for the content length, and that tells the connection use chunk encoding to transmit the request entity.  &lt;/p&gt;

&lt;p&gt;Where do you get the 40K number?  Is it from the log?  If so, that is the expected behavior &amp;#8211; the server continually processes documents until it reaches the end of the stream.  That may be 1 document that may be 1M docs...&lt;/p&gt;

&lt;p&gt;If you are filling up a Collection&amp;lt;SolrInputDocument&amp;gt; with 40K docs, then sending it of course it is going to hold on to 40K docs at once.&lt;/p&gt;


&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; We can enhance the SolrServer API by adding a method SolrServer#add(Iterator&amp;lt;SolrInputDocs&amp;gt; docs) . So CommonsHttpSolrServer can start writing the documents as and when you are producing your documents . We also have the advantage of not storing the huge list of docs in memory.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;I&apos;m not following...  with the StreamingHttpSolrServer, you can send documents one at a time and each documents starts sending as soon as it can.  There is a BlockingQueue&amp;lt;UpdateRequest&amp;gt; that holds all UpdateRequests that come through the &apos;request&apos; method.  BlockingQueue&apos;s only hold a fixed number of items and will block before adding something beyond the limit.&lt;/p&gt;

&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; Another enhancement is using a different format (&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-865&quot; title=&quot;Support document updates in binary format&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-865&quot;&gt;&lt;del&gt;SOLR-865&lt;/del&gt;&lt;/a&gt;). It uses javabin format and it can be extremely fast compared to XML and the payload can be reduced substantially.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;That is a different issue altogether.  That relates to having something different running on the server.  Once that is in, then this should be able to leverage that as well...&lt;/p&gt;
</comment>
                            <comment id="12660572" author="noble.paul" created="Sun, 4 Jan 2009 14:00:22 +0000"  >&lt;p&gt;Please ignore the number 40K docs. I just took it from your perf test numbers. I thought you were writing docs as a list&lt;/p&gt;

&lt;p&gt;I am referring to the client code .The method in UpdateRequest&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; Collection&amp;lt;ContentStream&amp;gt; getContentStreams() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; ClientUtils.toContentStreams( getXML(), ClientUtils.TEXT_XML );
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This means that the getXML() method actually constructs a huge String which is the entire xml. It is not very good if we are writing out very large no:of docs&lt;/p&gt;

&lt;p&gt;I am suggesting that ComonsHttpSolrServer has scope for improvement. Instead of building that String in memory  we can just start streaming it to the server. So the OutputStream can be passed on to UpdateRequest so that it can write the xml right into the stream. So there is streaming effectively on both ends&lt;/p&gt;

&lt;p&gt;This is valid where users do bulk updates. Not when they write one doc at a time. &lt;/p&gt;

&lt;p&gt;The new method SolrServer#add(Iterator&amp;lt;SolrInputDocs&amp;gt; docs) can start writing the docs immedietly and the docs can be uploaded as and when they are being produced. It is not related to these issue exactly, But the intend of this issue is to make upload faster.&lt;/p&gt;


&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-865&quot; title=&quot;Support document updates in binary format&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-865&quot;&gt;&lt;del&gt;SOLR-865&lt;/del&gt;&lt;/a&gt; is not very related to this issue. StreamingHttpSolrServer can use javabin format as well.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;with the StreamingHttpSolrServer, you can send documents one at a time and each documents starts sending as soon as it can&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;One drawback of a StreamingHttpSolrServer is that it ends up opening  multiple connections for uploading the documents&lt;/p&gt;

&lt;p&gt;Another enhancement . We can add one (or more ) extra thread in the server to do the call updaterequestprocessor.processAdd() . &lt;/p&gt;</comment>
                            <comment id="12660607" author="ryantxu" created="Sun, 4 Jan 2009 17:57:53 +0000"  >&lt;p&gt;Are you looking at the patch or just brainstorming how this could be implemented?&lt;/p&gt;

&lt;div class=&quot;panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;panelContent&quot;&gt;
&lt;p&gt;I am referring to the client code .The method in UpdateRequest&lt;/p&gt;

&lt;p&gt;public Collection&amp;lt;ContentStream&amp;gt; getContentStreams() throws IOException &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {
    return ClientUtils.toContentStreams( getXML(), ClientUtils.TEXT_XML );
}&lt;/span&gt; &lt;/div&gt;

&lt;p&gt;This means that the getXML() method actually constructs a huge String which is the entire xml. It is not very good if we are writing out very large no:of docs&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is not how the patch works...  for starters, it never calls getContentStreams() for UpdateRequest.  It opens a single connection and continually dumps the xml for each request.  Rather then call getXML() the patch adds a function writeXml( Writer ) that writes directly to the open buffer.&lt;/p&gt;

&lt;div class=&quot;panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;panelContent&quot;&gt;
&lt;p&gt;I am suggesting that ComonsHttpSolrServer has scope for improvement. Instead of building that String in memory we can just start streaming it to the server. So the OutputStream can be passed on to UpdateRequest so that it can write the xml right into the stream. So there is streaming effectively on both ends&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The ComonsHttpSolrServer is fine, but you are right that each UpdateRequest &lt;b&gt;may&lt;/b&gt; want to write the content directly to the open stream.  The ContentStream interface gives us all that control.  One thing to note is that if you do not specify the length, the HttpCommons server will use chunked encoding.&lt;/p&gt;

&lt;p&gt;But I think adding the StreammingUpdateSolrServer resolves that for everyone.  Uses have either option.&lt;/p&gt;

&lt;div class=&quot;panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;panelContent&quot;&gt;
&lt;p&gt; One drawback of a StreamingHttpSolrServer is that it ends up opening multiple connections for uploading the documents&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Nonsense &amp;#8211; that is exactly what this avoids.  It opens a single connection and writes everything to it.  You can configure how many threads you want emptying the queue; each one will open a connection.&lt;/p&gt;

&lt;div class=&quot;panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;panelContent&quot;&gt;
&lt;p&gt;Another enhancement . We can add one (or more ) extra thread in the server to do the call updaterequestprocessor.processAdd() . &lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;That opens a whole can of worms...  perhaps better discussed on java-dev.  For now I think sticking to the 1 thread/prequest is a good model.  If you want multiple threads running on the server use multiple connections (it is even an argument in the StreammingHttpSolrServer)&lt;/p&gt;</comment>
                            <comment id="12660610" author="ryantxu" created="Sun, 4 Jan 2009 17:59:38 +0000"  >&lt;p&gt;I would like to go ahead and commit this patch soon.  Shalin - did the changes in the latest patch resolve the issues you referred to?&lt;/p&gt;</comment>
                            <comment id="12660662" author="noble.paul" created="Mon, 5 Jan 2009 04:38:35 +0000"  >&lt;p&gt;Hi Ryan,&lt;br/&gt;
You got me wrong. I was trying to say how to make CommonsHttpSolrServer efficient by streaming docs as StreamingHttpSolrServer does when I add docs in bulk using &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
SolrServer.add(List&amp;lt;SolrInputDocument&amp;gt; docs)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Yes , StreamingHttpSolrServer uses only one connection per thread and it closes the connection after waiting for 250ms for a new document.&lt;/p&gt;</comment>
                            <comment id="12660784" author="shalinmangar" created="Mon, 5 Jan 2009 15:17:52 +0000"  >&lt;p&gt;Sorry Ryan for stalling this. The tests run fine. But we need to change the global lock to be final.&lt;/p&gt;

&lt;p&gt;What does the &quot;&amp;lt;stream&amp;gt;&quot; tag do?&lt;/p&gt;

&lt;p&gt;Please go ahead and take charge. It&apos;s all yours.&lt;/p&gt;</comment>
                            <comment id="12661285" author="ryantxu" created="Tue, 6 Jan 2009 19:57:12 +0000"  >&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; But we need to change the global lock to be final.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;why?  The global lock is used to block all the threads &amp;#8211; each worker checks if it is null to see if it should block or not.&lt;/p&gt;

&lt;p&gt;The only place that sets the lock is within the same synchronized block:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;

  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;synchronized&lt;/span&gt; void blockUntilFinished()
  {
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt;( lock == &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt; ) {
      lock = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ReentrantLock();
    }
    lock.lock();

    ...

    lock.unlock();
    lock = &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;;
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;since nothing else changes lock, i think it is ok.&lt;/p&gt;


&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; What does the &quot;&amp;lt;stream&amp;gt;&quot; tag do?&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;That is just there so that multiple &amp;lt;add&amp;gt; commands can be in the same XML document.  it is just an arbitrary parent tag.  The parser on the other end only validates once it hits a known cmd tag.&lt;/p&gt;</comment>
                            <comment id="12661434" author="noble.paul" created="Wed, 7 Jan 2009 04:12:52 +0000"  >&lt;blockquote&gt;&lt;p&gt;That is just there so that multiple &amp;lt;add&amp;gt; commands can be in the same XML document.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It may not be necessary to have the &amp;lt;strream&amp;gt; tag , if we modify the XMLLoader to accept multiple &amp;lt;add&amp;gt; we can do away with the extra &amp;lt;stream&amp;gt; tag.&lt;/p&gt;

&lt;p&gt;The XML format is a public interface and we should be conservative in adding stuff into that .&lt;/p&gt;

&lt;p&gt;It would be nice to document the public changes in the JIRA itself so that other committers can see the changes which are going to come in w/o going through the patch itself&lt;/p&gt;</comment>
                            <comment id="12661440" author="ryantxu" created="Wed, 7 Jan 2009 04:33:35 +0000"  >&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; if we modify the XMLLoader to accept multiple &amp;lt;add&amp;gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;The XMLLoader already accepts multiple &amp;lt;add&amp;gt; commands without any changes.&lt;/p&gt;

&lt;p&gt;The &amp;lt;stream&amp;gt; tag (or &amp;lt;whatever&amp;gt; tag) is added so that the xml is valid &amp;#8211; you can not have multiple roots in an xml document.&lt;/p&gt;

&lt;p&gt;Again &amp;#8211; &lt;b&gt;nothing&lt;/b&gt; has changed with the parser. &lt;/p&gt;</comment>
                            <comment id="12775603" author="gsingers" created="Tue, 10 Nov 2009 15:51:52 +0000"  >&lt;p&gt;Bulk close for Solr 1.4&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12396422" name="SOLR-906-StreamingHttpSolrServer.patch" size="13445" author="ryantxu" created="Thu, 18 Dec 2008 20:36:15 +0000"/>
                            <attachment id="12396409" name="SOLR-906-StreamingHttpSolrServer.patch" size="11993" author="ryantxu" created="Thu, 18 Dec 2008 18:42:46 +0000"/>
                            <attachment id="12395952" name="SOLR-906-StreamingHttpSolrServer.patch" size="11993" author="ryantxu" created="Fri, 12 Dec 2008 16:57:41 +0000"/>
                            <attachment id="12395760" name="SOLR-906-StreamingHttpSolrServer.patch" size="11957" author="ryantxu" created="Wed, 10 Dec 2008 21:08:23 +0000"/>
                            <attachment id="12395748" name="StreamingHttpSolrServer.java" size="6895" author="ryantxu" created="Wed, 10 Dec 2008 18:17:53 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>5.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 10 Dec 2008 23:33:17 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>6727</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hxxoh3:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>20284</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                </customfields>
    </item>
</channel>
</rss>