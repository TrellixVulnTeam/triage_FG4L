<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 05:24:56 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/SOLR-667/SOLR-667.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[SOLR-667] Alternate LRUCache implementation</title>
                <link>https://issues.apache.org/jira/browse/SOLR-667</link>
                <project id="12310230" key="SOLR">Solr</project>
                    <description>&lt;p&gt;The only available SolrCache i.e LRUCache is based on &lt;em&gt;LinkedHashMap&lt;/em&gt; which has &lt;em&gt;get()&lt;/em&gt; also synchronized. This can cause severe bottlenecks for faceted search. Any alternate implementation which can be faster/better must be considered. &lt;/p&gt;</description>
                <environment></environment>
        <key id="12401286">SOLR-667</key>
            <summary>Alternate LRUCache implementation</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="yseeley@gmail.com">Yonik Seeley</assignee>
                                    <reporter username="noble.paul">Noble Paul</reporter>
                        <labels>
                    </labels>
                <created>Wed, 30 Jul 2008 07:11:40 +0100</created>
                <updated>Thu, 13 Aug 2009 15:10:04 +0100</updated>
                            <resolved>Tue, 28 Oct 2008 20:14:16 +0000</resolved>
                                    <version>1.3</version>
                                    <fixVersion>1.4</fixVersion>
                                    <component>search</component>
                        <due></due>
                            <votes>2</votes>
                                    <watches>6</watches>
                                                                <comments>
                            <comment id="12618072" author="noble.paul" created="Wed, 30 Jul 2008 07:15:40 +0100"  >&lt;p&gt;A POC implementation based on &lt;em&gt;ConcurrentHashMap&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Gets are free&lt;/li&gt;
	&lt;li&gt;Puts are free till it touches the high water mark . It is expensive (very expensive) after the high watermark .&lt;/li&gt;
	&lt;li&gt;To lighten the load on put an extra thread is employed to do a concurrent mark and sweep&lt;/li&gt;
	&lt;li&gt;there is a high-water-mark and a low-water-mark . The extra thread cleans anything if low-water-mark is crossed. Put must removes if level crosses high-water-mark&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12618708" author="noble.paul" created="Thu, 31 Jul 2008 15:56:11 +0100"  >&lt;p&gt;If somebody can review the implementation we can add another cache implementation to Solr.&lt;/p&gt;</comment>
                            <comment id="12618715" author="yseeley@gmail.com" created="Thu, 31 Jul 2008 16:17:12 +0100"  >&lt;blockquote&gt;&lt;p&gt;Gets are free&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Not entirely... there are a few memory barriers that make things thread safe, so I wouldn&apos;t call it &quot;free&quot; (since this branched off of another issue where some had the idea that one could get away without any sort of locks or memory barriers).&lt;/p&gt;

&lt;p&gt;It&apos;s a good approach in genera, and should scale better with many CPUs under very high lookup load.  But I&apos;m not sure that it should use a separate cleaner thread... and if it does, I don&apos;t think it should be scheduled.&lt;/p&gt;

&lt;p&gt;After we got those details worked out, then we&apos;d need a SolrCache implementation that uses it.  Given where we are in the release cycle (and that the cache contention issue is only affecting 1 person that I&apos;ve seen), I think this should want until after 1.3&lt;/p&gt;</comment>
                            <comment id="12618750" author="funtick" created="Thu, 31 Jul 2008 17:27:13 +0100"  >&lt;blockquote&gt;&lt;p&gt;...safety, where nothing bad ever happens to an object. &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;When &lt;em&gt;SOLR&lt;/em&gt; adds object to cache or remove it from cache it does not change it, it manipulates with internal arrays of pointers to objects (which are probably atomic, but I don&apos;t know such JVM &amp;amp; GC internals in-depth...)&lt;/p&gt;

&lt;p&gt;Looks heavy with TreeSet...&lt;/p&gt;</comment>
                            <comment id="12618768" author="noble.paul" created="Thu, 31 Jul 2008 18:35:33 +0100"  >&lt;blockquote&gt;&lt;p&gt;It&apos;s a good approach in genera, and should scale better with many CPUs under very high lookup load. But I&apos;m not sure that it should use a separate cleaner thread... and if it does, I don&apos;t think it should be scheduled.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Thanks for the comments. I agree with you . I devised this approach because some user reported that heavy cache lookups are slowing things down for him. The cost benefit analysis is yet to be studied. Separate cleaner thread is optional in the implementation . I am yet to study the cost of sorting over a hundred thousand entries.  Do you recommend that the cleaner thread just keep running forever? That is fine, so there should be a sleep for the thread? &lt;/p&gt;

&lt;p&gt;BTW is the executorservice very expensive?&lt;/p&gt;

&lt;p&gt;I did not provide a SolrCache implementation because that is going to be drown the approach in details. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I think this should want until after 1.3&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;True. This is not marked for 1.3. But this can definitely live as a patch and anyone who needs it would benefit from it&lt;/p&gt;</comment>
                            <comment id="12618773" author="noblepaul" created="Thu, 31 Jul 2008 18:46:32 +0100"  >
&lt;p&gt;Fuad. You cannot trivialize concurrent programming so easily. Whatever&lt;br/&gt;
we have commented are from our experience (and wisdom)  . There is a&lt;br/&gt;
price to pay it. Java could have easily eliminated the&lt;br/&gt;
java.util.concurrent package by using &apos;volatile&apos; everywhere and no&lt;br/&gt;
need of AtomicInteger,AtomimcLong etc. So they are there for a reason&lt;/p&gt;



&lt;p&gt;BTW. Using TreeSet is not &apos;heavy&apos; . It is the right tool for right&lt;br/&gt;
purpose. If you need a sorted set that is best&lt;/p&gt;



&lt;p&gt;On Thu, Jul 31, 2008 at 9:58 PM, Fuad Efendi (JIRA) &amp;lt;jira@apache.org&amp;gt; wrote:&lt;/p&gt;



&lt;p&gt;&amp;#8211; &lt;br/&gt;
--Noble Paul&lt;/p&gt;</comment>
                            <comment id="12618805" author="funtick" created="Thu, 31 Jul 2008 19:40:53 +0100"  >&lt;p&gt;Paul, &lt;/p&gt;


&lt;p&gt;I have never ever suggested to use &apos;volatile&apos;  &apos;to avoid synchronization&apos; for concurrent programming. I only noticed some extremely stupid code where SOLR uses _double_synchronization and AtomicLong inside:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;synchronized&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt; put(&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt; key, &lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt; value) {
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (state == State.LIVE) {
      stats.inserts.incrementAndGet();
    }

    &lt;span class=&quot;code-keyword&quot;&gt;synchronized&lt;/span&gt; (map) {
      &lt;span class=&quot;code-comment&quot;&gt;// increment local inserts regardless of state???
&lt;/span&gt;      &lt;span class=&quot;code-comment&quot;&gt;// it does make it more consistent with the current size...
&lt;/span&gt;      inserts++;
      &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; map.put(key,value);
    }
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Each tool has an area of applicability, and even ConcurrentHashMap just slightly intersects with SOLR needs; SOLR does not need &apos;consistent view at a point in time&apos; on cached objects.&lt;/p&gt;

&lt;p&gt;&apos;volatile&apos; is part of Java Specs, and implemented differently by different vendors. I use volatile (instead of more expensive AtomicLong) only and only to prevent JVM HotSpot Optimizer from some &lt;em&gt;not-applicable&lt;/em&gt; staff...&lt;/p&gt;</comment>
                            <comment id="12618812" author="yseeley@gmail.com" created="Thu, 31 Jul 2008 20:03:57 +0100"  >&lt;blockquote&gt;&lt;p&gt;I only noticed some extremely stupid code where SOLR uses _double_synchronization and AtomicLong inside:&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;A simple typo I think... a remnant from way back when changing what object was being synchronized on.  That&apos;s why I like explicit synchronization rather than adding it to a method signature (easier to miss).   I just fixed this to be&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt; put(&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt; key, &lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt; value) {
    &lt;span class=&quot;code-keyword&quot;&gt;synchronized&lt;/span&gt; (map) {
      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (state == State.LIVE) {
        stats.inserts.incrementAndGet();
      }

      &lt;span class=&quot;code-comment&quot;&gt;// increment local inserts regardless of state???
&lt;/span&gt;      &lt;span class=&quot;code-comment&quot;&gt;// it does make it more consistent with the current size...
&lt;/span&gt;      inserts++;
      &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; map.put(key,value);
    }
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12618824" author="funtick" created="Thu, 31 Jul 2008 20:28:06 +0100"  >&lt;p&gt;Thanks Yonik, I even guess that in some cases synchronization is faster than sun.misc.Unsafe.compareAndSwapLong(this, valueOffset, expect, update);&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; incrementAndGet() {
        &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (;;) {
            &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; current = get();
            &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; next = current + 1;
            &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (compareAndSet(current, next))
                &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; next;
        }
    }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;extremal level of safety with some level of concurrency... Do we need exact value for &apos;stats.inserts&apos; (if it is not synchronized)?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;It can be &apos;long&apos; inside synchronized block...&lt;/p&gt;</comment>
                            <comment id="12618826" author="yseeley@gmail.com" created="Thu, 31 Jul 2008 20:36:29 +0100"  >&lt;p&gt;If you don&apos;t need the synchronized block, then an atomic variable for &quot;inserts&quot; (for example) would be a big win.&lt;br/&gt;
But if you have the synchronized block anyway, it&apos;s probably faster to just expand it&apos;s scope if the operations to be done are simple.&lt;/p&gt;</comment>
                            <comment id="12618934" author="noble.paul" created="Fri, 1 Aug 2008 06:21:25 +0100"  >&lt;p&gt;my findings on a simple perf test with no contention (single thread)&lt;/p&gt;

&lt;p&gt;The code is there in the &lt;em&gt;main()&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;cache size 1 million&lt;/p&gt;

&lt;p&gt;with Hashmap&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;time taken  for 1 million inserts = 2019ms&lt;/li&gt;
	&lt;li&gt;time taken for 1 million gets = 625&lt;/li&gt;
	&lt;li&gt;time taken  for cleanup  = 345ms&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;with ConcurrenthashHashMap&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;time taken  for 1 million inserts  = 2437(roughly 20% slower than hashmap but small in absolute numbers)&lt;/li&gt;
	&lt;li&gt;time taken for 1 million gets  = 393ms (actually faster than simple HashMap )&lt;/li&gt;
	&lt;li&gt;time taken  for cleanup = 298ms (actually faster)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;other observations &lt;br/&gt;
The extra thread may not be be necessary . The unlucky put() may take around .25 secs to .5secs for a cache size of 1 million .&lt;br/&gt;
If we keep the value of (highHaterMark -lowWaterMark) value very high cleanups will be infrequent&lt;/p&gt;


</comment>
                            <comment id="12621779" author="noble.paul" created="Tue, 12 Aug 2008 12:30:55 +0100"  >&lt;p&gt;Full SolrCache implementation &lt;br/&gt;
not tested &lt;/p&gt;

&lt;p&gt;the initialization parameters are same us the current LRUCache. &lt;/p&gt;</comment>
                            <comment id="12621827" author="noble.paul" created="Tue, 12 Aug 2008 15:13:33 +0100"  >&lt;p&gt;this is the right patch&lt;/p&gt;</comment>
                            <comment id="12623656" author="noble.paul" created="Tue, 19 Aug 2008 13:42:29 +0100"  >&lt;p&gt;with testcase&lt;/p&gt;</comment>
                            <comment id="12627592" author="adb" created="Tue, 2 Sep 2008 08:05:45 +0100"  >&lt;p&gt;I have also been considering a concurrent LRU cache for my own application and seeing this isse made me think about it again.  Wouldn&apos;t one option be to use a ReentrantReadWriteLock to synchronise the map rather than complete synchronisation on the map for both readers and writers.  Although that does not give a free get() it would at least allow concurrent get and still be able to use the LinkedHashMap and would not require the extra thread.  Not sure if SOLR is java 1.5, but if not you could still use Doug Lea&apos;s concurrent package for pre 1.5 Java.&lt;/p&gt;</comment>
                            <comment id="12627598" author="noble.paul" created="Tue, 2 Sep 2008 09:00:00 +0100"  >&lt;p&gt;The patch contains a implementation which uses the java 5 features (ConcurrentHashMap) .It is better than using a separate Lock&lt;/p&gt;</comment>
                            <comment id="12634373" author="noble.paul" created="Thu, 25 Sep 2008 05:39:35 +0100"  >&lt;p&gt;name change and some refactoring&lt;/p&gt;</comment>
                            <comment id="12635032" author="yseeley@gmail.com" created="Fri, 26 Sep 2008 22:28:34 +0100"  >&lt;p&gt;Here is a prototype of an idea I&apos;ve had for a while for an efficient concurrent LRU cache.&lt;br/&gt;
It is completely untested... consider it more &quot;code as design&quot;.  It &lt;b&gt;should&lt;/b&gt; feature faster cleaning - O( n ) when it works well.&lt;/p&gt;

&lt;p&gt;In addition to low and high water marks, it adds the concept of an &quot;acceptable&quot; water mark.  A cleaning phase will try to go to the low water mark, but will be considered successful if it hits the acceptable water mark.&lt;/p&gt;

&lt;p&gt;This is coupled with a multi-pass cleaning phase.  From the comments:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    &lt;span class=&quot;code-comment&quot;&gt;// &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; we want to keep at least 1000 entries, then timestamps of
&lt;/span&gt;    &lt;span class=&quot;code-comment&quot;&gt;// current through current-1000 are guaranteed not to be the oldest!
&lt;/span&gt;    &lt;span class=&quot;code-comment&quot;&gt;// Also, &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; we want to remove 500 entries, then
&lt;/span&gt;    &lt;span class=&quot;code-comment&quot;&gt;// oldestEntry through oldestEntry+500 are guaranteed to be
&lt;/span&gt;    &lt;span class=&quot;code-comment&quot;&gt;// removed.&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The oldestEntry and newestEntry in the set of entries currently being considered is recorded for each phase.  Entries that are new enough such that they are guaranteed to be kept are immediately removed from consideration, and entries that are old enough such that they are guaranteed to be removed are immediately removed (no sorting necessary).  After 2 phases of this (configurable) and we still haven&apos;t removed enough entries, a priority queue is used to find the oldest entries out of those remaining.&lt;/p&gt;

&lt;p&gt;There are undoubtedly some other tricks we can use, but this was the best I could come up with for now.&lt;/p&gt;</comment>
                            <comment id="12635108" author="noble.paul" created="Sat, 27 Sep 2008 07:50:41 +0100"  >&lt;p&gt;Looks good. This has a lot in common with my approach. The doClean() is done far more efficiently in your implementation . I can improve mine with your cleanup code (if you think it is fine)&lt;/p&gt;</comment>
                            <comment id="12635159" author="yseeley@gmail.com" created="Sat, 27 Sep 2008 16:17:40 +0100"  >&lt;blockquote&gt;&lt;p&gt;I can improve mine with your cleanup code (if you think it is fine)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1&lt;/p&gt;

&lt;p&gt;I&apos;d also include the manual tracking of size() that mine did... the ConcurrentHashMap.size() doesn&apos;t look fast.&lt;/p&gt;

&lt;p&gt;Another thing to think about : pass an optional Executor in the constructor instead of creating a cleaning thread... and if it&apos;s null, it means &quot;do it in the foreground&quot;.  That would add flexibility and the ability to avoid one thread per cache if desired.&lt;/p&gt;</comment>
                            <comment id="12635221" author="funtick" created="Sun, 28 Sep 2008 14:36:47 +0100"  >&lt;p&gt;Paul, Yonik,  thanks for your efforts; BTW &apos;Concurrent&apos;HashMap uses spinloops for &apos;safe&apos; updates in order to avoid synchronization (instead of giving up CPU cycles); there are always cases when it is not faster that simple HashMap with synchronization.&lt;/p&gt;

&lt;p&gt;LingPipe uses different approach, see last comment at &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-665&quot; title=&quot;FIFO Cache (Unsynchronized): 9x times performance boost&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-665&quot;&gt;&lt;del&gt;SOLR-665&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Also, why are you in-a-loop with LRU? LFU is logically better.&lt;/p&gt;

&lt;p&gt;+1 and thanks for sharing.&lt;/p&gt;</comment>
                            <comment id="12635230" author="noble.paul" created="Sun, 28 Sep 2008 16:06:46 +0100"  >&lt;p&gt;My implementation just uses a Map&amp;lt;K,V&amp;gt; internally. If we can get a Map implemenatation that is faster than  ConcurrentHashMap (and concurrent) we can replace it (after seeing the performance). &lt;/p&gt;


</comment>
                            <comment id="12637029" author="noble.paul" created="Mon, 6 Oct 2008 08:29:43 +0100"  >&lt;p&gt;borrowed some ideas from yonik&apos;s impl.&lt;br/&gt;
Probably this is a good enough first cut.  &lt;/p&gt;</comment>
                            <comment id="12639791" author="shalinmangar" created="Wed, 15 Oct 2008 11:32:33 +0100"  >&lt;p&gt;Yonik &amp;#8211; do you think this is good enough to go in now? Probably some users can try it out and report their experiences if we commit it early. I can take this up if you want.&lt;/p&gt;</comment>
                            <comment id="12643190" author="shalinmangar" created="Tue, 28 Oct 2008 12:11:15 +0000"  >&lt;ol&gt;
	&lt;li&gt;Added comments in the code&lt;/li&gt;
	&lt;li&gt;Fixed a few concurrency issues&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;I&apos;ll commit this shortly.&lt;/p&gt;</comment>
                            <comment id="12643328" author="shalinmangar" created="Tue, 28 Oct 2008 20:14:16 +0000"  >&lt;p&gt;Committed revision 708656.&lt;/p&gt;

&lt;p&gt;Thanks Fuad, Noble and Yonik!&lt;/p&gt;</comment>
                            <comment id="12643520" author="tfeak" created="Wed, 29 Oct 2008 15:45:32 +0000"  >&lt;p&gt;Huge thanks on this one. This was one of the bottlenecks I&apos;ve seen previously. For apples to apples load tests, this more then doubled my overall throughput.&lt;/p&gt;

&lt;p&gt;I do notice a sort of &quot;pulsing&quot; in the responses. It appears that everything flies along, but on occasion everything piles up for a second, then starts going again. This leads to a few response times that are over 1 second, but the average is way down closer to 20ms. Is this the cleanup involved when hitting a high-water mark?&lt;/p&gt;

&lt;p&gt;Overall, it&apos;s a huge improvement.&lt;/p&gt;</comment>
                            <comment id="12643535" author="noble.paul" created="Wed, 29 Oct 2008 17:31:09 +0000"  >&lt;blockquote&gt;&lt;p&gt;This leads to a few response times that are over 1 second, but the average is way down closer to 20ms.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;yeah you are right.&lt;br/&gt;
there is one more feature in ConcurrentLRUCache whcih enables cleanups to be done in a new thread .FastLRUCache is not using it yet .I&apos;ll give a patch soon . This will take care of that 1 sec delay.&lt;/p&gt;</comment>
                            <comment id="12643536" author="shalinmangar" created="Wed, 29 Oct 2008 17:31:26 +0000"  >&lt;blockquote&gt;&lt;p&gt;For apples to apples load tests, this more then doubled my overall throughput. &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Very good to hear that!&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Is this the cleanup involved when hitting a high-water mark?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes, a cleanup is attempted when the size crosses the high watermark (&apos;size&apos; config parameter). It is done in two stages. In the first stage, least recently used items are evicted. If, after the first stage, the cache size is still greater than &apos;acceptableSize&apos; config parameter (default is 0.95*maxSize), the second stage takes over. The second stage is more intensive and tries to bring down the cache size to the &apos;minSize&apos; config parameter (default is 0.9*maxSize).&lt;/p&gt;

&lt;p&gt;Note that the cleanup is done in the same thread which calls a put on the cache, hence the &apos;pulsing&apos; that you are seeing. The cache implementation supports using a separate cleanup thread too, however it is not used currently. We still need to evaluate the best way to use it and how much it can help.&lt;/p&gt;</comment>
                            <comment id="12643555" author="noble.paul" created="Wed, 29 Oct 2008 17:54:17 +0000"  >&lt;p&gt;run cleanup in new thread&lt;/p&gt;</comment>
                            <comment id="12643604" author="tfeak" created="Wed, 29 Oct 2008 18:58:10 +0000"  >&lt;p&gt;I&apos;m not sure if that helped out. I haven&apos;t run a profiler yet ,but I think the &quot;pulsing&quot; (for lack of a better term) is caused by something else.&lt;/p&gt;

&lt;p&gt;Here&apos;s why... I used the new FastLRUCache &lt;b&gt;only&lt;/b&gt; for my Document cache in my latest test. The document cache size is large enough to hold all of the documents in the test set, helping focus on cache behavior. The warming query is enough to get all documents into the cache on startup. So, the cache is essentially as full as it&apos;s gonna get. 100% hit rate, and not growing. Yet, it still exhibits this pulsing. Could it be associated with the overhead of maintaining the least recently used entries?&lt;/p&gt;

&lt;p&gt;The introduction of the background thread didn&apos;t address this. It also didn&apos;t appear to speed things up, in fact it dropped overall throughput a bit, though still better then they old LRUCache.&lt;/p&gt;</comment>
                            <comment id="12643738" author="noble.paul" created="Thu, 30 Oct 2008 03:49:41 +0000"  >&lt;blockquote&gt;&lt;p&gt;Could it be associated with the overhead of maintaining the least recently used entries?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The overhead is ~= 0. It just has to increment an AtomicLong everytime you do a get() .I suspect the &apos;pulsing&apos; may be because of GC pauses. enable GC logging and you will know&lt;/p&gt;</comment>
                            <comment id="12643968" author="yseeley@gmail.com" created="Thu, 30 Oct 2008 14:01:45 +0000"  >&lt;p&gt;Todd, what&apos;s the used size of your Document cache?&lt;br/&gt;
I&apos;ll review this latest incarnation of ConcurrentLRUCache to see if there&apos;s anything that might cause this.&lt;/p&gt;</comment>
                            <comment id="12643980" author="yseeley@gmail.com" created="Thu, 30 Oct 2008 14:27:25 +0000"  >&lt;p&gt;Some minor updates:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;fix thread saftey issue in finalizer (background thread may never see stop being set)&lt;/li&gt;
	&lt;li&gt;fix tracking of size in put() to only increment if oldValue != null&lt;/li&gt;
	&lt;li&gt;optimize cleaning check in put() since it will be called for every put until the size is back down.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12643984" author="yseeley@gmail.com" created="Thu, 30 Oct 2008 15:00:32 +0000"  >&lt;p&gt;Todd: have you verified that the hit rate stays at 100% (no new inserts, no new evictions, etc)?  If so, it might just be GC as Noble suggests.  Bigger heaps often increase GC pauses.&lt;/p&gt;</comment>
                            <comment id="12643988" author="tfeak" created="Thu, 30 Oct 2008 15:10:25 +0000"  >&lt;p&gt;I&apos;ll hook up profiling and see if my GC overhead has changed, or is seeing big peaks. Should be later today.&lt;/p&gt;

&lt;p&gt;My document cache is only 4000 for that test, but only about 3300 documents are in it. I wanted to focus on the overhead of getting things out of LRUCache, as in production we get &amp;gt;97% hit rates. I verified it&apos;s at 100% hit rate (warming fills it).&lt;/p&gt;</comment>
                            <comment id="12644008" author="shalinmangar" created="Thu, 30 Oct 2008 16:07:05 +0000"  >&lt;p&gt;Committed revision 709188.&lt;/p&gt;

&lt;p&gt;Thanks Yonik!&lt;/p&gt;</comment>
                            <comment id="12644023" author="yseeley@gmail.com" created="Thu, 30 Oct 2008 16:34:21 +0000"  >&lt;p&gt;The markAndSweep logic in the current code didn&apos;t replicate the logic I gave in my example ConcurrentLRUCache, and it can remove a lot more than it should.&lt;/p&gt;

&lt;p&gt;Specifically, my algorithm broke the results into 3 groups:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;those documents that are guaranteed to be in the top group (most recently accessed)&lt;/li&gt;
	&lt;li&gt;those documents guaranteed to be in the bottom group (immediately discard)&lt;/li&gt;
	&lt;li&gt;those documents where you can&apos;t tell.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The current code reversed this logic, assuming that one can remove everything that is not in the top group.  This isn&apos;t valid though, as lastAccess isn&apos;t uniform (and thus the size of the top group could be as small as 1).&lt;/p&gt;</comment>
                            <comment id="12644093" author="tfeak" created="Thu, 30 Oct 2008 20:38:04 +0000"  >&lt;p&gt;I ran with a profiler and I&apos;m not seeing any bursts of garbage collection. It&apos;s at a steady ~2%, with no major collections occurring (which is great!). However, the use of the profiler also slows things down about 10-20 % which seems to be enough that the pulsing goes away. I believe the pulsing may be some sort of limit of the testing I&apos;m doing on a limited local environment. I&apos;ll include the patch in my current Solr app and do a more accurate comparison with our production level load tests to see if it still exists. Though it may be a while before I can provide feedback from that one, as getting machines allocated for the heavy load testing can take a bit.&lt;/p&gt;

&lt;p&gt;As I said before, this is a huge improvement. Thanks for all the work on this one.&lt;/p&gt;</comment>
                            <comment id="12644097" author="yseeley@gmail.com" created="Thu, 30 Oct 2008 20:44:28 +0000"  >&lt;p&gt;I&apos;ll take a crack at updating the patch such that too many entries aren&apos;t removed.&lt;/p&gt;</comment>
                            <comment id="12644195" author="noble.paul" created="Fri, 31 Oct 2008 01:47:55 +0000"  >&lt;blockquote&gt;&lt;p&gt;The current code reversed this logic,&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I missed the point . I guess this post made it clear . As you mentioned it should be a 3 step cleanup&lt;/p&gt;</comment>
                            <comment id="12644216" author="shalinmangar" created="Fri, 31 Oct 2008 05:35:43 +0000"  >&lt;p&gt;I forgot to add license headers to the three source files for this issue. I&apos;ll hold off adding them lest it breaks patches that you guys are working on.&lt;/p&gt;</comment>
                            <comment id="12644222" author="noble.paul" created="Fri, 31 Oct 2008 06:13:04 +0000"  >&lt;p&gt;yonik&apos;s suggestions implemented&lt;/p&gt;</comment>
                            <comment id="12644235" author="noble.paul" created="Fri, 31 Oct 2008 08:15:30 +0000"  >&lt;p&gt;creating an array&lt;span class=&quot;error&quot;&gt;&amp;#91;map.size()&amp;#93;&lt;/span&gt; for every markAndSweep() is expensive. Iterator should be better&lt;/p&gt;</comment>
                            <comment id="12644321" author="yseeley@gmail.com" created="Fri, 31 Oct 2008 15:45:12 +0000"  >&lt;p&gt;Thanks Noble, we had a mid-air implementation collision &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
I&apos;m doing some quick performance testing the version I wrote now...I&apos;ll try it against your version after and then we can go from there.&lt;/p&gt;</comment>
                            <comment id="12644381" author="yseeley@gmail.com" created="Fri, 31 Oct 2008 18:04:25 +0000"  >&lt;p&gt;OK, here&apos;s the implementation based on my previous pseudo code, along with a very quick performance test.&lt;br/&gt;
The test uses random keys over a slightly bigger range than the table.&lt;br/&gt;
It also uses an upper water mark 10% higher than the lower water mark, and an acceptable water mark half way inbetween.  I haven&apos;t experimented to see what the best acceptable water mark is for either impl.  If anyone wants to do a more realistic test with real queries, they are welcome to it.&lt;/p&gt;

&lt;p&gt;I did 4 runs and took the lowest number for each sub-test.  Java6 -server, WinXP, P4.  Times in milliseconds.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    doPerfTest(2000000, 100000, 200000); &lt;span class=&quot;code-comment&quot;&gt;// big cache
&lt;/span&gt;noble=17063  yonik=9157
    doPerfTest(2000000, 100000, 120000);  &lt;span class=&quot;code-comment&quot;&gt;// smaller key space increases distance between 
&lt;/span&gt;oldest, newest and makes the first passes less effective.
noble=8391  yonik=5812
    doPerfTest(6000000, 1000, 2000);    &lt;span class=&quot;code-comment&quot;&gt;// small cache, smaller hit rate
&lt;/span&gt;noble=17578  yonik=12515
    doPerfTest(6000000, 1000, 1200);    &lt;span class=&quot;code-comment&quot;&gt;// small cache, bigger hit rate
&lt;/span&gt;noble=11500  yonik=8219
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12644389" author="yseeley@gmail.com" created="Fri, 31 Oct 2008 18:21:08 +0000"  >&lt;p&gt;Nobe, your latest patch contains code like this:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (!markAndSweepLock.tryLock()) &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt;;
    &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; oldestItem = &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.oldestItem;
    [...]
    markAndSweepLock.unlock();
    &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.oldestItem = oldestItem;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;oldestItem isn&apos;t volatile (and doesn&apos;t need to be if accessed correctly).&lt;br/&gt;
The lock will also serve as a read barrier, so the first part is OK.&lt;br/&gt;
The unlock will serve as a write barrier, but the set of oldestItem comes after it (not OK... another thread may not see the value).&lt;br/&gt;
Changing the order (the write to oldestItem before the unlock) will ensure that the next thread that crosses a read barrier will see the new value.&lt;/p&gt;</comment>
                            <comment id="12644532" author="yseeley@gmail.com" created="Sat, 1 Nov 2008 19:20:52 +0000"  >&lt;p&gt;Added some minor changes, making sure that minLimit &amp;gt;= 1 and limit &amp;gt;minLimit (needed for rounding with small cache sizes).&lt;br/&gt;
Also added test code for LRUCache vs FastLRUCache.&lt;br/&gt;
It appears that LRUCache is faster (at least on my single proc PC) when the hit ratio is low, and FastLRUCache is faster when the hit ratio is high.&lt;br/&gt;
Should FastLRUCache be made the default in the example schema for the filterCache?&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
time=2937 impl=LRUCache nThreads= 1 size=100000 maxKey=100000 gets=2000000 hitRatio=0.981608
time=2266 impl=FastLRUCache nThreads= 1 size=100000 maxKey=100000 gets=2000000 hitRatio=0.981608
time=3594 impl=LRUCache nThreads= 2 size=100000 maxKey=100000 gets=2000000 hitRatio=0.9816075
time=1484 impl=FastLRUCache nThreads= 2 size=100000 maxKey=100000 gets=2000000 hitRatio=0.981608
time=3203 impl=LRUCache nThreads= 1 size=100000 maxKey=120000 gets=2000000 hitRatio=0.835225
time=4593 impl=FastLRUCache nThreads= 1 size=100000 maxKey=120000 gets=2000000 hitRatio=0.751506
time=3781 impl=LRUCache nThreads= 2 size=100000 maxKey=120000 gets=2000000 hitRatio=0.834685
time=2656 impl=FastLRUCache nThreads= 2 size=100000 maxKey=120000 gets=2000000 hitRatio=0.8232835000000001
time=3234 impl=LRUCache nThreads= 1 size=100000 maxKey=200000 gets=2000000 hitRatio=0.523398
time=5047 impl=FastLRUCache nThreads= 1 size=100000 maxKey=200000 gets=2000000 hitRatio=0.3831675
time=4125 impl=LRUCache nThreads= 2 size=100000 maxKey=200000 gets=2000000 hitRatio=0.511871
time=3969 impl=FastLRUCache nThreads= 2 size=100000 maxKey=200000 gets=2000000 hitRatio=0.6665975
time=3390 impl=LRUCache nThreads= 1 size=100000 maxKey=1000000 gets=2000000 hitRatio=0.1445725
time=5687 impl=FastLRUCache nThreads= 1 size=100000 maxKey=1000000 gets=2000000 hitRatio=0.10041049999999996
time=4750 impl=LRUCache nThreads= 2 size=100000 maxKey=1000000 gets=2000000 hitRatio=0.10340150000000004
time=6875 impl=FastLRUCache nThreads= 2 size=100000 maxKey=1000000 gets=2000000 hitRatio=0.22233749999999997
time=1343 impl=LRUCache nThreads= 1 size=1000 maxKey=1000 gets=2000000 hitRatio=0.9998065
time=860 impl=FastLRUCache nThreads= 1 size=1000 maxKey=1000 gets=2000000 hitRatio=0.9998065
time=1547 impl=LRUCache nThreads= 2 size=1000 maxKey=1000 gets=2000000 hitRatio=0.9998065
time=703 impl=FastLRUCache nThreads= 2 size=1000 maxKey=1000 gets=2000000 hitRatio=0.9998065
time=1610 impl=LRUCache nThreads= 1 size=1000 maxKey=1200 gets=2000000 hitRatio=0.833648
time=2406 impl=FastLRUCache nThreads= 1 size=1000 maxKey=1200 gets=2000000 hitRatio=0.7404839999999999
time=2078 impl=LRUCache nThreads= 2 size=1000 maxKey=1200 gets=2000000 hitRatio=0.8334255
time=859 impl=FastLRUCache nThreads= 2 size=1000 maxKey=1200 gets=2000000 hitRatio=0.998974
time=1922 impl=LRUCache nThreads= 1 size=1000 maxKey=2000 gets=2000000 hitRatio=0.5003285
time=2875 impl=FastLRUCache nThreads= 1 size=1000 maxKey=2000 gets=2000000 hitRatio=0.3516785
time=2422 impl=LRUCache nThreads= 2 size=1000 maxKey=2000 gets=2000000 hitRatio=0.5002055000000001
time=1203 impl=FastLRUCache nThreads= 2 size=1000 maxKey=2000 gets=2000000 hitRatio=0.821195
time=2297 impl=LRUCache nThreads= 1 size=1000 maxKey=10000 gets=2000000 hitRatio=0.10054949999999996
time=2969 impl=FastLRUCache nThreads= 1 size=1000 maxKey=10000 gets=2000000 hitRatio=0.05416350000000003
time=3078 impl=LRUCache nThreads= 2 size=1000 maxKey=10000 gets=2000000 hitRatio=0.10003499999999999
time=3000 impl=FastLRUCache nThreads= 2 size=1000 maxKey=10000 gets=2000000 hitRatio=0.10475299999999999
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12644540" author="yseeley@gmail.com" created="Sat, 1 Nov 2008 21:07:41 +0000"  >&lt;p&gt;Latest patch:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;fixes an off-by-one (it was possible to go below minSize)&lt;/li&gt;
	&lt;li&gt;fixes tests to not expect a specific number of evictions.&lt;/li&gt;
	&lt;li&gt;makes FastLRUCache the default for filterCache in the example schema and main test schema.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I&apos;ll commit soon if there are no objections.&lt;/p&gt;</comment>
                            <comment id="12644554" author="noble.paul" created="Sun, 2 Nov 2008 02:27:12 +0000"  >&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
CacheEntry[] eset = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; CacheEntry[sz];
&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; eSize = 0;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Isn&apos;t it too expensive to create a potentially huge array every time we do a clean? (too much work for GC) .May be we do not even need it if the first loop is enough. Moreover this one thing has added more code.&lt;/p&gt;

&lt;p&gt;I didn&apos;t use lucene PriorityQueue because if somebody wished to lift the code they can easily do so if there is no other dependency. When I posted the requirement in google collections there was a lot of interest in such  a component. Can TreeSet do the trick?&lt;/p&gt;
</comment>
                            <comment id="12644594" author="yseeley@gmail.com" created="Sun, 2 Nov 2008 15:33:24 +0000"  >&lt;blockquote&gt;&lt;p&gt;Isn&apos;t it too expensive to create a potentially huge array every time we do a clean? (too much work for GC) &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That&apos;s what benchmarking is for &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;It&apos;s a single short-lived allocation that allows us to  greatly reduce the number of elements we need to evaluate on successive passes.  Inserts into a TreeSet may have a higher GC cost given it&apos;s an allocation per insert.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;May be we do not even need it if the first loop is enough.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right... although in my testing, it seemed like the first loop was rarely sufficient (although the second often was).&lt;/p&gt;</comment>
                            <comment id="12644656" author="shalinmangar" created="Mon, 3 Nov 2008 07:24:58 +0000"  >&lt;p&gt;Here&apos;s the performance test from the patch on a more recent machine &amp;#8211; Intel Quad Core, RHEL 64-bit, Java HotSpot(TM) 64-Bit Server VM (build 1.5.0_11-b03, mixed mode):&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
time=1456 impl=LRUCache nThreads= 1 size=100000 maxKey=100000 gets=2000000 hitRatio=0.981608
time=1041 impl=FastLRUCache nThreads= 1 size=100000 maxKey=100000 gets=2000000 hitRatio=0.981608
time=3256 impl=LRUCache nThreads= 2 size=100000 maxKey=100000 gets=2000000 hitRatio=0.981608
time=754 impl=FastLRUCache nThreads= 2 size=100000 maxKey=100000 gets=2000000 hitRatio=0.981608
time=1234 impl=LRUCache nThreads= 1 size=100000 maxKey=120000 gets=2000000 hitRatio=0.835225
time=1564 impl=FastLRUCache nThreads= 1 size=100000 maxKey=120000 gets=2000000 hitRatio=0.751506
time=3728 impl=LRUCache nThreads= 2 size=100000 maxKey=120000 gets=2000000 hitRatio=0.835006
time=1384 impl=FastLRUCache nThreads= 2 size=100000 maxKey=120000 gets=2000000 hitRatio=0.798109
time=1357 impl=LRUCache nThreads= 1 size=100000 maxKey=200000 gets=2000000 hitRatio=0.523398
time=1894 impl=FastLRUCache nThreads= 1 size=100000 maxKey=200000 gets=2000000 hitRatio=0.3831675
time=4556 impl=LRUCache nThreads= 2 size=100000 maxKey=200000 gets=2000000 hitRatio=0.512785
time=1514 impl=FastLRUCache nThreads= 2 size=100000 maxKey=200000 gets=2000000 hitRatio=0.4682115
time=1614 impl=LRUCache nThreads= 1 size=100000 maxKey=1000000 gets=2000000 hitRatio=0.1445725
time=1837 impl=FastLRUCache nThreads= 1 size=100000 maxKey=1000000 gets=2000000 hitRatio=0.10041049999999996
time=4710 impl=LRUCache nThreads= 2 size=100000 maxKey=1000000 gets=2000000 hitRatio=0.10963999999999996
time=1816 impl=FastLRUCache nThreads= 2 size=100000 maxKey=1000000 gets=2000000 hitRatio=0.11144399999999999
time=339 impl=LRUCache nThreads= 1 size=1000 maxKey=1000 gets=2000000 hitRatio=0.9998065
time=292 impl=FastLRUCache nThreads= 1 size=1000 maxKey=1000 gets=2000000 hitRatio=0.9998065
time=2511 impl=LRUCache nThreads= 2 size=1000 maxKey=1000 gets=2000000 hitRatio=0.9998065
time=351 impl=FastLRUCache nThreads= 2 size=1000 maxKey=1000 gets=2000000 hitRatio=0.9998065
time=383 impl=LRUCache nThreads= 1 size=1000 maxKey=1200 gets=2000000 hitRatio=0.833648
time=580 impl=FastLRUCache nThreads= 1 size=1000 maxKey=1200 gets=2000000 hitRatio=0.7404839999999999
time=2716 impl=LRUCache nThreads= 2 size=1000 maxKey=1200 gets=2000000 hitRatio=0.8337875
time=805 impl=FastLRUCache nThreads= 2 size=1000 maxKey=1200 gets=2000000 hitRatio=0.79799
time=570 impl=LRUCache nThreads= 1 size=1000 maxKey=2000 gets=2000000 hitRatio=0.5003285
time=794 impl=FastLRUCache nThreads= 1 size=1000 maxKey=2000 gets=2000000 hitRatio=0.3516785
time=3676 impl=LRUCache nThreads= 2 size=1000 maxKey=2000 gets=2000000 hitRatio=0.49959549999999997
time=1685 impl=FastLRUCache nThreads= 2 size=1000 maxKey=2000 gets=2000000 hitRatio=0.436728
time=712 impl=LRUCache nThreads= 1 size=1000 maxKey=10000 gets=2000000 hitRatio=0.10054949999999996
time=1022 impl=FastLRUCache nThreads= 1 size=1000 maxKey=10000 gets=2000000 hitRatio=0.05416350000000003
time=4395 impl=LRUCache nThreads= 2 size=1000 maxKey=10000 gets=2000000 hitRatio=0.100526
time=2562 impl=FastLRUCache nThreads= 2 size=1000 maxKey=10000 gets=2000000 hitRatio=0.08556600000000003
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With more number of threads this time (4 and 16):&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
time=1794 impl=LRUCache nThreads= 4 size=100000 maxKey=100000 gets=2000000 hitRatio=0.981608
time=594 impl=FastLRUCache nThreads= 4 size=100000 maxKey=100000 gets=2000000 hitRatio=0.9816075
time=1737 impl=LRUCache nThreads= 16 size=100000 maxKey=100000 gets=2000000 hitRatio=0.981607
time=602 impl=FastLRUCache nThreads= 16 size=100000 maxKey=100000 gets=2000000 hitRatio=0.981602
time=2387 impl=LRUCache nThreads= 4 size=100000 maxKey=120000 gets=2000000 hitRatio=0.830956
time=866 impl=FastLRUCache nThreads= 4 size=100000 maxKey=120000 gets=2000000 hitRatio=0.8892465
time=1793 impl=LRUCache nThreads= 16 size=100000 maxKey=120000 gets=2000000 hitRatio=0.8274485
time=706 impl=FastLRUCache nThreads= 16 size=100000 maxKey=120000 gets=2000000 hitRatio=0.9586865
time=2233 impl=LRUCache nThreads= 4 size=100000 maxKey=200000 gets=2000000 hitRatio=0.5025255
time=1228 impl=FastLRUCache nThreads= 4 size=100000 maxKey=200000 gets=2000000 hitRatio=0.654153
time=1905 impl=LRUCache nThreads= 16 size=100000 maxKey=200000 gets=2000000 hitRatio=0.500583
time=883 impl=FastLRUCache nThreads= 16 size=100000 maxKey=200000 gets=2000000 hitRatio=0.9067965
time=5336 impl=LRUCache nThreads= 4 size=100000 maxKey=1000000 gets=2000000 hitRatio=0.10182199999999997
time=1780 impl=FastLRUCache nThreads= 4 size=100000 maxKey=1000000 gets=2000000 hitRatio=0.25870800000000005
time=2911 impl=LRUCache nThreads= 16 size=100000 maxKey=1000000 gets=2000000 hitRatio=0.10132300000000005
time=1941 impl=FastLRUCache nThreads= 16 size=100000 maxKey=1000000 gets=2000000 hitRatio=0.508488
time=687 impl=LRUCache nThreads= 4 size=1000 maxKey=1000 gets=2000000 hitRatio=0.9998065
time=421 impl=FastLRUCache nThreads= 4 size=1000 maxKey=1000 gets=2000000 hitRatio=0.9998065
time=782 impl=LRUCache nThreads= 16 size=1000 maxKey=1000 gets=2000000 hitRatio=0.9998065
time=452 impl=FastLRUCache nThreads= 16 size=1000 maxKey=1000 gets=2000000 hitRatio=0.9998065
time=813 impl=LRUCache nThreads= 4 size=1000 maxKey=1200 gets=2000000 hitRatio=0.8333735
time=678 impl=FastLRUCache nThreads= 4 size=1000 maxKey=1200 gets=2000000 hitRatio=0.9988885
time=794 impl=LRUCache nThreads= 16 size=1000 maxKey=1200 gets=2000000 hitRatio=0.8331635
time=503 impl=FastLRUCache nThreads= 16 size=1000 maxKey=1200 gets=2000000 hitRatio=0.977526
time=1554 impl=LRUCache nThreads= 4 size=1000 maxKey=2000 gets=2000000 hitRatio=0.500093
time=928 impl=FastLRUCache nThreads= 4 size=1000 maxKey=2000 gets=2000000 hitRatio=0.802332
time=1102 impl=LRUCache nThreads= 16 size=1000 maxKey=2000 gets=2000000 hitRatio=0.5002759999999999
time=566 impl=FastLRUCache nThreads= 16 size=1000 maxKey=2000 gets=2000000 hitRatio=0.954131
time=1543 impl=LRUCache nThreads= 4 size=1000 maxKey=10000 gets=2000000 hitRatio=0.10062899999999997
time=1039 impl=FastLRUCache nThreads= 4 size=1000 maxKey=10000 gets=2000000 hitRatio=0.7582409999999999
time=1372 impl=LRUCache nThreads= 16 size=1000 maxKey=10000 gets=2000000 hitRatio=0.10031000000000001
time=604 impl=FastLRUCache nThreads= 16 size=1000 maxKey=10000 gets=2000000 hitRatio=0.935282
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now with 8 and 32 threads:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
time=2109 impl=LRUCache nThreads= 8 size=100000 maxKey=100000 gets=2000000 hitRatio=0.9816075
time=608 impl=FastLRUCache nThreads= 8 size=100000 maxKey=100000 gets=2000000 hitRatio=0.981606
time=1502 impl=LRUCache nThreads= 32 size=100000 maxKey=100000 gets=2000000 hitRatio=0.9816045
time=648 impl=FastLRUCache nThreads= 32 size=100000 maxKey=100000 gets=2000000 hitRatio=0.981592
time=3876 impl=LRUCache nThreads= 8 size=100000 maxKey=120000 gets=2000000 hitRatio=0.8267995
time=748 impl=FastLRUCache nThreads= 8 size=100000 maxKey=120000 gets=2000000 hitRatio=0.915961
time=2176 impl=LRUCache nThreads= 32 size=100000 maxKey=120000 gets=2000000 hitRatio=0.8271935
time=694 impl=FastLRUCache nThreads= 32 size=100000 maxKey=120000 gets=2000000 hitRatio=0.9652565
time=2038 impl=LRUCache nThreads= 8 size=100000 maxKey=200000 gets=2000000 hitRatio=0.5005305
time=1088 impl=FastLRUCache nThreads= 8 size=100000 maxKey=200000 gets=2000000 hitRatio=0.789179
time=2147 impl=LRUCache nThreads= 32 size=100000 maxKey=200000 gets=2000000 hitRatio=0.4997505
time=884 impl=FastLRUCache nThreads= 32 size=100000 maxKey=200000 gets=2000000 hitRatio=0.926915
time=2343 impl=LRUCache nThreads= 8 size=100000 maxKey=1000000 gets=2000000 hitRatio=0.10397699999999999
time=2207 impl=FastLRUCache nThreads= 8 size=100000 maxKey=1000000 gets=2000000 hitRatio=0.34063
time=3440 impl=LRUCache nThreads= 32 size=100000 maxKey=1000000 gets=2000000 hitRatio=0.10123850000000001
time=2087 impl=FastLRUCache nThreads= 32 size=100000 maxKey=1000000 gets=2000000 hitRatio=0.5367375
time=909 impl=LRUCache nThreads= 8 size=1000 maxKey=1000 gets=2000000 hitRatio=0.9998065
time=443 impl=FastLRUCache nThreads= 8 size=1000 maxKey=1000 gets=2000000 hitRatio=0.9998065
time=682 impl=LRUCache nThreads= 32 size=1000 maxKey=1000 gets=2000000 hitRatio=0.9998065
time=447 impl=FastLRUCache nThreads= 32 size=1000 maxKey=1000 gets=2000000 hitRatio=0.9998065
time=1189 impl=LRUCache nThreads= 8 size=1000 maxKey=1200 gets=2000000 hitRatio=0.832726
time=605 impl=FastLRUCache nThreads= 8 size=1000 maxKey=1200 gets=2000000 hitRatio=0.919104
time=1463 impl=LRUCache nThreads= 32 size=1000 maxKey=1200 gets=2000000 hitRatio=0.8337005
time=489 impl=FastLRUCache nThreads= 32 size=1000 maxKey=1200 gets=2000000 hitRatio=0.9845845
time=1256 impl=LRUCache nThreads= 8 size=1000 maxKey=2000 gets=2000000 hitRatio=0.500149
time=678 impl=FastLRUCache nThreads= 8 size=1000 maxKey=2000 gets=2000000 hitRatio=0.907774
time=1013 impl=LRUCache nThreads= 32 size=1000 maxKey=2000 gets=2000000 hitRatio=0.49962399999999996
time=503 impl=FastLRUCache nThreads= 32 size=1000 maxKey=2000 gets=2000000 hitRatio=0.976796
time=1504 impl=LRUCache nThreads= 8 size=1000 maxKey=10000 gets=2000000 hitRatio=0.10030550000000005
time=754 impl=FastLRUCache nThreads= 8 size=1000 maxKey=10000 gets=2000000 hitRatio=0.9151345
time=1245 impl=LRUCache nThreads= 32 size=1000 maxKey=10000 gets=2000000 hitRatio=0.10028899999999996
time=499 impl=FastLRUCache nThreads= 32 size=1000 maxKey=10000 gets=2000000 hitRatio=0.978823
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When the number of threads are increased, FastLRUCache is true to its name &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="12644730" author="yseeley@gmail.com" created="Mon, 3 Nov 2008 14:31:45 +0000"  >&lt;p&gt;That benchmark really isn&apos;t valid for a high number of threads though: notice the difference in hitRatio.&lt;br/&gt;
If you have many threads quickly adding items and only one thread at a time removing items, the FastLRUCache goes over it&apos;s target size and thus increases it&apos;s hitRatio, making it artificially faster.&lt;/p&gt;

&lt;p&gt;This isn&apos;t a concern for it&apos;s use in Solr though, since the generation of a cache value will be much slower than clearing the cache.&lt;/p&gt;</comment>
                            <comment id="12644738" author="tfeak" created="Mon, 3 Nov 2008 15:48:53 +0000"  >&lt;p&gt;Is there an &lt;b&gt;easy&lt;/b&gt; way to get this patched into 1.3.0?&lt;/p&gt;

&lt;p&gt;Right now, I think I have to grab 7 patches and apply them in order. Will that give me the correct content? Is there an easier way to do this from the repository?&lt;/p&gt;</comment>
                            <comment id="12644889" author="noble.paul" created="Tue, 4 Nov 2008 04:04:29 +0000"  >&lt;p&gt;There is another number which we have ignored. If the cleanup is done in a separate thread , FastLRUCache consistently outperforms the legacy one. (shalin forgot to put the numbers). For a very large cache size , the cleanup takes ~200-300 ms. Which means a request can end up paying a huge price. &lt;/p&gt;

&lt;p&gt;We need to add a new  &apos;newCleanupThread&apos; option to FastLRUCache (it is there in my old patch). I guess with that we can make FastLRUcache the default with newCleanupThread=true. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Is there an easy way to get this patched into 1.3.0? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If you apply yonik&apos;s latest patch on trunk you get two extra files . You can straightaway copy those two files to 1.3 and use it.&lt;/p&gt;</comment>
                            <comment id="12644942" author="shalinmangar" created="Tue, 4 Nov 2008 10:55:02 +0000"  >&lt;p&gt;I ran it again with a new thread for each cleanup. Time taken for markAndSweep is printed:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
time=1550 impl=LRUCache nThreads= 1 size=100000 maxKey=120000 gets=2000000 hitRatio=0.835225
MarkAndSweepTime = 138
MarkAndSweepTime = 35
MarkAndSweepTime = 36
MarkAndSweepTime = 39
MarkAndSweepTime = 41
MarkAndSweepTime = 42
MarkAndSweepTime = 42
MarkAndSweepTime = 44
MarkAndSweepTime = 43
MarkAndSweepTime = 43
MarkAndSweepTime = 44
MarkAndSweepTime = 43
MarkAndSweepTime = 43
MarkAndSweepTime = 44
MarkAndSweepTime = 43
MarkAndSweepTime = 44
MarkAndSweepTime = 44
MarkAndSweepTime = 43
time=1378 impl=FastLRUCache nThreads= 1 size=100000 maxKey=120000 gets=2000000 hitRatio=0.8130459999999999

time=3942 impl=LRUCache nThreads= 2 size=100000 maxKey=120000 gets=2000000 hitRatio=0.835045
MarkAndSweepTime = 58
MarkAndSweepTime = 165
MarkAndSweepTime = 32
MarkAndSweepTime = 34
MarkAndSweepTime = 37
MarkAndSweepTime = 37
MarkAndSweepTime = 46
MarkAndSweepTime = 40
MarkAndSweepTime = 61
MarkAndSweepTime = 53
MarkAndSweepTime = 51
MarkAndSweepTime = 44
MarkAndSweepTime = 47
MarkAndSweepTime = 47
MarkAndSweepTime = 48
MarkAndSweepTime = 48
MarkAndSweepTime = 46
time=1062 impl=FastLRUCache nThreads= 2 size=100000 maxKey=120000 gets=2000000 hitRatio=0.8560415
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12645302" author="yseeley@gmail.com" created="Wed, 5 Nov 2008 19:35:42 +0000"  >&lt;p&gt;I just committed a fix to the setting of acceptableSize - it was always being set to maxSize, which would normally cause the cleaning routine to return after the first phase (and thus be called more often than normal).&lt;/p&gt;</comment>
                            <comment id="12646094" author="noble.paul" created="Sun, 9 Nov 2008 16:23:30 +0000"  >&lt;p&gt;added a new boolean attribute &lt;tt&gt;newCleanThread&lt;/tt&gt; . Default is set to false&lt;/p&gt;</comment>
                            <comment id="12647587" author="shalinmangar" created="Fri, 14 Nov 2008 11:21:09 +0000"  >&lt;p&gt;Yonik, what do you think about using a new cleanup thread?&lt;/p&gt;</comment>
                            <comment id="12647657" author="yseeley@gmail.com" created="Fri, 14 Nov 2008 16:42:41 +0000"  >&lt;p&gt;The ability to use a separate cleanup thread is interesting... but I&apos;m not sure that having the ability to spin off a new thread for &lt;b&gt;each&lt;/b&gt; cleanup is something one would ever want to do.  The cleanup thread logic should probably be fixed too (no sleeping and polling... it should wait until notified that a cleanup is needed)&lt;/p&gt;</comment>
                            <comment id="12647798" author="noble.paul" created="Sat, 15 Nov 2008 04:54:48 +0000"  >&lt;p&gt;OK that is a good idea. But this is an important functinality. &lt;/p&gt;</comment>
                            <comment id="12648076" author="noble.paul" created="Mon, 17 Nov 2008 04:17:37 +0000"  >&lt;ul&gt;
	&lt;li&gt;cleanup thread does wait() and get notified when needed&lt;/li&gt;
	&lt;li&gt;ConcurrentLRUCache is generified&lt;/li&gt;
	&lt;li&gt;A new interface added to ConcurrentLRUCache called EvictionListener .This gets callback for each entry that is evicted&lt;/li&gt;
	&lt;li&gt;FastLRUcache has a new configuration &apos;cleanupThread&apos; . default is set to &apos;false&apos; . I believe it should be true by default&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12648918" author="noble.paul" created="Wed, 19 Nov 2008 06:26:21 +0000"  >&lt;p&gt;made CacheEntry non static &lt;/p&gt;</comment>
                            <comment id="12649342" author="shalinmangar" created="Thu, 20 Nov 2008 11:43:21 +0000"  >&lt;p&gt;Yonik, not trying to be pushy but can this patch be committed? &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;I want to create a build for internal use out of trunk with this feature.&lt;/p&gt;</comment>
                            <comment id="12649982" author="yseeley@gmail.com" created="Sat, 22 Nov 2008 22:26:37 +0000"  >&lt;p&gt;Does this have a thread leak?  Where is FastLRUCache.destroy() ever called?&lt;br/&gt;
It&apos;s called from the finalizer (yuck), &lt;b&gt;but&lt;/b&gt; that finalizer will never be called because the cleaning thread references the cache (the definition of liveness).  Issues with having the cache deal with the thread lifecycle is why I previously recommended exploring the use of an Executor that the user passes in.&lt;/p&gt;</comment>
                            <comment id="12650031" author="noble.paul" created="Sun, 23 Nov 2008 16:22:52 +0000"  >&lt;p&gt; Yonik, Nice catch . There was a thread leak.&lt;/p&gt;

&lt;p&gt;I hope this patch fixes that. The cleanup thread now holds a WeakReference to the cache &lt;br/&gt;
The close() of Solrcache ensures that it is destroyed.&lt;/p&gt;</comment>
                            <comment id="12650035" author="yseeley@gmail.com" created="Sun, 23 Nov 2008 17:12:49 +0000"  >&lt;p&gt;Thanks Noble, looks like that solution should work.&lt;/p&gt;

&lt;p&gt;Funny thing with the latest patch though - I get compile errors with &quot;ant test&quot; from the command line:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
compile-common:
    [mkdir] Created dir: f:\code\solr\build\common
    [javac] Compiling 39 source files to f:\code\solr\build\common
    [javac] f:\code\solr\src\java\org\apache\solr\common\util\ConcurrentLRUCache.java:201: &lt;span class=&quot;code-keyword&quot;&gt;generic&lt;/span&gt; array creation
    [javac]       CacheEntry[] eset = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; CacheEntry[sz];
    [javac]                           ^
    [javac] f:\code\solr\src\java\org\apache\solr\common\util\ConcurrentLRUCache.java:379: non-&lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; class org.apache.solr.common.util.ConcurrentLRUCache.Cache
Entry cannot be referenced from a &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; context
    [javac]       &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; ((CacheEntry)b).lastAccessedCopy &amp;lt; ((CacheEntry)a).lastAccessedCopy;
   [...]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It looks like the compiler thinks that the method is static.  IntelliJ doesn&apos;t flag any errors, and I can&apos;t see anything wrong after a quick glance at the code.  Does &quot;ant test&quot; from the command line work for you?&lt;/p&gt;</comment>
                            <comment id="12650055" author="yseeley@gmail.com" created="Sun, 23 Nov 2008 21:44:24 +0000"  >&lt;p&gt;OK, committed latest patch after some minor logic changes (including changing CacheEntry from an inner class to a static inner class, which solved the compilation errors).&lt;/p&gt;</comment>
                            <comment id="12650103" author="noble.paul" created="Mon, 24 Nov 2008 04:30:53 +0000"  >&lt;ul&gt;
	&lt;li&gt;Should we keep the cleanupThread= true default for FastLRUCache?&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12650104" author="yseeley@gmail.com" created="Mon, 24 Nov 2008 04:42:48 +0000"  >&lt;p&gt;I think the default should remain cleanupThread=false.  It&apos;s simpler behavior, and for normal cache sizes, the pause an individual request may see is less than what one would see from a GC pause.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12391057" name="ConcurrentLRUCache.java" size="9401" author="yseeley@gmail.com" created="Fri, 26 Sep 2008 22:28:34 +0100"/>
                            <attachment id="12387323" name="ConcurrentLRUCache.java" size="4576" author="noble.paul" created="Fri, 1 Aug 2008 06:21:25 +0100"/>
                            <attachment id="12387164" name="ConcurrentLRUCache.java" size="3608" author="noble.paul" created="Wed, 30 Jul 2008 07:15:40 +0100"/>
                            <attachment id="12393199" name="SOLR-667-alternate.patch" size="21469" author="yseeley@gmail.com" created="Sat, 1 Nov 2008 19:20:52 +0000"/>
                            <attachment id="12393171" name="SOLR-667-alternate.patch" size="16112" author="yseeley@gmail.com" created="Fri, 31 Oct 2008 18:04:25 +0000"/>
                            <attachment id="12393062" name="SOLR-667-updates.patch" size="2874" author="yseeley@gmail.com" created="Thu, 30 Oct 2008 14:27:25 +0000"/>
                            <attachment id="12394511" name="SOLR-667.patch" size="8869" author="noble.paul" created="Sun, 23 Nov 2008 16:22:52 +0000"/>
                            <attachment id="12394223" name="SOLR-667.patch" size="7484" author="noble.paul" created="Wed, 19 Nov 2008 06:26:21 +0000"/>
                            <attachment id="12394038" name="SOLR-667.patch" size="10023" author="noble.paul" created="Mon, 17 Nov 2008 04:17:37 +0000"/>
                            <attachment id="12393588" name="SOLR-667.patch" size="1397" author="noble.paul" created="Sun, 9 Nov 2008 16:23:30 +0000"/>
                            <attachment id="12393134" name="SOLR-667.patch" size="5757" author="noble.paul" created="Fri, 31 Oct 2008 08:15:30 +0000"/>
                            <attachment id="12393129" name="SOLR-667.patch" size="5921" author="noble.paul" created="Fri, 31 Oct 2008 06:13:04 +0000"/>
                            <attachment id="12392998" name="SOLR-667.patch" size="1141" author="noble.paul" created="Wed, 29 Oct 2008 17:54:17 +0000"/>
                            <attachment id="12392927" name="SOLR-667.patch" size="19565" author="shalinmangar" created="Tue, 28 Oct 2008 12:11:15 +0000"/>
                            <attachment id="12391522" name="SOLR-667.patch" size="17853" author="noble.paul" created="Mon, 6 Oct 2008 08:29:43 +0100"/>
                            <attachment id="12390898" name="SOLR-667.patch" size="17112" author="noble.paul" created="Thu, 25 Sep 2008 05:39:35 +0100"/>
                            <attachment id="12388519" name="SOLR-667.patch" size="16380" author="noble.paul" created="Tue, 19 Aug 2008 13:42:29 +0100"/>
                            <attachment id="12388052" name="SOLR-667.patch" size="13375" author="noble.paul" created="Tue, 12 Aug 2008 15:15:55 +0100"/>
                            <attachment id="12388051" name="SOLR-667.patch" size="13356" author="noble.paul" created="Tue, 12 Aug 2008 15:13:33 +0100"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>19.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 31 Jul 2008 15:17:12 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>6947</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hxxpxz:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>20522</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                </customfields>
    </item>
</channel>
</rss>