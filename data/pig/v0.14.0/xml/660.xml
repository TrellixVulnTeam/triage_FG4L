<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 05:02:08 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/PIG-660/PIG-660.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[PIG-660] Integration with Hadoop 0.20</title>
                <link>https://issues.apache.org/jira/browse/PIG-660</link>
                <project id="12310730" key="PIG">Pig</project>
                    <description>&lt;p&gt;With Hadoop 0.20, it will be possible to query the status of each map and reduce in a map reduce job. This will allow better error reporting. Some of the other items that could be on Hadoop&apos;s feature requests/bugs are documented here for tracking.&lt;/p&gt;

&lt;p&gt;1. Hadoop should return objects instead of strings when exceptions are thrown&lt;br/&gt;
2. The JobControl should handle all exceptions and report them appropriately. For example, when the JobControl fails to launch jobs, it should handle exceptions appropriately and should support APIs that query this state, i.e., failure to launch jobs.&lt;/p&gt;</description>
                <environment>&lt;p&gt;Hadoop 0.20&lt;/p&gt;</environment>
        <key id="12414403">PIG-660</key>
            <summary>Integration with Hadoop 0.20</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="sms">Santhosh Srinivasan</assignee>
                                    <reporter username="sms">Santhosh Srinivasan</reporter>
                        <labels>
                    </labels>
                <created>Mon, 9 Feb 2009 17:20:00 +0000</created>
                <updated>Wed, 24 Mar 2010 22:14:38 +0000</updated>
                            <resolved>Tue, 29 Sep 2009 01:31:22 +0100</resolved>
                                    <version>0.4.0</version>
                                    <fixVersion>0.5.0</fixVersion>
                                    <component>impl</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>8</watches>
                                                                <comments>
                            <comment id="12671936" author="sms" created="Mon, 9 Feb 2009 17:29:41 +0000"  >&lt;p&gt;JIRAs in Hadoop corresponding to items 1 and 2.&lt;/p&gt;

&lt;p&gt;1. &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-5201&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HADOOP-5201&lt;/a&gt;&lt;br/&gt;
2. &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-5202&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HADOOP-5202&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="12678996" author="sms" created="Wed, 4 Mar 2009 23:50:32 +0000"  >&lt;p&gt;Patch to integrate PIG with Hadoop 20. This patch switches off the deprecation warnings and fixes a NPE.&lt;/p&gt;</comment>
                            <comment id="12679056" author="sms" created="Thu, 5 Mar 2009 03:29:45 +0000"  >&lt;p&gt;New patch with TestHBaseStorage.java excluded from unit testing.&lt;/p&gt;</comment>
                            <comment id="12679734" author="sms" created="Fri, 6 Mar 2009 21:45:01 +0000"  >&lt;p&gt;New patch that ensures that pig specific properties are picked up and reasonable error messages are returned when backend errors (that cannot be parsed) occur.&lt;/p&gt;</comment>
                            <comment id="12679759" author="sms" created="Fri, 6 Mar 2009 23:03:46 +0000"  >&lt;p&gt;Updated patch in synchrony with the latest sources.&lt;/p&gt;</comment>
                            <comment id="12680718" author="sms" created="Wed, 11 Mar 2009 01:04:19 +0000"  >&lt;p&gt;Latest patch in synchrony with PIG trunk. Also has a fix for the number of reducers for order by when parallel is not used in the script.&lt;/p&gt;</comment>
                            <comment id="12726677" author="olgan" created="Thu, 2 Jul 2009 21:58:26 +0100"  >&lt;p&gt;Updated patch:&lt;/p&gt;

&lt;p&gt;(1) Applies without warnings to the current trunk.&lt;br/&gt;
(2) Resolves TestCounter failures. Thanks, Arun, for help with this.&lt;/p&gt;</comment>
                            <comment id="12733414" author="dvryaboy" created="Tue, 21 Jul 2009 01:19:01 +0100"  >&lt;p&gt;Updating the patch to set PIG_HADOOP_VERSION to 20 by default.&lt;/p&gt;</comment>
                            <comment id="12736264" author="rangadi" created="Tue, 28 Jul 2009 21:20:49 +0100"  >&lt;p&gt;Currently, hadoop jar for 0.18 under lib/ is called hadoop18.jar. Should we change build.xml to use hadoop20.jar instead of hadoop18.jar?&lt;/p&gt;

&lt;p&gt;I can file a jira to commit hadoop20.jar. This might be replaced by updated jar when this jira is committed.&lt;/p&gt;</comment>
                            <comment id="12736283" author="sms" created="Tue, 28 Jul 2009 21:50:08 +0100"  >&lt;p&gt;The build.xml in the patch(es) have the reference to hadoop20.jar. The missing part is the hadoop20.jar that Pig can use to build its sources. Pig cannot use the hadoop20.jar coming from the Hadoop release.&lt;/p&gt;</comment>
                            <comment id="12736286" author="olgan" created="Tue, 28 Jul 2009 21:55:41 +0100"  >&lt;p&gt;Raghu, please, add the hadoop20.jar that Zebra is using. We can commit it with the understanding that we will overwrite once we commit hadoop 20 support into PIg&lt;/p&gt;</comment>
                            <comment id="12736297" author="rangadi" created="Tue, 28 Jul 2009 22:11:23 +0100"  >&lt;p&gt;Thanks Olga and Santosh.&lt;/p&gt;

&lt;p&gt;build.xml change is already in the patch. Thanks.&lt;/p&gt;

&lt;p&gt;I will attach hadoop20.jar that works with PIG. This is useful for anyone to tryout the patch. This will also be used by zebra (&lt;a href=&quot;https://issues.apache.org/jira/browse/PIG-833&quot; title=&quot;Storage access layer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PIG-833&quot;&gt;&lt;del&gt;PIG-833&lt;/del&gt;&lt;/a&gt;). Please commit the jar file to PIG trunk. It could be updated with a later version of hadoop-0.20 branch.&lt;/p&gt;</comment>
                            <comment id="12736311" author="rangadi" created="Tue, 28 Jul 2009 22:28:03 +0100"  >&lt;p&gt;Updated patch fixes two minor conflicts with the current pig trunk.&lt;/p&gt;</comment>
                            <comment id="12736313" author="dvryaboy" created="Tue, 28 Jul 2009 22:28:53 +0100"  >&lt;p&gt;Santosh and Olga &amp;#8211; could you document the differences between a version of 20 Pig can use and that in the Hadoop release? Links to necessary patches, etc?&lt;/p&gt;</comment>
                            <comment id="12736319" author="olgan" created="Tue, 28 Jul 2009 22:34:45 +0100"  >&lt;p&gt;removed the latest attachment - I think there is a bit of confusion. We don&apos;t need a new patch, just a separate hadoop jar that works with the official hadoop 20 release.&lt;/p&gt;</comment>
                            <comment id="12738628" author="pkamath" created="Mon, 3 Aug 2009 22:50:15 +0100"  >&lt;p&gt;Attached a patch for &quot;branch-0.3&quot; based on &lt;a href=&quot;https://issues.apache.org/jira/browse/PIG-660&quot; title=&quot;Integration with Hadoop 0.20&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PIG-660&quot;&gt;&lt;del&gt;PIG-660&lt;/del&gt;&lt;/a&gt;_5.patch. The only difference is that a couple of files (HConfiguration.java and HDataStorage.java) need ctrl-M end of lines for the patch to apply correctly to branch-0.3&lt;/p&gt;</comment>
                            <comment id="12739317" author="dvryaboy" created="Wed, 5 Aug 2009 03:42:39 +0100"  >&lt;p&gt;Attached patch, pig_660_shims.patch, introduces an compatibility layer similar to that in &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-487&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HIVE-487&lt;/a&gt; . HadoopShims.java contains wrappers that hide interface differences between Hadoop 18 and 20; when an interface change affects Pig, a shim is added into this class, and used by Pig.&lt;/p&gt;

&lt;p&gt;Separate versions of the shims are maintained for different Hadoop versions.&lt;/p&gt;

&lt;p&gt;This way, Pig users can compile against either Hadoop 18 or Hadoop 20 by simply changing an ant property, either via the -D flag, or build.properties, instead of having to go through the process of patching.&lt;/p&gt;

&lt;p&gt;There has been discussion of officially moving Pig to 0.20; this way, we sidestep the whole question, and only need to worry about version compatibility when using specific Hadoop APIs.&lt;/p&gt;

&lt;p&gt;I propose that we use this mechanism until Pig is moved to use the new, future-proofed API.  &lt;/p&gt;

&lt;p&gt;Pig compiled against 18 won&apos;t be able to use some of the newest features, such as Zebra storage. Ant can be configured not to build ant if Hadoop version is &amp;lt; 20.&lt;/p&gt;</comment>
                            <comment id="12739348" author="daijy" created="Wed, 5 Aug 2009 06:52:35 +0100"  >&lt;p&gt;Hi, Dmitriy, &lt;br/&gt;
I like your idea. One comment, in src/20/java/org/apache/pig/shims/HadoopShims.java, the package line is &quot;org.apache.hadoop.hive.shims&quot;, I guess it is a typo right?&lt;/p&gt;</comment>
                            <comment id="12739569" author="dvryaboy" created="Wed, 5 Aug 2009 16:31:48 +0100"  >&lt;p&gt;Sure is.. uploading a patch with the fixed package name. &lt;/p&gt;</comment>
                            <comment id="12740241" author="dvryaboy" created="Thu, 6 Aug 2009 21:39:55 +0100"  >&lt;p&gt;The shim patch posted above doesn&apos;t work as cleanly as desired; the current build.xml has junit.hadoop.conf points to a directory in $&lt;/p&gt;
{user.home}&lt;br/&gt;
&lt;br/&gt;
This has an undesired effect &amp;#8211; a hadoop config file gets created the first time you run ant, which among other things sets what class implements the FileSytem interface. When ant gets re-run with a different hadoop version, &apos;ant clean&apos; does not clean out this file &amp;#8211; so an incorrect fs class name gets used.  Deleting the directory created by junit.hadoop.conf before rerunning fixes the problem; so does putting the value of junit.hadoop.conf relative to ${build.dir} instead of ${user.home}
&lt;p&gt;.  &lt;/p&gt;

&lt;p&gt;As I am not sure how the Y! developers use their pigconf directories this thing references, I do not know the appropriate way to proceed. Comments?&lt;/p&gt;</comment>
                            <comment id="12740300" author="dvryaboy" created="Thu, 6 Aug 2009 23:58:43 +0100"  >&lt;p&gt;The attached patch fixes the mentioned issue with junit.hadoop.conf by setting it to $build.dir/conf&lt;br/&gt;
This can be overridden by build.properties if individual contributors want to revert to the old behavior.&lt;/p&gt;

&lt;p&gt;Also added a compatibility shim for hadoop19 (from &lt;a href=&quot;https://issues.apache.org/jira/browse/PIG-573&quot; title=&quot;Changes to make Pig run with Hadoop 19&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PIG-573&quot;&gt;&lt;del&gt;PIG-573&lt;/del&gt;&lt;/a&gt;)&lt;/p&gt;</comment>
                            <comment id="12740333" author="jashmenn" created="Fri, 7 Aug 2009 01:50:15 +0100"  >&lt;p&gt;By applying this patch to r801032 and changing the hadoop.version = 20, I&apos;m still getting problems when trying to use pig against hadoop 20.&lt;/p&gt;

&lt;p&gt;Error and trace:&lt;/p&gt;

&lt;p&gt;2009-08-07 00:45:48,549 &lt;span class=&quot;error&quot;&gt;&amp;#91;main&amp;#93;&lt;/span&gt; INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: hdfs://x.x.x.x:54310&lt;br/&gt;
2009-08-07 00:45:48,834 &lt;span class=&quot;error&quot;&gt;&amp;#91;main&amp;#93;&lt;/span&gt; ERROR org.apache.pig.Main - ERROR 2999: Unexpected internal error. Failed to create DataStorage&lt;br/&gt;
2009-08-07 00:45:48,834 &lt;span class=&quot;error&quot;&gt;&amp;#91;main&amp;#93;&lt;/span&gt; ERROR org.apache.pig.Main - java.lang.RuntimeException: Failed to create DataStorage&lt;br/&gt;
        at org.apache.pig.backend.hadoop.datastorage.HDataStorage.init(HDataStorage.java:75)&lt;br/&gt;
        at org.apache.pig.backend.hadoop.datastorage.HDataStorage.&amp;lt;init&amp;gt;(HDataStorage.java:58)&lt;br/&gt;
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.init(HExecutionEngine.java:198)&lt;br/&gt;
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.init(HExecutionEngine.java:137)&lt;br/&gt;
        at org.apache.pig.impl.PigContext.connect(PigContext.java:180)&lt;br/&gt;
        at org.apache.pig.PigServer.&amp;lt;init&amp;gt;(PigServer.java:169)&lt;br/&gt;
        at org.apache.pig.PigServer.&amp;lt;init&amp;gt;(PigServer.java:158)&lt;br/&gt;
        at org.apache.pig.tools.grunt.Grunt.&amp;lt;init&amp;gt;(Grunt.java:54)&lt;br/&gt;
        at org.apache.pig.Main.main(Main.java:347)&lt;br/&gt;
Caused by: java.io.IOException: Call failed on local exception&lt;br/&gt;
        at org.apache.hadoop.ipc.Client.call(Client.java:718)&lt;br/&gt;
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:216)&lt;br/&gt;
        at org.apache.hadoop.dfs.$Proxy0.getProtocolVersion(Unknown Source)&lt;br/&gt;
        at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:319)&lt;br/&gt;
        at org.apache.hadoop.dfs.DFSClient.createRPCNamenode(DFSClient.java:103)&lt;br/&gt;
        at org.apache.hadoop.dfs.DFSClient.&amp;lt;init&amp;gt;(DFSClient.java:173)&lt;br/&gt;
        at org.apache.hadoop.dfs.DistributedFileSystem.initialize(DistributedFileSystem.java:67)&lt;br/&gt;
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1339)&lt;br/&gt;
        at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:56)&lt;br/&gt;
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1351)&lt;br/&gt;
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:213)&lt;br/&gt;
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:118)&lt;br/&gt;
        at org.apache.pig.backend.hadoop.datastorage.HDataStorage.init(HDataStorage.java:72)&lt;br/&gt;
        ... 8 more&lt;br/&gt;
Caused by: java.io.EOFException&lt;br/&gt;
        at java.io.DataInputStream.readInt(DataInputStream.java:375)&lt;br/&gt;
        at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:499)&lt;br/&gt;
        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:441&lt;/p&gt;</comment>
                            <comment id="12740339" author="dvryaboy" created="Fri, 7 Aug 2009 02:01:12 +0100"  >&lt;p&gt;Nate,&lt;br/&gt;
Your stacktrace shows hadoop.dfs calls (as opposed to hdfs) which tells me it&apos;s looking for &amp;#8211; and finding &amp;#8211; hadoop 18 classes.&lt;/p&gt;

&lt;p&gt;Can you do this:&lt;/p&gt;

&lt;p&gt;export PIG_HADOOP_VERSION=20&lt;br/&gt;
ant clean; ant -Dhadoop.version=20&lt;/p&gt;

&lt;p&gt;any try again?&lt;/p&gt;

&lt;p&gt;Just to be sure, try moving hadoop1* out of the lib directory (so that it for sure fails if it&apos;s trying to look for 18).&lt;/p&gt;</comment>
                            <comment id="12740665" author="jashmenn" created="Fri, 7 Aug 2009 19:20:52 +0100"  >&lt;p&gt;Dmitriy, thanks for the feedback. I did an ant clean and ant -Dhadoop.version=20 as you suggested. That alone did not work, however, when I deleted hadoop17.jar and hadoop18.jar &lt;b&gt;then&lt;/b&gt; it worked perfectly.&lt;/p&gt;

&lt;p&gt;Thanks again,&lt;/p&gt;

&lt;p&gt;Nate&lt;/p&gt;</comment>
                            <comment id="12746135" author="olgan" created="Fri, 21 Aug 2009 19:40:30 +0100"  >&lt;p&gt;Compressed to fit JIRA size limit. Please, uncompress before placing in lib directory&lt;/p&gt;</comment>
                            <comment id="12746136" author="olgan" created="Fri, 21 Aug 2009 19:43:40 +0100"  >&lt;p&gt;Submitted &lt;/p&gt;

&lt;p&gt;(1) Patch to apply to the latest trunk to work with the official Hadoop 20 release&lt;br/&gt;
(2) Compressed version of hadoop20.jar. Need to be uncompressed and placed in the lib dir.&lt;/p&gt;

&lt;p&gt;This should allow running Pig against Hadoop 20 cluster&lt;/p&gt;</comment>
                            <comment id="12752654" author="daijy" created="Tue, 8 Sep 2009 19:12:11 +0100"  >&lt;p&gt;Resync with trunk.&lt;/p&gt;</comment>
                            <comment id="12760452" author="olgan" created="Tue, 29 Sep 2009 01:31:22 +0100"  >&lt;p&gt;patch was committed a while back&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310010">
                    <name>Incorporates</name>
                                                                <inwardlinks description="is part of">
                                        <issuelink>
            <issuekey id="12433124">PIG-924</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12415414" name="PIG-660-for-branch-0.3.patch" size="8208" author="pkamath" created="Mon, 3 Aug 2009 22:50:15 +0100"/>
                            <attachment id="12401470" name="PIG-660.patch" size="4982" author="sms" created="Wed, 4 Mar 2009 23:50:32 +0000"/>
                            <attachment id="12401481" name="PIG-660_1.patch" size="5471" author="sms" created="Thu, 5 Mar 2009 03:29:45 +0000"/>
                            <attachment id="12401653" name="PIG-660_2.patch" size="14443" author="sms" created="Fri, 6 Mar 2009 23:03:46 +0000"/>
                            <attachment id="12401885" name="PIG-660_3.patch" size="5648" author="sms" created="Wed, 11 Mar 2009 01:04:19 +0000"/>
                            <attachment id="12412421" name="PIG-660_4.patch" size="7417" author="olgan" created="Thu, 2 Jul 2009 21:58:26 +0100"/>
                            <attachment id="12414043" name="PIG-660_5.patch" size="8194" author="dvryaboy" created="Tue, 21 Jul 2009 01:19:01 +0100"/>
                            <attachment id="12417297" name="PIG-660_trunk.patch" size="8208" author="olgan" created="Fri, 21 Aug 2009 19:42:19 +0100"/>
                            <attachment id="12418947" name="PIG-660_trunk_2.patch" size="8137" author="daijy" created="Tue, 8 Sep 2009 19:12:11 +0100"/>
                            <attachment id="12417296" name="hadoop20.jar.gz" size="10432632" author="olgan" created="Fri, 21 Aug 2009 19:40:30 +0100"/>
                            <attachment id="12415567" name="pig_660_shims.patch" size="18909" author="dvryaboy" created="Wed, 5 Aug 2009 03:42:39 +0100"/>
                            <attachment id="12415618" name="pig_660_shims_2.patch" size="18901" author="dvryaboy" created="Wed, 5 Aug 2009 16:31:48 +0100"/>
                            <attachment id="12415790" name="pig_660_shims_3.patch" size="26304" author="dvryaboy" created="Thu, 6 Aug 2009 23:58:43 +0100"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>13.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 2 Jul 2009 20:58:26 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>164243</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hyahef:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>95130</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                </customfields>
    </item>
</channel>
</rss>