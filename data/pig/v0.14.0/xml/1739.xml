<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 04:58:17 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/PIG-1739/PIG-1739.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[PIG-1739] Pig 0.8 zero return code when pig script fails; also error is dumped on screen instead of logfile</title>
                <link>https://issues.apache.org/jira/browse/PIG-1739</link>
                <project id="12310730" key="PIG">Pig</project>
                    <description>&lt;p&gt;I have pig script where one input directory cannot be accessed. The pig script obviously fails but the return code is zero.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
A = LOAD &apos;/user/viraj/testdata1&apos; USING PigStorage(&apos;:&apos;) AS (ia, na);
B = FOREACH A GENERATE $0 AS id;
C = LOAD &apos;/user/tstusr/test/&apos; USING PigStorage(&apos;:&apos;) AS (ib, nb);
D = FOREACH C GENERATE $0 AS id;
--dump B;
E = JOIN A by ia, C by ib USING &apos;replicated&apos;;
store E into &apos;id.out&apos;;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here is the console output:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$ java -cp $PIG_HOME/pig.jar org.apache.pig.Main script.pig&lt;br/&gt;
2010-11-19 06:51:32,780 &lt;span class=&quot;error&quot;&gt;&amp;#91;main&amp;#93;&lt;/span&gt; INFO  org.apache.pig.Main - Logging error messages to: /home/viraj/pigscripts/pig_1290149492775.log&lt;br/&gt;
...&lt;br/&gt;
2010-11-19 06:51:39,136 &lt;span class=&quot;error&quot;&gt;&amp;#91;main&amp;#93;&lt;/span&gt; INFO  org.apache.pig.tools.pigstats.ScriptState - Pig features used in the script: REPLICATED_JOIN&lt;br/&gt;
2010-11-19 06:51:39,187 &lt;span class=&quot;error&quot;&gt;&amp;#91;main&amp;#93;&lt;/span&gt; INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - (Name: E: Store(hdfs://mynamenode/user/viraj/id.out:org.apache.pig.builtin.PigStorage) - 1-38 Operator Key: 1-38)&lt;br/&gt;
2010-11-19 06:51:39,198 &lt;span class=&quot;error&quot;&gt;&amp;#91;main&amp;#93;&lt;/span&gt; INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler - File concatenation threshold: 100 optimistic? false&lt;br/&gt;
2010-11-19 06:51:39,344 &lt;span class=&quot;error&quot;&gt;&amp;#91;main&amp;#93;&lt;/span&gt; WARN  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler - failed to get number of input files&lt;br/&gt;
org.apache.hadoop.security.AccessControlException: org.apache.hadoop.security.AccessControlException: Permission denied: user=viraj, access=EXECUTE, inode=&quot;tstusr&quot;:tstusr:users:rwx------&lt;br/&gt;
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)&lt;br/&gt;
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)&lt;br/&gt;
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)&lt;br/&gt;
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)&lt;br/&gt;
        at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:95)&lt;br/&gt;
        at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:57)&lt;br/&gt;
        at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:678)&lt;br/&gt;
        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:521)&lt;br/&gt;
        at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:692)&lt;br/&gt;
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.hasTooManyInputFiles(MRCompiler.java:1302)&lt;br/&gt;
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.visitFRJoin(MRCompiler.java:1210)&lt;br/&gt;
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFRJoin.visit(POFRJoin.java:188)&lt;br/&gt;
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.compile(MRCompiler.java:472)&lt;br/&gt;
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.compile(MRCompiler.java:451)&lt;br/&gt;
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.compile(MRCompiler.java:333)&lt;br/&gt;
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.compile(MapReduceLauncher.java:469)&lt;br/&gt;
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:117)&lt;br/&gt;
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.execute(HExecutionEngine.java:378)&lt;br/&gt;
        at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1198)&lt;br/&gt;
        at org.apache.pig.PigServer.execute(PigServer.java:1190)&lt;br/&gt;
        at org.apache.pig.PigServer.access$100(PigServer.java:128)&lt;br/&gt;
        at org.apache.pig.PigServer$Graph.execute(PigServer.java:1517)&lt;br/&gt;
        at org.apache.pig.PigServer.executeBatchEx(PigServer.java:362)&lt;br/&gt;
        at org.apache.pig.PigServer.executeBatch(PigServer.java:329)&lt;br/&gt;
        at org.apache.pig.tools.grunt.GruntParser.executeBatch(GruntParser.java:112)&lt;br/&gt;
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:169)&lt;br/&gt;
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:141)&lt;br/&gt;
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:90)&lt;br/&gt;
        at org.apache.pig.Main.run(Main.java:498)&lt;br/&gt;
        at org.apache.pig.Main.main(Main.java:107)&lt;br/&gt;
2010-11-19 06:51:56,712 &lt;span class=&quot;error&quot;&gt;&amp;#91;main&amp;#93;&lt;/span&gt; ERROR org.apache.pig.tools.pigstats.PigStats - ERROR 2997: Unable to recreate exception from backend error: org.apache.pig.backend.executionengine.ExecException: ERROR 2118: org.apache.hadoop.security.AccessControlException: Permission denied: user=viraj, access=EXECUTE, inode=&quot;tstusr&quot;:tstusr:users:rwx------&lt;br/&gt;
2010-11-19 06:51:56,712 &lt;span class=&quot;error&quot;&gt;&amp;#91;main&amp;#93;&lt;/span&gt; ERROR org.apache.pig.tools.pigstats.PigStatsUtil - 1 map reduce job(s) failed!&lt;br/&gt;
2010-11-19 06:51:56,714 &lt;span class=&quot;error&quot;&gt;&amp;#91;main&amp;#93;&lt;/span&gt; INFO  org.apache.pig.tools.pigstats.PigStats - Script Statistics:&lt;br/&gt;
...&lt;br/&gt;
HadoopVersion   PigVersion      UserId  StartedAt       FinishedAt      Features&lt;br/&gt;
0.20.1   0.8.0..1011012300       viraj   2010-11-19 06:51:41     2010-11-19 06:51:56     REPLICATED_JOIN&lt;/p&gt;

&lt;p&gt;Failed!&lt;/p&gt;

&lt;p&gt;Failed Jobs:&lt;br/&gt;
JobId   Alias   Feature Message Outputs&lt;br/&gt;
N/A     C       MAP_ONLY        Message: org.apache.pig.backend.executionengine.ExecException: ERROR 2118: org.apache.hadoop.security.AccessControlException: Permission denied: user=viraj, access=EXECUTE, inode=&quot;tstusr&quot;:tstusr:users:rwx------&lt;br/&gt;
Input(s):&lt;br/&gt;
Failed to read data from &quot;/user/tstusr/test/&quot;&lt;/p&gt;

&lt;p&gt;Output(s):&lt;/p&gt;

&lt;p&gt;Counters:&lt;br/&gt;
Total records written : 0&lt;br/&gt;
Total bytes written : 0&lt;br/&gt;
Spillable Memory Manager spill count : 0&lt;br/&gt;
Total bags proactively spilled: 0&lt;br/&gt;
Total records proactively spilled: 0&lt;/p&gt;

&lt;p&gt;Job DAG:&lt;br/&gt;
null    -&amp;gt;      null,&lt;br/&gt;
null&lt;/p&gt;

&lt;p&gt;2010-11-19 06:51:56,714 &lt;span class=&quot;error&quot;&gt;&amp;#91;main&amp;#93;&lt;/span&gt; INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Failed!&lt;/p&gt;

&lt;p&gt;$echo $?&lt;br/&gt;
0&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Clearly users depending on this return code to run their workflows are affected.&lt;/p&gt;

&lt;p&gt;Viraj&lt;/p&gt;</description>
                <environment></environment>
        <key id="12480403">PIG-1739</key>
            <summary>Pig 0.8 zero return code when pig script fails; also error is dumped on screen instead of logfile</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="yanz">Yan Zhou</assignee>
                                    <reporter username="viraj">Viraj Bhat</reporter>
                        <labels>
                    </labels>
                <created>Fri, 19 Nov 2010 07:12:07 +0000</created>
                <updated>Fri, 17 Dec 2010 22:47:01 +0000</updated>
                            <resolved>Mon, 22 Nov 2010 17:30:27 +0000</resolved>
                                    <version>0.8.0</version>
                                    <fixVersion>0.8.0</fixVersion>
                                    <component>impl</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="12934076" author="yanz" created="Sat, 20 Nov 2010 01:49:27 +0000"  >&lt;p&gt;The problem is that when a job status is checked, its output status is checked. But the temporary file output status is never checked, which makes sense in case of successful jobs since nobody is interested in temporary file stats. But if a job producing temporary output fails, the output status need to be added otherwise the job status check won&apos;t find the failed job.&lt;/p&gt;</comment>
                            <comment id="12934079" author="yanz" created="Sat, 20 Nov 2010 02:14:02 +0000"  >&lt;p&gt;With the fix, the log files contains the following, which I believe is good enough for informational and debugging purposes as well. The &quot;failed to get number of input files&quot; message is just a warning,  by itself does not cause the failure,  and only sent to the console.&lt;/p&gt;

&lt;p&gt;Backend error message during job submission&lt;br/&gt;
-------------------------------------------&lt;br/&gt;
org.apache.pig.backend.executionengine.ExecException: ERROR 2118: org.apache.hadoop.security.AccessControlException: Permission denied: user=yanz, access=EXECUTE, inode=&quot;test1&quot;:yanz:hdfs:---------&lt;br/&gt;
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.getSplits(PigInputFormat.java:280)&lt;br/&gt;
	at org.apache.hadoop.mapred.JobClient.writeNewSplits(JobClient.java:907)&lt;br/&gt;
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:801)&lt;br/&gt;
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:752)&lt;br/&gt;
	at org.apache.hadoop.mapred.jobcontrol.Job.submit(Job.java:378)&lt;br/&gt;
	at org.apache.hadoop.mapred.jobcontrol.JobControl.startReadyJobs(JobControl.java:247)&lt;br/&gt;
	at org.apache.hadoop.mapred.jobcontrol.JobControl.run(JobControl.java:279)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:619)&lt;br/&gt;
Caused by: org.apache.hadoop.security.AccessControlException: org.apache.hadoop.security.AccessControlException: Permission denied: user=yanz, access=EXECUTE, inode=&quot;test1&quot;:yanz:hdfs:---------&lt;br/&gt;
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)&lt;br/&gt;
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)&lt;br/&gt;
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)&lt;br/&gt;
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)&lt;br/&gt;
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:96)&lt;br/&gt;
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:58)&lt;br/&gt;
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:617)&lt;br/&gt;
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:453)&lt;br/&gt;
	at org.apache.hadoop.fs.FileSystem.getFileStatus(FileSystem.java:1330)&lt;br/&gt;
	at org.apache.hadoop.fs.FileSystem.globStatusInternal(FileSystem.java:924)&lt;br/&gt;
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:866)&lt;br/&gt;
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:209)&lt;br/&gt;
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigTextInputFormat.listStatus(PigTextInputFormat.java:36)&lt;br/&gt;
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:246)&lt;br/&gt;
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.getSplits(PigInputFormat.java:268)&lt;br/&gt;
	... 7 more&lt;br/&gt;
Caused by: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.security.AccessControlException: Permission denied: user=yanz, access=EXECUTE, inode=&quot;test1&quot;:yanz:hdfs:---------&lt;br/&gt;
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:159)&lt;br/&gt;
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:115)&lt;br/&gt;
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:85)&lt;br/&gt;
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:4575)&lt;br/&gt;
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkTraverse(FSNamesystem.java:4554)&lt;br/&gt;
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:1751)&lt;br/&gt;
	at org.apache.hadoop.hdfs.server.namenode.NameNode.getFileInfo(NameNode.java:572)&lt;br/&gt;
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:597)&lt;br/&gt;
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)&lt;br/&gt;
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:969)&lt;br/&gt;
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:965)&lt;br/&gt;
	at java.security.AccessController.doPrivileged(Native Method)&lt;br/&gt;
	at javax.security.auth.Subject.doAs(Subject.java:396)&lt;br/&gt;
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:963)&lt;/p&gt;

&lt;p&gt;	at org.apache.hadoop.ipc.Client.call(Client.java:740)&lt;br/&gt;
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)&lt;br/&gt;
	at $Proxy0.getFileInfo(Unknown Source)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:597)&lt;br/&gt;
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)&lt;br/&gt;
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)&lt;br/&gt;
	at $Proxy0.getFileInfo(Unknown Source)&lt;br/&gt;
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:615)&lt;br/&gt;
	... 15 more&lt;/p&gt;

&lt;p&gt;Pig Stack Trace&lt;br/&gt;
---------------&lt;br/&gt;
ERROR 2244: Job failed, hadoop does not return any error message&lt;/p&gt;

&lt;p&gt;org.apache.pig.backend.executionengine.ExecException: ERROR 2244: Job failed, hadoop does not return any error message&lt;br/&gt;
	at org.apache.pig.tools.grunt.GruntParser.executeBatch(GruntParser.java:117)&lt;br/&gt;
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:169)&lt;br/&gt;
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:141)&lt;br/&gt;
	at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:90)&lt;br/&gt;
	at org.apache.pig.Main.run(Main.java:509)&lt;br/&gt;
	at org.apache.pig.Main.main(Main.java:107)&lt;br/&gt;
================================================================================&lt;/p&gt;</comment>
                            <comment id="12934220" author="yanz" created="Sun, 21 Nov 2010 02:33:49 +0000"  >&lt;p&gt;test-core passes ok. test-patch has the following output:&lt;/p&gt;

&lt;p&gt;     &lt;span class=&quot;error&quot;&gt;&amp;#91;exec&amp;#93;&lt;/span&gt; -1 overall.&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;exec&amp;#93;&lt;/span&gt;&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;exec&amp;#93;&lt;/span&gt;     +1 @author.  The patch does not contain any @author tags.&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;exec&amp;#93;&lt;/span&gt;&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;exec&amp;#93;&lt;/span&gt;     -1 tests included.  The patch doesn&apos;t appear to include any new or modified tests.&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;exec&amp;#93;&lt;/span&gt;                         Please justify why no tests are needed for this patch.&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;exec&amp;#93;&lt;/span&gt;&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;exec&amp;#93;&lt;/span&gt;     +1 javadoc.  The javadoc tool did not generate any warning messages.&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;exec&amp;#93;&lt;/span&gt;&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;exec&amp;#93;&lt;/span&gt;     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;exec&amp;#93;&lt;/span&gt;&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;exec&amp;#93;&lt;/span&gt;     +1 findbugs.  The patch does not introduce any new Findbugs warnings.&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;exec&amp;#93;&lt;/span&gt;&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;exec&amp;#93;&lt;/span&gt;     +1 release audit.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;


&lt;p&gt;The test case is hard to automate, I manually verified the fix.&lt;/p&gt;</comment>
                            <comment id="12934515" author="rding" created="Mon, 22 Nov 2010 17:14:54 +0000"  >&lt;p&gt;+1&lt;/p&gt;</comment>
                            <comment id="12934524" author="yanz" created="Mon, 22 Nov 2010 17:30:27 +0000"  >&lt;p&gt;Committed to both the trunk and the 0.8 branch.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12460081" name="PIG-1739.patch" size="772" author="yanz" created="Sat, 20 Nov 2010 01:49:27 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Sat, 20 Nov 2010 01:49:27 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>165168</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hyatgf:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>97083</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                </customfields>
    </item>
</channel>
</rss>