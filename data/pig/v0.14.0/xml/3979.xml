<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 05:08:20 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/PIG-3979/PIG-3979.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[PIG-3979] group all performance, garbage collection, and incremental aggregation</title>
                <link>https://issues.apache.org/jira/browse/PIG-3979</link>
                <project id="12310730" key="PIG">Pig</project>
                    <description>&lt;p&gt;I have a PIG statement similar to:&lt;br/&gt;
summary = foreach (group data ALL) generate &lt;br/&gt;
COUNT(data.col1), SUM(data.col2), SUM(data.col2)&lt;br/&gt;
, Moments(col3)&lt;br/&gt;
, Moments(data.col4)&lt;/p&gt;

&lt;p&gt;There are a couple of hundred columns.&lt;/p&gt;

&lt;p&gt;I set the following:&lt;br/&gt;
SET pig.exec.mapPartAgg true;&lt;br/&gt;
SET pig.exec.mapPartAgg.minReduction 3;&lt;br/&gt;
SET pig.cachedbag.memusage 0.05;&lt;/p&gt;

&lt;p&gt;I found that when I ran this on a JVM with insufficient memory, the process eventually timed out because of an infinite garbage collection loop.&lt;/p&gt;

&lt;p&gt;The problem was invariant to the memusage setting.&lt;/p&gt;

&lt;p&gt;I solved the problem by making changes to:&lt;br/&gt;
org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperator.POPartialAgg.java&lt;/p&gt;

&lt;p&gt;Rather than reading in 10000 records to establish an estimate of the reduction, I make an estimate after reading in enough tuples to fill pig.cachedbag.memusage percent of Runtime.getRuntime().maxMemory().&lt;/p&gt;

&lt;p&gt;I also made a change to guarantee at least one record allowed in second tier storage. In the current implementation, if the reduction is very high 1000:1, space in second tier storage is zero.&lt;/p&gt;

&lt;p&gt;With these changes, I can summarize large data sets with small JVMs. I also find that setting pig.cachedbag.memusage to a small number such as 0.05 results in much better garbage collection performance without reducing throughput. I suppose tuning GC would also solve a problem with excessive garbage collection.&lt;/p&gt;

&lt;p&gt;The performance is sweet. &lt;/p&gt;</description>
                <environment></environment>
        <key id="12717785">PIG-3979</key>
            <summary>group all performance, garbage collection, and incremental aggregation</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="rohini">Rohini Palaniswamy</assignee>
                                    <reporter username="ddreyfus">David Dreyfus</reporter>
                        <labels>
                    </labels>
                <created>Sun, 1 Jun 2014 20:37:06 +0100</created>
                <updated>Fri, 21 Nov 2014 05:59:02 +0000</updated>
                            <resolved>Mon, 3 Nov 2014 15:17:52 +0000</resolved>
                                    <version>0.12.0</version>
                    <version>0.11.1</version>
                                    <fixVersion>0.14.0</fixVersion>
                                    <component>impl</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                <comments>
                            <comment id="14015070" author="ddreyfus" created="Sun, 1 Jun 2014 20:41:57 +0100"  >&lt;p&gt;Here is my version of the modified file. I&apos;m sure there are better ways to express the changes. I hope you can incorporate them into a future release. You might also want to consider a separate variable to control the size of the second tier storage and not rely on a reduction ratio from the first tier.&lt;/p&gt;</comment>
                            <comment id="14015091" author="chitnis" created="Sun, 1 Jun 2014 22:20:42 +0100"  >&lt;p&gt;Thanks for this JIRA David. Can you attach just the diff patch so its easier to review the changes? you can use command &lt;tt&gt;git diff --no-prefix &amp;gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/PIG-3979&quot; title=&quot;group all performance, garbage collection, and incremental aggregation&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PIG-3979&quot;&gt;&lt;del&gt;PIG-3979&lt;/del&gt;&lt;/a&gt;-v1.patch&lt;/tt&gt; in your local repository&lt;/p&gt;</comment>
                            <comment id="14015345" author="ddreyfus" created="Mon, 2 Jun 2014 13:28:36 +0100"  >&lt;p&gt;Attached patch file as requested. Again, if you approve the approach taken in this patch, I&apos;d suggest a little refactoring.&lt;/p&gt;</comment>
                            <comment id="14015818" author="aniket486" created="Mon, 2 Jun 2014 21:04:07 +0100"  >&lt;p&gt;Does this need to go in with 0.13? Can we move this to 0.13.1 instead?&lt;/p&gt;</comment>
                            <comment id="14015830" author="ddreyfus" created="Mon, 2 Jun 2014 21:14:59 +0100"  >&lt;p&gt;The changes can go into 0.13.0 or 0.13.1. Before they do, you should do a little refactoring, however.&lt;/p&gt;</comment>
                            <comment id="14015833" author="ddreyfus" created="Mon, 2 Jun 2014 21:15:58 +0100"  >&lt;p&gt;I do not know what is implied by making me the assignee. Perhaps you can elaborate.&lt;/p&gt;</comment>
                            <comment id="14015839" author="aniket486" created="Mon, 2 Jun 2014 21:20:48 +0100"  >&lt;p&gt;I just added you to the list of contributors. Assigning a jira to you means you are submitting and contributing a fix for it. I haven&apos;t reviewed the patch yet, but seems like a good idea. Thanks for the contribution!&lt;/p&gt;</comment>
                            <comment id="14028742" author="mrflip" created="Thu, 12 Jun 2014 03:10:53 +0100"  >&lt;p&gt;This is great. Using partial aggregation with TOP was causing constant spills and terrible performance; this patch reduced the runtime by a factor of 10. I&apos;d prefer that most of the log messages be at DEBUG priority, but in that case it was very helpful.&lt;/p&gt;

&lt;p&gt;However, I also have some questions about whether there&apos;s more going on than the patch currently addresses &amp;#8211; there are still pathological cases that cause the SpillableMemoryManager to enter seppuku mode.&lt;/p&gt;

&lt;p&gt;With a large heap size set, the initial FIRST_TIER_THRESHOLD of 20,000 is hit way before the sample size is, and so it never actually adjusts that threshold. Is that behavior desirable, or should the code still include the test for (numRecsInRawMap &amp;gt;= NUM_RECS_TO_SAMPLE) as follows?&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
            &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (!sizeReductionChecked   &amp;amp;&amp;amp; ((sampleSize &amp;gt;= sampleMem) || (numRecsInRawMap &amp;gt;= NUM_RECS_TO_SAMPLE))) {
                checkSizeReduction();
                sampleSize = 0;
            }
            &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (!estimatedMemThresholds &amp;amp;&amp;amp; ((sampleSize &amp;gt;= sampleMem) || (numRecsInRawMap &amp;gt;= NUM_RECS_TO_SAMPLE))) {
                estimateMemThresholds();
            }
 &lt;span class=&quot;code-comment&quot;&gt;// ...&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As long as estimateMemThresholds() hasn&apos;t been called, avgTupleSize has its initial value of zero &amp;#8211; meaning that getMemorySize() returns zero as well. If I&apos;m reading this correctly, with large heap sizes and this patch getMemorySize() will always return zero.&lt;/p&gt;

&lt;p&gt;Lastly, I&apos;m concerned there&apos;s still an interaction between POPartialAgg and SpillableMemoryManager this doesn&apos;t address. I&apos;m not deeply familiar with what&apos;s going on, so read my questions as &quot;I don&apos;t understand why X is&quot; rather than &quot;I don&apos;t think X should be&quot;.&lt;/p&gt;

&lt;p&gt;With a large JVM heap size, when the POPartialAgg does actually get to a certain size the SpillableMemoryManager goes into a gc hell of its own creation. Here&apos;s a taste:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2014-06-11 21:07:23,248 4974  INFO  o.a.p.b.h.e.m.PigGenericMapReduce$Map     | Aliases being processed per job phase (AliasName[line,offset]): M: events[36,9],events[-1,-1],events_final_event_1[59,23],1-3[59,32] C: events_final_event_1[59,23],1-3[59,32] R: events_final_event_1[59,23]
2014-06-11 21:07:24,432 6158  INFO  o.a.p.b.h.e.p.r.POPartialAgg              | After reduction, processed map: 126; raw map: 0
2014-06-11 21:07:24,433 6159  INFO  o.a.p.b.h.e.p.r.POPartialAgg              | Observed reduction factor: from 10000 to 126 =&amp;gt; 79.
2014-06-11 21:07:24,714 6440  INFO  o.a.p.b.h.e.p.r.POPartialAgg              | Getting mem limits; considering 1 POPArtialAgg objects.
2014-06-11 21:07:24,721 6447  INFO  o.a.p.b.h.e.p.r.POPartialAgg              | Estimated total tuples to buffer, based on 10000 tuples that took up 14954552 bytes: 191715
2014-06-11 21:07:24,721 6447  INFO  o.a.p.b.h.e.p.r.POPartialAgg              | Setting thresholds. Primary: 189288. Secondary: 2427
2014-06-11 21:07:24,827 6553  INFO  o.a.p.i.u.SpillableMemoryManager          | first memory handler call- Usage threshold init = 5505024(5376K) used = 795475864(776831K) committed = 795607040(776960K) max = 1048576000(1024000K)
2014-06-11 21:07:24,832 6558  INFO  o.a.p.b.h.e.p.r.POPartialAgg              | Spill triggered by SpillableMemoryManager
2014-06-11 21:07:24,832 6558  INFO  o.a.p.b.h.e.p.r.POPartialAgg              | In startSpill(), aggregating raw inputs. 13545 tuples.
2014-06-11 21:07:24,832 6558  INFO  o.a.p.i.u.SpillableMemoryManager          | Spilled an estimate of 20423195 bytes from 1 objects. init = 5505024(5376K) used = 795475864(776831K) committed = 795607040(776960K) max = 1048576000(1024000K)
2014-06-11 21:07:24,852 6578  INFO  o.a.p.b.h.e.p.r.POPartialAgg              | processed inputs: 302 tuples.
2014-06-11 21:07:24,852 6578  INFO  o.a.p.b.h.e.p.r.POPartialAgg              | In startSpill(), aggregating processed inputs. 302 tuples.
2014-06-11 21:07:24,855 6581  INFO  o.a.p.b.h.e.p.r.POPartialAgg              | processed inputs: 301 tuples.
2014-06-11 21:07:24,928 6654  INFO  o.a.p.b.h.e.p.r.POPartialAgg              | In spillResults(), processed map is empty -- done spilling.
2014-06-11 21:07:30,052 11778 INFO  o.a.p.b.h.e.p.r.POPartialAgg              | Aggregating 189289 raw records at first level.
2014-06-11 21:07:32,416 14142 INFO  o.a.p.i.u.SpillableMemoryManager          | first memory handler call - Collection threshold init = 5505024(5376K) used = 645973328(630833K) committed = 795607040(776960K) max = 1048576000(1024000K)
2014-06-11 21:07:32,418 14144 INFO  o.a.p.b.h.e.p.r.POPartialAgg              | Spill triggered by SpillableMemoryManager
2014-06-11 21:07:32,418 14144 INFO  o.a.p.b.h.e.p.r.POPartialAgg              | In startSpill(), aggregating raw inputs. 84343 tuples.
2014-06-11 21:07:32,418 14144 INFO  o.a.p.i.u.SpillableMemoryManager          | hard invoking GC: accumulatedFreeSize 150120425 gcActivationSize 40000000 estimatedFreed 129697230 toFree: 383829328.
2014-06-11 21:07:33,049 14775 INFO  o.a.p.i.u.SpillableMemoryManager          | Spilled an estimate of 129697230 bytes from 1 objects. init = 5505024(5376K) used = 645973328(630833K) committed = 795607040(776960K) max = 1048576000(1024000K)
2014-06-11 21:07:33,051 14777 INFO  o.a.p.b.h.e.p.r.POPartialAgg              | Spill triggered by SpillableMemoryManager
...
... (Repeats several hundred times, causing a GC on each)
... (Three minutes later, it becomes able to carry on...)
...
2014-06-11 21:08:58,423 100149 INFO  o.a.p.b.h.e.p.r.POPartialAgg              | Spill triggered by SpillableMemoryManager
2014-06-11 21:08:58,423 100149 INFO  o.a.p.i.u.SpillableMemoryManager          | hard invoking GC: accumulatedFreeSize 129706200 gcActivationSize 40000000 estimatedFreed 129706200 toFree: 262180472.
2014-06-11 21:08:58,697 100423 INFO  o.a.p.i.u.SpillableMemoryManager          | Spilled an estimate of 129706200 bytes from 1 objects. init = 5505024(5376K) used = 524324472(512035K) committed = 1048576000(1024000K) max = 1048576000(1024000K)
2014-06-11 21:08:58,761 100487 INFO  o.a.p.b.h.e.p.r.POPartialAgg              | processed inputs: 3481 tuples.
2014-06-11 21:08:58,762 100488 INFO  o.a.p.b.h.e.p.r.POPartialAgg              | In startSpill(), aggregating processed inputs. 3481 tuples.
2014-06-11 21:08:58,789 100515 INFO  o.a.p.b.h.e.p.r.POPartialAgg              | processed inputs: 3480 tuples.
2014-06-11 21:08:58,916 100642 INFO  o.a.p.b.h.e.p.r.POPartialAgg              | In spillResults(), processed map is empty -- done spilling.
2014-06-11 21:09:02,949 104675 INFO  o.a.p.b.h.e.p.r.POPartialAgg              | Aggregating 189289 raw records at first level.
2014-06-11 21:09:05,812 107538 INFO  o.a.p.b.h.e.p.r.POPartialAgg              | Spill triggered by SpillableMemoryManager
2014-06-11 21:09:05,813 107539 INFO  o.a.p.b.h.e.p.r.POPartialAgg              | In startSpill(), aggregating raw inputs. 120387 tuples.
2014-06-11 21:09:05,813 107539 INFO  o.a.p.i.u.SpillableMemoryManager          | hard invoking GC: accumulatedFreeSize 183538160 gcActivationSize 40000000 estimatedFreed 183538160 toFree: 404073608.
2014-06-11 21:09:06,717 108443 INFO  o.a.p.i.u.SpillableMemoryManager          | Spilled an estimate of 183538160 bytes from 1 objects. init = 5505024(5376K) used = 771075208(753003K) committed = 1048576000(1024000K) max = 1048576000(1024000K)
2014-06-11 21:09:06,717 108443 INFO  o.a.p.b.h.e.p.r.POPartialAgg              | Spill triggered by SpillableMemoryManager
2014-06-11 21:09:06,717 108443 INFO  o.a.p.i.u.SpillableMemoryManager          | hard invoking GC: accumulatedFreeSize 183571050 gcActivationSize 40000000 estimatedFreed 183571050 toFree: 353267104.
2014-06-11 21:09:07,422 109148 INFO  o.a.p.i.u.SpillableMemoryManager          | Spilled an estimate of 183571050 bytes from 1 objects. init = 5505024(5376K) used = 615411104(600987K) committed = 1048576000(1024000K) max = 1048576000(1024000K)
...
... (and so forth)
...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There are two thresholds &amp;#8211; the user adjustable pig.spill.gc.activation.size and the hard-coded extraGCThresholdFraction &amp;#8211; that cause SpillableMemoryManager to force a hard GC.&lt;br/&gt;
Somehow or another in a way that SpillableMemoryManager doesn&apos;t expect POPartialAgg either exceeds those thresholds or enters the sort order of spillables or something.&lt;/p&gt;

&lt;p&gt;When I disable the GC thresholds (by setting pig.spill.gc.activation.size and by manually changing extraGCThresholdFraction in the code each to a large value),&lt;br/&gt;
the job finishes in under a minute:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2014-06-11 20:53:47,951 5265  INFO  o.a.p.b.h.e.m.PigGenericMapReduce$Map     | Aliases being processed per job phase (AliasName[line,offset]): M: events[36,9],events[-1,-1],events_final_event_1[59,23],1-3[59,32] C: events_final_event_1[59,23],1-3[59,32] R: events_final_event_1[59,23]
2014-06-11 20:53:49,057 6371  INFO  o.a.p.b.h.e.p.r.POPartialAgg              | After reduction, processed map: 126; raw map: 0
2014-06-11 20:53:49,058 6372  INFO  o.a.p.b.h.e.p.r.POPartialAgg              | Observed reduction factor: from 10000 to 126 =&amp;gt; 79.
2014-06-11 20:53:49,256 6570  INFO  o.a.p.b.h.e.p.r.POPartialAgg              | Getting mem limits; considering 1 POPArtialAgg objects.
2014-06-11 20:53:49,266 6580  INFO  o.a.p.b.h.e.p.r.POPartialAgg              | Estimated total tuples to buffer, based on 10000 tuples that took up 14954552 bytes: 191715
2014-06-11 20:53:49,266 6580  INFO  o.a.p.b.h.e.p.r.POPartialAgg              | Setting thresholds. Primary: 189288. Secondary: 2427
2014-06-11 20:53:53,835 11149 INFO  o.a.p.b.h.e.p.r.POPartialAgg              | Aggregating 189289 raw records at first level.
2014-06-11 20:53:54,023 11337 INFO  o.a.p.i.u.SpillableMemoryManager          | first memory handler call - Collection threshold init = 5505024(5376K) used = 661747360(646237K) committed = 795602944(776956K) max = 1048576000(1024000K)
2014-06-11 20:53:54,152 11466 INFO  o.a.p.b.h.e.p.r.POPartialAgg              | Spill triggered by SpillableMemoryManager
2014-06-11 20:53:54,153 11467 INFO  o.a.p.i.u.SpillableMemoryManager          | Spilled an estimate of 283175425 bytes from 1 objects. init = 5505024(5376K) used = 661747360(646237K) committed = 795602944(776956K) max = 1048576000(1024000K)
2014-06-11 20:53:54,391 11705 INFO  o.a.p.b.h.e.p.r.POPartialAgg              | Aggregating 2543 secondary records to be combined.
2014-06-11 20:53:54,435 11749 INFO  o.a.p.b.h.e.p.r.POPartialAgg              | Aggregating 2542 secondary records to be combined.
2014-06-11 20:53:54,435 11749 INFO  o.a.p.b.h.e.p.r.POPartialAgg              | Starting spill.
2014-06-11 20:53:54,435 11749 INFO  o.a.p.b.h.e.p.r.POPartialAgg              | In startSpill(), aggregating processed inputs. 2542 tuples.
2014-06-11 20:53:54,466 11780 INFO  o.a.p.b.h.e.p.r.POPartialAgg              | processed inputs: 2542 tuples.
2014-06-11 20:53:54,699 12013 INFO  o.a.p.b.h.e.p.r.POPartialAgg              | In spillResults(), processed map is empty -- done spilling.
...
... about a dozen spills in all
...
2014-06-11 20:54:37,085 54399 INFO  o.a.p.b.h.e.p.r.POPartialAgg              | In startSpill(), aggregating raw inputs. 129522 tuples.
2014-06-11 20:54:37,086 54400 INFO  o.a.p.i.u.SpillableMemoryManager          | Spilled an estimate of 193614460 bytes from 1 objects. init = 5505024(5376K) used = 568816808(555485K) committed = 1048576000(1024000K) max = 1048576000(1024000K)
2014-06-11 20:54:37,244 54558 INFO  o.a.p.b.h.e.p.r.POPartialAgg              | processed inputs: 1616 tuples.
2014-06-11 20:54:37,245 54559 INFO  o.a.p.b.h.e.p.r.POPartialAgg              | In startSpill(), aggregating processed inputs. 1616 tuples.
2014-06-11 20:54:37,251 54565 INFO  o.a.p.b.h.e.p.r.POPartialAgg              | processed inputs: 1616 tuples.
2014-06-11 20:54:37,260 54574 INFO  o.a.p.b.h.e.p.r.POPartialAgg              | In spillResults(), processed map is empty -- done spilling.
2014-06-11 20:54:41,159 58473 INFO  o.a.p.b.h.e.p.r.POPartialAgg              | In startSpill(), aggregating raw inputs. 180461 tuples.
2014-06-11 20:54:41,486 58800 INFO  o.a.p.b.h.e.p.r.POPartialAgg              | processed inputs: 2270 tuples.
2014-06-11 20:54:41,486 58800 INFO  o.a.p.b.h.e.p.r.POPartialAgg              | In startSpill(), aggregating processed inputs. 2270 tuples.
2014-06-11 20:54:41,502 58816 INFO  o.a.p.b.h.e.p.r.POPartialAgg              | processed inputs: 2270 tuples.
2014-06-11 20:54:41,502 58816 INFO  o.a.p.b.h.e.p.r.POPartialAgg              | Spilling last bits.
2014-06-11 20:54:41,523 58837 INFO  o.a.p.b.h.e.p.r.POPartialAgg              | In spillResults(), processed map is empty -- done spilling.
2014-06-11 20:54:41,528 58842 INFO  o.a.h.m.MapTask                           | Starting flush of map output
2014-06-11 20:54:41,528 58842 INFO  o.a.h.m.MapTask                           | Spilling map output
2014-06-11 20:54:41,528 58842 INFO  o.a.h.m.MapTask                           | bufstart = 0; bufend = 4240460; bufvoid = 471859200
2014-06-11 20:54:41,528 58842 INFO  o.a.h.m.MapTask                           | kvstart = 117964796(471859184); kvend = 117861448(471445792); length = 103349/29491200
2014-06-11 20:54:41,644 58958 INFO  o.a.h.i.c.CodecPool                       | Got brand-&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; compressor [.deflate]
2014-06-11 20:54:41,689 59003 INFO  o.a.p.b.h.e.m.PigCombiner$Combine         | Aliases being processed per job phase (AliasName[line,offset]): M: events[36,9],events[-1,-1],events_final_event_1[59,23],1-3[59,32] C: events_final_event_1[59,23],1-3[59,32] R: events_final_event_1[59,23]
2014-06-11 20:54:43,913 61227 INFO  o.a.h.m.MapTask                           | Finished spill 0
2014-06-11 20:54:43,917 61231 INFO  o.a.h.m.Task                              | Task:attempt_1402538004132_0001_m_000000_0 is done. And is in the process of committing
2014-06-11 20:54:43,922 61236 INFO  o.a.h.m.Task                              | Task &apos;attempt_1402538004132_0001_m_000000_0&apos; done.
2014-06-11 20:54:44,023 61337 INFO  o.a.h.m.i.MetricsSystemImpl               | Stopping MapTask metrics system...
2014-06-11 20:54:44,023 61337 INFO  o.a.h.m.i.MetricsSystemImpl               | MapTask metrics system stopped.
2014-06-11 20:54:44,023 61337 INFO  o.a.h.m.i.MetricsSystemImpl               | MapTask metrics system shutdown complete.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
	&lt;li&gt;POPartialAgg&apos;s spill() method is lazy &amp;#8211; it only sets the doSpill flag for a later iteration. Since the SpillableMemoryManager action is synchronized, I think there&apos;s no way for the spill to actually take place. If I understand this right, the invokeGC can&apos;t ever help with a POPartialAgg spillable, as no memory could possibly have been freed.&lt;/li&gt;
	&lt;li&gt;Why is SpillableMemoryManager looping the way it is in the pathological case?&lt;/li&gt;
	&lt;li&gt;Manually forcing a GC seems brutish. How confident are folks that this helps compared to proper JVM gc tuning?&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14028812" author="mrflip" created="Thu, 12 Jun 2014 05:14:07 +0100"  >&lt;p&gt;As a workaround, changing getMemorySize to under-report by returning avgTupleSize * numRecsInProcessedMap prevents the hard GCs, and lets me avoid overriding the global forced-GC behavior. This probably isn&apos;t correct but it is effective.&lt;/p&gt;</comment>
                            <comment id="14029973" author="ddreyfus" created="Fri, 13 Jun 2014 00:06:16 +0100"  >&lt;p&gt;Hi Philip,&lt;/p&gt;

&lt;p&gt;Like yourself, I entered the murky world of spillable aggregates because I had jobs that took forever or died (seppuku). I noticed the problem was triggered by the 20K record read. I am no expert on this code, so really can&apos;t offer a lot of suggestions on how to fix excessive spilling and a battle between GC and spilling. I&apos;m glad the patch helped you.&lt;/p&gt;

&lt;p&gt;I have no objection to your change to use the smaller of 20K records or a certain amount of memory, though I don&apos;t know what it would solve. Once the memory constraint is reached, the records get reduced. If this means 200K records are read, so what.&lt;/p&gt;

&lt;p&gt;POPartialAgg.getMemorySize() will return 0 whenever all the records to read fit into the memory allotted. This will be the case with or without the patch. The only thing the patch does is make the number of records read proportional to the the size of the record and the amount of memory allocated to the task.&lt;/p&gt;

&lt;p&gt;I also played with the GC issues but didn&apos;t find a good solution that I understood well enough. Spilling seems to be triggered by the GC running. If spilling is triggered by GC and spilling tries to allocate memory which triggers more GC, we have a death spiral. If we could disable GC while spilling, we might avoid death by GC. There really aught to be a better method than keeping memory utilization to a minimum. Solving the GC problem would be a coup.&lt;/p&gt;
</comment>
                            <comment id="14032395" author="ddreyfus" created="Mon, 16 Jun 2014 13:44:08 +0100"  >&lt;p&gt;Updates to attachments:&lt;br/&gt;
SpillableMemoryManager - Made sorting stable to avoid java.lang.IllegalArgumentException: Comparison method violates its general contract! (see JIRA-4012). Eliminated calls to System.gc() from within the gc notification handler. This seems to eliminate the gc death spiral. Since gc() is supposed to be just a hint, and because calling it would lead to a new notification call, the hope is that removing this won&apos;t cause problems for use cases when the spillable that is being called isn&apos;t POPartialAgg.&lt;/p&gt;

&lt;p&gt;POPartialAgg. -&lt;br/&gt;
Made sure we have a default memory size.&lt;br/&gt;
Made it so the SpillableMemoryManager triggers an aggregation pass that may yet spill, but doesn&apos;t always force a spill.&lt;/p&gt;

&lt;p&gt;In both, set some of the log messages to debug level from info level.&lt;/p&gt;</comment>
                            <comment id="14156611" author="ddreyfus" created="Thu, 2 Oct 2014 15:27:40 +0100"  >&lt;p&gt;Hi Daniel,&lt;br/&gt;
Do I need to do anything at this time with the patch?&lt;/p&gt;

</comment>
                            <comment id="14157688" author="daijy" created="Fri, 3 Oct 2014 06:32:48 +0100"  >&lt;p&gt;I can take a look. If I understand correctly, there are two different issues. SpillableMemoryManager.java.patch is addressing conparison violation issue in &lt;a href=&quot;https://issues.apache.org/jira/browse/PIG-4012&quot; title=&quot;java.lang.IllegalArgumentException: Comparison method violates its general contract! SpillableMemoryManager&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PIG-4012&quot;&gt;&lt;del&gt;PIG-4012&lt;/del&gt;&lt;/a&gt;, and  &lt;a href=&quot;https://issues.apache.org/jira/browse/PIG-3979&quot; title=&quot;group all performance, garbage collection, and incremental aggregation&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PIG-3979&quot;&gt;&lt;del&gt;PIG-3979&lt;/del&gt;&lt;/a&gt;-v1.patch is addressing the partial aggregation issue, right?&lt;/p&gt;</comment>
                            <comment id="14161277" author="ddreyfus" created="Tue, 7 Oct 2014 01:32:40 +0100"  >&lt;p&gt;The SpillableMemoryManager.java.patch addresses both &lt;a href=&quot;https://issues.apache.org/jira/browse/PIG-4012&quot; title=&quot;java.lang.IllegalArgumentException: Comparison method violates its general contract! SpillableMemoryManager&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PIG-4012&quot;&gt;&lt;del&gt;PIG-4012&lt;/del&gt;&lt;/a&gt; as well as&lt;br/&gt;
other GC related issues, such as Pig auguring itself into the ground trying&lt;br/&gt;
to free memory, as eloquently described by Philip Kromer. &lt;a href=&quot;https://issues.apache.org/jira/browse/PIG-3979&quot; title=&quot;group all performance, garbage collection, and incremental aggregation&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PIG-3979&quot;&gt;&lt;del&gt;PIG-3979&lt;/del&gt;&lt;/a&gt; also&lt;br/&gt;
addresses partial aggregation issues. There are also dependencies between&lt;br/&gt;
my patched files in &lt;a href=&quot;https://issues.apache.org/jira/browse/PIG-3979&quot; title=&quot;group all performance, garbage collection, and incremental aggregation&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PIG-3979&quot;&gt;&lt;del&gt;PIG-3979&lt;/del&gt;&lt;/a&gt;. I&apos;d say they come as a matched set.&lt;/p&gt;

</comment>
                            <comment id="14165899" author="daijy" created="Thu, 9 Oct 2014 23:34:58 +0100"  >&lt;p&gt;Here is a summary of changes I read:&lt;br/&gt;
1. Create a strong reference list spillablesSR for sorting: +1, this is to fix comparison contract exception. The list itself could be big and exacerbate the memory stress, but I cannot think of a better solution&lt;br/&gt;
2. Avoid invoking System.gc: I am not familiar with the gc detail, but the explanation sounds fine to me&lt;br/&gt;
3. POPartialAgg sample size should use memory percentage instead of numTuple : +1&lt;br/&gt;
4. LOG.info =&amp;gt; LOG.debug: +1&lt;/p&gt;

&lt;p&gt;I also make some minor comments in RB: &lt;a href=&quot;https://reviews.apache.org/r/26527/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.apache.org/r/26527/&lt;/a&gt;. Many of the issues I have already addressed in &lt;a href=&quot;https://issues.apache.org/jira/browse/PIG-3979&quot; title=&quot;group all performance, garbage collection, and incremental aggregation&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PIG-3979&quot;&gt;&lt;del&gt;PIG-3979&lt;/del&gt;&lt;/a&gt;-3.patch. I leave the change history in RB for your reference.&lt;/p&gt;

&lt;p&gt;One more thing, this does not seems to completely solve &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mrflip&quot; class=&quot;user-hover&quot; rel=&quot;mrflip&quot;&gt;Philip (flip) Kromer&lt;/a&gt;&apos;s infinite loop issue. To solve that, we shall do an actually spill inside POPartialAgg.spill instead of putting a doContingentSpill flag. Otherwise, during the actual spill in getNextTuple, we could enter into handleNotification constantly. If we do actual spill inside POPartialAgg.spill, since this is inside handleNotification, the spill process will not be interrupted by another handleNotification. Isn&apos;t it?&lt;/p&gt;</comment>
                            <comment id="14168449" author="ddreyfus" created="Sun, 12 Oct 2014 02:17:04 +0100"  >&lt;p&gt;Regarding Philip (flip) Kromer&apos;s death spiral, do you have a test case that shows that the problem isn&apos;t resolved? It was certainly my hope that I did solve it. My own testing suggested it did (although I didn&apos;t have a test case that made it real easy to confirm). Before I made the changes, tight memory parameters would cause a death spiral with ease. Afterwords, not so much.&lt;/p&gt;

&lt;p&gt;My sense is that trying to do useful work within the GC notification thread/handler can cause problems. By using the handler to set a flag and letting the main thread to do the useful work, we avoid that problem. I haven&apos;t researched how GC notification works. I&apos;m not convinced that doing processing within the handler guarantees no further GC calls. Moreover, it&apos;s not clear that the POPartialAgg class is designed for access by multiple threads. &lt;/p&gt;

&lt;p&gt;In short, i&apos;d keep it as simple as possible with all spill work handled by one spiller that operates on the main thread.&lt;/p&gt;
</comment>
                            <comment id="14168538" author="daijy" created="Sun, 12 Oct 2014 06:59:30 +0100"  >&lt;p&gt;I don&apos;t have a test case, in theory, this could happen when we have memory stress, the actual spill code uses more memory so could trigger constant GC. But I agree to keep the code simple for now. We need more research to figure out if we can stop additional GC within the spill, and there are additional complexity to make POPartialAgg multi-thread safe. &lt;/p&gt;

&lt;p&gt;There are only 2 minor comments left in RB, can you take a look?&lt;/p&gt;</comment>
                            <comment id="14168695" author="ddreyfus" created="Sun, 12 Oct 2014 17:04:30 +0100"  >&lt;p&gt;Comment at line 279:&lt;br/&gt;
The second tier should at least allow one tuple before it tries to aggregate.&lt;br/&gt;
This code retains the total number of tuples in the buffer while guaranteeing &lt;br/&gt;
the second tier has at least one tuple.&lt;/p&gt;

&lt;p&gt;The difference between doSpill and doContingentSpill:&lt;br/&gt;
The doSpill flag is set when spilling is running or needs to run.&lt;br/&gt;
It is set by POPartialAgg when its buffers are full after having run aggregations.&lt;br/&gt;
The doContingentSpill flag is set when the SpillableMemoryManager is notified&lt;br/&gt;
by GC that the runtime is low on memory and the SpillableMemoryManager identifies&lt;br/&gt;
the particular buffer as a good spill candidate because it is large. The contingent spill logic tries &lt;br/&gt;
to satisfy the memory manager&apos;s request for freeing memory by aggregating data&lt;br/&gt;
rather than just spilling records to disk. &lt;/p&gt;</comment>
                            <comment id="14168855" author="daijy" created="Mon, 13 Oct 2014 01:20:59 +0100"  >&lt;p&gt;Attach &lt;a href=&quot;https://issues.apache.org/jira/browse/PIG-3979&quot; title=&quot;group all performance, garbage collection, and incremental aggregation&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PIG-3979&quot;&gt;&lt;del&gt;PIG-3979&lt;/del&gt;&lt;/a&gt;-4.patch incorporates David&apos;s comments.&lt;/p&gt;

&lt;p&gt;Patch committed to trunk and 0.14 branch. Thanks David, Philip!&lt;/p&gt;</comment>
                            <comment id="14173038" author="rohini" created="Wed, 15 Oct 2014 23:01:40 +0100"  >&lt;p&gt;This patch makes almost all spill related log messages from info-&amp;gt;debug and it will make debugging really hard. We need to revert them back and only make something debug if it is really causing spam.&lt;/p&gt;</comment>
                            <comment id="14173053" author="rohini" created="Wed, 15 Oct 2014 23:11:00 +0100"  >&lt;p&gt;Also I see issues and performance impact removing the System.gc() introduced for issue in &lt;a href=&quot;https://issues.apache.org/jira/browse/PIG-3148&quot; title=&quot;OutOfMemory exception while spilling stale DefaultDataBag. Extra option to gc() before spilling large bag.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PIG-3148&quot;&gt;&lt;del&gt;PIG-3148&lt;/del&gt;&lt;/a&gt;. I think it already had threshold checks to only invoke System.gc() when necessary to avoid the infinte loop spending cpu cycles in gc as mentioned in &lt;a href=&quot;https://issues.apache.org/jira/browse/PIG-3148?focusedCommentId=13577743&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13577743&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/PIG-3148?focusedCommentId=13577743&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13577743&lt;/a&gt; . Can you give the scenario why the extra System.gc() caused the problem? May be it just needs some tweaking of that threshold. &lt;/p&gt;</comment>
                            <comment id="14173099" author="rohini" created="Wed, 15 Oct 2014 23:51:12 +0100"  >&lt;p&gt;Another comment. The tuplesize is calculated twice now.&lt;br/&gt;
sampleSize += tupleSize&lt;br/&gt;
estTotalMem += mem;&lt;/p&gt;

&lt;p&gt;  As it is sampleSize += tupleSize is a overhead when the number of records is less. Previously if the number of records per map is less than 10K memory size would never be estimated. But I think it is ok to pay that cost to avoid issues like this as sampling on memory usage is better than record count. But should avoid doing it twice as estTotalMem will be same as sampleSize. &lt;/p&gt;</comment>
                            <comment id="14173884" author="ddreyfus" created="Thu, 16 Oct 2014 16:50:03 +0100"  >&lt;p&gt;Hi Rohini,&lt;br/&gt;
1) Debug messages can always be enabled. Separating Info and Debug messages allows better performance and smaller logs.&lt;br/&gt;
2) System.gc() isn&apos;t guaranteed to force GC; it is a suggestion. The issue we ran into is the memory manager auguring itself into the ground with repeated looping. The thought is to let the JVM orchestrate its GC operations and to just use the notification as a chance to shrink memory used by spillable objects. I didn&apos;t determine if the auguring was due to a specific GC call or to the general issue of looping on GC calls. For &lt;a href=&quot;https://issues.apache.org/jira/browse/PIG-3148&quot; title=&quot;OutOfMemory exception while spilling stale DefaultDataBag. Extra option to gc() before spilling large bag.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PIG-3148&quot;&gt;&lt;del&gt;PIG-3148&lt;/del&gt;&lt;/a&gt;, I would look at either threshold settings, releasing locks, or a method of not spilling something not worth spilling. Looping on GC doesn&apos;t seem to be a good solution to a memory management issue. &lt;br/&gt;
3) I agree that duplication in code is probably not a great idea.&lt;/p&gt;</comment>
                            <comment id="14174230" author="rohini" created="Thu, 16 Oct 2014 21:59:10 +0100"  >&lt;blockquote&gt;&lt;p&gt;Debug messages can always be enabled. Separating Info and Debug messages allows better performance and smaller logs.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;    These log messages do not actually cause much spam and are very helpful to analyze a running job or already completed job. If there are too many of spill messages, then there is a bigger problem and the job needs to be tuned. When there are production issues, it is not possible to go ask users to turn on debug logging to determine why a job is slow. All spill information of even hadoop is info logs because spilling is something you need to know about and should not be debug. Also turning on debug logging logs way too much. A system should have enough information to pinpoint often encountered scenarios without having to turn on debug logging.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;System.gc() isn&apos;t guaranteed to force GC; it is a suggestion. &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;  Theoretically yes.  &lt;a href=&quot;https://issues.apache.org/jira/browse/PIG-3148&quot; title=&quot;OutOfMemory exception while spilling stale DefaultDataBag. Extra option to gc() before spilling large bag.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PIG-3148&quot;&gt;&lt;del&gt;PIG-3148&lt;/del&gt;&lt;/a&gt; just added extra GC to avoid big stale bags from being spilled and it did fix that issue. The SpillableManager relied on invoking System.gc() from the beginning and it has been working so far based on that. Just removing that is going to break a lot of existing jobs. &lt;/p&gt;

&lt;p&gt;Problem I see here is actually that &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
estimatedFreed += toBeFreed;
accumulatedFreeSize += toBeFreed;
&lt;span class=&quot;code-comment&quot;&gt;// This should significantly reduce the number of small files
&lt;/span&gt;                &lt;span class=&quot;code-comment&quot;&gt;// in &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; that we have a lot of nested bags
&lt;/span&gt;                &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (accumulatedFreeSize &amp;gt; gcActivationSize) {
                    invokeGC = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;;
                }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;may not exactly apply to POPartialAgg as it behaves differently from bag spills. Bags actually spill the data and free up the memory. While POPartialAgg only sets up the spill flag and does not actually spill till the next record is processed. So when System.gc() is invoked because POPartialAgg was &amp;gt; 40MB it actually is useless because the maps have not been processed and aggregated and there is no space freed yet. If that is fixed, I think it might work with the regular SpillableMemoryManager code. Changing SpillableMemoryManager to make POPartialAgg work should not cause regression issues with normal bags as that is more critical. &lt;/p&gt;
</comment>
                            <comment id="14175345" author="ddreyfus" created="Fri, 17 Oct 2014 19:30:18 +0100"  >&lt;p&gt;For counting spilled bytes, I rely on the counters. Does this not work in your use case?&lt;br/&gt;
If it doesn&apos;t and you want to have &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
            &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt;(estimatedFreed &amp;gt; 0){
                &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; msg = &lt;span class=&quot;code-quote&quot;&gt;&quot;Spilled an estimate of &quot;&lt;/span&gt; + estimatedFreed +
                &lt;span class=&quot;code-quote&quot;&gt;&quot; bytes from &quot;&lt;/span&gt; + numObjSpilled + &lt;span class=&quot;code-quote&quot;&gt;&quot; objects. &quot;&lt;/span&gt; + info.getUsage();;
                log.info(msg);
            }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;I have no problem with that.&lt;/p&gt;

&lt;p&gt;I understand how the extra GC solved &lt;a href=&quot;https://issues.apache.org/jira/browse/PIG-3148&quot; title=&quot;OutOfMemory exception while spilling stale DefaultDataBag. Extra option to gc() before spilling large bag.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PIG-3148&quot;&gt;&lt;del&gt;PIG-3148&lt;/del&gt;&lt;/a&gt;. I also think it generates the problem that causes PIG to augur itself into the ground.&lt;br/&gt;
I think the challenge is to come up with a better solution to &lt;a href=&quot;https://issues.apache.org/jira/browse/PIG-3148&quot; title=&quot;OutOfMemory exception while spilling stale DefaultDataBag. Extra option to gc() before spilling large bag.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PIG-3148&quot;&gt;&lt;del&gt;PIG-3148&lt;/del&gt;&lt;/a&gt; that avoids spilling stale bags and avoids relying on multiple calls to GC to clean stuff up.&lt;/p&gt;</comment>
                            <comment id="14175552" author="daijy" created="Fri, 17 Oct 2014 22:18:57 +0100"  >&lt;p&gt;This also cause TestPOPartialAgg fail. I need to rollback the patch until we reach consensus and fix the unit test failure.&lt;/p&gt;</comment>
                            <comment id="14176061" author="ddreyfus" created="Sat, 18 Oct 2014 18:38:06 +0100"  >&lt;p&gt;Daniel,&lt;br/&gt;
Will you be waiting on me to resolve the TestPOPartialAgg fail?&lt;/p&gt;</comment>
                            <comment id="14177063" author="rohini" created="Mon, 20 Oct 2014 17:17:08 +0100"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ddreyfus&quot; class=&quot;user-hover&quot; rel=&quot;ddreyfus&quot;&gt;David Dreyfus&lt;/a&gt;,&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;For counting spilled bytes, I rely on the counters.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;   Problem is for killed or failed tasks the counters are discarded and the log messages are very useful then.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I understand how the extra GC solved &lt;a href=&quot;https://issues.apache.org/jira/browse/PIG-3148&quot; title=&quot;OutOfMemory exception while spilling stale DefaultDataBag. Extra option to gc() before spilling large bag.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PIG-3148&quot;&gt;&lt;del&gt;PIG-3148&lt;/del&gt;&lt;/a&gt;. &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;   There were two System.gc() in the SpillableMemoryManager. There was always a System.gc() invoked after all the spill() calls which was there from the beginning. Then there is an extraGC that was introduced to do a System.gc() before the spill() call if object size exceeds a certain threshold. You have removed both the System.gc() calls which I believe will lead to lot of regressions. It would be nice to add another API to confirm before invoking extra GC but that will cause backward incompatibility unless we move to Java 8. For now we can hack by doing instanceof POPartialAgg and not invoke extraGC. I am posting a patch that makes the spill in POPartialAgg synchronous. Can you test if it fixes your problem? &lt;/p&gt;</comment>
                            <comment id="14177180" author="rohini" created="Mon, 20 Oct 2014 18:49:20 +0100"  >&lt;p&gt; Attached a initial version of the patch which does not do extraGC for POPartialAgg and makes the spill synchronous to have the second invokeGC actually free memory. Can you check if that works for you? It still needs fixing of TestPOPartialAgg, modification of existing tests or addition of new tests for the behaviour change.&lt;/p&gt;</comment>
                            <comment id="14183593" author="rohini" created="Fri, 24 Oct 2014 22:52:32 +0100"  >&lt;p&gt;Attached patch hopefully fixes all those issues. I also reverted back to using num records for samples as it was hitting default threshold of 20K records and aggregating and spilling couple of times well in advance before sample size was reached when I ran some modified tests on bigdata.conf. I made the spill() estimateThresholds() so that should solve the problem the sampleSize check was put in. Also it avoids the penalty of collecting the sample size in advance when it might not actually be needed if sizeReduction factor was not met.&lt;/p&gt;

&lt;p&gt;Review board link - &lt;a href=&quot;https://reviews.apache.org/r/27169/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.apache.org/r/27169/&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14193826" author="rohini" created="Sun, 2 Nov 2014 13:25:40 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/PIG-3979&quot; title=&quot;group all performance, garbage collection, and incremental aggregation&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PIG-3979&quot;&gt;&lt;del&gt;PIG-3979&lt;/del&gt;&lt;/a&gt;-6.patch - &lt;a href=&quot;https://reviews.apache.org/r/27169/diff/3/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.apache.org/r/27169/diff/3/&lt;/a&gt; &lt;/p&gt;</comment>
                            <comment id="14194197" author="rohini" created="Mon, 3 Nov 2014 02:49:10 +0000"  >&lt;p&gt;Updated BigData_5 e2e test to have -Dpig.exec.mapPartAgg=true.&lt;/p&gt;</comment>
                            <comment id="14194281" author="daijy" created="Mon, 3 Nov 2014 05:59:57 +0000"  >&lt;p&gt;+1&lt;/p&gt;</comment>
                            <comment id="14194614" author="rohini" created="Mon, 3 Nov 2014 15:17:52 +0000"  >&lt;p&gt;Committed to branch-0.14 and trunk. Thanks Daniel for the review.&lt;/p&gt;

&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ddreyfus&quot; class=&quot;user-hover&quot; rel=&quot;ddreyfus&quot;&gt;David Dreyfus&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mrflip&quot; class=&quot;user-hover&quot; rel=&quot;mrflip&quot;&gt;Philip (flip) Kromer&lt;/a&gt; for the patches and analysis. It would be great if you guys could test out the new patch. &lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12674030" name="PIG-3979-3.patch" size="20503" author="daijy" created="Thu, 9 Oct 2014 23:34:58 +0100"/>
                            <attachment id="12674441" name="PIG-3979-4.patch" size="21382" author="daijy" created="Mon, 13 Oct 2014 01:20:59 +0100"/>
                            <attachment id="12677026" name="PIG-3979-5.patch" size="33251" author="rohini" created="Fri, 24 Oct 2014 22:52:32 +0100"/>
                            <attachment id="12678792" name="PIG-3979-6.patch" size="35235" author="rohini" created="Sun, 2 Nov 2014 13:25:40 +0000"/>
                            <attachment id="12678855" name="PIG-3979-7.patch" size="35716" author="rohini" created="Mon, 3 Nov 2014 02:49:10 +0000"/>
                            <attachment id="12675880" name="PIG-3979-synchronous-spill.patch" size="20185" author="rohini" created="Mon, 20 Oct 2014 18:49:20 +0100"/>
                            <attachment id="12647889" name="PIG-3979-v1.patch" size="3173" author="ddreyfus" created="Mon, 2 Jun 2014 13:28:36 +0100"/>
                            <attachment id="12650557" name="POPartialAgg.java.patch" size="38834" author="ddreyfus" created="Mon, 16 Jun 2014 13:44:08 +0100"/>
                            <attachment id="12650558" name="SpillableMemoryManager.java.patch" size="9380" author="ddreyfus" created="Mon, 16 Jun 2014 13:44:08 +0100"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Sun, 1 Jun 2014 21:20:42 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>395989</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hzq16n:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>396115</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                </customfields>
    </item>
</channel>
</rss>