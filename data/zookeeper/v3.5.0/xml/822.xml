<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 05:43:38 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/ZOOKEEPER-822/ZOOKEEPER-822.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[ZOOKEEPER-822] Leader election taking a long time  to complete</title>
                <link>https://issues.apache.org/jira/browse/ZOOKEEPER-822</link>
                <project id="12310801" key="ZOOKEEPER">ZooKeeper</project>
                    <description>&lt;p&gt;Created a 3 node cluster.&lt;/p&gt;

&lt;p&gt;1 Fail the ZK leader&lt;br/&gt;
2. Let leader election finish. Restart the leader and let it join the &lt;br/&gt;
3. Repeat &lt;/p&gt;

&lt;p&gt;After a few rounds leader election takes anywhere 25- 60 seconds to finish. Note- we didn&apos;t have any ZK clients and no new znodes were created.&lt;/p&gt;

&lt;p&gt;zoo.cfg is shown below:&lt;/p&gt;

&lt;p&gt;#Mon Jul 19 12:15:10 UTC 2010&lt;br/&gt;
server.1=192.168.4.12\:2888\:3888&lt;br/&gt;
server.0=192.168.4.11\:2888\:3888&lt;br/&gt;
clientPort=2181&lt;br/&gt;
dataDir=/var/zookeeper&lt;br/&gt;
syncLimit=2&lt;br/&gt;
server.2=192.168.4.13\:2888\:3888&lt;br/&gt;
initLimit=5&lt;br/&gt;
tickTime=2000&lt;/p&gt;

&lt;p&gt;I have attached logs from two nodes that took a long time to form the cluster after failing the leader. The leader was down anyways so logs from that node shouldn&apos;t matter.&lt;br/&gt;
Look for &quot;START HERE&quot;. Logs after that point should be of our interest.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12469627">ZOOKEEPER-822</key>
            <summary>Leader election taking a long time  to complete</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.png">Blocker</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="vishalmlst">Vishal Kher</assignee>
                                    <reporter username="vishalmlst">Vishal Kher</reporter>
                        <labels>
                    </labels>
                <created>Mon, 19 Jul 2010 16:51:01 +0100</created>
                <updated>Wed, 23 Nov 2011 19:22:27 +0000</updated>
                            <resolved>Wed, 6 Oct 2010 18:03:01 +0100</resolved>
                                    <version>3.3.0</version>
                                    <fixVersion>3.3.2</fixVersion>
                    <fixVersion>3.4.0</fixVersion>
                                    <component>quorum</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>2</watches>
                                                                <comments>
                            <comment id="12889896" author="vishalmlst" created="Mon, 19 Jul 2010 16:54:31 +0100"  >&lt;p&gt;Attaching logs from two nodes that took too long to complete leader election. &lt;/p&gt;</comment>
                            <comment id="12889902" author="vishalmlst" created="Mon, 19 Jul 2010 17:06:36 +0100"  >&lt;p&gt;I would like that add that the problem is highly reproducible.&lt;/p&gt;</comment>
                            <comment id="12889906" author="fpj" created="Mon, 19 Jul 2010 17:27:12 +0100"  >&lt;p&gt;Vishal, I can&apos;t reproduce your problem. I just tried twice to kill the leader and rejoin it 20 times each, and I can&apos;t see the problem you&apos;re mentioning.  I wonder if there is anything special about your setup. I also can see in your logs lots of exceptions related to connections, and as a first cut, it sounds like this is preventing the severs from exchanging notifications, and therefore the delay. &lt;/p&gt;

&lt;p&gt;Two minor comments: your log file for server 2 does not contain &quot;START HERE&quot; and each file duplicates every message.&lt;/p&gt;
</comment>
                            <comment id="12889907" author="ikelly" created="Mon, 19 Jul 2010 17:27:24 +0100"  >&lt;p&gt;Could you try putting the logs through loggraph (in zookeeper/src/contrib)? Perhaps a graphical view will give some insight?&lt;/p&gt;</comment>
                            <comment id="12890052" author="vishalmlst" created="Mon, 19 Jul 2010 22:46:56 +0100"  >&lt;p&gt;Hi Flavio,&lt;/p&gt;

&lt;p&gt;I have Zookeeper servers running in a VM. To kill ZK server, I power off a VM. On the other hand, I tried several times killing ZK process and restarting it and I did not see any issues.&lt;br/&gt;
So there is something about the reboot that is causing this problem (TCP session not getting cleaned-up?).&lt;/p&gt;

&lt;p&gt;I don&apos;t see many connection exceptions in the log.&lt;/p&gt;

&lt;p&gt;Once the leader election starts  we start seeing &quot;Notification time out&quot; messages.&lt;/p&gt;

&lt;p&gt;However, before this we do see that the connection was established (show below):&lt;/p&gt;

&lt;p&gt;2010-07-19 14:40:52,562 - DEBUG &lt;span class=&quot;error&quot;&gt;&amp;#91;WorkerSender Thread:QuorumCnxManager@366&amp;#93;&lt;/span&gt; - There is a connection already for server 0&lt;br/&gt;
2010-07-19 14:40:52,563 - DEBUG &lt;span class=&quot;error&quot;&gt;&amp;#91;WorkerSender Thread:QuorumCnxManager@346&amp;#93;&lt;/span&gt; - Opening channel to server 2&lt;/p&gt;

&lt;p&gt;Do you still think this is a communication problem?&lt;/p&gt;</comment>
                            <comment id="12890199" author="fpj" created="Tue, 20 Jul 2010 08:07:27 +0100"  >&lt;p&gt;Hi Vishal, Do you think you can uploaded all three log files for a problematic run? We would like to put it on loggraph to visualize what&apos;s going on there. It sounds like it is somehow related to the VM reboots, I don&apos;t know why yet.&lt;/p&gt;</comment>
                            <comment id="12891386" author="vishalmlst" created="Fri, 23 Jul 2010 00:03:33 +0100"  >&lt;p&gt;Hi Flavio,&lt;/p&gt;

&lt;p&gt;I have attached the logs. Hope this helps. Do you have more info on loggraph (doc, etc)?&lt;/p&gt;

&lt;p&gt;Thanks.&lt;/p&gt;</comment>
                            <comment id="12891582" author="fpj" created="Fri, 23 Jul 2010 14:10:53 +0100"  >&lt;p&gt;Thanks for the logs, Vishal. The jira discussing loggraph is &lt;a href=&quot;https://issues.apache.org/jira/browse/ZOOKEEPER-773&quot; title=&quot;Log visualisation&quot; class=&quot;issue-link&quot; data-issue-key=&quot;ZOOKEEPER-773&quot;&gt;&lt;del&gt;ZOOKEEPER-773&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="12892761" author="ikelly" created="Tue, 27 Jul 2010 14:21:34 +0100"  >&lt;p&gt;Hi Vishal,&lt;/p&gt;

&lt;p&gt;The logs in zk_leader_election.tar.gz seem to be from different runs. node0 starts at 2010-07-22 17:33:54,166, node1 at 2010-07-22 22:21:11,979 and node2 at 2010-07-22 22:22:17,249. Are the clocks on the machine in sync?&lt;/p&gt;

&lt;p&gt;-Ivan&lt;/p&gt;</comment>
                            <comment id="12892830" author="phunt" created="Tue, 27 Jul 2010 17:11:44 +0100"  >&lt;p&gt;Ivan, do the clocks need to be in sync? Perhaps you should use the xid (cxid) instead?&lt;/p&gt;</comment>
                            <comment id="12893121" author="ikelly" created="Wed, 28 Jul 2010 10:54:22 +0100"  >&lt;p&gt;currently the timestamp is used as the zxid isn&apos;t always available in the message logs. But yes, zxid would be more desirable. Perhaps I can extract that from the context some other way. &lt;/p&gt;
</comment>
                            <comment id="12894571" author="vishalmlst" created="Mon, 2 Aug 2010 16:29:52 +0100"  >&lt;p&gt;attaching new logs.&lt;/p&gt;</comment>
                            <comment id="12894576" author="vishalmlst" created="Mon, 2 Aug 2010 16:36:22 +0100"  >
&lt;p&gt;I have attached new logs. I don&apos;t use ntp, but all l the nodes should be at the most a few seconds apart. &lt;/p&gt;

&lt;p&gt;I have marked start and end of the faulty election. look at  zookeeper-192.168.10.3-log and search for &quot;vishal&quot;, &lt;/p&gt;

&lt;p&gt;Note - it is super easy to reproduce the bug. Create a 3 node cluster and reboot the leader (or shutdown the network interface). You may need to repeat the test several times. &lt;/p&gt;

&lt;p&gt;If you do a clean shutdown of the leader (zkServer.sh stop), then you won&apos;t see this bug. I feel that there is something releated to TCP timeout/ session management of failed node that is causing this problem.&lt;/p&gt;</comment>
                            <comment id="12896043" author="ikelly" created="Fri, 6 Aug 2010 15:20:48 +0100"  >&lt;p&gt;I&apos;ve tried to repro this with 3 zookeepers running on the same machine, and 3 zookeepers running on virtual machine and I cannot get it to repro. I was taking out the leader by shutting down the network interface. Have you been able to repro this on another set of machines other than the ones you first observed it on?&lt;/p&gt;

&lt;p&gt;Also, could you try this with the latest trunk? Some improvements were made around the FLE which may shed some more light.&lt;/p&gt;</comment>
                            <comment id="12896061" author="vishalmlst" created="Fri, 6 Aug 2010 16:12:00 +0100"  >&lt;p&gt;Hi Ivan.&lt;/p&gt;

&lt;p&gt;Were the logs of any help? I might be worth having 3 VMs and rebooting the leader instead of shutting down the interface. We have seen this on all of our dev cluster. Al tough all the dev clusters are based on same VM images. So it won&apos;t be fair to claim that the problem was seen on different set of machines. I will try with the latest trunk and let you know the result. What FLE changes do you think would have fixed this problem?&lt;/p&gt;

&lt;p&gt;Thanks.&lt;/p&gt;

&lt;p&gt;-Vishal&lt;/p&gt;

</comment>
                            <comment id="12896063" author="ikelly" created="Fri, 6 Aug 2010 16:28:48 +0100"  >&lt;p&gt;They logs were of some help, but I don&apos;t understand what&apos;s happening. I looks like multiple nodes are claiming leadership at the same time, but that can&apos;t be right.&lt;/p&gt;

&lt;p&gt;The FLE changes won&apos;t fix it, but they do log more information, so they may make it easier to see what is happening.&lt;/p&gt;</comment>
                            <comment id="12896980" author="ikelly" created="Tue, 10 Aug 2010 18:46:14 +0100"  >&lt;p&gt;Could this be related?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/ZOOKEEPER-785&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/ZOOKEEPER-785&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="12896983" author="ikelly" created="Tue, 10 Aug 2010 18:50:09 +0100"  >&lt;p&gt;Actually, ignore that. Read it wrong.&lt;/p&gt;</comment>
                            <comment id="12899156" author="vishalmlst" created="Mon, 16 Aug 2010 23:45:32 +0100"  >&lt;p&gt;Hi Ivan,&lt;/p&gt;

&lt;p&gt;Can you describe me your setup?&lt;/p&gt;

&lt;p&gt;My setup info:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;3 ESX boxes&lt;/li&gt;
	&lt;li&gt;1 SLES 11 VMs on each&lt;/li&gt;
	&lt;li&gt;Cluster of 3 nodes&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I hit this problem consistently after rebooting the leader.&lt;/p&gt;

&lt;p&gt;Thanks.&lt;br/&gt;
-Vishal&lt;/p&gt;</comment>
                            <comment id="12899974" author="vishalmlst" created="Wed, 18 Aug 2010 20:22:30 +0100"  >&lt;p&gt;Hi Ivan,&lt;/p&gt;

&lt;p&gt;Per your suggetion, I have reproduced the same problem on Red Hat Enterprise Linux Server release 5.5.&lt;br/&gt;
(3 RHEL VMs on 3 ESX).&lt;/p&gt;

&lt;p&gt;I used the code from trunk (may be a week old but that shouldn&apos;t matter). &lt;br/&gt;
The leader election with the latest code is taking way more time to finish (2 mins 43 seconds).&lt;/p&gt;

&lt;p&gt;The reproduction procedure was the same :&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;reboot -n the leader.&lt;/li&gt;
	&lt;li&gt;let the rebooted node come up&lt;/li&gt;
	&lt;li&gt;start zookeeper on the rebooted node&lt;/li&gt;
	&lt;li&gt;repeat until we run into the issue&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Look for the last round of election (marked ****** Rebooting LEADER) in 10.17.119.101-zookeeper.log and 10.17.119.102-zookeeper.log.&lt;/p&gt;


&lt;p&gt;Thanks.&lt;br/&gt;
-Vishal&lt;/p&gt;
</comment>
                            <comment id="12899983" author="vishalmlst" created="Wed, 18 Aug 2010 20:42:59 +0100"  >&lt;p&gt;While going through the code yesterday,  I found two potential problems that I though might be worth reporting in the context of this bug.&lt;/p&gt;

&lt;p&gt;1. In FastLeaderElection.java&lt;br/&gt;
    /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Check if all queues are empty, indicating that all messages have been delivered.&lt;br/&gt;
     */&lt;br/&gt;
    boolean haveDelivered() {&lt;br/&gt;
        for (ArrayBlockingQueue&amp;lt;ByteBuffer&amp;gt; queue : queueSendMap.values()) 
{
            LOG.debug(&quot;Queue size: &quot; + queue.size());
            if (queue.size() == 0)
                return true;
        }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;        return false;&lt;br/&gt;
    }&lt;/p&gt;

&lt;p&gt;the haveDelivered()  function returns true without checking if rest of the queus are empty.&lt;/p&gt;

&lt;p&gt;2. QuorumCnxManager.connectAll() function connects to one peer at a time and it uses a blocking connect (SocketChannle.open). I added a timeout to the SocketChannel.open and that did not fix the problem. &lt;/p&gt;</comment>
                            <comment id="12900010" author="fpj" created="Wed, 18 Aug 2010 21:08:33 +0100"  >&lt;p&gt;Hi Vishal, Thanks for reporting. Here are some quick comments:&lt;/p&gt;

&lt;p&gt;Issue 1: I think that just the javadoc message is incorrect. We really just want to check that some process has received notifications.&lt;/p&gt;

&lt;p&gt;Issue2: The connection will eventually timeout if not established, so setting a different value should not make a difference. The point about blocking connect is a good one. I think it is worthwhile creating a jira for it.  &lt;/p&gt;</comment>
                            <comment id="12900108" author="vishalmlst" created="Thu, 19 Aug 2010 00:31:12 +0100"  >&lt;p&gt;I am suspecting that one of the node (10.17.119.101) is not sending the notification to the other node. sendNotifications() is called to send notification to all peers. This functions enteres the notification in sendqueue. However, either the entry was not put in the queue (sendqueue.offer failed) or the thread that polls sendqueue did not wake up. I am not sure what the cause is yet.&lt;/p&gt;

&lt;p&gt;I had added extra debug messages. Three messages are of main interest:&lt;/p&gt;

&lt;p&gt;1. in sendNotifications():  Print &quot;IN FLE sending notification to server id = 1&quot; for each server. Also print  &quot;proposedLeader, proposedZxid, logicalclock&quot;&lt;br/&gt;
2. In FastLeaderElection.lookForLeader() print &quot;Updating proposa&quot; before calling upgradeProposal if (totalOrderPredicate(n.leader, n.zxid, proposedZxid) is true&lt;br/&gt;
3. in WorkerSender.process(), log -  LOG.info(&quot;WorkSender.process() QUEUEING m.state= &quot; + m.state + &quot; m.leader=&quot; + m.leader + &quot; m.sid=&quot; + m.sid);&lt;/p&gt;

&lt;p&gt;Suppporting log entries from 10.17.119.101-zookeeper.log.  I  have added description inline..&lt;br/&gt;
------------------&lt;br/&gt;
2010-08-18 14:53:56,451 - INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@475&amp;#93;&lt;/span&gt; - IN FLE sending notification to server id = 1&lt;br/&gt;
2010-08-18 14:53:56,451 - INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@476&amp;#93;&lt;/span&gt; - proposedLeader, proposedZxid, logicalclock 0343597383684&lt;br/&gt;
2010-08-18 14:53:56,452 - INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;WorkerSender Thread:FastLeaderElection$Messenger$WorkerSender@352&amp;#93;&lt;/span&gt; - WorkSender.process() QUEUEING m.state= LOOKING m.leader=0 m.sid=1&lt;br/&gt;
2010-08-18 14:53:56,452 - DEBUG &lt;span class=&quot;error&quot;&gt;&amp;#91;WorkerSender Thread:QuorumCnxManager@347&amp;#93;&lt;/span&gt; - Opening channel to server 1&lt;br/&gt;
2010-08-18 14:53:56,453 - INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@475&amp;#93;&lt;/span&gt; - IN FLE sending notification to server id = 2&lt;br/&gt;
2010-08-18 14:53:56,453 - INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@476&amp;#93;&lt;/span&gt; - proposedLeader, proposedZxid, logicalclock 0343597383684&lt;br/&gt;
2010-08-18 14:53:56,453 - INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@690&amp;#93;&lt;/span&gt; - Notification: 1, 34359738368, 4, 0, LOOKING, LOOKING, 1&lt;br/&gt;
2010-08-18 14:53:56,454 - DEBUG &lt;span class=&quot;error&quot;&gt;&amp;#91;QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@496&amp;#93;&lt;/span&gt; - id: 1, proposed id: 0, zxid: 34359738368, proposed zxid: 34359738368&lt;br/&gt;
2010-08-18 14:53:56,454 - INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@710&amp;#93;&lt;/span&gt; - Updating proposal&lt;br/&gt;
2010-08-18 14:53:56,454 - INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;WorkerSender Thread:QuorumCnxManager@162&amp;#93;&lt;/span&gt; - Have smaller server identifier, so dropping the connection: (1, 0)&lt;br/&gt;
2010-08-18 14:53:56,455 - INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;WorkerSender Thread:FastLeaderElection$Messenger$WorkerSender@352&amp;#93;&lt;/span&gt; - WorkSender.process() QUEUEING m.state= LOOKING m.leader=0 m.sid=2&lt;br/&gt;
2010-08-18 14:53:56,458 - DEBUG &lt;span class=&quot;error&quot;&gt;&amp;#91;WorkerSender Thread:QuorumCnxManager@347&amp;#93;&lt;/span&gt; - Opening channel to server 2&lt;br/&gt;
2010-08-18 14:53:56,458 - WARN  &lt;span class=&quot;error&quot;&gt;&amp;#91;Thread-19:QuorumCnxManager$RecvWorker@659&amp;#93;&lt;/span&gt; - Connection broken:&lt;br/&gt;
java.io.IOException: Channel eof&lt;br/&gt;
        at org.apache.zookeeper.server.quorum.QuorumCnxManager$RecvWorker.run(QuorumCnxManager.java:631)&lt;br/&gt;
2010-08-18 14:53:56,459 - INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@475&amp;#93;&lt;/span&gt; - IN FLE sending notification to server id = 0&lt;/p&gt;

&lt;p&gt;2010-08-18 14:53:56,460 - INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@476&amp;#93;&lt;/span&gt; - proposedLeader, proposedZxid, logicalclock 1343597383684  &lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;
	&lt;ul&gt;
		&lt;li&gt;
		&lt;ul&gt;
			&lt;li&gt;
			&lt;ul&gt;
				&lt;li&gt;
				&lt;ul&gt;
					&lt;li&gt;The above line shows that this node (server 0) is going to vote for 1. see - proposedLeader, proposedZxid, logicalclock 1 34359738368 4  Forgot to add spaces in the message  &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; ****&lt;/li&gt;
				&lt;/ul&gt;
				&lt;/li&gt;
			&lt;/ul&gt;
			&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;2010-08-18 14:53:56,460 - DEBUG &lt;span class=&quot;error&quot;&gt;&amp;#91;Thread-1:QuorumCnxManager$Listener@446&amp;#93;&lt;/span&gt; - Connection request /10.17.119.102:41597&lt;br/&gt;
2010-08-18 14:53:56,461 - DEBUG &lt;span class=&quot;error&quot;&gt;&amp;#91;Thread-1:QuorumCnxManager$Listener@449&amp;#93;&lt;/span&gt; - Connection request: 0&lt;br/&gt;
2010-08-18 14:53:56,461 - DEBUG &lt;span class=&quot;error&quot;&gt;&amp;#91;Thread-1:QuorumCnxManager$SendWorker@505&amp;#93;&lt;/span&gt; - Address of remote peer: 1&lt;br/&gt;
2010-08-18 14:53:56,461 - INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@475&amp;#93;&lt;/span&gt; - IN FLE sending notification to server id = 1&lt;br/&gt;
2010-08-18 14:53:56,462 - INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@476&amp;#93;&lt;/span&gt; - proposedLeader, proposedZxid, logicalclock 1343597383684 &lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;
	&lt;ul&gt;
		&lt;li&gt;
		&lt;ul&gt;
			&lt;li&gt;
			&lt;ul&gt;
				&lt;li&gt;
				&lt;ul&gt;
					&lt;li&gt;Above, server 0 queued a notification to be sent to server 1. The notfication is saying that it accepts 1 as the leader. But the notification never got sent. process() was not called at all from WorkerSender. Its almost as if the notification was never entered in sendqueue (in sendNotifications). *****&lt;/li&gt;
				&lt;/ul&gt;
				&lt;/li&gt;
			&lt;/ul&gt;
			&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;2010-08-18 14:53:56,462 - INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@475&amp;#93;&lt;/span&gt; - IN FLE sending notification to server id = 2&lt;br/&gt;
2010-08-18 14:53:56,462 - INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@476&amp;#93;&lt;/span&gt; - proposedLeader, proposedZxid, logicalclock 1343597383684&lt;br/&gt;
2010-08-18 14:53:56,463 - DEBUG &lt;span class=&quot;error&quot;&gt;&amp;#91;QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@716&amp;#93;&lt;/span&gt; - Adding vote: From = 1, Proposed leader = 1, Porposed zxid = 34359738368, Proposed epoch = 4&lt;br/&gt;
2010-08-18 14:53:56,463 - INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@690&amp;#93;&lt;/span&gt; - Notification: 0, 34359738368, 4, 0, LOOKING, LOOKING, 0&lt;br/&gt;
2010-08-18 14:53:56,463 - DEBUG &lt;span class=&quot;error&quot;&gt;&amp;#91;QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@496&amp;#93;&lt;/span&gt; - id: 0, proposed id: 1, zxid: 34359738368, proposed zxid: 34359738368&lt;br/&gt;
2010-08-18 14:53:56,463 - DEBUG &lt;span class=&quot;error&quot;&gt;&amp;#91;QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@716&amp;#93;&lt;/span&gt; - Adding vote: From = 0, Proposed leader = 0, Porposed zxid = 34359738368, Proposed epoch = 4&lt;/p&gt;</comment>
                            <comment id="12900111" author="vishalmlst" created="Thu, 19 Aug 2010 00:38:33 +0100"  >&lt;p&gt;at line 852 in 10.17.119.101-zookeeper.log WorkSender finally finds something in sendqueue and starts sending the notification to server 1.&lt;/p&gt;</comment>
                            <comment id="12900165" author="fpj" created="Thu, 19 Aug 2010 05:08:31 +0100"  >&lt;p&gt;Vishal, You don&apos;t seem to be using trunk code. The current trunk code would report notifications using the following format when report level info is enabled:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;LOG.info(&quot;Notification: &quot; + n.leader + &quot; (n.leader), &quot; + n.zxid +
                &quot; (n.zxid), &quot; + n.epoch + &quot; (n.round), &quot; + n.state +
                &quot; (n.state), &quot; + n.sid + &quot; (n.sid), &quot; + self.getPeerState() +
                &quot; (my state)&quot;);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And I&apos;m seeing the following in the excerpt above:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Notification: 0, 34359738368, 4, 0, LOOKING, LOOKING, 0
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Also, it would be great if we could use loggraph to visualize what is going on in your situation. &lt;/p&gt;</comment>
                            <comment id="12900401" author="vishalmlst" created="Thu, 19 Aug 2010 20:07:19 +0100"  >&lt;p&gt;Hi Flavio,&lt;/p&gt;

&lt;p&gt;Ah! my trunk is quite old then. But I don&apos;t think it is necessary to run with the latest code for debugging this issue.&lt;/p&gt;

&lt;p&gt;I have identified one problem in WorkerSender.process(). This function calls manager.toSend() whicih calls connectOne. connectOne does a blocking connect (which takes order of minutes to return if a node is down). Thus, WorkerSender.run() will  block and not send any successive notifications to other nodes.&lt;/p&gt;

&lt;p&gt;Let met know what you think&lt;/p&gt;

&lt;p&gt;I tired with adding timeouts to connectOne, but I am running into similar issue somewhere else. So that didnt fix the problem&lt;/p&gt;


</comment>
                            <comment id="12900897" author="vishalmlst" created="Fri, 20 Aug 2010 23:26:21 +0100"  >&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;To rule out any setup specific issues (VM/ESX/ect), I tried to reproduce this problem with 3 physical machines (no VMs). I was able to reproduce the same problem after 15 reboots or so. It took a little more number of reboots on the physical setup. I used the latest code this time. I  have attached the logs. &lt;/p&gt;

&lt;p&gt;I am suspecting 3 problems:&lt;br/&gt;
1. blocking connect in WorkerSender.process as described in my earlier comments.&lt;br/&gt;
2. SendWorker.run() calls finish at the end. This could result in finish() getting called twice (e.g., finish called from receiveConnection), thus, causing  senderWorkerMap.remove(sid) called twice and removing an entry that should be removed.&lt;br/&gt;
3. Race condition that causes one of the peers to disconnect other peer&apos;s connection (receiveConnection/initiateConnection issue). I am still verifying/investigating the this.&lt;/p&gt;

&lt;p&gt;In logs of server id 0 and 1, search for **&lt;b&gt;REBOOTING LEADER&lt;/b&gt;** from the bottom. This line marks the start of faulty FLE.&lt;/p&gt;

&lt;p&gt;-Vishal&lt;/p&gt;</comment>
                            <comment id="12900898" author="vishalmlst" created="Fri, 20 Aug 2010 23:30:01 +0100"  >&lt;p&gt;Correction:&lt;br/&gt;
2. SendWorker.run() calls finish at the end. This could result in finish() getting called twice (e.g., finish called from receiveConnection), thus, causing senderWorkerMap.remove(sid) called twice and removing an entry that should &lt;b&gt;not&lt;/b&gt; be removed.&lt;/p&gt;</comment>
                            <comment id="12902988" author="vishalmlst" created="Thu, 26 Aug 2010 20:00:08 +0100"  >&lt;p&gt;The fix for problem 1 and 2 above eliminates the bug. I will have a patch out soon.&lt;/p&gt;</comment>
                            <comment id="12904684" author="fpj" created="Tue, 31 Aug 2010 18:04:49 +0100"  >&lt;p&gt;Hi VIshal, Good catches:&lt;/p&gt;

&lt;p&gt;1- It sounds right that blocking the connection establishment might increase the time to election unnecessarily when the other party is not up. Here is my interpretation. If the machine is up but the the zk server is not running, then we simply get a connection failure and move on. The same doesn&apos;t happen when the the machine is down, since we need to wait for the connection establishment to time out;&lt;br/&gt;
2- It sounds right that a connection can be dropped erroneously due to a race, but I don&apos;t see in which case it can cause the election time to increase substantially, unless the race is triggered multiple times in a row. A server will try to connect upon every new notification, and a server only calls SendWorker.finish() in receiveNotification if it has a higher identifier. In this case, it creates a new connection immediately after, so it would need a previous connection being dropped right before to have the case you&apos;re describing;&lt;br/&gt;
3- Servers with higher identifiers decline connection requests from servers with lower identifiers; it is part of the protocol. Is this what you&apos;re referring to?&lt;/p&gt;</comment>
                            <comment id="12905528" author="vishalmlst" created="Thu, 2 Sep 2010 15:44:10 +0100"  >&lt;p&gt;Hi Flavio,&lt;/p&gt;

&lt;p&gt;I was planning to send out a mail explaining the problems in the FLE implementation that I have found so far. For now, I will put the info here. We can create new JIRAs if needed. I am waiting to hear back from our legal department to resolve copyright issues so that I can share my fixes as well.&lt;/p&gt;

&lt;p&gt;1. Blocking connects and accepts:&lt;br/&gt;
You are right, when the node is down TCP timeouts rule.&lt;/p&gt;

&lt;p&gt;a) The first problem is in manager.toSend(). This invokes connectOne(), which does a blocking connect. While testing, I changed the code so that connectOne() starts a new thread called AsyncConnct(). AsyncConnect.run() does a socketChannel.connect(). After starting AsyncConnect, connectOne starts a timer. connectOne continues with normal operations if the connection is established before the timer expires, otherwise, when the timer expires it interrupts AsyncConnect() thread and returns. In this way, I can have an upper bound on the amount of time we need to wait for connect to succeed. Of course, this was a quick fix for my testing. Ideally, we should use Selector to do non-blocking connects/accepts. I am planning to do that later once we at least have a quick fix for the problem and consensus from others for the real fix (this problem is big blocker for us). Note that it is OK to do blocking IO in SenderWorker and RecvWorker threads since they block IO to the respective peer.&lt;/p&gt;

&lt;p&gt;b) The blocking IO problem is not just restricted to connectOne(), but also in receiveConnection(). The Listener thread calls receiveConnection() for each incoming connection request. receiveConnection does blocking IO to get peer&apos;s info (s.read(msgBuffer)). Worse, it invokes connectOne() back to the peer that had sent the connection request. All of this is happening from the Listener. In short, if a peer fails after initiating a connection, the Listener thread won&apos;t be able to accept connections from other peers, because it would be stuck in read() or connetOne(). Also the code has an inherent cycle. initiateConnection() and receiveConnection() will have to be very carefully synchronized otherwise, we could run into deadlocks. This code is going to be difficult to maintain/modify.&lt;/p&gt;

&lt;p&gt;2. Buggy senderWorkerMap handling:&lt;br/&gt;
The code that manages senderWorkerMap is very buggy. It is causing multiple election rounds. While debugging I found that sometimes after FLE a node will have its sendWorkerMap empty even if it has SenderWorker and RecvWorker threads for each peer.&lt;/p&gt;

&lt;p&gt;a) The receiveConnection() method calls the finish() method, which removes an entry from the map. Additionally, the thread itself calls finish() which could remove the newly added entry from the map. In short, receiveConnection is causing the exact condition that you mentioned above.&lt;/p&gt;

&lt;p&gt;b) Apart from the bug in finish(), receiveConnection is making an entry in senderWorkerMap at the wrong place. Here&apos;s the buggy code:&lt;br/&gt;
            SendWorker vsw = senderWorkerMap.get(sid);&lt;br/&gt;
            senderWorkerMap.put(sid, sw);&lt;br/&gt;
            if(vsw != null)&lt;br/&gt;
                vsw.finish();&lt;br/&gt;
It makes an entry for the new thread and then calls finish, which causes the new thread to be removed from the Map. The old thread will also get terminated since finish() will interrupt the thread.&lt;/p&gt;

&lt;p&gt;3. Race condition in receiveConnection and initiateConnection:&lt;/p&gt;

&lt;p&gt;&lt;b&gt;In theory&lt;/b&gt;, two peers can keep disconnecting each other&apos;s connection.&lt;/p&gt;

&lt;p&gt;Example:&lt;br/&gt;
T0: Peer 0 initiates a connection (request 1)&lt;br/&gt;
                                                                                               T1: Peer 1 receives connection from peer 0&lt;br/&gt;
                                                                                               T2: Peer 1 calls receiveConnection()&lt;br/&gt;
T2: Peer 0 closes connection to Peer 1 because its ID is lower.&lt;br/&gt;
T3: Peer 0 re-initiates connection to Peer 1 from manger.toSend() (request 2)&lt;br/&gt;
T3: Peer 1 terminates older connection to peer 0&lt;br/&gt;
T4: Peer 1 calls connectOne() which starts new sendWorker threads for peer 0&lt;br/&gt;
T5: Peer 1 kills connection created in T3 because it receives another (request 2) connect request from 0&lt;/p&gt;

&lt;p&gt;The problem here is that while Peer 0 is accepting a connection from Peer 1 it can also be initiating a connection to Peer 1. So if they hit the right frequencies they could sit in a connect/disconnect loop and cause multiple rounds of leader election.&lt;/p&gt;

&lt;p&gt;I think the cause here is again blocking connects()/accepts(). A peer starts to take action (to kill existing threads and start new threads) as soon as a connection is established at the &lt;b&gt;TCP level&lt;/b&gt;. That is, it does not give us any control to synchronized connect and accepts. We could use non-blocking connects and accepts. This will allow us to a) tell a thread to not initiate a connection because the listener is about to accept a connection from the remote peer (use isAcceptable() and isConnectable()methods of SelectionKey) and b) prevent a thread from initiating multiple connect request to the same peer. It will simplify synchronization.&lt;/p&gt;

&lt;p&gt;Any thoughts?&lt;/p&gt;

&lt;p&gt;-Vishal&lt;/p&gt;</comment>
                            <comment id="12905836" author="fpj" created="Fri, 3 Sep 2010 10:07:28 +0100"  >&lt;blockquote&gt;
&lt;p&gt;1. Blocking connects and accepts:&lt;br/&gt;
You are right, when the node is down TCP timeouts rule.&lt;/p&gt;

&lt;p&gt;a) The first problem is in manager.toSend(). This invokes connectOne(), which does a blocking connect. While testing, I changed the code so that connectOne() starts a new thread called AsyncConnct(). AsyncConnect.run() does a socketChannel.connect(). After starting AsyncConnect, connectOne starts a timer. connectOne continues with normal operations if the connection is established before the timer expires, otherwise, when the timer expires it interrupts AsyncConnect() thread and returns. In this way, I can have an upper bound on the amount of time we need to wait for connect to succeed. Of course, this was a quick fix for my testing. Ideally, we should use Selector to do non-blocking connects/accepts. I am planning to do that later once we at least have a quick fix for the problem and consensus from others for the real fix (this problem is big blocker for us). Note that it is OK to do blocking IO in SenderWorker and RecvWorker threads since they block IO to the respective peer.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;As I commented before, it might be ok to make it asynchronous, especially if we have a way of checking that there is an attempt to establish a connection in progress. &lt;br/&gt;
I&apos;m also still intrigued about why this is a problem for you. I haven&apos;t seen any of this being a problem before, which of course doesn&apos;t mean we shouldn&apos;t fix it. It would be nice to understand what&apos;s special about your setup or if others have seen similar problems and I missed the reports.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;b) The blocking IO problem is not just restricted to connectOne(), but also in receiveConnection(). The Listener thread calls receiveConnection() for each incoming connection request. receiveConnection does blocking IO to get peer&apos;s info (s.read(msgBuffer)). Worse, it invokes connectOne() back to the peer that had sent the connection request. All of this is happening from the Listener. In short, if a peer fails after initiating a connection, the Listener thread won&apos;t be able to accept connections from other peers, because it would be stuck in read() or connetOne(). Also the code has an inherent cycle. initiateConnection() and receiveConnection() will have to be very carefully synchronized otherwise, we could run into deadlocks. This code is going to be difficult to maintain/modify.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If I remember correctly, we currently synchronize connectOne and make all connection establishments through connectOne so that we make sure that we do one at a time. My understanding is that this should reduce the number of rounds of attempts to establish connections, perhaps at the cost of a longer delay in some runs. &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2. Buggy senderWorkerMap handling:&lt;br/&gt;
The code that manages senderWorkerMap is very buggy. It is causing multiple election rounds. While debugging I found that sometimes after FLE a node will have its sendWorkerMap empty even if it has SenderWorker and RecvWorker threads for each peer.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don&apos;t think that having multiple rounds is bad; in fact, I think it is unavoidable using reasonable timeout values. The second part, however, sounds like a problem we should fix. &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;a) The receiveConnection() method calls the finish() method, which removes an entry from the map. Additionally, the thread itself calls finish() which could remove the newly added entry from the map. In short, receiveConnection is causing the exact condition that you mentioned above.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I thought that we were increasing the intervals between notifications, and if so I believe the case you mention above should not happen more than a few times. Now, to fix it, it sounds like we need to check that the finish call is removing the correct object in sendWorkerMap. That is, obj.finish() should remove obj and do nothing if the SendWorker object in sendWorkerMap is a different one. What do you think?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;b) Apart from the bug in finish(), receiveConnection is making an entry in senderWorkerMap at the wrong place. Here&apos;s the buggy code:&lt;br/&gt;
SendWorker vsw = senderWorkerMap.get(sid);&lt;br/&gt;
senderWorkerMap.put(sid, sw);&lt;br/&gt;
if(vsw != null)&lt;br/&gt;
vsw.finish();&lt;br/&gt;
It makes an entry for the new thread and then calls finish, which causes the new thread to be removed from the Map. The old thread will also get terminated since finish() will interrupt the thread.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;See my comment above. Perhaps I should wait to see your proposed modifications, but I wonder if works to check that we are removing the correct SendWorker object.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;3. Race condition in receiveConnection and initiateConnection:&lt;/p&gt;

&lt;p&gt;In theory, two peers can keep disconnecting each other&apos;s connection.&lt;/p&gt;

&lt;p&gt;Example:&lt;br/&gt;
T0: Peer 0 initiates a connection (request 1)&lt;br/&gt;
T1: Peer 1 receives connection from peer 0&lt;br/&gt;
T2: Peer 1 calls receiveConnection()&lt;br/&gt;
T2: Peer 0 closes connection to Peer 1 because its ID is lower.&lt;br/&gt;
T3: Peer 0 re-initiates connection to Peer 1 from manger.toSend() (request 2)&lt;br/&gt;
T3: Peer 1 terminates older connection to peer 0&lt;br/&gt;
T4: Peer 1 calls connectOne() which starts new sendWorker threads for peer 0&lt;br/&gt;
T5: Peer 1 kills connection created in T3 because it receives another (request 2) connect request from 0&lt;/p&gt;

&lt;p&gt;The problem here is that while Peer 0 is accepting a connection from Peer 1 it can also be initiating a connection to Peer 1. So if they hit the right frequencies they could sit in a connect/disconnect loop and cause multiple rounds of leader election.&lt;/p&gt;

&lt;p&gt;I think the cause here is again blocking connects()/accepts(). A peer starts to take action (to kill existing threads and start new threads) as soon as a connection is established at the TCP level. That is, it does not give us any control to synchronized connect and accepts. We could use non-blocking connects and accepts. This will allow us to a) tell a thread to not initiate a connection because the listener is about to accept a connection from the remote peer (use isAcceptable() and isConnectable()methods of SelectionKey) and b) prevent a thread from initiating multiple connect request to the same peer. It will simplify synchronization.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Even though it is true that you could have an infinite run where the two processes bump into each other forever, we increase the interval between new batches of notifications, so it should eventually stop, and in my experience it typically doesn&apos;t go beyond two rounds. However, I agree that if there were a way of verifying that there is a connection establishment in progress we could stop earlier. It sounds like a good idea to give it a try.&lt;/p&gt;</comment>
                            <comment id="12906221" author="mahadev" created="Sat, 4 Sep 2010 06:18:42 +0100"  >&lt;p&gt;Marking this for 3.3.2, to see if we want this included in 3.3.2.&lt;/p&gt;
</comment>
                            <comment id="12906346" author="fpj" created="Sun, 5 Sep 2010 15:10:34 +0100"  >&lt;p&gt;I think we need some time to converge on problems and fixes. My understanding is that we want to have 3.3.2 out soon, and my feeling is that this is not a blocker for 3.3.2 given Vishal&apos;s description and our experience with the system so far, but it would be good to hear from Vishal.&lt;/p&gt;</comment>
                            <comment id="12906815" author="vishalmlst" created="Tue, 7 Sep 2010 15:28:57 +0100"  >&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I have attached the diff of my changes. It is a pretty simple fix. My  intention was to have a fix with minimal code changes.&lt;/p&gt;

&lt;p&gt;connectOne waits for 2 seconds for a connection to establish. Right now the value is hard-coded. I was planning to add a new property that would specify the amount of time to wait for a connect request. If you think that the proposed changes are good enough, I will make the change for using the property value and resubmit a second patch. &lt;/p&gt;

&lt;p&gt;Let me know.&lt;/p&gt;

&lt;p&gt;-Vishal&lt;/p&gt;</comment>
                            <comment id="12906854" author="vishalmlst" created="Tue, 7 Sep 2010 17:11:30 +0100"  >&lt;p&gt;Hi Flavio,&lt;/p&gt;

&lt;p&gt;&amp;gt; I think we need some time to converge on problems and fixes. &lt;/p&gt;

&lt;p&gt;I don&apos;t think it would take a long time to converge. I think the patch that I attached is quite simple. After adding a new property for timeout we should be good to go.&lt;/p&gt;

&lt;p&gt;&amp;gt; My understanding is that we want to have 3.3.2 out soon, and my feeling is that this is not a blocker for 3.3.2 given Vishal&apos;s description and our experience with the system so far, but it would be good to hear from Vishal.&lt;/p&gt;

&lt;p&gt;From our earlier email exchanges I have a feeling that in most cases FLE was tested by restarting the ZooKeeper service (and not by rebooting/shutting down the host). I am a bit concerned that enough time may not have been spent in testing/reproducing this problem. In my opinion, this fix should go in 3.3.2. I know for sure that we won&apos;t be able to use the next release as is without this fix.&lt;/p&gt;

&lt;p&gt;Thanks.&lt;br/&gt;
-Vishal&lt;/p&gt;</comment>
                            <comment id="12906872" author="fpj" created="Tue, 7 Sep 2010 17:52:56 +0100"  >&lt;p&gt;I&apos;ll have a look at it, Vishal. Thanks for posting it.&lt;/p&gt;</comment>
                            <comment id="12909434" author="fpj" created="Tue, 14 Sep 2010 21:35:59 +0100"  >&lt;p&gt;Hi Vishal, I have taken a look at your patch. As I said before, it sounds good to me to make SocketChannel non-blocking, but I don&apos;t like very much the approach of creating one thread per connection attempt. Instead, I was thinking that we should try to use a selector. What do you think?&lt;/p&gt;</comment>
                            <comment id="12909556" author="vishalmlst" created="Wed, 15 Sep 2010 02:25:30 +0100"  >&lt;p&gt;Hi Flavio,&lt;/p&gt;

&lt;p&gt;As I mentioned earlier, this is a temporary patch until the selector based approach (non-blocking IO)  is ready.&lt;/p&gt;

&lt;p&gt;In general, what is the concern with the current fix? There will be only one thread running at a time. The thread just makes sure that we can bound the connection time. This patch is working well for us as a temporary fix. Apart from the overhead of starting a thread I don&apos;t see anything wrong with the fix.&lt;/p&gt;

&lt;p&gt;Again, given that this bug is a blocker for us, we certainly cannot wait until the non-blocking implementation  is done and released.&lt;/p&gt;

&lt;p&gt;Thanks.&lt;br/&gt;
-Vishal&lt;/p&gt;</comment>
                            <comment id="12909558" author="mahadev" created="Wed, 15 Sep 2010 02:31:12 +0100"  >&lt;p&gt;visha, flavio,&lt;br/&gt;
  If there is just one thread running at one point in time, then its ok. Also, I am really worried about the code structure in LeaderElection.java. Its ok to have a temporary fix, but it would be great to see some commitment from someone on doing it right in 3.4. &lt;/p&gt;</comment>
                            <comment id="12909560" author="vishalmlst" created="Wed, 15 Sep 2010 02:46:54 +0100"  >&lt;p&gt;I agree with Mahadev. &lt;/p&gt;</comment>
                            <comment id="12909592" author="mahadev" created="Wed, 15 Sep 2010 06:03:59 +0100"  >&lt;p&gt;vishal,&lt;br/&gt;
 I was expecting some commitment from you for making it use a selector &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;.&lt;/p&gt;</comment>
                            <comment id="12909678" author="fpj" created="Wed, 15 Sep 2010 10:50:23 +0100"  >&lt;p&gt;I believe the patch I&apos;m attaching achieves the same goal and is even simpler, but I&apos;d like to make sure it suits your needs, Vishal.&lt;/p&gt;

&lt;p&gt;If you agree with the modifications, I can work on a test. I was also thinking that the 2-second timeout you used before is too low and I&apos;ve raised to 5 seconds. But, instead of trying to argue which value is ideal, I&apos;d rather use a system property and use a default value of at least 5 seconds.&lt;/p&gt;

&lt;p&gt;I also commit to redesigning QuorumCnxManager for either 3.4.0 or 4.0.0 to use a selector or some other approach we agree upon. I&apos;ve been wanting to do it for a while anyway, and I actually thought there was a jira open for it... Maybe not, I can&apos;t find it right now. &lt;/p&gt;</comment>
                            <comment id="12910722" author="fpj" created="Fri, 17 Sep 2010 19:40:18 +0100"  >&lt;p&gt;I&apos;m adding a test to the patch. It tries to send a message to an address for which a connection request receives no response, so it has to timeout. The test then checks that the amount of time elapsed is less than 6s (the timeout value is hardcoded 5s). Raising the timeout from 5s to say 7s makes the test fail.&lt;/p&gt;</comment>
                            <comment id="12910737" author="fpj" created="Fri, 17 Sep 2010 20:05:39 +0100"  >&lt;p&gt;Attaching patch for 3.3.2.&lt;/p&gt;</comment>
                            <comment id="12912483" author="vishalmlst" created="Mon, 20 Sep 2010 15:51:42 +0100"  >&lt;p&gt;Hi Flavio,&lt;/p&gt;

&lt;p&gt;Thanks. I will take a look at the patches.&lt;/p&gt;

&lt;p&gt;-Vishal&lt;/p&gt;</comment>
                            <comment id="12912494" author="fpj" created="Mon, 20 Sep 2010 16:18:44 +0100"  >&lt;p&gt;Going to submit patches that introduce a system property.&lt;/p&gt;</comment>
                            <comment id="12912546" author="fpj" created="Mon, 20 Sep 2010 16:41:53 +0100"  >&lt;p&gt;I have added a system property called &quot;cnxtimeout&quot; to change the timeout value in QuorumCnxManager. Tests pass for me.&lt;/p&gt;</comment>
                            <comment id="12913307" author="vishalmlst" created="Wed, 22 Sep 2010 00:22:29 +0100"  >&lt;p&gt;Hi Flavio,&lt;/p&gt;

&lt;p&gt;+1. Looks good. I remember looking at the socket.connect() method, but I don&apos;t remember why I ruled it out in the favor of thread.&lt;br/&gt;
Minor point - missing space before &quot;error&quot; in LOG.warn(&quot;Connection broken: for id &quot; + sid + &quot;my id = &quot; + self.getId() + &quot;error..).&lt;/p&gt;

&lt;p&gt;Thank you.&lt;/p&gt;

&lt;p&gt;-Vishal&lt;/p&gt;
</comment>
                            <comment id="12914979" author="fpj" created="Sun, 26 Sep 2010 15:23:35 +0100"  >&lt;p&gt;Thanks for reviewing it, Vishal. I have fixed the LOG.warn you pointed out and uploaded new patch files.&lt;/p&gt;</comment>
                            <comment id="12915796" author="breed" created="Tue, 28 Sep 2010 16:54:01 +0100"  >&lt;p&gt;looks good overall flavio. just a quick questions: i notice that operations on senderWorkerMap in initiateConnection are not synchronized. senderWorkerMap is concurrent, but there could be a race between the get, put, and vsw.finish if initiateConnection is called concurrently for the same sid. right?&lt;/p&gt;

&lt;p&gt;also you need to add a blurb to the config doc for the timeout system variable, which should be &quot;zookeeper.cnxtimeout&quot; so that it can be set from the configuration file.&lt;/p&gt;</comment>
                            <comment id="12915857" author="fpj" created="Tue, 28 Sep 2010 18:53:51 +0100"  >&lt;p&gt;Thanks for the comments, Ben. I have modified zookeeperAdmin and added the &quot;zookeeper.&quot; prefix to the code.&lt;/p&gt;

&lt;p&gt;Regarding your question, initiateConnection is called from two methods: testInitiateConnection (used only in tests) and connectOne. connectOne is synchronized. Do you still see an issue?&lt;/p&gt;</comment>
                            <comment id="12918428" author="breed" created="Wed, 6 Oct 2010 07:12:30 +0100"  >&lt;p&gt;+1 looks good. ready to commit.&lt;/p&gt;</comment>
                            <comment id="12918595" author="phunt" created="Wed, 6 Oct 2010 18:03:01 +0100"  >&lt;p&gt;Committed to trunk/33 - thanks Vishal and everyone who pushed this through!&lt;/p&gt;</comment>
                            <comment id="12918878" author="hudson" created="Thu, 7 Oct 2010 11:50:49 +0100"  >&lt;p&gt;Integrated in ZooKeeper-trunk #959 (See &lt;a href=&quot;https://hudson.apache.org/hudson/job/ZooKeeper-trunk/959/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://hudson.apache.org/hudson/job/ZooKeeper-trunk/959/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/ZOOKEEPER-822&quot; title=&quot;Leader election taking a long time  to complete&quot; class=&quot;issue-link&quot; data-issue-key=&quot;ZOOKEEPER-822&quot;&gt;&lt;del&gt;ZOOKEEPER-822&lt;/del&gt;&lt;/a&gt;. Leader election taking a long time to complete&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12451046" name="822.tar.gz" size="47683" author="vishalmlst" created="Mon, 2 Aug 2010 16:29:52 +0100"/>
                            <attachment id="12455854" name="ZOOKEEPER-822-3.3.2.patch" size="8849" author="fpj" created="Tue, 28 Sep 2010 18:43:34 +0100"/>
                            <attachment id="12455607" name="ZOOKEEPER-822-3.3.2.patch" size="7810" author="fpj" created="Sun, 26 Sep 2010 15:19:48 +0100"/>
                            <attachment id="12455045" name="ZOOKEEPER-822-3.3.2.patch" size="7387" author="fpj" created="Mon, 20 Sep 2010 16:19:33 +0100"/>
                            <attachment id="12454889" name="ZOOKEEPER-822-3.3.2.patch" size="6689" author="fpj" created="Fri, 17 Sep 2010 20:05:39 +0100"/>
                            <attachment id="12455855" name="ZOOKEEPER-822.patch" size="8995" author="fpj" created="Tue, 28 Sep 2010 18:43:50 +0100"/>
                            <attachment id="12455608" name="ZOOKEEPER-822.patch" size="7955" author="fpj" created="Sun, 26 Sep 2010 15:22:08 +0100"/>
                            <attachment id="12455046" name="ZOOKEEPER-822.patch" size="7532" author="fpj" created="Mon, 20 Sep 2010 16:20:05 +0100"/>
                            <attachment id="12454890" name="ZOOKEEPER-822.patch" size="6834" author="fpj" created="Fri, 17 Sep 2010 20:06:15 +0100"/>
                            <attachment id="12454886" name="ZOOKEEPER-822.patch" size="6834" author="fpj" created="Fri, 17 Sep 2010 19:40:18 +0100"/>
                            <attachment id="12454641" name="ZOOKEEPER-822.patch" size="4769" author="fpj" created="Wed, 15 Sep 2010 10:50:23 +0100"/>
                            <attachment id="12454017" name="ZOOKEEPER-822.patch_v1" size="6063" author="vishalmlst" created="Tue, 7 Sep 2010 15:28:57 +0100"/>
                            <attachment id="12452439" name="rhel.tar.gz" size="92687" author="vishalmlst" created="Wed, 18 Aug 2010 20:22:30 +0100"/>
                            <attachment id="12449853" name="test_zookeeper_1.log" size="217291" author="vishalmlst" created="Mon, 19 Jul 2010 16:54:31 +0100"/>
                            <attachment id="12449852" name="test_zookeeper_2.log" size="219930" author="vishalmlst" created="Mon, 19 Jul 2010 16:54:31 +0100"/>
                            <attachment id="12450233" name="zk_leader_election.tar.gz" size="54702" author="vishalmlst" created="Fri, 23 Jul 2010 00:03:33 +0100"/>
                            <attachment id="12452676" name="zookeeper-3.4.0.tar.gz" size="33333" author="vishalmlst" created="Fri, 20 Aug 2010 23:26:21 +0100"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>17.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Mon, 19 Jul 2010 16:27:12 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>47583</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hxzu27:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>32854</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                </customfields>
    </item>
</channel>
</rss>