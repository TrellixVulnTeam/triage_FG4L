<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 05:45:47 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/ZOOKEEPER-1049/ZOOKEEPER-1049.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[ZOOKEEPER-1049] Session expire/close flooding renders heartbeats to delay significantly</title>
                <link>https://issues.apache.org/jira/browse/ZOOKEEPER-1049</link>
                <project id="12310801" key="ZOOKEEPER">ZooKeeper</project>
                    <description>&lt;p&gt;Let&apos;s say we have 100 clients (group A) already connected to three-node ZK ensemble with session timeout of 15 second.  And we have 1000 clients (group B) already connected to the same ZK ensemble, all watching several nodes (with 15 second session timeout)&lt;/p&gt;

&lt;p&gt;Consider a case in which All clients in group B suddenly hung or deadlocked (JVM OOME) all at the same time. 15 seconds later, all sessions in group B gets expired, creating session closing stampede. Depending on the number of this clients in group B, all request/response ZK ensemble should process get delayed up to 8 seconds (1000 clients we have tested).&lt;/p&gt;

&lt;p&gt;This delay causes some clients in group A their sessions expired due to delay in getting heartbeat response. This causes normal servers to drop out of clusters. This is a serious problem in our installation, since some of our services running batch servers or CI servers creating the same scenario as above almost everyday.&lt;/p&gt;

&lt;p&gt;I am attaching a graph showing ping response time delay.&lt;/p&gt;

&lt;p&gt;I think ordering of creating/closing sessions and ping exchange isn&apos;t important (quorum state machine). at least ping request / response should be handle independently (different queue and different thread) to keep realtime-ness of ping.&lt;/p&gt;

&lt;p&gt;As a workaround, we are raising session timeout to 50 seconds.&lt;br/&gt;
But this causes max. failover of cluster to significantly increased, thus initial QoS we promised cannot be met.&lt;/p&gt;






</description>
                <environment>&lt;p&gt;CentOS 5.3, three node ZK ensemble&lt;/p&gt;</environment>
        <key id="12504432">ZOOKEEPER-1049</key>
            <summary>Session expire/close flooding renders heartbeats to delay significantly</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.png">Critical</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="tru64ufs">Chang Song</assignee>
                                    <reporter username="tru64ufs">Chang Song</reporter>
                        <labels>
                    </labels>
                <created>Sat, 16 Apr 2011 04:42:55 +0100</created>
                <updated>Wed, 23 Nov 2011 19:22:20 +0000</updated>
                            <resolved>Tue, 3 May 2011 22:30:42 +0100</resolved>
                                    <version>3.3.2</version>
                                    <fixVersion>3.3.4</fixVersion>
                    <fixVersion>3.4.0</fixVersion>
                                    <component>server</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                <comments>
                            <comment id="13020549" author="tru64ufs" created="Sat, 16 Apr 2011 04:43:51 +0100"  >&lt;p&gt;Ping delay with respect to the number of clients closing session.&lt;/p&gt;</comment>
                            <comment id="13020550" author="tru64ufs" created="Sat, 16 Apr 2011 04:47:01 +0100"  >&lt;p&gt;We have monitored ZK ensemble server. There are no significant I/O activity, this workload is CPU bound. 0-3% io wait (this is normal system I/O wait due to system log flush, page cache flush daemon).  User CPU goes up to 30-40% during this closing stampede.&lt;/p&gt;

&lt;p&gt;We&apos;ll attach ZK config file.&lt;/p&gt;</comment>
                            <comment id="13020551" author="tru64ufs" created="Sat, 16 Apr 2011 04:48:26 +0100"  >&lt;p&gt;Log4J log level is &quot;error&quot; during this test.&lt;/p&gt;</comment>
                            <comment id="13020552" author="tru64ufs" created="Sat, 16 Apr 2011 04:48:52 +0100"  >&lt;p&gt;ZK version is 3.3.3&lt;/p&gt;</comment>
                            <comment id="13020553" author="tru64ufs" created="Sat, 16 Apr 2011 05:00:29 +0100"  >&lt;p&gt;tickTime=2000&lt;br/&gt;
initLimit=10&lt;br/&gt;
syncLimit=5&lt;/p&gt;

&lt;p&gt;dataDir=/???/zookeeper/data&lt;br/&gt;
dataLogDir=/???/zookeeper/log&lt;/p&gt;

&lt;p&gt;clientPort=17288&lt;/p&gt;

&lt;p&gt;maxClientCnxns=0&lt;/p&gt;

&lt;p&gt;#globalOutstandingLimit=1000, we tried both (no difference)&lt;br/&gt;
globalOutstandingLimit=50000&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;Arcus ZooKeeper Servers&lt;br/&gt;
server.1=????&lt;br/&gt;
server.2=????&lt;br/&gt;
server.3=1????&lt;/li&gt;
&lt;/ol&gt;
</comment>
                            <comment id="13020555" author="tru64ufs" created="Sat, 16 Apr 2011 05:02:37 +0100"  >&lt;p&gt;log4j.rootLogger=ERROR, ROLLINGFILE&lt;/p&gt;

&lt;p&gt;log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender&lt;br/&gt;
log4j.appender.CONSOLE.Threshold=INFO&lt;br/&gt;
log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout&lt;br/&gt;
log4j.appender.CONSOLE.layout.ConversionPattern=%d&lt;/p&gt;
{ISO8601} - %-5p &lt;span class=&quot;error&quot;&gt;&amp;#91;%t:%C{1}@%L&amp;#93;&lt;/span&gt; - %m%n&lt;br/&gt;
&lt;br/&gt;
log4j.appender.ROLLINGFILE=org.apache.log4j.RollingFileAppender&lt;br/&gt;
log4j.appender.ROLLINGFILE.Threshold=DEBUG&lt;br/&gt;
log4j.appender.ROLLINGFILE.File=/???/zookeeper/log/zookeeper.log&lt;br/&gt;
&lt;br/&gt;
log4j.appender.ROLLINGFILE.MaxFileSize=50MB&lt;br/&gt;
log4j.appender.ROLLINGFILE.layout=org.apache.log4j.PatternLayout&lt;br/&gt;
log4j.appender.ROLLINGFILE.layout.ConversionPattern=%d{ISO8601}
&lt;p&gt; - %-5p [%t:%C&lt;/p&gt;
{1}@%L] - %m%n&lt;br/&gt;
&lt;br/&gt;
log4j.appender.TRACEFILE=org.apache.log4j.FileAppender&lt;br/&gt;
log4j.appender.TRACEFILE.Threshold=TRACE&lt;br/&gt;
log4j.appender.TRACEFILE.File=/???/zookeeper/log/zookeeper_trace.log&lt;br/&gt;
&lt;br/&gt;
log4j.appender.TRACEFILE.layout=org.apache.log4j.PatternLayout&lt;br/&gt;
log4j.appender.TRACEFILE.layout.ConversionPattern=%d{ISO8601} - %-5p [%t:%C{1}
&lt;p&gt;@%L]&lt;span class=&quot;error&quot;&gt;&amp;#91;%x&amp;#93;&lt;/span&gt; - %m%n&lt;/p&gt;</comment>
                            <comment id="13020564" author="tru64ufs" created="Sat, 16 Apr 2011 07:28:54 +0100"  >&lt;blockquote&gt;&lt;p&gt;1) are you running in a virtualized environment?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;NO&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2) are you co-locating other services on the same host(s) that make up&lt;br/&gt;
the ZK serving cluster?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;NO. Dedicated ZK ensemble&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;3) have you followed the admin guide&apos;s &quot;things to avoid&quot;?&lt;br/&gt;
&lt;a href=&quot;http://zookeeper.apache.org/doc/r3.3.3/zookeeperAdmin.html#sc_commonProblems&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://zookeeper.apache.org/doc/r3.3.3/zookeeperAdmin.html#sc_commonProblems&lt;/a&gt;&lt;br/&gt;
In particular ensuring that you are not swapping or going into gc&lt;br/&gt;
pause (both on the server and the client)&lt;br/&gt;
a) try turning on GC logging and ensure that you are not going into GC&lt;br/&gt;
pause, see the troubleshooting guide, this is the most common cause of&lt;br/&gt;
high latency for the clients&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;No full GC (jstat output)&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;b) ensure that you are not swapping&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;No swapping. No significant IO (0-3% iowait utilization)&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;c) ensure that other processes are not causing log writing&lt;br/&gt;
(transactional logging) to be slow.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;No other processes are running on these hosts.&lt;/p&gt;
</comment>
                            <comment id="13020565" author="tru64ufs" created="Sat, 16 Apr 2011 07:35:36 +0100"  >&lt;blockquote&gt;
&lt;p&gt;1) what is the connectivity like btw the servers?&lt;br/&gt;
What is the ping time btw them?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;ping time is under 1ms (inter-IDC)&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Is the system perhaps loading down the network during this test,&lt;br/&gt;
causing network latency to increase? Are all the nic cards (server and&lt;br/&gt;
client) configured correctly? I&apos;ve seen a number of cases where&lt;br/&gt;
clients and/or server had incorrectly configured nics (ethtool&lt;br/&gt;
reported 10 MB/sec half duplex for what should be 1gigeth)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;They all are configured Full-Duplex 1Gbps NIC.&lt;br/&gt;
Switches are all operational.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2) regarding IO, if you run &apos;iostat -x 2&apos; on the zk servers while your&lt;br/&gt;
issue is happening, what&apos;s the %util of the disk? what&apos;s the iowait&lt;br/&gt;
look like?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;0-3% at the time of testing. It varies. There is no significant IO&lt;br/&gt;
from Zookeeper process. Just normal page cache flush activity.&lt;/p&gt;</comment>
                            <comment id="13020567" author="tru64ufs" created="Sat, 16 Apr 2011 07:41:13 +0100"  >&lt;p&gt;How to reproduce&lt;/p&gt;

&lt;p&gt;1. 50 clients connect to ZK ensemble watching a node with 15 sec session timeout (calling group A)&lt;/p&gt;

&lt;p&gt;2. Let 1000 clients in other group (called group B) connect to the ZK ensemble, but hang the JVM (or intentionally create OOM case). Again with 15 sec session timeout&lt;/p&gt;

&lt;p&gt;3. Watch clients in group A drop out of the cluster due to ping delay (session expired)&lt;/p&gt;</comment>
                            <comment id="13020568" author="tru64ufs" created="Sat, 16 Apr 2011 07:45:38 +0100"  >&lt;blockquote&gt;
&lt;p&gt;What about using multiple ZK clusters for this, then?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Sorry, we cannot do that due to service deployment policy&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Have you tested a cluster where the machines are set up correctly with separate snapshot and log disks?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;We have two test/staging ZK ensemble setup, and one production. They reproduce the same result.&lt;br/&gt;
Since IO wasn&apos;t an issue, there is no point of separating snapshot and log.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Are your ZK machines doing any other tasks? &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;This is a dedicated ZK ensemble&lt;/p&gt;
</comment>
                            <comment id="13021169" author="fournc" created="Mon, 18 Apr 2011 19:38:54 +0100"  >&lt;p&gt;This might be related to this email chain from February:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/zookeeper-user/201102.mbox/%3C6642FC1CAF133548AA8FDF497C547F0A23C0C5265B@NYWEXMBX2126.msad.ms.com%3E&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://mail-archives.apache.org/mod_mbox/zookeeper-user/201102.mbox/%3C6642FC1CAF133548AA8FDF497C547F0A23C0C5265B@NYWEXMBX2126.msad.ms.com%3E&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I theorized at the time that the issue might be due to synchronization on the session table, but never got enough information to finish the investigation.&lt;/p&gt;</comment>
                            <comment id="13021438" author="tru64ufs" created="Tue, 19 Apr 2011 08:15:20 +0100"  >&lt;p&gt;We have uploaded a simple reproducer scripts.&lt;/p&gt;</comment>
                            <comment id="13021447" author="tru64ufs" created="Tue, 19 Apr 2011 08:31:49 +0100"  >&lt;p&gt;I have uploaded a python-based reproducer, and test results.&lt;/p&gt;

&lt;p&gt;This probably is the worst iowait CPU% (you can find them in Result directory.&lt;/p&gt;

&lt;p&gt;avg-cpu:  %user   %nice %system %iowait  %steal   %idle&lt;br/&gt;
           6.15    0.00    2.05    2.56    0.00   89.23&lt;/p&gt;


&lt;p&gt;The resulting latency delay (with this reproducer)- You will find 2 second delay for some clients&lt;/p&gt;

&lt;p&gt;close session elapsed : 0ms&lt;br/&gt;
close session elapsed : 0ms&lt;br/&gt;
close session elapsed : 2001ms&lt;br/&gt;
close session elapsed : 1ms&lt;br/&gt;
close session elapsed : 0ms&lt;/p&gt;

&lt;p&gt;.....&lt;/p&gt;

&lt;p&gt;close session elapsed : 0ms&lt;br/&gt;
close session elapsed : 0ms&lt;br/&gt;
close session elapsed : 2001ms&lt;br/&gt;
close session elapsed : 0ms&lt;br/&gt;
close session elapsed : 0ms&lt;br/&gt;
....&lt;/p&gt;

&lt;p&gt;close session elapsed : 0ms&lt;br/&gt;
close session elapsed : 2000ms&lt;br/&gt;
close session elapsed : 2000ms&lt;br/&gt;
close session elapsed : 0ms&lt;br/&gt;
close session elapsed : 0ms&lt;/p&gt;


</comment>
                            <comment id="13021448" author="tru64ufs" created="Tue, 19 Apr 2011 08:36:56 +0100"  >&lt;p&gt;When this happens &lt;br/&gt;
Below thread becomes BLOCKED&lt;/p&gt;

&lt;p&gt;&quot;NIOServerCxn.Factory:0.0.0.0/0.0.0.0:17288&quot; - Thread t@11&lt;br/&gt;
   java.lang.Thread.State: BLOCKED on org.apache.zookeeper.server.NIOServerCnxn$Factory@399831c3 owned by: CommitProcessor:3&lt;br/&gt;
	at org.apache.zookeeper.server.NIOServerCnxn$Factory.run(NIOServerCnxn.java:234)&lt;br/&gt;
   Locked ownable synchronizers:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;None&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;If you look at CommitProcessor:3&lt;/p&gt;

&lt;p&gt;&quot;CommitProcessor:3&quot; - Thread t@27&lt;br/&gt;
   java.lang.Thread.State: RUNNABLE&lt;br/&gt;
	at sun.nio.ch.FileDispatcher.preClose0(Native Method)&lt;br/&gt;
	at sun.nio.ch.SocketDispatcher.preClose(SocketDispatcher.java:41)&lt;br/&gt;
	at sun.nio.ch.SocketChannelImpl.implCloseSelectableChannel(SocketChannelImpl.java:684)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;locked java.lang.Object@@3186c91&lt;br/&gt;
	at java.nio.channels.spi.AbstractSelectableChannel.implCloseChannel(AbstractSelectableChannel.java:201)&lt;br/&gt;
	at java.nio.channels.spi.AbstractInterruptibleChannel.close(AbstractInterruptibleChannel.java:97)&lt;/li&gt;
	&lt;li&gt;locked java.lang.Object@@7fc600&lt;br/&gt;
	at sun.nio.ch.SocketAdaptor.close(SocketAdaptor.java:352)&lt;br/&gt;
	at org.apache.zookeeper.server.NIOServerCnxn.closeSock(NIOServerCnxn.java:1463)&lt;br/&gt;
	at org.apache.zookeeper.server.NIOServerCnxn.close(NIOServerCnxn.java:1412)&lt;/li&gt;
	&lt;li&gt;locked java.util.HashSet@@4b94a39&lt;br/&gt;
	at org.apache.zookeeper.server.NIOServerCnxn$Factory.closeSessionWithoutWakeup(NIOServerCnxn.java:343)&lt;br/&gt;
	at org.apache.zookeeper.server.NIOServerCnxn$Factory.closeSession(NIOServerCnxn.java:330)&lt;/li&gt;
	&lt;li&gt;locked org.apache.zookeeper.server.NIOServerCnxn$Factory@@399831c&lt;br/&gt;
	at org.apache.zookeeper.server.FinalRequestProcessor.processRequest(FinalRequestProcessor.java:133)&lt;br/&gt;
	at org.apache.zookeeper.server.quorum.Leader$ToBeAppliedRequestProcessor.processRequest(Leader.java:540)&lt;br/&gt;
	at org.apache.zookeeper.server.quorum.CommitProcessor.run(CommitProcessor.java:73)&lt;br/&gt;
   Locked ownable synchronizers:&lt;/li&gt;
	&lt;li&gt;None&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;scxn.closeSession(request.sessionId)&lt;br/&gt;
if (request.hdr != null &amp;amp;&amp;amp; request.hdr.getType() == OpCode.closeSession) {&lt;br/&gt;
    Factory scxn = zks.getServerCnxnFactory();&lt;br/&gt;
    // this might be possible since&lt;br/&gt;
    // we might just be playing diffs from the leader&lt;br/&gt;
    if (scxn != null &amp;amp;&amp;amp; request.cnxn == null) &lt;/p&gt;
{
        // calling this if we have the cnxn results in the client&apos;s
        // close session response being lost - we&apos;ve already closed
        // the session/socket here before we can send the closeSession
        // in the switch block below
        scxn.closeSession(request.sessionId);
        return;
    }
&lt;p&gt;}&lt;/p&gt;



&lt;p&gt;Below is a synchronized method&lt;/p&gt;

&lt;p&gt;NIOServerCnxn.Factory.closeSession()&lt;br/&gt;
synchronized void closeSession(long sessionId) &lt;/p&gt;
{
    selector.wakeup();
    closeSessionWithoutWakeup(sessionId);
}


&lt;p&gt;@SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
private void closeSessionWithoutWakeup(long sessionId) {&lt;br/&gt;
    HashSet&amp;lt;NIOServerCnxn&amp;gt; cnxns;&lt;br/&gt;
    synchronized (this.cnxns) &lt;/p&gt;
{
        cnxns = (HashSet&amp;lt;NIOServerCnxn&amp;gt;)this.cnxns.clone();
    }

&lt;p&gt;    for (NIOServerCnxn cnxn : cnxns) {&lt;br/&gt;
        if (cnxn.sessionId == sessionId) {&lt;br/&gt;
            try &lt;/p&gt;
{
                cnxn.close();
            }
&lt;p&gt; catch (Exception e) &lt;/p&gt;
{
                LOG.warn(&quot;exception during session close&quot;, e);
            }
&lt;p&gt;            break;&lt;br/&gt;
        }&lt;br/&gt;
    }&lt;br/&gt;
}&lt;/p&gt;



&lt;p&gt;We measured the time spent in scxn.closeSession(request.sessionId), and the result is in the previous post.&lt;/p&gt;


&lt;p&gt;if (request.hdr != null &amp;amp;&amp;amp; request.hdr.getType() == OpCode.closeSession) {&lt;br/&gt;
    Factory scxn = zks.getServerCnxnFactory();&lt;/p&gt;

&lt;p&gt;    if (scxn != null &amp;amp;&amp;amp; request.cnxn == null) &lt;/p&gt;
{

        scxn.closeSession(request.sessionId);

        return;
    }
&lt;p&gt;}&lt;/p&gt;

</comment>
                            <comment id="13021452" author="tru64ufs" created="Tue, 19 Apr 2011 08:48:05 +0100"  >&lt;p&gt;I think we found a culprit&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; NIOServerCnxn(ZooKeeperServer zk, SocketChannel sock,
            SelectionKey sk, Factory factory) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
        &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.zk = zk;
        &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.sock = sock;
        &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.sk = sk;
        &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.factory = factory;
        sock.socket().setTcpNoDelay(&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;);
        sock.socket().setSoLinger(&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, 2); &lt;span class=&quot;code-comment&quot;&gt;// socket linger option of 
&lt;/span&gt;        InetAddress addr = ((InetSocketAddress) sock.socket()
                .getRemoteSocketAddress()).getAddress();
        authInfo.add(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Id(&lt;span class=&quot;code-quote&quot;&gt;&quot;ip&quot;&lt;/span&gt;, addr.getHostAddress()));
        sk.interestOps(SelectionKey.OP_READ);
    }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;it is the socket linger option.&lt;br/&gt;
since clients session already expired, do we need to wait 2 second to wait until clients to consume socket receive buffer?&lt;/p&gt;

&lt;p&gt;we have set to linger option to 0 to send TCP RESET, and the problem went away.&lt;/p&gt;

&lt;p&gt;We&apos;ll try to test original case, and report back.&lt;/p&gt;

</comment>
                            <comment id="13021524" author="tru64ufs" created="Tue, 19 Apr 2011 11:20:20 +0100"  >&lt;p&gt;We just verified that the original problem goes away when linger option is off (0)&lt;br/&gt;
Mystery solved.&lt;/p&gt;

&lt;p&gt;We will patch our production server right away.&lt;/p&gt;</comment>
                            <comment id="13021601" author="mahadev" created="Tue, 19 Apr 2011 15:17:12 +0100"  >&lt;p&gt;Chang,&lt;br/&gt;
  I am quite confused. Not sure how setting the tcp linger time to 0 is helping you? What OS are you using? I find it surprising that setting the linger time to be 0 doesnt delay packets to other sockets. &lt;/p&gt;</comment>
                            <comment id="13021695" author="mahadev" created="Tue, 19 Apr 2011 19:14:02 +0100"  >&lt;p&gt;Ben and I looked at the man page of socket and here is what it says:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;SO_LINGER
Sets or gets the SO_LINGER option. The argument is a linger structure.
struct linger {
    int l_onoff;    /* linger active */
    int l_linger;   /* how many seconds to linger for */
};
When enabled, a close(2) or shutdown(2) will not return until all queued messages for the socket have been successfully sent or the linger timeout has been reached. Otherwise, the call returns immediately and the closing is done in the background. When the socket is closed as part of exit(2), it always lingers in the background.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So, since we have a single thread that calls close() and IO on sockets, it is possible that this can cause a huge delay in IO for other sockets. Anyone else hasmore information on this please feel free to update!&lt;/p&gt;



</comment>
                            <comment id="13021788" author="tdunning" created="Tue, 19 Apr 2011 21:46:27 +0100"  >&lt;p&gt;Is it possible that somebody on the server side has a lock while trying to close a socket to a failed machine?&lt;/p&gt;

&lt;p&gt;Or is it possible that there is a limited number of threads available for this closing activity?&lt;/p&gt;
</comment>
                            <comment id="13021827" author="tru64ufs" created="Tue, 19 Apr 2011 23:17:56 +0100"  >&lt;p&gt;OK.&lt;/p&gt;

&lt;p&gt;Linger option with 2 second timeout on Apache web server works because it is connection per process/thread model. Blocking in one process does not block other request processing.&lt;/p&gt;

&lt;p&gt;If you look at the code below, I don&apos;t see why it shouldn&apos;t block.&lt;br/&gt;
One 2 sec delay causes some other closing activity to 4 or 6 or 8 seconds, and so forth.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
NIOServerCnxn.Factory.closeSession()
&lt;span class=&quot;code-keyword&quot;&gt;synchronized&lt;/span&gt; void closeSession(&lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; sessionId) { selector.wakeup(); closeSessionWithoutWakeup(sessionId); }

@SuppressWarnings(&lt;span class=&quot;code-quote&quot;&gt;&quot;unchecked&quot;&lt;/span&gt;)
&lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; void closeSessionWithoutWakeup(&lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; sessionId) {
  HashSet&amp;lt;NIOServerCnxn&amp;gt; cnxns;
  &lt;span class=&quot;code-keyword&quot;&gt;synchronized&lt;/span&gt; (&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.cnxns) { cnxns = (HashSet&amp;lt;NIOServerCnxn&amp;gt;)&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.cnxns.clone(); }

  &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (NIOServerCnxn cnxn : cnxns) {
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (cnxn.sessionId == sessionId) {
      &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; { cnxn.close(); } 
      &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (Exception e) { LOG.warn(&lt;span class=&quot;code-quote&quot;&gt;&quot;exception during session close&quot;&lt;/span&gt;, e); }
      &lt;span class=&quot;code-keyword&quot;&gt;break&lt;/span&gt;;
    }
  }
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;One thing I need to make correction, my team didn&apos;t set linger to 0.&lt;br/&gt;
They disabled the linger option, close() returns immediately (lingering done by TCP stack)&lt;/p&gt;

&lt;p&gt;This is different than setting linger to 0, which simply flush all the buffers and send TCP RST.&lt;br/&gt;
By disabling linger option, system will hold on to the socket in FIN_WAIT_1 state for dead clients.&lt;br/&gt;
note that we have no timeout for FIN_WAIT_1 sockets.&lt;/p&gt;

&lt;p&gt;We&apos;ll have to experiment with setting linger option 0 to move socket from ESTABLISHED to CLOSED states immediately. &lt;/p&gt;


</comment>
                            <comment id="13022691" author="tru64ufs" created="Thu, 21 Apr 2011 10:10:33 +0100"  >&lt;p&gt;I don&apos;t see any problem with disabling linger option due to net.ipv4.tcp_orphan_retries being 200ms on Linux. socket without lingering option to dead server closed to FIN_WAIT_1 setting times out immediately (after 200ms) and immediately CLOSED.&lt;/p&gt;

&lt;p&gt;So our final fix is &lt;/p&gt;

&lt;p&gt;setSoLinger(false, -1)&lt;/p&gt;

&lt;p&gt;Please consider this fix for next release.&lt;br/&gt;
Thank you.&lt;/p&gt;</comment>
                            <comment id="13023635" author="netspider" created="Sun, 24 Apr 2011 07:44:17 +0100"  >&lt;p&gt;This patch set NIOServerCnxn linger option to &apos;false, -1&apos;.&lt;/p&gt;</comment>
                            <comment id="13026378" author="hadoopqa" created="Thu, 28 Apr 2011 18:13:32 +0100"  >&lt;p&gt;-1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12477238/ZOOKEEPER-1049.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12477238/ZOOKEEPER-1049.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision 1091841.&lt;/p&gt;

&lt;p&gt;    +1 @author.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    -1 tests included.  The patch doesn&apos;t appear to include any new or modified tests.&lt;br/&gt;
                        Please justify why no new tests are needed for this patch.&lt;br/&gt;
                        Also please list what manual steps were performed to verify this patch.&lt;/p&gt;

&lt;p&gt;    +1 javadoc.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    -1 findbugs.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    +1 release audit.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    +1 core tests.  The patch passed core unit tests.&lt;/p&gt;

&lt;p&gt;    +1 contrib tests.  The patch passed contrib unit tests.&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/hudson/job/PreCommit-ZOOKEEPER-Build/235//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/hudson/job/PreCommit-ZOOKEEPER-Build/235//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/hudson/job/PreCommit-ZOOKEEPER-Build/235//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/hudson/job/PreCommit-ZOOKEEPER-Build/235//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/hudson/job/PreCommit-ZOOKEEPER-Build/235//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/hudson/job/PreCommit-ZOOKEEPER-Build/235//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13028441" author="mahadev" created="Tue, 3 May 2011 22:30:42 +0100"  >&lt;p&gt;I just committed this! Very nice catch! thanks chang!&lt;/p&gt;</comment>
                            <comment id="13028443" author="mahadev" created="Tue, 3 May 2011 22:32:50 +0100"  >&lt;p&gt;BTW, I added a tiny comment with the patch so that its clear for others.&lt;/p&gt;</comment>
                            <comment id="13028555" author="tru64ufs" created="Wed, 4 May 2011 02:55:33 +0100"  >&lt;p&gt;Kudos to &quot;Chisu Ryu&quot;, who found the root cause and came up with the fix.&lt;br/&gt;
Thank you.&lt;/p&gt;
</comment>
                            <comment id="13028703" author="hudson" created="Wed, 4 May 2011 11:55:05 +0100"  >&lt;p&gt;Integrated in ZooKeeper-trunk #1172 (See &lt;a href=&quot;https://builds.apache.org/hudson/job/ZooKeeper-trunk/1172/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/hudson/job/ZooKeeper-trunk/1172/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/ZOOKEEPER-1049&quot; title=&quot;Session expire/close flooding renders heartbeats to delay significantly&quot; class=&quot;issue-link&quot; data-issue-key=&quot;ZOOKEEPER-1049&quot;&gt;&lt;del&gt;ZOOKEEPER-1049&lt;/del&gt;&lt;/a&gt;. Session expire/close flooding renders heartbeats to delay significantly. (chang song via mahadev)&lt;/p&gt;</comment>
                            <comment id="13125996" author="phunt" created="Wed, 12 Oct 2011 18:54:42 +0100"  >&lt;p&gt;Recently a couple of users have reported this - I&apos;m including it in 3.3.4 as well.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12527976">ZOOKEEPER-1238</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12477238" name="ZOOKEEPER-1049.patch" size="695" author="netspider" created="Sun, 24 Apr 2011 07:44:17 +0100"/>
                            <attachment id="12476696" name="ZookeeperPingTest.zip" size="18970" author="tru64ufs" created="Tue, 19 Apr 2011 08:15:20 +0100"/>
                            <attachment id="12476516" name="zk_ping_latency.pdf" size="3626" author="tru64ufs" created="Sat, 16 Apr 2011 04:43:51 +0100"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Mon, 18 Apr 2011 18:38:54 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>47501</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hxztfz:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>32754</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                </customfields>
    </item>
</channel>
</rss>