<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 03:35:52 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/DERBY-5416/DERBY-5416.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[DERBY-5416] SYSCS_COMPRESS_TABLE causes an OutOfMemoryError when the heap is full at call time and then gets mostly garbage collected later on</title>
                <link>https://issues.apache.org/jira/browse/DERBY-5416</link>
                <project id="10594" key="DERBY">Derby</project>
                    <description>&lt;p&gt;When compressing a table with an index that is larger than the maximum heap size and therefore cannot be hold in memory as a whole an OutOfMemoryError can occur. &lt;/p&gt;

&lt;p&gt;For this to happen the heap usage must be close to the maximum heap size at the start of the index recreation and then while the entries are sorted a garbage collection run must clean out most of the heap. This can happen because a concurrent process releases a huge chunk of memory or just because the buffer of a previous table compression has not yet been garbage collected. &lt;br/&gt;
The internally used heuristics to guess when more memory can be used for the merge inserter estimates that more memory is available and then the sort buffer gets doubled. The buffer size gets doubled until the heap usage is back to the level when the merge inserter was first initialized or when the OOM occurs.&lt;/p&gt;

&lt;p&gt;The problem lies in MergeInsert.insert(...). The check if the buffer can be doubled contains the expression &quot;estimatedMemoryUsed &amp;lt; 0&quot; where estimatedMemoryUsed is the difference in current heap usage and heap usage at initialization. Unfortunately, in the aforementioned scenario this will be true until the heap usage will reach close to maximum heap size before doubling the buffer size will be stopped.&lt;/p&gt;

&lt;p&gt;I&apos;ve tested it with 10.6.2.1, 10.7.1.1 and 10.8.1.2 but the actual bug most likely exists in prior versions too.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12523325">DERBY-5416</key>
            <summary>SYSCS_COMPRESS_TABLE causes an OutOfMemoryError when the heap is full at call time and then gets mostly garbage collected later on</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.png">Critical</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="knutanders">Knut Anders Hatlen</assignee>
                                    <reporter username="rbaradari">Ramin Baradari</reporter>
                        <labels>
                            <label>derby_triage10_9</label>
                    </labels>
                <created>Fri, 16 Sep 2011 14:04:20 +0100</created>
                <updated>Wed, 21 Jan 2015 00:23:08 +0000</updated>
                            <resolved>Wed, 26 Feb 2014 22:21:58 +0000</resolved>
                                    <version>10.6.2.1</version>
                    <version>10.7.1.1</version>
                    <version>10.8.1.2</version>
                                    <fixVersion>10.10.2.0</fixVersion>
                    <fixVersion>10.11.1.1</fixVersion>
                                    <component>Store</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                <comments>
                            <comment id="13106042" author="rbaradari" created="Fri, 16 Sep 2011 14:13:24 +0100"  >&lt;p&gt;Patch &quot;compress_test_5416.patch&quot; adds a new test method to CompressTableTest.java that reproduces the crash. The test reproduced the crash on my systems each time I ran it but the bug depends a bit on the proper timing so it might actually succeed on systems. The test is a bit slow takes about 10minutes to complete.&lt;/p&gt;

&lt;p&gt;The test creates a table with an index that is larger in data size than the 512mb maximum heap size the test runner has. (should be about 600mb+). It then fills the heap in chunks of several megabytes until an OOM occurs that is catched and ignore. The reference to that dummy data is then released and the table compression is called.&lt;/p&gt;</comment>
                            <comment id="13106072" author="kristwaa" created="Fri, 16 Sep 2011 14:54:47 +0100"  >&lt;p&gt;Thanks for taking the time to write up a bug report and to provide a repro, Ramin.&lt;/p&gt;

&lt;p&gt;FYI, we do have a separate low-memory test suite that&apos;s run with a 16 MB heap. It can be run with &apos;ant junit-lowmem&apos;, the suite is found in java/testing/org/apache/derbyTesting/functionTests/tests/memory&lt;/p&gt;</comment>
                            <comment id="13164675" author="kmarsden" created="Wed, 7 Dec 2011 20:21:35 +0000"  >&lt;p&gt;I wonder, is this really a bug or just a case where we simply do not have enough heap to execute the task so it fails with an OOM?&lt;/p&gt;</comment>
                            <comment id="13210632" author="mikem" created="Fri, 17 Feb 2012 22:46:20 +0000"  >&lt;p&gt;Any suggestions on a better way to determine if sort should use more in-memory.  It seems for performance derby should be &lt;br/&gt;
using more memory for these operations automatically, but as this issue points out operations can fail if the choice is wrong.&lt;br/&gt;
Solutions should be zero-admin by default, with maybe tuning knobs to work around issues like this one where the default does&lt;br/&gt;
not work.  &lt;/p&gt;</comment>
                            <comment id="13760086" author="dagw" created="Fri, 6 Sep 2013 10:37:48 +0100"  >&lt;p&gt;I was able to repro by applying the patch supplied.&lt;/p&gt;</comment>
                            <comment id="13843115" author="knutanders" created="Mon, 9 Dec 2013 12:40:48 +0000"  >&lt;p&gt;If the heap is filled right before the CREATE INDEX statement, the OOME happens in CREATE INDEX instead of SYSCS_COMPRESS_TABLE. (They both use the same code for sorting.) The OOME is reproduced faster that way.&lt;/p&gt;</comment>
                            <comment id="13843219" author="knutanders" created="Mon, 9 Dec 2013 15:00:52 +0000"  >&lt;p&gt;The code that decides whether or not to grow the sort buffer, essentially works like this in the failing case:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;When the sort buffer is initialized, it records the amount of memory currently in use, and allocates a small buffer.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;When the buffer is full, it checks the amount of memory currently in use. It intends to use the difference between the current usage and the initial usage as an estimate of how much memory a doubling of the sort buffer requires. However, since a gc has happened, the difference is negative. Since there is more memory available now than when the buffer was initialized, it assumes that it is safe to allocate as much extra space now as the amount that it successfully allocated with less available memory. So it doubles the buffer size. This sounds like a fair assumption.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;The next time the buffer is full, it still sees that the memory usage is smaller than the initial memory usage. Again it assumes that it is safe to double the buffer size, and does exactly that. However, at this point, the assumption is not as fair. Notice the difference between the assumption in this step and in the previous step: In the previous step, it was assumed safe to grow the buffer with as much space as we added when the buffer was initialized. In this step, we don&apos;t grow the buffer by the same amount as we initially gave the buffer; we actually grow it by twice that amount. This step is repeated each time the buffer gets full, and each time the amount we add gets doubled (way beyond the initial amount that we regarded as a safe increment). Eventually, the buffer gets too large for the heap, and we get an OOME.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I see at least three ways we could improve the heuristic to avoid this problem:&lt;/p&gt;

&lt;p&gt;1. Instead of using the difference between the current memory usage and the initial memory usage for estimating the memory requirements, we could use the difference between the current memory usage and the memory usage the previous time the buffer was doubled. Then a big gc right after the allocation of the buffer won&apos;t affect all upcoming estimates, only the estimate calculated the first time the buffer is full.&lt;/p&gt;

&lt;p&gt;2. When we don&apos;t have an estimate of the memory requirement for doubling the buffer (because of a gc), and the current memory usage is smaller than the initial memory usage, don&apos;t assume blindly that it is OK to double the buffer. Instead, grow it by the amount of memory that we found it was safe to add initially, when the memory usage was at least as high as it is now. This would mean a doubling of the buffer the first time the buffer gets full, but less than that from the second time the buffer gets full. (In the common case, where we do have an estimate of the memory usage, a doubling will happen each time the buffer gets full, as long as the estimate suggests there&apos;s enough free heap space.) In other words, use a more conservative approach and grow the buffer more slowly when we don&apos;t have a good estimate for the actual memory requirements.&lt;/p&gt;

&lt;p&gt;3. Since the buffer contains arrays of DataValueDescriptors, we may be able to estimate the memory requirements the same way as we do for BackingStoreHashtable. That is, by calling estimateMemoryUsage() on the DataValueDescriptors to see approximately how much space a single row takes. (Currently, this approach underestimates the actual memory requirements. See &lt;a href=&quot;https://issues.apache.org/jira/browse/DERBY-4620&quot; title=&quot;Query optimizer causes OOM error on a complex query&quot; class=&quot;issue-link&quot; data-issue-key=&quot;DERBY-4620&quot;&gt;DERBY-4620&lt;/a&gt;.)&lt;/p&gt;</comment>
                            <comment id="13843296" author="knutanders" created="Mon, 9 Dec 2013 16:29:49 +0000"  >&lt;p&gt;Attaching lowmem-test.diff, which scales down the test case so that it fits into the lowmem test suite that runs with 16 MB heap, as Kristian suggested. It still shows the OOME in my environment, and it runs a lot faster since it uses a much smaller test table.&lt;/p&gt;</comment>
                            <comment id="13843310" author="dagw" created="Mon, 9 Dec 2013 16:42:13 +0000"  >&lt;p&gt;All the proposed changes sound like improvements to me. 2) sounds more conservative (less efficient but also less risky) than 1. If we could do 3) that&apos;s the best as far as accuracy is concerned; not sure how expensive it is?&lt;/p&gt;</comment>
                            <comment id="13844224" author="knutanders" created="Tue, 10 Dec 2013 12:13:36 +0000"  >&lt;p&gt;Unfortunately, the scaled-down test case in lowmem-test.diff does not work as a good repro for this bug, since it also fails with OOME if the code that grows the sort buffer is completely disabled. So that test case only shows that 16MB heap is too small when compressing a table of that shape and size. It would of course be good reduce the minimum memory requirements too, but that&apos;s not what&apos;s requested in this bug report.&lt;/p&gt;</comment>
                            <comment id="13844275" author="knutanders" created="Tue, 10 Dec 2013 13:43:41 +0000"  >&lt;p&gt;Thanks for the feedback, Dag.&lt;/p&gt;

&lt;p&gt;You&apos;re right that 3) would probably give better accuracy. I don&apos;t think it&apos;s particularly expensive, and in any case I think we&apos;d only call estimateMemoryUsage() on a single row each time the buffer is full, so it won&apos;t be called frequently.&lt;/p&gt;

&lt;p&gt;The downsides with 3) are:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;It will underestimate the memory requirements until &lt;a href=&quot;https://issues.apache.org/jira/browse/DERBY-4620&quot; title=&quot;Query optimizer causes OOM error on a complex query&quot; class=&quot;issue-link&quot; data-issue-key=&quot;DERBY-4620&quot;&gt;DERBY-4620&lt;/a&gt; is fixed, which might lead to OOME. (I&apos;ve been reluctant to fix the estimates in &lt;a href=&quot;https://issues.apache.org/jira/browse/DERBY-4620&quot; title=&quot;Query optimizer causes OOM error on a complex query&quot; class=&quot;issue-link&quot; data-issue-key=&quot;DERBY-4620&quot;&gt;DERBY-4620&lt;/a&gt;, because that might degrade the performance of existing applications unless we increase the max hash table size at the same time.)&lt;/li&gt;
	&lt;li&gt;With variable-width data types, the rows may vary greatly in size, so the estimate may be way off with that approach too if the sample row is not close to the average row size.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I experimented with 2), but it has some weaknesses too. The buffer will indeed grow more slowly with that approach, but if the heap was completely full when the sort buffer was initialized (like it is in this case), it simply means that the buffer will grow slowly until it has filled up the heap again. So it is very likely to eventually hit the ceiling and fail with OOME (and Ramin&apos;s test case does still fail with OOME when this approach is used).&lt;/p&gt;

&lt;p&gt;Another experiment I ran, one that looks more promising, is a simplified variant of 1). It changes the meaning of beginTotalMemory and beginFreeMemory so that they represent the low-water mark of the memory usage. In the common case, where the memory usage grows as the buffer fills up, they have the same meaning as today. But once it detects that the memory usage is lower than when the buffer was initialized, those fields are changed to the current state. Although this doesn&apos;t make the estimates completely accurate (they still underestimate the actual memory requirement), it makes them more accurate than they currently are. Ramin&apos;s test case succeeds when this approach is used.&lt;/p&gt;</comment>
                            <comment id="13844369" author="knutanders" created="Tue, 10 Dec 2013 15:57:00 +0000"  >&lt;p&gt;The attached patch, d5416-1a.diff, implements the variant of 1) mentioned above, where we simply replace the initial memory usage with the current memory usage if we detect that the memory usage has gone down.&lt;/p&gt;

&lt;p&gt;Although this is not a perfect solution (the memory estimates are still inaccurate), I don&apos;t think it will make the estimates worse in any situations. In the common case, it won&apos;t affect the estimates at all. And when it does affect the estimates, it changes them from &quot;way too low&quot; (negative values) to just &quot;too low&quot;.&lt;/p&gt;

&lt;p&gt;All the regression tests ran cleanly with the patch. So did Ramin&apos;s test case.&lt;/p&gt;</comment>
                            <comment id="13844644" author="dagw" created="Tue, 10 Dec 2013 20:41:10 +0000"  >&lt;p&gt;Seems like a net improvement to me, code looks good. +1&lt;/p&gt;</comment>
                            <comment id="13845286" author="jira-bot" created="Wed, 11 Dec 2013 10:57:36 +0000"  >&lt;p&gt;Commit 1550103 from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=knutanders&quot; class=&quot;user-hover&quot; rel=&quot;knutanders&quot;&gt;Knut Anders Hatlen&lt;/a&gt; in branch &apos;code/trunk&apos;&lt;br/&gt;
[ &lt;a href=&quot;https://svn.apache.org/r1550103&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://svn.apache.org/r1550103&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/DERBY-5416&quot; title=&quot;SYSCS_COMPRESS_TABLE causes an OutOfMemoryError when the heap is full at call time and then gets mostly garbage collected later on&quot; class=&quot;issue-link&quot; data-issue-key=&quot;DERBY-5416&quot;&gt;&lt;del&gt;DERBY-5416&lt;/del&gt;&lt;/a&gt;: SYSCS_COMPRESS_TABLE causes an OutOfMemoryError when the&lt;br/&gt;
heap is full at call time and then gets mostly garbage collected later&lt;br/&gt;
on.&lt;/p&gt;

&lt;p&gt;Improve the accuracy of the code that estimates the memory requirement&lt;br/&gt;
of the sort buffer. When it detects that the current memory usage is&lt;br/&gt;
lower than the initial memory usage, it now records the current usage&lt;br/&gt;
and uses that value instead of the initial memory usage in future&lt;br/&gt;
calculations.&lt;/p&gt;

&lt;p&gt;This compensates to some degree, but not fully, for the skew in the&lt;br/&gt;
estimates due to garbage collection happening after the initial memory&lt;br/&gt;
usage. The memory requirement will not be as badly underestimated, and&lt;br/&gt;
the likelihood of OutOfMemoryErrors is reduced.&lt;/p&gt;

&lt;p&gt;There is no regression test case for this bug, since the only&lt;br/&gt;
reliable, reproducible test case that we currently have, needs too&lt;br/&gt;
much time, disk space and memory to be included in the regression test&lt;br/&gt;
suite.&lt;/p&gt;</comment>
                            <comment id="13845301" author="knutanders" created="Wed, 11 Dec 2013 10:58:09 +0000"  >&lt;p&gt;Thanks for reviewing the patch, Dag. I committed it to trunk with revision 1550103.&lt;/p&gt;</comment>
                            <comment id="13913298" author="mikem" created="Wed, 26 Feb 2014 18:44:51 +0000"  >&lt;p&gt;consider for 10.10 backport&lt;/p&gt;</comment>
                            <comment id="13913316" author="mikem" created="Wed, 26 Feb 2014 18:56:18 +0000"  >&lt;p&gt;temporarily assigning myself ownership while working on 10.10 backport.&lt;/p&gt;</comment>
                            <comment id="13913354" author="jira-bot" created="Wed, 26 Feb 2014 19:15:34 +0000"  >&lt;p&gt;Commit 1572203 from mikem@apache.org in branch &apos;code/branches/10.10&apos;&lt;br/&gt;
[ &lt;a href=&quot;https://svn.apache.org/r1572203&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://svn.apache.org/r1572203&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/DERBY-5416&quot; title=&quot;SYSCS_COMPRESS_TABLE causes an OutOfMemoryError when the heap is full at call time and then gets mostly garbage collected later on&quot; class=&quot;issue-link&quot; data-issue-key=&quot;DERBY-5416&quot;&gt;&lt;del&gt;DERBY-5416&lt;/del&gt;&lt;/a&gt;: SYSCS_COMPRESS_TABLE causes an OutOfMemoryError when the&lt;br/&gt;
heap is full at call time and then gets mostly garbage collected later&lt;br/&gt;
on.&lt;/p&gt;

&lt;p&gt;backporting change #1550103 from trunk to 10.10.&lt;/p&gt;

&lt;p&gt;Improve the accuracy of the code that estimates the memory requirement&lt;br/&gt;
of the sort buffer. When it detects that the current memory usage is&lt;br/&gt;
lower than the initial memory usage, it now records the current usage&lt;br/&gt;
and uses that value instead of the initial memory usage in future&lt;br/&gt;
calculations.&lt;/p&gt;

&lt;p&gt;This compensates to some degree, but not fully, for the skew in the&lt;br/&gt;
estimates due to garbage collection happening after the initial memory&lt;br/&gt;
usage. The memory requirement will not be as badly underestimated, and&lt;br/&gt;
the likelihood of OutOfMemoryErrors is reduced.&lt;/p&gt;

&lt;p&gt;There is no regression test case for this bug, since the only&lt;br/&gt;
reliable, reproducible test case that we currently have, needs too&lt;br/&gt;
much time, disk space and memory to be included in the regression test&lt;br/&gt;
suite.&lt;/p&gt;</comment>
                            <comment id="13913355" author="mikem" created="Wed, 26 Feb 2014 19:16:39 +0000"  >&lt;p&gt;opening to work on backport to 10.10&lt;/p&gt;</comment>
                            <comment id="13913614" author="mikem" created="Wed, 26 Feb 2014 22:21:58 +0000"  >&lt;p&gt;backported to 10.10.  This is as far as I plan to backport right now, but could be backported more if anyone is interested.&lt;/p&gt;
</comment>
                            <comment id="14284751" author="myrna" created="Wed, 21 Jan 2015 00:23:08 +0000"  >&lt;p&gt;bulk change to close all issues resolved but not closed and not changed since June 1, 2014.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12599854">DERBY-5876</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12695601">DERBY-6481</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12494789" name="compress_test_5416.patch" size="3229" author="rbaradari" created="Fri, 16 Sep 2011 14:07:01 +0100"/>
                            <attachment id="12618053" name="d5416-1a.diff" size="4207" author="knutanders" created="Tue, 10 Dec 2013 15:57:00 +0000"/>
                            <attachment id="12617853" name="lowmem-test.diff" size="6239" author="knutanders" created="Mon, 9 Dec 2013 16:29:49 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310200" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Bug behavior facts</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10365"><![CDATA[Crash]]></customfieldvalue>
    <customfieldvalue key="10421"><![CDATA[Seen in production]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Fri, 16 Sep 2011 13:54:47 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>24832</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12310090" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Issue &amp; fix info</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10424"><![CDATA[Repro attached]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hy0b0v:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>35604</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12310050" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Urgency</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10052"><![CDATA[Normal]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>
</channel>
</rss>