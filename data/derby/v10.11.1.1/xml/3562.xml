<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 03:46:24 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/DERBY-3562/DERBY-3562.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[DERBY-3562] Number of log files (and log dir size) on the slave increases continuously</title>
                <link>https://issues.apache.org/jira/browse/DERBY-3562</link>
                <project id="10594" key="DERBY">Derby</project>
                    <description>&lt;p&gt;I did a simple test inserting tuples in a table during replication:&lt;/p&gt;

&lt;p&gt;The attached file &apos;master_slave-db_size-6.jpg&apos; shows that &lt;br/&gt;
the size of the log directory (and number of files in the log directory)&lt;br/&gt;
increases continuously during replication, while on master the size &lt;br/&gt;
(and number of files) never exceeds ~12Mb (12 files?) in this scenario.&lt;/p&gt;

&lt;p&gt;The seg0 directory on the slave stays at the same size as the master &lt;br/&gt;
seg0 directory.&lt;/p&gt;</description>
                <environment>-</environment>
        <key id="12391994">DERBY-3562</key>
            <summary>Number of log files (and log dir size) on the slave increases continuously</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="jorgenlo">J&#248;rgen L&#248;land</assignee>
                                    <reporter username="olesolberg">Ole Solberg</reporter>
                        <labels>
                    </labels>
                <created>Thu, 20 Mar 2008 19:40:04 +0000</created>
                <updated>Fri, 21 Jan 2011 17:51:49 +0000</updated>
                            <resolved>Wed, 2 Apr 2008 14:01:19 +0100</resolved>
                                    <version>10.4.1.3</version>
                    <version>10.5.1.1</version>
                                    <fixVersion>10.4.1.3</fixVersion>
                    <fixVersion>10.5.1.1</fixVersion>
                                    <component>Replication</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="12581024" author="narayanan" created="Fri, 21 Mar 2008 07:58:47 +0000"  >&lt;p&gt;Not sure if the analysis here points at the rate of increase in the slave or the increase&lt;br/&gt;
in the slave relative to the master. &lt;/p&gt;

&lt;p&gt;But isn&apos;t an increase in the log size of a slave expected, because the Slave receives log &lt;br/&gt;
records from the master and use LogToFile#appendLogRecord to write these to disk. &lt;br/&gt;
So basically the slave uses the logs received from the master to recover and move to a &lt;br/&gt;
transaction consistent state.&lt;/p&gt;

&lt;p&gt;Incase you have not seen the following points from Derby-3071 (Modify logging subsystem for slave replication mode)&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;SlaveFactory (&lt;a href=&quot;https://issues.apache.org/jira/browse/DERBY-3021&quot; title=&quot;Replication: Add a ReplicationSlave controller that will manage replication on the slave side&quot; class=&quot;issue-link&quot; data-issue-key=&quot;DERBY-3021&quot;&gt;&lt;del&gt;DERBY-3021&lt;/del&gt;&lt;/a&gt;) will receive log records from the master and use LogToFile#appendLogRecord to write these to disk. While in slave mode, only SlaveFactory will be allowed to append log records.&lt;/li&gt;
	&lt;li&gt;The thread running LogToFile#recover will recover (redo) one log file at a time (like now), but will not be allowed to open a log file X until that file is no longer being written to. Thus, while appenLogFile writes to logX.dat, recover will be allowed to read all log files up to and including logX-1.dat but will then have to wait until appendLogRecord starts writing to logX+1.dat.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12581046" author="oysteing" created="Fri, 21 Mar 2008 10:05:27 +0000"  >&lt;p&gt;I think the issue may be that checkpointing is only happening at the master.  Hence, log files are garbage-collected on the master, but not on the slave.  If that is the case, one risk that with a long-running replication, the disk on the slave side will fill up.  I do not think this is acceptable, and we need to look into how the recovery thread can be modified to garbage collect log files when checkpoint log records are encountered.&lt;/p&gt;</comment>
                            <comment id="12581816" author="jorgenlo" created="Tue, 25 Mar 2008 07:55:42 +0000"  >&lt;p&gt;&amp;gt; I do not think this is acceptable, and we need to look into how the recovery thread can be modified to garbage collect log files when checkpoint log records are encountered.&lt;/p&gt;

&lt;p&gt;I agree. This has to be fixed. &lt;/p&gt;

&lt;p&gt;I think it will be possible to use the checkpoints received from the master to discard old log files on the slave. The checkpoint contains a low water mark pointing to the oldest log record that needs to be kept. Log older than the LWM should be discardable on the slave as well. Need some more investigation, though...&lt;/p&gt;</comment>
                            <comment id="12581946" author="jorgenlo" created="Tue, 25 Mar 2008 14:04:59 +0000"  >&lt;p&gt;When a checkpoint operation is encountered on the slave, cached data blocks are synced (flushed to disk). The log.ctrl file is then updated to point to this cp. It appears that the only thing needed is to call truncateLog with the low water mark of the cp. Manual tests confirm this (failover succeeds on the slave and old log files are removed continuously) , but I haven&apos;t run all the tests yet. &lt;/p&gt;</comment>
                            <comment id="12582163" author="jorgenlo" created="Wed, 26 Mar 2008 07:52:34 +0000"  >&lt;p&gt;Patch 1a deletes unnecessary, old log files on the slave when a checkpoint is encountered in the log received from the master. All tests (except those failing tinderbox) passed, including the replication test suite.&lt;/p&gt;

&lt;p&gt;Requesting review.&lt;/p&gt;</comment>
                            <comment id="12582949" author="jorgenlo" created="Fri, 28 Mar 2008 09:15:34 +0000"  >&lt;p&gt;Mike Matrigali writes:&lt;br/&gt;
&amp;gt; I didn&apos;t quite follow all of this, and admit i am not up on replication.&lt;br/&gt;
&amp;gt; It would be nice if this process was the exact code as the normal&lt;br/&gt;
&amp;gt; checkpoint processing.  So a checkpoint would be triggered and then&lt;br/&gt;
&amp;gt; after it had done it&apos;s work it would do the appropriate cleanup.  If you&lt;br/&gt;
&amp;gt; do the cleanup too soon then redo recovery of the slave won&apos;t work - is&lt;br/&gt;
&amp;gt; that expected to work or at that point to you just restart from scratch&lt;br/&gt;
&amp;gt; from master.&lt;/p&gt;

&lt;p&gt;&amp;gt; The existing code that replay&apos;s multiple checkpoints may be wierd as it&lt;br/&gt;
&amp;gt; may assume that this is recovery of a backed up database that is meant&lt;br/&gt;
&amp;gt; to keep all of it&apos;s log files.  Make sure to not break that.&lt;/p&gt;

&lt;p&gt;&amp;gt; Is there a concept of a &quot;fully&quot; recoverable slave, ie. one that is&lt;br/&gt;
&amp;gt; supposed to keep all of it&apos;s log files so that it is recoverable in&lt;br/&gt;
&amp;gt; case of a data crash.  As I said may not be necessary as there is&lt;br/&gt;
&amp;gt; always the master.  Just good to know what is expected.&lt;br/&gt;
Mike,&lt;/p&gt;

&lt;p&gt;Thank you for expressing your concerns. I&apos;ll do my best to explain why I think the proposed solution will work.&lt;/p&gt;

&lt;p&gt;The patch adds functionality to the checkpoint processing used during recovery (LogToFile#checkpointInRFR). During recovery, the dirty data pages are flushed to disk, and the log.ctrl file is updated to point to the new checkpoint currently being processed.&lt;/p&gt;

&lt;p&gt;With the patch &lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;, the log files that are older than the currently processed checkpoint&apos;s Undo Low Water Mark (undo LWM) are then deleted. The undo LWM points to the earliest log record that may be required to do recovery &lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt;. Since the log files are processed sequentially and the data pages have been flushed, the undo LWM in the checkpoint is equally valid during recovery (aka slave replication mode) as during normal transaction processing.&lt;/p&gt;

&lt;p&gt;Once replication has successfully started, the slave database will always be recoverable &lt;span class=&quot;error&quot;&gt;&amp;#91;3&amp;#93;&lt;/span&gt;, but not in case of corrupted data blocks &lt;span class=&quot;error&quot;&gt;&amp;#91;4&amp;#93;&lt;/span&gt;. You may at any time crash the Derby serving the slave database and then reboot it. The used-to-be-slave database will then recover to a transaction consistent state including the modifications from all transactions whose commit log record was written to disk on the slave before the crash.&lt;/p&gt;

&lt;p&gt;Please follow up if you think I may have misunderstood anything or did not answer your questions good enough.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; The patch only applies to slave replication mode. Backup is not affected as to not break the &quot;fully&quot; recoverability feature for backups.&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt; The first log record of the oldest transaction in the checkpoint&apos;s transaction table.&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;3&amp;#93;&lt;/span&gt; If &quot;fully&quot; recoverable means recovering in presence of corrupted data blocks, this is currently not supported for replication.&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;4&amp;#93;&lt;/span&gt; Not including jar files, as explained in &lt;a href=&quot;https://issues.apache.org/jira/browse/DERBY-3552&quot; title=&quot;Handle jar files that are installed when replication is enabled&quot; class=&quot;issue-link&quot; data-issue-key=&quot;DERBY-3552&quot;&gt;&lt;del&gt;DERBY-3552&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="12584552" author="oysteing" created="Wed, 2 Apr 2008 14:01:19 +0100"  >&lt;p&gt;Thanks for the patch J&#248;rgen.  I have verified that log files now get garbage collected at slave.&lt;br/&gt;
Merged to trunk at revision 643336.&lt;br/&gt;
Merged to 10.4 branch at revision 643892.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12378611" name="derby-3562-1a.diff" size="5036" author="jorgenlo" created="Wed, 26 Mar 2008 07:52:34 +0000"/>
                            <attachment id="12378612" name="derby-3562-1a.stat" size="432" author="jorgenlo" created="Wed, 26 Mar 2008 07:52:34 +0000"/>
                            <attachment id="12378338" name="master_slave-db_size-6.jpg" size="48476" author="olesolberg" created="Thu, 20 Mar 2008 19:40:47 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Fri, 21 Mar 2008 07:58:47 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>23714</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hy0lhz:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>37301</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                            </customfields>
    </item>
</channel>
</rss>