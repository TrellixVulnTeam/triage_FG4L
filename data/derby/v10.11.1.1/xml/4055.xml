<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 03:37:36 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/DERBY-4055/DERBY-4055.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[DERBY-4055] Space may not be reclaimed if  row locks are not available after three retries </title>
                <link>https://issues.apache.org/jira/browse/DERBY-4055</link>
                <project id="10594" key="DERBY">Derby</project>
                    <description>&lt;p&gt;In a multithreaded clob update where the same row is being updated, space will not be reclaimed.  The offending code is in ReclaimSpaceHelper:&lt;/p&gt;

&lt;p&gt;	RecordHandle headRecord = work.getHeadRowHandle();&lt;/p&gt;

&lt;p&gt;		if (!container_rlock.lockRecordForWrite(&lt;br/&gt;
                tran, headRecord, false /* not insert &lt;b&gt;/, false /&lt;/b&gt; nowait */))&lt;br/&gt;
		{&lt;br/&gt;
			// cannot get the row lock, retry&lt;br/&gt;
			tran.abort();&lt;br/&gt;
			if (work.incrAttempts() &amp;lt; 3)&lt;/p&gt;
            {
				return Serviceable.REQUEUE;
            }&lt;br/&gt;
			else&lt;br/&gt;
            {&lt;br/&gt;
                // If code gets here, the space will be lost forever, and&lt;br/&gt;
                // can only be reclaimed by a full offline compress of the&lt;br/&gt;
                // table/index.&lt;br/&gt;
&lt;br/&gt;
                if (SanityManager.DEBUG)&lt;br/&gt;
                {&lt;br/&gt;
                    if (SanityManager.DEBUG_ON(DaemonService.DaemonTrace))&lt;br/&gt;
                    {
                        SanityManager.DEBUG(
                            DaemonService.DaemonTrace, 
                            &quot;  gave up after 3 tries to get row lock &quot; + 
                            work);
                    }&lt;br/&gt;
                }&lt;br/&gt;
				return Serviceable.DONE;&lt;br/&gt;
            }&lt;br/&gt;
		}&lt;br/&gt;
&lt;br/&gt;
&lt;br/&gt;
If we cannot get the lock after three tries we give up.  The reproduction for this issue is in the test store.ClobReclamationTest.xtestMultiThreadUpdateSingleRow().&lt;br/&gt;
&lt;br/&gt;
&lt;br/&gt;
This issue also used to reference the code below and has some references to trying to get a reproduction for that issue, but that work has moved to &lt;a href=&quot;https://issues.apache.org/jira/browse/DERBY-4054&quot; title=&quot;Multithreaded clob update with exclusive table locking causes table growth that is not reclaimed&quot; class=&quot;issue-link&quot; data-issue-key=&quot;DERBY-4054&quot;&gt;DERBY-4054&lt;/a&gt;.  Please see &lt;a href=&quot;https://issues.apache.org/jira/browse/DERBY-4054&quot; title=&quot;Multithreaded clob update with exclusive table locking causes table growth that is not reclaimed&quot; class=&quot;issue-link&quot; data-issue-key=&quot;DERBY-4054&quot;&gt;DERBY-4054&lt;/a&gt; for work on the container lock issue.&lt;br/&gt;
&lt;br/&gt;
ContainerHandle containerHdl = &lt;br/&gt;
			openContainerNW(tran, container_rlock, work.getContainerId());&lt;br/&gt;
&lt;br/&gt;
		if (containerHdl == null)&lt;br/&gt;
		{&lt;br/&gt;
			tran.abort();&lt;br/&gt;
&lt;br/&gt;
			if (SanityManager.DEBUG)&lt;br/&gt;
            {&lt;br/&gt;
                if (SanityManager.DEBUG_ON(DaemonService.DaemonTrace))&lt;br/&gt;
                {
                    SanityManager.DEBUG(
                        DaemonService.DaemonTrace, &quot; aborted &quot; + work + 
                        &quot; because container is locked or dropped&quot;);
                }&lt;br/&gt;
            }&lt;br/&gt;
&lt;br/&gt;
			if (work.incrAttempts() &amp;lt; 3) // retry this for serveral times&lt;br/&gt;
            {
				return Serviceable.REQUEUE;
            }
&lt;p&gt;			else&lt;br/&gt;
            {&lt;br/&gt;
                // If code gets here, the space will be lost forever, and&lt;br/&gt;
                // can only be reclaimed by a full offline compress of the&lt;br/&gt;
                // table/index.&lt;/p&gt;

&lt;p&gt;                if (SanityManager.DEBUG)&lt;br/&gt;
                {&lt;br/&gt;
                    if (SanityManager.DEBUG_ON(DaemonService.DaemonTrace))&lt;/p&gt;
                    {
                        SanityManager.DEBUG(
                            DaemonService.DaemonTrace, 
                            &quot;  gave up after 3 tries to get container lock &quot; + 
                            work);
                    }
&lt;p&gt;                }&lt;/p&gt;

&lt;p&gt;				return Serviceable.DONE;&lt;br/&gt;
            }&lt;br/&gt;
		}	&lt;/p&gt;

</description>
                <environment></environment>
        <key id="12414691">DERBY-4055</key>
            <summary>Space may not be reclaimed if  row locks are not available after three retries </summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png" description="The issue is open and ready for the assignee to start work on it.">Open</status>
                    <statusCategory id="2" key="new" colorName="blue-gray"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="kmarsden">Kathey Marsden</reporter>
                        <labels>
                            <label>derby_triage10_11</label>
                    </labels>
                <created>Thu, 12 Feb 2009 19:41:30 +0000</created>
                <updated>Wed, 3 Jul 2013 16:26:43 +0100</updated>
                                            <version>10.1.3.1</version>
                    <version>10.2.2.0</version>
                    <version>10.3.3.0</version>
                    <version>10.4.2.0</version>
                    <version>10.5.1.1</version>
                                                    <component>Store</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>2</watches>
                                                                <comments>
                            <comment id="12673044" author="kmarsden" created="Thu, 12 Feb 2009 20:29:02 +0000"  >&lt;p&gt;With revision 743867, checked in reproduction for the case where it cannot get the row lock after three tries.&lt;br/&gt;
enable store.ClobReclamationTest.xtestMultiThreadUpdateSingleRow()&lt;br/&gt;
If multiple threads are trying to update the same row, reclamation doesn&apos;t happen and with derby.debug.true=Daemon trace see in the log many instances of:&lt;br/&gt;
DEBUG DaemonTrace OUTPUT:   gave up after 3 tries to get row lock Reclaim COLUMN_CHAIN...&lt;/p&gt;


</comment>
                            <comment id="12673134" author="kmarsden" created="Fri, 13 Feb 2009 03:57:47 +0000"  >&lt;p&gt;I noticed in the code coverage output that we do cover the case where we give  up after 3 tries to get container lock, so I put in an ASSERT and ran the store tests and found it pops with T_RawStoreFactory.unit. Attached is the derby.log with the assertion.  I am not sure how the failing   unit tests  would translate into a user case.  I&apos;ll ponder that tomorrow unless it becomes obvious to someone else in the meanwhile.&lt;/p&gt;</comment>
                            <comment id="12673325" author="kmarsden" created="Fri, 13 Feb 2009 17:54:44 +0000"  >&lt;p&gt;I looked at one of the test cases that triggered my assertion,  P703.  In this case the test drops the container before commit.&lt;br/&gt;
if (segment != ContainerHandle.TEMPORARY_SEGMENT) &lt;/p&gt;
{
			t_util.t_dropContainer(t, segment, cid);	// cleanup
		}

&lt;p&gt;		t.commit();&lt;/p&gt;

&lt;p&gt;So I think that&apos;s why we give up.  I don&apos;t really see how I could emulate this in JDBC and it would be the lock case that I would want to trigger anyway.  So, I don&apos;t think this unit test gives me a clue how to reproduce the  &quot;  gave up after 3 tries to get container lock &quot; case with JDBC.&lt;/p&gt;



</comment>
                            <comment id="12673401" author="kmarsden" created="Fri, 13 Feb 2009 22:20:49 +0000"  >&lt;p&gt;For the row lock case, changing container_rlock.lockRecordForWrite to wait for the lock seemed to correct the problem, but I don&apos;t know what ramifications that might have.&lt;/p&gt;

&lt;p&gt;Always requeuing the request  if we couldn&apos;t get the lock seemed to work only if I put a sleep in before closing the connection to give the post commit tasks a chance to catch up. I had to sleep over a minute to get 10,000 updates by each of two threads to catch up.  Even then I had a lot of free pages, so the space would ultimately get reclaimed, but  we&apos;d still use a lot of space.  In a very active system I could also see the post commit tasks piling up and becoming a resource issue, so this solution doesn&apos;t seem like a good option.&lt;/p&gt;

&lt;p&gt;Questions:&lt;/p&gt;

&lt;p&gt;1) Is it expected that the cleanup tasks will just be aborted if if the connection is closed or is that another bug?&lt;/p&gt;

&lt;p&gt;2) Anyone have any ideas on an acceptable approach to make sure our reclamation  gets done without causing unacceptable lock contention?&lt;/p&gt;</comment>
                            <comment id="12673436" author="mikem" created="Sat, 14 Feb 2009 00:29:45 +0000"  >&lt;p&gt;Wait on the lock or bump on the retry are the only &quot;easy&quot; bug fixes I can &lt;br/&gt;
think of.  There are downsides to both.&lt;/p&gt;

&lt;p&gt;wait on lock:&lt;br/&gt;
Currently we only have a single background thread to do these kinds of tasks,&lt;br/&gt;
it is single threaded through each task.  So if you wait on lock, then every&lt;br/&gt;
background task waits until that can get finished.  Stuff like checkpoints,&lt;br/&gt;
and other space reclamation tasks will wait.  The queue may grow and become&lt;br/&gt;
unmanageable.&lt;/p&gt;

&lt;p&gt;bump retry:&lt;br/&gt;
Could lead to just cpu spinning and no actual guarantee of getting the resource&lt;br/&gt;
as it could keep getting unlucky and lose a cycle schedule when the resource&lt;br/&gt;
is actually available.&lt;/p&gt;

&lt;p&gt;Bigger feature projects could do resolve these issues.&lt;/p&gt;

&lt;p&gt;1) Enhance the background daemon schedule utility to support more than one&lt;br/&gt;
   thread.  &lt;br/&gt;
   o I think it is important that we don&apos;t just randomly grow the number&lt;br/&gt;
     of background threads, as we already have complaints about the one&lt;br/&gt;
     background thread per db.  But it would be fine to short term allocate &lt;br/&gt;
     a thread and then destroy it.&lt;br/&gt;
   o not sure exactly what is best, but in this case it would be nice to &lt;br/&gt;
     be able to REQUE the post commit task to the daemon saying add it to the&lt;br/&gt;
     WAIT allowed queue.  At that point maybe the daemon starts a new thread&lt;br/&gt;
     for each one or maybe for each N or something else.  Maybe it is &lt;br/&gt;
     configurable.&lt;br/&gt;
   o There are already other problems for which this may be the solution:&lt;/p&gt;

&lt;p&gt;     1) As number of cores/cpu&apos;s grow it becomes obvious that one background&lt;br/&gt;
        thread to N possible concurrent user thread which could each generate&lt;br/&gt;
        work is not correct.&lt;br/&gt;
     2) There are some tasks that are even more critical than others that could&lt;br/&gt;
        benefit from better priority.  Things like checkpoint may want their&lt;br/&gt;
        own thread rather than share with others.&lt;/p&gt;


&lt;p&gt;2) The problem with space being lost &quot;forever&quot; (which really means until an&lt;br/&gt;
   offline compress, in part stems from the row format of the overflow pieces.&lt;/p&gt;

&lt;p&gt;   o we could change the overflow pieces to have back pointers to the main&lt;br/&gt;
     head page which would make it much easier to figure out a stranded piece&lt;br/&gt;
     during online compress.&lt;/p&gt;

&lt;p&gt;   o We could write some sort of brute force search which could come up with&lt;br/&gt;
     the stranded pieces and call reclaim on them.  It is actually not that&lt;br/&gt;
     hard of code to write if one doesn&apos;t worry about memory.  It would go&lt;br/&gt;
     something like - probably many ways to optimize it. :&lt;/p&gt;

&lt;p&gt;     for (every page at raw store level)&lt;br/&gt;
     {&lt;br/&gt;
         if (main page)&lt;br/&gt;
         {&lt;br/&gt;
             for every row on page&lt;/p&gt;
             {
                 if (has an overflow piece)
                     add main row handle to main hash table, and overflow handle
             }
&lt;p&gt;         }&lt;/p&gt;

&lt;p&gt;         if (overflow page)&lt;br/&gt;
         {&lt;br/&gt;
             for every row on page&lt;/p&gt;
             {
                 add main row handle, and next overflow handle
             }
&lt;p&gt;         }&lt;/p&gt;

&lt;p&gt;     }&lt;/p&gt;

&lt;p&gt;     for (every row in main hash table)&lt;/p&gt;
     {
         delete complete overflow chain from overflow hash table
     }

&lt;p&gt;     I believe this leaves the disconnected chains in the overflow hash table,&lt;br/&gt;
     with some graph algorithm to determine the head of the chains.&lt;/p&gt;

&lt;p&gt;     The above works for X lock on the whole table, I am not sure if row locking&lt;br/&gt;
     works.  If you have to get X lock then an offline compress is not that much&lt;br/&gt;
     worse which is why it never got implemented.&lt;/p&gt;

&lt;p&gt;3) This post commit work is all still really at the first implmentation level,&lt;br/&gt;
   when most choices were what was simple and mostly work.  Not much has &lt;br/&gt;
   happened since then.  Also in first implementation big CLOB&apos;s and BLOB&apos;s were&lt;br/&gt;
   not even implemented so the downside of losing some stuff was not much.&lt;/p&gt;

&lt;p&gt;   As Kathey pointed out I think there are holes in a lot of the post commit&lt;br/&gt;
   stuff where a crash can lose them.  It seems the only way to make sure we&lt;br/&gt;
   don&apos;t lose them is to somehow store them transactionally in the db, but at&lt;br/&gt;
   what cost?  We could implement some sort of internal table, and then store&lt;br/&gt;
   rows with the post commit work.&lt;/p&gt;

&lt;p&gt;Personally I think 1 and 2 are better paths than 3.  It would be interesting&lt;br/&gt;
to know what other db&apos;s do.  I am pretter sure postgress for instance rely&apos;s&lt;br/&gt;
on scheduling a &quot;scrubber&quot; to do a lot of what we do in post commit.  I would&lt;br/&gt;
rather see us make changes that would allow online compress table to handle&lt;br/&gt;
these overflow losses and then enhance the post commit deamon to better handle&lt;br/&gt;
them asap without ever needing the compress table.&lt;/p&gt;
</comment>
                            <comment id="12680196" author="kmarsden" created="Mon, 9 Mar 2009 16:56:23 +0000"  >&lt;p&gt;I was working with a user who found they could work around this issue with use of better indexing. They had a simultaneous update and select which were accessing different rows, but still they saw this problem occur.  Putting an index on the column from which they were doing the select avoided a table scan and thus avoided the issue.  Hopefully this will work as a workaround for others until we can get this issue fixed.&lt;/p&gt;</comment>
                            <comment id="12726871" author="knutanders" created="Fri, 3 Jul 2009 11:20:45 +0100"  >&lt;p&gt;Triaged for 10.5.2.&lt;/p&gt;</comment>
                            <comment id="13212437" author="mikem" created="Tue, 21 Feb 2012 08:06:33 +0000"  >&lt;p&gt;Triaged for 10.9, no changes.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12414163">DERBY-4050</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12515565">DERBY-5356</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12599854">DERBY-5876</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12400145" name="derby.log.T_RawStoreFactoryWithAssert" size="750736" author="kmarsden" created="Fri, 13 Feb 2009 03:57:47 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Sat, 14 Feb 2009 00:29:45 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>23997</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12310090" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Issue &amp; fix info</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10422"><![CDATA[High Value Fix]]></customfieldvalue>
    <customfieldvalue key="10427"><![CDATA[Workaround attached]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hy0azj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>35598</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310050" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Urgency</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10052"><![CDATA[Normal]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>
</channel>
</rss>