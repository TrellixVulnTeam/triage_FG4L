<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 03:35:10 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/DERBY-5108/DERBY-5108.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[DERBY-5108] Intermittent failure in AutomaticIndexStatisticsTest.testShutdownWhileScanningThenDelete on Windows</title>
                <link>https://issues.apache.org/jira/browse/DERBY-5108</link>
                <project id="10594" key="DERBY">Derby</project>
                    <description>&lt;p&gt;The test AutomaticIndexStatisticsTest.testShutdownWhileScanningThenDelete fails intermittently on Windows platforms because the test is unable to delete a database directory.&lt;br/&gt;
Even after several retries and sleeps (the formula should be (attempt -1) * 2000, resulting in a total sleep time of 12 seconds), the conglomerate system\singleUse\copyShutdown\seg0\c481.dat cannot be deleted.&lt;/p&gt;

&lt;p&gt;For instance from &lt;a href=&quot;http://dbtg.foundry.sun.com/derby/test/Daily/jvm1.6/testing/testlog/w2003/1078855-suitesAll_diff.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://dbtg.foundry.sun.com/derby/test/Daily/jvm1.6/testing/testlog/w2003/1078855-suitesAll_diff.txt&lt;/a&gt; :&lt;br/&gt;
(truncated paths)&lt;br/&gt;
testShutdownWhileScanningThenDelete &amp;lt;assertDirectoryDeleted&amp;gt; attempt 1 left 3 files/dirs behind: 0=system\singleUse\copyShutdown\seg0\c481.dat 1=system\singleUse\copyShutdown\seg0 2=system\singleUse\copyShutdown&lt;br/&gt;
&amp;lt;assertDirectoryDeleted&amp;gt; attempt 2 left 3 files/dirs behind: 0=system\singleUse\copyShutdown\seg0\c481.dat 1=system\singleUse\copyShutdown\seg0 2=system\singleUse\copyShutdown&lt;br/&gt;
&amp;lt;assertDirectoryDeleted&amp;gt; attempt 3 left 3 files/dirs behind: 0=system\singleUse\copyShutdown\seg0\c481.dat 1=system\singleUse\copyShutdown\seg0 2=system\singleUse\copyShutdown&lt;br/&gt;
&amp;lt;assertDirectoryDeleted&amp;gt; attempt 4 left 3 files/dirs behind: 0=system\singleUse\copyShutdown\seg0\c481.dat 1=system\singleUse\copyShutdown\seg0 2=system\singleUse\copyShutdown&lt;br/&gt;
used 205814 ms F.&lt;/p&gt;

&lt;p&gt;Maybe the database isn&apos;t shut down, or some specific timing of events causes a file to be reopened when it shouldn&apos;t have been (i.e. after the database shutdown has been initiated).&lt;/p&gt;</description>
                <environment>Windows platforms.</environment>
        <key id="12500787">DERBY-5108</key>
            <summary>Intermittent failure in AutomaticIndexStatisticsTest.testShutdownWhileScanningThenDelete on Windows</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.png">Blocker</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="kristwaa">Kristian Waagan</assignee>
                                    <reporter username="kristwaa">Kristian Waagan</reporter>
                        <labels>
                    </labels>
                <created>Tue, 8 Mar 2011 17:46:14 +0000</created>
                <updated>Tue, 25 Oct 2011 18:09:39 +0100</updated>
                            <resolved>Fri, 9 Sep 2011 22:01:30 +0100</resolved>
                                    <version>10.8.1.2</version>
                                    <fixVersion>10.8.2.2</fixVersion>
                    <fixVersion>10.9.1.0</fixVersion>
                                    <component>Services</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="13004107" author="kmarsden" created="Tue, 8 Mar 2011 18:12:12 +0000"  >&lt;p&gt;I saw this on weme 6.2  10.8.0.1 alpha - (1079089) 3/7/2011&lt;/p&gt;
</comment>
                            <comment id="13004116" author="myrna" created="Tue, 8 Mar 2011 18:28:37 +0000"  >&lt;p&gt;I saw this with ibm 1.6 - 10.8.0.1 alpha (1078654), on windows XP.&lt;/p&gt;</comment>
                            <comment id="13004683" author="mikem" created="Wed, 9 Mar 2011 17:57:35 +0000"  >&lt;p&gt;more detail on the 3/7 weme occurrence:&lt;br/&gt;
&lt;a href=&quot;http://people.apache.org/~myrnavl/derby_test_results/main/windows/testlog/weme6.2/1079089-suites.All_diff.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://people.apache.org/~myrnavl/derby_test_results/main/windows/testlog/weme6.2/1079089-suites.All_diff.txt&lt;/a&gt;&lt;br/&gt;
There was 1 failure:&lt;br/&gt;
1) testShutdownWhileScanningThenDelete(org.apache.derbyTesting.functionTests.tests.store.AutomaticIndexStatisticsTest)junit.framework.AssertionFailedError: Failed to delete 3 files (root=C:\jartest\JarResults.2011-03-07\weme6.2_suites.All\system\singleUse\copyShutdown: C:\jartest\JarResults.2011-03-07\weme6.2_suites.All\system\singleUse\copyShutdown\seg0\c481.dat (isDir=false, canRead=true, canWrite=true, size=22351872), C:\jartest\JarResults.2011-03-07\weme6.2_suites.All\system\singleUse\copyShutdown\seg0 (isDir=true, canRead=true, canWrite=true, size=0), C:\jartest\JarResults.2011-03-07\weme6.2_suites.All\system\singleUse\copyShutdown (isDir=true, canRead=true, canWrite=true, size=0)&lt;br/&gt;
	at org.apache.derbyTesting.junit.BaseJDBCTestCase.assertDirectoryDeleted(BaseJDBCTestCase.java:1526)&lt;br/&gt;
	at org.apache.derbyTesting.functionTests.tests.store.AutomaticIndexStatisticsTest.testShutdownWhileScanningThenDelete(AutomaticIndexStatisticsTest.java:188)&lt;br/&gt;
	at java.lang.reflect.AccessibleObject.invokeV(AccessibleObject.java:195)&lt;br/&gt;
	at org.apache.derbyTesting.junit.BaseTestCase.runBare(BaseTestCase.java:112)&lt;br/&gt;
	at junit.extensions.TestDecorator.basicRun(TestDecorator.java:22)&lt;br/&gt;
	at junit.extensions.TestSetup$1.protect(TestSetup.java:19)&lt;br/&gt;
	at junit.extensions.TestSetup.run(TestSetup.java:23)&lt;br/&gt;
	at org.apache.derbyTesting.junit.BaseTestSetup.run(BaseTestSetup.java:57)&lt;br/&gt;
	at junit.extensions.TestDecorator.basicRun(TestDecorator.java:22)&lt;br/&gt;
	at junit.extensions.TestSetup$1.protect(TestSetup.java:19)&lt;br/&gt;
	at junit.extensions.TestSetup.run(TestSetup.java:23)&lt;/p&gt;
</comment>
                            <comment id="13004812" author="mikem" created="Wed, 9 Mar 2011 21:47:12 +0000"  >&lt;p&gt;This issue seems to be reproducible in my environment.  3 out of 3 times so far just running the AutomaticIndexStatisticsTest alone - I will try more later.  As a test I bumped up the retry to 100 and went to lunch and came back to the test still stuck trying to delete the same file.  So it seems likely a bug in the code rather than not enough wait time in the test.  I am going to attach a java core I took while it was looping.  I see a derby.rawStoreDaemon but have not debugged enough to know if this is expected or not in this case.  I don&apos;t know if the harness when running just this test case might still create the default db and leave it up while then creating and starting a separate one for the single use database case that this test uses.&lt;/p&gt;</comment>
                            <comment id="13004815" author="mikem" created="Wed, 9 Mar 2011 21:49:12 +0000"  >&lt;p&gt;snapshot of system looping while trying to delete last file in the seg0 directory.&lt;/p&gt;</comment>
                            <comment id="13005396" author="mikem" created="Thu, 10 Mar 2011 23:24:41 +0000"  >&lt;p&gt;I am actively working on this one.  Right now I am concentrating on SANE, classes, windows failure of just this one fixture.  For me on my laptop this fails every time.&lt;br/&gt;
Unless anyone protests I am going to check in a change that disables just this one fixture while I am working on it.  I have verified that the file that is the problem is the&lt;br/&gt;
data file that istat is scanning when the shutdown happens.  It stuck around for over an hour so I am going to assume the resource is stuck open at least until the thread&lt;br/&gt;
that started the server goes away and in worst case until the jvm comes down.  I &lt;/p&gt;

&lt;p&gt;I am marking this a regression, because a user previous to 10.8 that shuts down when his threads are doing nothing won&apos;t see this.  But in 10.8 istat might be running&lt;br/&gt;
and he will run into it.  I could see this leading to problems with subsquent ddl which has to do deletes or renames of the associated table.&lt;/p&gt;

&lt;p&gt;My current guess is that &lt;a href=&quot;https://issues.apache.org/jira/browse/DERBY-5037&quot; title=&quot;Assertion failure from index-stat-thread when running AutomaticIndexStatisticsTest&quot; class=&quot;issue-link&quot; data-issue-key=&quot;DERBY-5037&quot;&gt;&lt;del&gt;DERBY-5037&lt;/del&gt;&lt;/a&gt; fixed the symptom but shutdown is still failing and leaving a real resource problem.  I still need to do some more debugging to &lt;br/&gt;
verify what is going on.  In my case I am consistently getting the ASSERT.  But I believe we see the issue in nightly runs against SANE=false so it is not specific to stuff&lt;br/&gt;
we do in ASSERT logic.  I am hoping that fixing the SANE path will apply to the SANE=false path also.  &lt;/p&gt;

&lt;p&gt;Logically it seems like what should happen on &quot;clean&quot; shutdown is first user threads should be shutdown, then istat, and finally store.  I wonder if in 10.8 we can take advantage of the interrupt work to quickly and safely shutdown the user and istat threads?&lt;/p&gt;</comment>
                            <comment id="13005402" author="kmarsden" created="Thu, 10 Mar 2011 23:39:23 +0000"  >&lt;p&gt;+1 to making this a blocker.  I have seen cases in the past where I have instructed users to wait for a bit after shutdown before deleting the database to avoid deletion errors.    If that won&apos;t work reliably no matter how long they wait it would be extremely problematic.&lt;/p&gt;</comment>
                            <comment id="13005562" author="kristwaa" created="Fri, 11 Mar 2011 09:12:29 +0000"  >&lt;p&gt;Thanks for looking into this, Mike.&lt;br/&gt;
If you enable istat logging and tracing, you should be able to see which exception/error is raised when the istat daemon is shut down (i.e. is it always being raised in the same place?)&lt;/p&gt;

&lt;p&gt;Kathey, even though I agree this could be extremely problematic, there are some points to consider:&lt;br/&gt;
 o the istat daemon must be doing work as the database is shut down&lt;br/&gt;
 o the bug is timing dependent (it doesn&apos;t fail every time on the regression test machines, nor on my Windows Vista quad core)&lt;br/&gt;
 o the user must try to delete the database files&lt;br/&gt;
 o affects Windows platforms only&lt;/p&gt;

&lt;p&gt;With the points above, this issue is still not a blocker for me, especially since the feature can be disabled. I still think this issue is the most important istat bug to fix, so it would be great if it made it into 10.8.&lt;/p&gt;

&lt;p&gt;It is unclear to me at this point what happens resource-wise if you just skip the delete-step, reboot the database and repeat the sequence (without killing the JVM).&lt;br/&gt;
One thing that will definitely make it a blocker is if the database can&apos;t be booted and/or be written into after the bug occurs (again, without killing the JVM, as would typically by the case in appserver environments).&lt;/p&gt;</comment>
                            <comment id="13005663" author="rhillegas" created="Fri, 11 Mar 2011 14:55:34 +0000"  >&lt;p&gt;Changing the urgency to Blocker. That makes this issue a showstopper for 10.8.1. If I understand Mike&apos;s analysis, we could see problems with DDL on the table in the following scenario:&lt;/p&gt;

&lt;p&gt;o Running on Windows&lt;br/&gt;
o The application shuts down the database but the application continues to do other work.&lt;br/&gt;
o The application then reboots the database for more work.&lt;/p&gt;</comment>
                            <comment id="13005680" author="lilywei" created="Fri, 11 Mar 2011 15:36:41 +0000"  >&lt;p&gt;+1 to making this a blocker for this application shuts down the database then delete the database is problematic on Windows.&lt;/p&gt;</comment>
                            <comment id="13005876" author="mikem" created="Fri, 11 Mar 2011 22:22:23 +0000"  >&lt;p&gt;It looks like one of the last things that happens in the istat thread prior to store peforming it&apos;s shutdown and closing all of it&apos;s open files is that &lt;br/&gt;
it recovers an open from an interrupt.  I&apos;ll include the stacks below that show this.  What should we do if the istat thread get&apos;s an interrupt.  I am leaning&lt;br/&gt;
toward it should check for interrupts at critical places and then give up on the work it is doing.  Maybe there is a place it could check whether it should be&lt;br/&gt;
shutting down that needs to be checked more often?  &lt;/p&gt;

&lt;p&gt;The store shutdown code only closes files that are not currently &quot;kept&quot; (store&apos;s term for a file that is open by a store client), so not surprising that file is&lt;br/&gt;
not closed if it is executing while the istat thread is busy reading from the file).  Here is the stack of the store closing it&apos;s open files:&lt;br/&gt;
    at org.apache.derby.impl.store.raw.data.RAFContainer.closeContainer(RAFContainer.java:216)^M&lt;br/&gt;
    at org.apache.derby.impl.store.raw.data.RAFContainer4.closeContainer(RAFContainer4.java:227)^M&lt;br/&gt;
    at org.apache.derby.impl.store.raw.data.FileContainer.clearIdentity(FileContainer.java:476)^M&lt;br/&gt;
    at org.apache.derby.impl.services.cache.ConcurrentCache.removeEntry(ConcurrentCache.java:167)^M&lt;br/&gt;
    at org.apache.derby.impl.services.cache.ConcurrentCache.ageOut(ConcurrentCache.java:583)^M&lt;br/&gt;
    at org.apache.derby.impl.services.cache.ConcurrentCache.shutdown(ConcurrentCache.java:598)^M&lt;br/&gt;
    at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.stop(BaseDataFileFactory.java:519)^M&lt;br/&gt;
    at org.apache.derby.impl.services.monitor.TopService.stop(TopService.java:443)^M&lt;br/&gt;
    at org.apache.derby.impl.services.monitor.TopService.shutdown(TopService.java:394)^M&lt;br/&gt;
    at org.apache.derby.impl.services.monitor.BaseMonitor.shutdown(BaseMonitor.java:229)^M&lt;/p&gt;

&lt;p&gt;Here is some debug showing the stacks where the istat daemon is recovering from an interrupt: on the file of interest c481.dat - conglom/container  id  1153&lt;br/&gt;
Thu Mar 10 16:22:59 PST 2011 Thread&lt;span class=&quot;error&quot;&gt;&amp;#91;index-stat-thread,5,main&amp;#93;&lt;/span&gt; &lt;/p&gt;
{istat,trace@637806084} worker thread started (xid=24016) [q/p/s=1/0/1,err:k/u/c=0/0/0,rej:f/d/&lt;br/&gt;
/0/0]^M&lt;br/&gt;
Thu Mar 10 16:22:59 PST 2011 Thread&lt;span class=&quot;error&quot;&gt;&amp;#91;index-stat-thread,5,main&amp;#93;&lt;/span&gt; {istat,trace@637806084}
&lt;p&gt;     processing &quot;APP&quot;.&quot;BIG_TABLE&quot; ^M&lt;br/&gt;
DEBUG IndexStatisticsDaemonImpl OUTPUT: opened conglomerateNumber&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; = 1153^M&lt;br/&gt;
DEBUG RAFContainer OUTPUT: about to close file C:\derby\s1\systest\out\junit\system\singleUse\copyShutdown\seg0\c481.dat^M&lt;br/&gt;
Exception trace: ^M&lt;br/&gt;
java.lang.Throwable^M&lt;br/&gt;
    at org.apache.derby.impl.store.raw.data.RAFContainer.closeContainer(RAFContainer.java:216)^M&lt;br/&gt;
    at org.apache.derby.impl.store.raw.data.RAFContainer4.closeContainer(RAFContainer4.java:227)^M&lt;br/&gt;
    at org.apache.derby.impl.store.raw.data.RAFContainer4.recoverContainerAfterInterrupt(RAFContainer4.java:860)^M&lt;br/&gt;
    at org.apache.derby.impl.store.raw.data.RAFContainer4.readPage(RAFContainer4.java:368)^M&lt;br/&gt;
    at org.apache.derby.impl.store.raw.data.RAFContainer4.readPage(RAFContainer4.java:246)^M&lt;br/&gt;
    at org.apache.derby.impl.store.raw.data.CachedPage.readPage(CachedPage.java:673)^M&lt;br/&gt;
    at org.apache.derby.impl.store.raw.data.CachedPage.setIdentity(CachedPage.java:193)^M&lt;br/&gt;
    at org.apache.derby.impl.services.cache.ConcurrentCache.find(ConcurrentCache.java:295)^M&lt;br/&gt;
    at org.apache.derby.impl.store.raw.data.FileContainer.getUserPage(FileContainer.java:2530)^M&lt;br/&gt;
    at org.apache.derby.impl.store.raw.data.FileContainer.getPage(FileContainer.java:2580)^M&lt;br/&gt;
    at org.apache.derby.impl.store.raw.data.BaseContainerHandle.getPage(BaseContainerHandle.java:319)^M&lt;br/&gt;
    at org.apache.derby.impl.store.access.btree.ControlRow.get(ControlRow.java:833)^M&lt;br/&gt;
    at org.apache.derby.impl.store.access.btree.ControlRow.get(ControlRow.java:820)^M&lt;br/&gt;
    at org.apache.derby.impl.store.access.btree.ControlRow.getRightSibling(ControlRow.java:531)^M&lt;br/&gt;
    at org.apache.derby.impl.store.access.btree.BTreeScan.positionAtNextPage(BTreeScan.java:493)^M&lt;br/&gt;
    at org.apache.derby.impl.store.access.btree.BTreeForwardScan.fetchRows(BTreeForwardScan.java:464)^M&lt;br/&gt;
    at org.apache.derby.impl.store.access.btree.BTreeScan.fetchNextGroup(BTreeScan.java:1681)^M&lt;br/&gt;
    at org.apache.derby.impl.services.daemon.IndexStatisticsDaemonImpl$KeyComparator.fetchRows(IndexStatisticsDaemonImpl.java:1174)^M&lt;br/&gt;
    at org.apache.derby.impl.services.daemon.IndexStatisticsDaemonImpl.updateIndexStatsMinion(IndexStatisticsDaemonImpl.java:470)^M&lt;br/&gt;
    at org.apache.derby.impl.services.daemon.IndexStatisticsDaemonImpl.generateStatistics(IndexStatisticsDaemonImpl.java:327)^M&lt;br/&gt;
    at org.apache.derby.impl.services.daemon.IndexStatisticsDaemonImpl.processingLoop(IndexStatisticsDaemonImpl.java:777)^M&lt;br/&gt;
    at org.apache.derby.impl.services.daemon.IndexStatisticsDaemonImpl.run(IndexStatisticsDaemonImpl.java:687)^M&lt;br/&gt;
    at java.lang.Thread.run(Thread.java:736)^M&lt;br/&gt;
DEBUG RAFContainer OUTPUT: closed file C:\derby\s1\systest\out\junit\system\singleUse\copyShutdown\seg0\c481.dat^M&lt;br/&gt;
DEBUG RAFContainer OUTPUT: opening file: C:\derby\s1\systest\out\junit\system\singleUse\copyShutdown\seg0\c481.dat^M&lt;br/&gt;
Exception trace: ^M&lt;br/&gt;
java.lang.Throwable^M&lt;br/&gt;
    at org.apache.derby.impl.store.raw.data.RAFContainer.run(RAFContainer.java:1587)^M&lt;br/&gt;
    at java.security.AccessController.doPrivileged(AccessController.java:251)^M&lt;br/&gt;
    at org.apache.derby.impl.store.raw.data.RAFContainer.openContainerMinion(RAFContainer.java:939)^M&lt;br/&gt;
    at org.apache.derby.impl.store.raw.data.RAFContainer.reopenContainer(RAFContainer.java:914)^M&lt;br/&gt;
    at org.apache.derby.impl.store.raw.data.RAFContainer4.recoverContainerAfterInterrupt(RAFContainer4.java:861)^M&lt;br/&gt;
    at org.apache.derby.impl.store.raw.data.RAFContainer4.readPage(RAFContainer4.java:368)^M&lt;br/&gt;
    at org.apache.derby.impl.store.raw.data.RAFContainer4.readPage(RAFContainer4.java:246)^M&lt;br/&gt;
    at org.apache.derby.impl.store.raw.data.CachedPage.readPage(CachedPage.java:673)^M&lt;br/&gt;
    at org.apache.derby.impl.store.raw.data.CachedPage.setIdentity(CachedPage.java:193)^M&lt;br/&gt;
    at org.apache.derby.impl.services.cache.ConcurrentCache.find(ConcurrentCache.java:295)^M&lt;br/&gt;
    at org.apache.derby.impl.store.raw.data.FileContainer.getUserPage(FileContainer.java:2530)^M&lt;br/&gt;
    at org.apache.derby.impl.store.raw.data.FileContainer.getPage(FileContainer.java:2580)^M&lt;br/&gt;
    at org.apache.derby.impl.store.raw.data.BaseContainerHandle.getPage(BaseContainerHandle.java:319)^M&lt;br/&gt;
    at org.apache.derby.impl.store.access.btree.ControlRow.get(ControlRow.java:833)^M&lt;br/&gt;
    at org.apache.derby.impl.store.access.btree.ControlRow.get(ControlRow.java:820)^M&lt;br/&gt;
    at org.apache.derby.impl.store.access.btree.ControlRow.getRightSibling(ControlRow.java:531)^M&lt;br/&gt;
    at org.apache.derby.impl.store.access.btree.BTreeScan.positionAtNextPage(BTreeScan.java:493)^M&lt;br/&gt;
    at org.apache.derby.impl.store.access.btree.BTreeForwardScan.fetchRows(BTreeForwardScan.java:464)^M&lt;br/&gt;
    at org.apache.derby.impl.store.access.btree.BTreeScan.fetchNextGroup(BTreeScan.java:1681)^M&lt;br/&gt;
    at org.apache.derby.impl.services.daemon.IndexStatisticsDaemonImpl$KeyComparator.fetchRows(IndexStatisticsDaemonImpl.java:1174)^M&lt;br/&gt;
    at org.apache.derby.impl.services.daemon.IndexStatisticsDaemonImpl.updateIndexStatsMinion(IndexStatisticsDaemonImpl.java:470)^M&lt;br/&gt;
    at org.apache.derby.impl.services.daemon.IndexStatisticsDaemonImpl.generateStatistics(IndexStatisticsDaemonImpl.java:327)^M&lt;br/&gt;
    at org.apache.derby.impl.services.daemon.IndexStatisticsDaemonImpl.processingLoop(IndexStatisticsDaemonImpl.java:777)^M&lt;br/&gt;
    at org.apache.derby.impl.services.daemon.IndexStatisticsDaemonImpl.run(IndexStatisticsDaemonImpl.java:687)^M&lt;br/&gt;
    at java.lang.Thread.run(Thread.java:736)^M&lt;br/&gt;
----------------------------------------------------------------^M&lt;br/&gt;
----------------------------------------------------------------^M&lt;br/&gt;
Thu Mar 10 16:22:59 PST 2011:&lt;br/&gt;
Shutting down instance 35924132-012e-a249-3f92-ffffe527380d on database directory C:\derby\s1\systest\out\junit\system\singleUse\copyShutdown with class loade&lt;br/&gt;
sun.misc.Launcher$AppClassLoader@49be49be ^M&lt;br/&gt;
shutting down:&lt;br/&gt;
isCorrupt = false&lt;br/&gt;
pageCache = org.apache.derby.impl.services.cache.ConcurrentCache@1c2a1c2a&lt;br/&gt;
containerCache = org.apache.derby.impl.services.cache.ConcurrentCache@20fa20fa^M&lt;/p&gt;</comment>
                            <comment id="13005899" author="mikem" created="Fri, 11 Mar 2011 23:36:56 +0000"  >&lt;p&gt;The more I look at this issue I think the problem is that the istat daemon should shutdown and not return until it has completed this &lt;br/&gt;
shutdown when indexRefresher.stop(); is called from the DataDictionary&apos;s stop call().  For a clean shutdown of the system the store&lt;br/&gt;
needs all it&apos;s clients shutdown first and then it can cleanly shutdown, and force the database files and transaction logs insuring &lt;br/&gt;
a clean shutdown with no recovery work necessary on the next boot.&lt;/p&gt;

&lt;p&gt;By leaving the istat daemon running we can run into a number of errors that I don&apos;t think can be solved.  We might fix a specific one shown&lt;br/&gt;
up by this test but the system is just not designed to handle clean shutdown while stuff is still running without first waiting for the running&lt;br/&gt;
stuff to stop somehow.&lt;/p&gt;

&lt;p&gt;Kristian noted in &lt;a href=&quot;https://issues.apache.org/jira/browse/DERBY-5037&quot; title=&quot;Assertion failure from index-stat-thread when running AutomaticIndexStatisticsTest&quot; class=&quot;issue-link&quot; data-issue-key=&quot;DERBY-5037&quot;&gt;&lt;del&gt;DERBY-5037&lt;/del&gt;&lt;/a&gt;:&lt;br/&gt;
&amp;gt; I think Mike&apos;s comments/observations above agree pretty much with my thinking when writing the code. Seems there are several error-handling issues to iron out though...&lt;br/&gt;
&amp;gt;A few specific comments:&lt;br/&gt;
&amp;gt;o I decided to not make Derby wait for the background thread to finish on shutdown, as it might potentially be scanning a very large table.&lt;br/&gt;
&amp;gt;o Logging is rather verbose now during testing, but I agree it should be less verbose (or maybe turned off completely) when released.&lt;br/&gt;
&amp;gt;o I&apos;m logging a lot of exceptions to aid testing/debugging. These should also go away, or be enabled by a property if the user wishes to do so. &lt;/p&gt;

&lt;p&gt;I now think that it was wrong to not wait for the background thread.  This would match the behavior of the rawStoreDaemon thread which is &quot;owned&quot;&lt;br/&gt;
by the raw store module - the module stops the daemon and the daemon waits around for work to stop/complete before returning from the stop, and&lt;br/&gt;
then the raw store continues with it&apos;s data and transaction file cleanup prior to stopping.   I agree it would be a nice optimization to somehow stop the background thread in&lt;br/&gt;
the middle of a big scan, and it seems like with the better interrupt support this should be much easier than was the case before 10.8.   I would like&lt;br/&gt;
some feedback before proceding from those more knowledgeable about the istat work.&lt;/p&gt;

&lt;p&gt;I do think that the work rick did for &lt;a href=&quot;https://issues.apache.org/jira/browse/DERBY-5037&quot; title=&quot;Assertion failure from index-stat-thread when running AutomaticIndexStatisticsTest&quot; class=&quot;issue-link&quot; data-issue-key=&quot;DERBY-5037&quot;&gt;&lt;del&gt;DERBY-5037&lt;/del&gt;&lt;/a&gt; is still valuable as it will handle much better the non-clean shutdowns that Derby can experience.  ButFor&lt;br/&gt;
a non-clean shutdown we might have to just live with a file left open until the thread or jvm exits.   But for a requested orderly shutdown of the system I &lt;br/&gt;
think we should go with the top down shutdown supported by the architecture rather than try to fix errors encountered when top level modules are still&lt;br/&gt;
running while lower level modules are trying to shut down.&lt;/p&gt;</comment>
                            <comment id="13006010" author="knutanders" created="Sat, 12 Mar 2011 09:14:58 +0000"  >&lt;p&gt;Engine shutdown interrupts all threads that are active inside Derby code. I&apos;m wondering if that&apos;s the interrupt the istat thread is experiencing. &lt;a href=&quot;https://issues.apache.org/jira/browse/DERBY-4920&quot; title=&quot;suites.All stuck in RAFContainer4.awaitRestoreChannel()&quot; class=&quot;issue-link&quot; data-issue-key=&quot;DERBY-4920&quot;&gt;&lt;del&gt;DERBY-4920&lt;/del&gt;&lt;/a&gt; was similar, only then it was a checkpoint that was interrupted by the engine shutdown and reopened the channel. I don&apos;t know if we want to handle interrupts differently for the istat daemon than we do for other threads. I&apos;d rather see that the reopening logic in store knew that it shouldn&apos;t attempt to reopen channels if the engine is about to shut down. I thought it already had some logic to take care of that, but I&apos;m not sure.&lt;/p&gt;

&lt;p&gt;Waiting for the istat thread before completing shutdown sounds OK. I think the reason why it doesn&apos;t do that now, was a concern that it may take a very long time if it&apos;s processing a large index. But shutdown also performs a checkpoint, which may take a long time if the page cache is big. And if the istat thread is also interrupted by the engine shutdown, it should stop rather quickly (if we only can make it stop reopening the channel).&lt;/p&gt;

&lt;p&gt;I&apos;m wondering, though, if this problem is istat-specific, or if we would see the same problem if we called SYSCS_UPDATE_STATISTICS on a large index just before shutting down the database?&lt;/p&gt;</comment>
                            <comment id="13006212" author="knutanders" created="Sun, 13 Mar 2011 15:50:02 +0000"  >&lt;p&gt;I invoked an engine shutdown while a call to SYSCS_UTIL.SYSCS_UPDATE_STATISTICS was in progress, and then I observed that the process still had an open file handle to the index file after the shutdown had completed. I had disabled istat when I ran this experiment.&lt;/p&gt;</comment>
                            <comment id="13006437" author="knutanders" created="Mon, 14 Mar 2011 14:48:03 +0000"  >&lt;p&gt;I don&apos;t think we need to use interrupts to shut down the istat thread more quickly. IndexStatisticsDaemonImpl.stop() sets a flag daemonDisabled that we can check every now and then while scanning the indexes. Currently, the flag is checked right before the scan starts, but I was thinking we could check that flag for example each time we&apos;ve fetched a new chunk of rows from store. The attached patch does just that, and also makes stop() wait for the istat thread to complete if there is an active thread.&lt;/p&gt;

&lt;p&gt;Does that help on the machine where you&apos;ll able to reproduce the problem, Mike?&lt;/p&gt;</comment>
                            <comment id="13006478" author="lilywei" created="Mon, 14 Mar 2011 15:57:57 +0000"  >&lt;p&gt;Thanks Knut for the patch. I applied to my windows 7 machine. I used to able to reproduce the delete problem quickly. After running AutomaticIndexStatisticsTest 5 times, the delete problem did not show up. With waitOnShutdown.diff patch, checking the daemonDisabled flag each time we fetch a new chuck of rows from store and making stop() wait for istat thread to complete if there is an active thread definitely help. Does this approach fulfill the top down shutdown supported by the architecture?&lt;/p&gt;</comment>
                            <comment id="13006492" author="knutanders" created="Mon, 14 Mar 2011 16:27:46 +0000"  >&lt;p&gt;Thanks for testing the patch, Lily!&lt;/p&gt;

&lt;p&gt;&amp;gt; Does this approach fulfill the top down shutdown supported by the architecture?&lt;/p&gt;

&lt;p&gt;Yes, I think so. The istat service is shut down from the data dictionary, which is top-down. That was done before as well, but the actual worker thread would be left running until it finished by itself. With the patch, stop() won&apos;t complete until the worker thread is done, so when the higher levels proceed in the shutdown code, we know that the istat thread won&apos;t be causing any bad interactions.&lt;/p&gt;</comment>
                            <comment id="13006542" author="mikem" created="Mon, 14 Mar 2011 18:08:21 +0000"  >&lt;p&gt;thanks for the work knut.  Your patch looks good to me so far, I am running my single test now.&lt;/p&gt;

&lt;p&gt;Even though update statistics in a user thread exhibits the same symptom I would argue that it is much more serious in a default on istat release.  &lt;br/&gt;
A user shutting down&lt;br/&gt;
while actively running can work around the issue.  But the background process is not in their control (other than disabling it entirely).  So doing more&lt;br/&gt;
work to behave nicely during shutdown in istat important in my opinion.  We should probably log a separate JIRA for your update statistics test case.  I&lt;br/&gt;
assume other work by user threads probably has a similar problem.  It would be good to know if this is a regression caused by the improved interrupt&lt;br/&gt;
handling.&lt;/p&gt;

&lt;p&gt;You bring up some interesting thoughts on interrupts and shutdown.  I think we should start a thread and discuss that separately from this JIRA, deciding first&lt;br/&gt;
 what is the right thing to do then how to do it.&lt;/p&gt;</comment>
                            <comment id="13006567" author="mikem" created="Mon, 14 Mar 2011 18:46:48 +0000"  >&lt;p&gt;In my environment just got 4 for 4 errors without the patch and 4 for 4 sucesses with the patch.  I will submit a fix once I can get a clean full test run, probably late night pacific time.&lt;/p&gt;

&lt;p&gt;4 out of 4 worked and 5th failed.  So fix is better but something still going on, will continue working ...&lt;/p&gt;</comment>
                            <comment id="13006602" author="rhillegas" created="Mon, 14 Mar 2011 19:57:41 +0000"  >&lt;p&gt;Reducing the urgency of this issue. Now that istat is not turned on by default for 10.8.1, this issue is not a blocker for the release.&lt;/p&gt;</comment>
                            <comment id="13006891" author="knutanders" created="Tue, 15 Mar 2011 12:25:05 +0000"  >&lt;p&gt;There may be other issues than the istat daemon coming into play here. I see that RawStore.stop() doesn&apos;t wait for the checkpoint to complete if there already is a checkpoint in progress, so there may be similar problems in that code. A related problem with checkpoints reopening containers during shutdown was fixed in &lt;a href=&quot;https://issues.apache.org/jira/browse/DERBY-4920&quot; title=&quot;suites.All stuck in RAFContainer4.awaitRestoreChannel()&quot; class=&quot;issue-link&quot; data-issue-key=&quot;DERBY-4920&quot;&gt;&lt;del&gt;DERBY-4920&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="13006941" author="knutanders" created="Tue, 15 Mar 2011 14:35:40 +0000"  >&lt;p&gt;I added some println statements to TopService.shutdown() to see in which order the modules were taken down. I found that BaseDataFileFactoryJ4 was stopped before DataDictionaryImpl (which also stops the istat daemon). BaseDataFileFactoryJ4 shuts down the page cache and the container cache, so ideally no I/O should happen after that point. Since we don&apos;t stop the istat thread until after the caches have been shut down, that may explain why we still see that files are being reopened. I think it&apos;s the container cache that closes the files.&lt;/p&gt;

&lt;p&gt;BaseMonitor sends an interrupt to all active threads before invoking TopService.shutdown(), so maybe what&apos;s happening is something like this:&lt;/p&gt;

&lt;p&gt;1) BaseMonitor interrupts the threads and goes on shutting down the modules.&lt;/p&gt;

&lt;p&gt;2) The istat thread notices the interrupt inside the store code and tries reopening the container that was closed by the interrupt. But before it has completed that task, ...&lt;/p&gt;

&lt;p&gt;3) The data file factory is shut down, and all data files are closed when the container cache shuts down.&lt;/p&gt;

&lt;p&gt;4) The istat thread finishes the reopening of the index file.&lt;/p&gt;

&lt;p&gt;5) DataDictionaryImpl stops the istat thread, but too late.&lt;/p&gt;</comment>
                            <comment id="13006983" author="mikem" created="Tue, 15 Mar 2011 16:00:36 +0000"  >&lt;p&gt;I am not actively working on this any more.  I think it should be left open until after istat is re-enabled or the istat tests are changed to run even if istat is disabled by default.   Then should be closed if we go a reasonable amount of time without seeing it in nightly&apos;s or reported by anyone.&lt;/p&gt;</comment>
                            <comment id="13007247" author="knutanders" created="Tue, 15 Mar 2011 22:45:11 +0000"  >&lt;p&gt;&amp;gt; if we go a reasonable amount of time without seeing it in nightly&apos;s or reported by anyone&lt;/p&gt;

&lt;p&gt;Didn&apos;t you say you still saw the problem in one out of five runs with the fix?&lt;/p&gt;</comment>
                            <comment id="13007279" author="mikem" created="Tue, 15 Mar 2011 23:33:06 +0000"  >&lt;p&gt;yesterday I tried exactly 5 runs using the exact submitted patch and it failed once.  I then modified the patch, mostly adding comments and slightly different break structure.  I then ran it exactly 40 times and it did not reproduce.  I then ran full set of tests in a codeline with istat still enabled as default and they all passed and checked it in - but had to update my codeline to do the submit.  I have not run it since.  It is a little harder to run since istat does not run by default - which was why I was looking for a checked in solution to running the tests no matter what the default happened to be.  I just have not gotten around to hacking a codeline or test run to do it since.  I wish the disabling had waited and happened solely in the branch if necessary, so that we could be getting testing on istat in the trunk now.  &lt;/p&gt;
</comment>
                            <comment id="13007504" author="knutanders" created="Wed, 16 Mar 2011 15:10:50 +0000"  >&lt;p&gt;Thanks, Mike. Sounds like it fixed most cases then. I still suspect the shutdown order is problematic, so I&apos;d like to keep the bug open at least until we&apos;ve investigated and found out if it actually is a problem.&lt;/p&gt;

&lt;p&gt;As to running AutomaticIndexStatisticsTest on head of trunk when istat is disabled, I found that I could do that by adding -Dderby.storage.indexStats.auto=true to the java command when invoking the test runner. I would think that it should also work on suites.All.&lt;/p&gt;</comment>
                            <comment id="13008044" author="dagw" created="Thu, 17 Mar 2011 18:04:14 +0000"  >&lt;p&gt;Attaching a small fix to handle the case of the thread stopping the daemon being interrupted while doing the IndexStatisticsDaemonImpl#join introduced in this issue: note the interrupt and retry the join. I think this is more in keeping with what we do elsewhere now.&lt;/p&gt;

&lt;p&gt;Running regressions.&lt;/p&gt;</comment>
                            <comment id="13010082" author="dagw" created="Wed, 23 Mar 2011 11:57:25 +0000"  >&lt;p&gt;Regression ran ok with my patch.&lt;/p&gt;</comment>
                            <comment id="13010701" author="knutanders" created="Thu, 24 Mar 2011 15:18:43 +0000"  >&lt;p&gt;Thanks for the followup patch, Dag. I think it makes sense to change the interrupt handling to match what we do in the rest of the engine.&lt;/p&gt;

&lt;p&gt;I applied the patch and added a call to Thread.interrupt() to make join() always throw an exception. I verified that it saved the interrupt status and continued waiting until the thread had completed. However, when I checked the interrupt flag after DriverManager.getConnection(&quot;...;shutdown=true&quot;) had completed, the interrupt flag was not set in the thread. Perhaps the shutdown code doesn&apos;t have the mechanism in place to restore the interrupt flag?&lt;/p&gt;</comment>
                            <comment id="13010745" author="dagw" created="Thu, 24 Mar 2011 16:22:53 +0000"  >&lt;p&gt;Committed patch as svn 1085027.&lt;/p&gt;

&lt;p&gt;Thanks for looking at this, Knut. I thought it should restore the flag, I&apos;ll investigate it.&lt;/p&gt;</comment>
                            <comment id="13010815" author="dagw" created="Thu, 24 Mar 2011 18:23:15 +0000"  >&lt;p&gt;During shutdown there is a corner case where the lcc (which contains the interrupt status) is deleted before we reach the place where the flag is attempted restored. I&apos;ll file a separate issue for this.&lt;/p&gt;</comment>
                            <comment id="13012238" author="mikem" created="Mon, 28 Mar 2011 22:37:16 +0100"  >&lt;p&gt;I tried out the latest in trunk as of 3/26/2011 and ran the AutomaticIndexStatisticsTest suite 64 times and got 61 good runs and 3 failures all in the shutdown fixture as follows:&lt;br/&gt;
There was 1 failure:&lt;br/&gt;
1) testShutdownWhileScanningThenDelete(org.apache.derbyTesting.functionTests.tes&lt;br/&gt;
ts.store.AutomaticIndexStatisticsTest)junit.framework.AssertionFailedError: Fail&lt;br/&gt;
ed to delete 3 files (root=C:\derby\s2\newout\system\singleUse\copyShutdown: C:\&lt;br/&gt;
derby\s2\newout\system\singleUse\copyShutdown\seg0\c481.dat (isDir=false, canRea&lt;br/&gt;
d=true, canWrite=true, size=22351872), C:\derby\s2\newout\system\singleUse\copyS&lt;br/&gt;
hutdown\seg0 (isDir=true, canRead=true, canWrite=true, size=0), C:\derby\s2\newo&lt;br/&gt;
ut\system\singleUse\copyShutdown (isDir=true, canRead=true, canWrite=true, size=&lt;br/&gt;
0)&lt;br/&gt;
    at org.apache.derbyTesting.junit.BaseJDBCTestCase.assertDirectoryDeleted(Bas&lt;br/&gt;
eJDBCTestCase.java:1526)&lt;br/&gt;
    at org.apache.derbyTesting.functionTests.tests.store.AutomaticIndexStatistic&lt;br/&gt;
sTest.testShutdownWhileScanningThenDelete(AutomaticIndexStatisticsTest.java:188)&lt;br/&gt;
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java&lt;br/&gt;
:60)&lt;br/&gt;
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorI&lt;br/&gt;
mpl.java:37)&lt;br/&gt;
    at org.apache.derbyTesting.junit.BaseTestCase.runBare(BaseTestCase.java:112)&lt;br/&gt;
    at junit.extensions.TestDecorator.basicRun(TestDecorator.java:22)&lt;br/&gt;
    at junit.extensions.TestSetup$1.protect(TestSetup.java:19)&lt;br/&gt;
    at junit.extensions.TestSetup.run(TestSetup.java:23)&lt;br/&gt;
    at org.apache.derbyTesting.junit.BaseTestSetup.run(BaseTestSetup.java:57)&lt;br/&gt;
    at junit.extensions.TestDecorator.basicRun(TestDecorator.java:22)&lt;br/&gt;
    at junit.extensions.TestSetup$1.protect(TestSetup.java:19)&lt;br/&gt;
    at junit.extensions.TestSetup.run(TestSetup.java:23)&lt;/p&gt;

&lt;p&gt;There didn&apos;t seem to be anything interesting in derby.log.  I think we should enable the &quot;extra&quot; logging by default in&lt;br/&gt;
this test while there is still and issue. I would be happy to run in my environment and reproduce if there is any &lt;br/&gt;
additional logging that we think will track down the issue (even if it something we don&apos;t want checked into the&lt;br/&gt;
actual codeline).  Tonight I will at least run the test with the extra logging enabled from the command line.&lt;/p&gt;</comment>
                            <comment id="13039375" author="kristwaa" created="Wed, 25 May 2011 23:13:25 +0100"  >&lt;p&gt;Another occurrence seen on May 8th 2011: &lt;a href=&quot;http://people.apache.org/~myrnavl/derby_test_results/main/windows/testSummary-1100426.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://people.apache.org/~myrnavl/derby_test_results/main/windows/testSummary-1100426.html&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13043343" author="kristwaa" created="Fri, 3 Jun 2011 14:15:53 +0100"  >&lt;p&gt;Attaching patch 1a, which will make Derby shut down the istat daemon before throwing the shutdown exception.&lt;br/&gt;
I also tweaked the log entry to tell if the scan was aborted.&lt;/p&gt;

&lt;p&gt;I have tested the patch on a Windows machine and I have run the regression tests, but I had problems reproducing the error on my machine. Nonetheless, I never saw the error with the patch applied.&lt;br/&gt;
If someone wants to test the patch on Windows before I commit, please let me know and I&apos;ll hold back.&lt;/p&gt;

&lt;p&gt;Patch ready for review.&lt;/p&gt;</comment>
                            <comment id="13043895" author="dagw" created="Fri, 3 Jun 2011 18:33:38 +0100"  >&lt;p&gt;Thanks, Kristian. &lt;/p&gt;

&lt;p&gt;I see that you put the call to disableIndexStatsRefresher at one point in  EmbedConnection&apos;s constructor correspoding to a requested shutdown. Database shutdown is also initiated from other locations, e.g from TransactionResourceImpl#handleException and others (many to do with replication handling). Is there no need to similarly stop the stat daemon when the db is shut down from these other locations?&lt;/p&gt;</comment>
                            <comment id="13045346" author="kristwaa" created="Tue, 7 Jun 2011 10:58:12 +0100"  >&lt;p&gt;Hi Dag,&lt;/p&gt;

&lt;p&gt;There is a need to stop the istat daemon in other cases too to guarantee that this issue will never happen. There are several criteria that must occur for the issue to happen, and a few more to be able to observe it. Nevertheless, I think the takeaway is that Derby will leak file handles. How/if this affects normal Derby operation is unclear to me, and a JVM restart will clean this up.&lt;/p&gt;

&lt;p&gt;Attaching patch 2a.&lt;br/&gt;
To make the fix broader, I moved the logic to stop the daemon to DatabaseContextImpl.cleanupOnError.&lt;br/&gt;
Do you reckon this is sufficient, or are you aware of other location that will bypass that code?&lt;/p&gt;

&lt;p&gt;The regression tests ran without failures with patch 2a, but I have not yet tested this on Windows (I&apos;ll try to start a test loop later today when I can access a Windows machine).&lt;/p&gt;

&lt;p&gt;Patch ready for review. &lt;/p&gt;</comment>
                            <comment id="13045408" author="dagw" created="Tue, 7 Jun 2011 13:59:43 +0100"  >&lt;p&gt;I think the move should cover it. +1&lt;/p&gt;</comment>
                            <comment id="13045411" author="dagw" created="Tue, 7 Jun 2011 14:17:26 +0100"  >&lt;p&gt;I was just wondering about the code ca line 339-342 in TransactionResourceImpl.. could that bypass it,? Note that&lt;br/&gt;
this is part of the old interrupt machinery to close down threads when shutting down the engine. But presumably by then you have already shut down the thread. Depends what happens first, the shutdown of the statistics  thread or the call to notifyAllActiveThreads, but I see in DataBaseContextImpl#cleanupOnError you have inserted the logic &lt;b&gt;before&lt;/b&gt; the call to notifyAllActiveThreads so you should be golden.&lt;/p&gt;
</comment>
                            <comment id="13045860" author="kristwaa" created="Wed, 8 Jun 2011 10:29:08 +0100"  >&lt;p&gt;Thanks for looking at the patch, Dag.&lt;/p&gt;

&lt;p&gt;I think your reasoning is sound.&lt;br/&gt;
Do you happen to know if other exceptions than StandardExceptions can be passed in to cleanupOnError? What happens with InterruptedExceptions?&lt;br/&gt;
I assume the normal case is that all exceptions are either StandardException or other exceptions wrapped in a StandardException such that we can ask for the severity. In any case, if another kind of exception is passed into DatabaseContextImpl.cleanupOnError the code to shut down the istat daemon won&apos;t be run.&lt;/p&gt;

&lt;p&gt;The test case testShutdownWhileScanningThenDelete ran 236 times without a failure on a Windows machine with the patch applied (I then aborted the loop).&lt;/p&gt;

&lt;p&gt;Committed patch 1b to trunk with revision 1133304.&lt;br/&gt;
I&apos;ll monitor the automated tests (Windows platforms) to see if the issue shows itself again with this fix.&lt;/p&gt;</comment>
                            <comment id="13045871" author="kristwaa" created="Wed, 8 Jun 2011 10:51:24 +0100"  >&lt;p&gt;Attaching patch 3a, which adjusts a log message if a scan is aborted.&lt;/p&gt;

&lt;p&gt;Committed to trunk with revision 1133317.&lt;/p&gt;</comment>
                            <comment id="13046878" author="dagw" created="Thu, 9 Jun 2011 23:40:27 +0100"  >&lt;p&gt;Re cleanUpOnError: I believe the Throwable can be an SQLException, but essentially anything. The InterruptedExceptions&lt;br/&gt;
are handled, I think, before we get this far, although I think I saw a couple of unhandled wait&apos;s the other day which is on my list of things to check. &lt;/p&gt;

&lt;p&gt;But there could be other unhandled Exceptions and Errors as well, cf. the call to DCI#cleanUpOnError from ContextManager#cleanupOnError.called from TRI#handleException called from EmbedConnection#handleException which is called from many API methods in catch-all (Throwable) constructs.&lt;/p&gt;

&lt;p&gt;EmbedConnection#handleException is also called from ConnectionChild#handleException which is used from ResultSet API methods etc...&lt;/p&gt;

&lt;p&gt;Note the call to wrap &quot;other&quot; exceptions in the call TRI#warpInSQLException towards the end of TRI#handleException.&lt;/p&gt;</comment>
                            <comment id="13049747" author="kristwaa" created="Wed, 15 Jun 2011 13:31:23 +0100"  >&lt;p&gt;Thanks, Dag.&lt;/p&gt;

&lt;p&gt;Although there seems to be situations where the code shutting down the istat daemon as part of the cleanup is bypassed, I plan to leave the fix as it is for now.&lt;br/&gt;
As far as I understand, non-StandardException exceptions are supposed to be wrapped (although I haven&apos;t verified if this happens before or during/after the cleanup), and unless the exception brings down the database/system there the daemon isn&apos;t supposed to be shut down. I think it should be fairly easy to change this at a later time if we find the current fix inadequate.&lt;/p&gt;

&lt;p&gt;I haven&apos;t seen any more occurrences of the bug so far, but I&apos;ll continue to monitor the test results for a while longer before closing. I&apos;d also like to see if the error shows up in the 10.8 branch testing.&lt;/p&gt;

&lt;p&gt;Will backport to 10.8 later.&lt;/p&gt;</comment>
                            <comment id="13056491" author="kristwaa" created="Tue, 28 Jun 2011 14:14:33 +0100"  >&lt;p&gt;Merged missing fixes (2a and 3a) to 10.8 with revision 1140583.&lt;/p&gt;</comment>
                            <comment id="13056492" author="kristwaa" created="Tue, 28 Jun 2011 14:14:49 +0100"  >&lt;p&gt;Closing issue.&lt;/p&gt;</comment>
                            <comment id="13101560" author="myrna" created="Fri, 9 Sep 2011 21:59:50 +0100"  >&lt;p&gt;changing issue to change component for 10.8.2 release notes.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12480847">DERBY-4915</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12498403">DERBY-5026</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12501060">DERBY-5123</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12501351">DERBY-5131</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12481337" name="derby-5108-1a-early_istats_shutdown.diff" size="2248" author="kristwaa" created="Fri, 3 Jun 2011 14:15:53 +0100"/>
                            <attachment id="12473929" name="derby-5108-2.diff" size="989" author="dagw" created="Thu, 17 Mar 2011 18:04:14 +0000"/>
                            <attachment id="12481677" name="derby-5108-2a-early_istats_shutdown_broad.diff" size="2931" author="kristwaa" created="Tue, 7 Jun 2011 10:58:12 +0100"/>
                            <attachment id="12481795" name="derby-5108-3a-istat_log_abort.diff" size="1278" author="kristwaa" created="Wed, 8 Jun 2011 10:51:24 +0100"/>
                            <attachment id="12473202" name="javacore.20110309.125807.4048.0001.txt" size="330997" author="mikem" created="Wed, 9 Mar 2011 21:49:12 +0000"/>
                            <attachment id="12473569" name="waitOnShutdown.diff" size="2610" author="knutanders" created="Mon, 14 Mar 2011 14:48:03 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>6.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310200" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Bug behavior facts</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10420"><![CDATA[Regression]]></customfieldvalue>
    <customfieldvalue key="10369"><![CDATA[Regression Test Failure]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Tue, 8 Mar 2011 18:12:12 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>24654</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hy0euv:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>36225</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12310050" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Urgency</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10052"><![CDATA[Normal]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>
</channel>
</rss>