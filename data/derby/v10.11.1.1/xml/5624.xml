<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 03:09:57 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/DERBY-5624/DERBY-5624.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[DERBY-5624] System can run out of stack space while processing DropOnCommit requests.</title>
                <link>https://issues.apache.org/jira/browse/DERBY-5624</link>
                <project id="10594" key="DERBY">Derby</project>
                    <description>&lt;p&gt;The system currently recursively calls xact.notifyObservers() from DropOnCommit.update().  It does this because in some cases&lt;br/&gt;
new observers can be added while processing the list of notifyObservers and those were being missed before the change, causing&lt;br/&gt;
Assertions in the tests and possibly files not properly dropped on commit.&lt;/p&gt;

&lt;p&gt;Multiple users on the Derby user list have had failures running SYSCS_UTIL.SYSCS_COMPRESS_TABLE(), running out of stack track&lt;br/&gt;
with a heavily recursive stack trace of the form (see more detail from these reports in subsequent comments):&lt;br/&gt;
Caused by: java.lang.StackOverflowError&lt;br/&gt;
at java.lang.ThreadLocal.get(ThreadLocal.java:125)&lt;br/&gt;
at java.lang.StringCoding.deref(StringCoding.java:46)&lt;br/&gt;
at java.lang.StringCoding.encode(StringCoding.java:258)&lt;br/&gt;
at java.lang.String.getBytes(String.java:946)&lt;br/&gt;
at java.io.UnixFileSystem.getBooleanAttributes0(Native Method)&lt;br/&gt;
at java.io.UnixFileSystem.getBooleanAttributes(UnixFileSystem.java:228)&lt;br/&gt;
at java.io.File.exists(File.java:733)&lt;br/&gt;
at org.apache.derby.impl.store.raw.data.StreamFileContainer.run(Unknown Source)&lt;br/&gt;
at java.security.AccessController.doPrivileged(Native Method)&lt;br/&gt;
at org.apache.derby.impl.store.raw.data.StreamFileContainer.privExists(Unknown Source)&lt;br/&gt;
at org.apache.derby.impl.store.raw.data.StreamFileContainer.open(Unknown Source)&lt;br/&gt;
at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.openStreamContainer(Unknown Source)&lt;br/&gt;
at org.apache.derby.impl.store.raw.xact.Xact.openStreamContainer(Unknown Source)&lt;br/&gt;
at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.dropStreamContainer(Unknown Source)&lt;br/&gt;
at org.apache.derby.impl.store.raw.xact.Xact.dropStreamContainer(Unknown Source)&lt;br/&gt;
at org.apache.derby.impl.store.raw.data.DropOnCommit.update(Unknown Source)&lt;br/&gt;
at java.util.Observable.notifyObservers(Observable.java:142)&lt;br/&gt;
at org.apache.derby.iapi.store.raw.xact.RawTransaction.notifyObservers(Unknown Source)&lt;br/&gt;
at org.apache.derby.impl.store.raw.data.DropOnCommit.update(Unknown Source)&lt;br/&gt;
at java.util.Observable.notifyObservers(Observable.java:142)&lt;br/&gt;
at org.apache.derby.iapi.store.raw.xact.RawTransaction.notifyObservers(Unknown Source)&lt;br/&gt;
at org.apache.derby.impl.store.raw.data.DropOnCommit.update(Unknown Source)&lt;br/&gt;
at java.util.Observable.notifyObservers(Observable.java:142)&lt;br/&gt;
at org.apache.derby.iapi.store.raw.xact.RawTransaction.notifyObservers(Unknown Source)&lt;br/&gt;
at org.apache.derby.impl.store.raw.data.DropOnCommit.update(Unknown Source)&lt;br/&gt;
at java.util.Observable.notifyObservers(Observable.java:142)&lt;/p&gt;</description>
                <environment></environment>
        <key id="12543377">DERBY-5624</key>
            <summary>System can run out of stack space while processing DropOnCommit requests.</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="mikem">Mike Matrigali</assignee>
                                    <reporter username="mikem">Mike Matrigali</reporter>
                        <labels>
                            <label>derby_triage10_9</label>
                    </labels>
                <created>Mon, 20 Feb 2012 17:38:58 +0000</created>
                <updated>Fri, 25 Apr 2014 15:12:31 +0100</updated>
                            <resolved>Mon, 2 Apr 2012 19:45:00 +0100</resolved>
                                    <version>10.5.3.2</version>
                    <version>10.8.2.2</version>
                                    <fixVersion>10.5.3.2</fixVersion>
                    <fixVersion>10.6.2.4</fixVersion>
                    <fixVersion>10.7.1.4</fixVersion>
                    <fixVersion>10.8.3.0</fixVersion>
                    <fixVersion>10.9.1.0</fixVersion>
                                    <component>Store</component>
                        <due></due>
                            <votes>3</votes>
                                    <watches>3</watches>
                                                                <comments>
                            <comment id="13212086" author="mikem" created="Mon, 20 Feb 2012 21:30:40 +0000"  >&lt;p&gt;initial proof on concept patch, no extensive tests run yet.  Passes store test for original 3993 error.  The included test&lt;br/&gt;
failed before the fix and passes after the fix.  I probably will alter the test to be less of a resource hog, or look at putting it into a different suite.  It currently creates a table with 1000 columns and an index on each of those columns, and then runs compress table.&lt;/p&gt;</comment>
                            <comment id="13212092" author="mikem" created="Mon, 20 Feb 2012 21:35:21 +0000"  >&lt;p&gt;I have repro&apos;d what I think is the problem using a very un-real world test case (table with 1000 columns and an index&lt;br/&gt;
on every column).  It would be nice to get a more real world test case that just uses a big table and whatever the ddl on that table that is causing the issue.  &lt;/p&gt;

&lt;p&gt;The 2 users running into this seem to be both using relatively big tables, but I have not seen ddl yet.  I am wondering if the problem they are seeing is related to either on-disk sort files or blobs. &lt;/p&gt;</comment>
                            <comment id="13212634" author="mmccawley" created="Tue, 21 Feb 2012 14:58:56 +0000"  >&lt;p&gt;This seems to have fixed the problem for me. It also decreases the runtime as it doesn&apos;t have to create such a large stack. However, I was running 10.8, and I built the patch on main (10.9 beta), so I need to rebuild on 10.8 to make sure that nothing else is different as well as test on a larger database.&lt;/p&gt;</comment>
                            <comment id="13212757" author="mmccawley" created="Tue, 21 Feb 2012 17:33:59 +0000"  >&lt;p&gt;Built on 10.8 and run with a large table (over 45 million rows). I&apos;ll update if something changes, but it looks like this patch fixed the issue.&lt;/p&gt;</comment>
                            <comment id="13213117" author="mikem" created="Tue, 21 Feb 2012 23:53:52 +0000"  >&lt;p&gt;checked in fix to trunk.  Will let it sit there and make sure it passes tests across the nightly platforms and look at backporting it next week.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/DERBY-5624&quot; title=&quot;System can run out of stack space while processing DropOnCommit requests.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;DERBY-5624&quot;&gt;&lt;del&gt;DERBY-5624&lt;/del&gt;&lt;/a&gt; System can run out of stack space while processing DropOnCommit reque&lt;br/&gt;
sts.&lt;/p&gt;

&lt;p&gt;Taking care of cleanup after a commit is handled by notifying all &quot;Observers&quot;&lt;br/&gt;
that an event has taken place that they might want to act on and cleanup.  In&lt;br/&gt;
the added test case this is triggered by off line commit which effectively&lt;br/&gt;
drops and recreates the base table and all of its indexes after loading the&lt;br/&gt;
data into them.&lt;/p&gt;

&lt;p&gt;Sometimes these Observers may execute work which adds to the Observer queue,&lt;br/&gt;
and that queue can &quot;miss&quot; them in the first pass through.&lt;/p&gt;

&lt;p&gt;A previous fix for this problem added a recursive call to notifyObservers in&lt;br/&gt;
the place that could cause this addition of observers.  This recursive call&lt;br/&gt;
was causing stack problems when the number of Observers became large.  For&lt;br/&gt;
the checked in test case this was 1000 indexes on 1000 columns of the table.&lt;br/&gt;
For other users I believe the cause was a by product of sorts on large disk&lt;br/&gt;
based sorts for multi-gigabyte tables and indexes.  2 users were reporting&lt;br/&gt;
similar failed stacks for failing compresses of large tables, and one was&lt;br/&gt;
able to take this fix to their environment and then successfully run the&lt;br/&gt;
compress.&lt;/p&gt;

&lt;p&gt;The fix was to remove the recursion and instead loop at the outermost point&lt;br/&gt;
until there were no Observers.&lt;/p&gt;

&lt;p&gt;Adding the test to the largedata suite as it takes over 10 minutes to run&lt;br/&gt;
on my machine.&lt;/p&gt;</comment>
                            <comment id="13213981" author="myrna" created="Wed, 22 Feb 2012 20:34:18 +0000"  >&lt;p&gt;I&apos;m reopening, because I saw a failure in my nightly test of largeDataTests suite, looks like this test is now pushing past the # of open files supported for my account:&lt;/p&gt;

&lt;p&gt;(emb)largedata.Derby5624Test.testDERBY_5624 java.io.FileNotFoundException: /local1/cloudtst/dev/src/NightlyBuildResults.2012-02-21/largeDataTests/fail/Embedded_40/Derby5624Test/testDERBY_5624/derby.log (Too many open files)&lt;br/&gt;
	at java.io.FileOutputStream.open(Native Method)&lt;br/&gt;
	at java.io.FileOutputStream.&amp;lt;init&amp;gt;(FileOutputStream.java:190)&lt;br/&gt;
	at java.io.FileOutputStream.&amp;lt;init&amp;gt;(FileOutputStream.java:142)&lt;br/&gt;
	at org.apache.derbyTesting.functionTests.util.PrivilegedFileOpsForTests.copySingleFile(PrivilegedFileOpsForTests.java:305)&lt;br/&gt;
	at org.apache.derbyTesting.functionTests.util.PrivilegedFileOpsForTests.recursiveCopy(PrivilegedFileOpsForTests.java:261)&lt;br/&gt;
	at org.apache.derbyTesting.functionTests.util.PrivilegedFileOpsForTests.access$000(PrivilegedFileOpsForTests.java:49)&lt;br/&gt;
	at org.apache.derbyTesting.functionTests.util.PrivilegedFileOpsForTests$8.run(PrivilegedFileOpsForTests.java:235)&lt;br/&gt;
	at java.security.AccessController.doPrivileged(AccessController.java:251)&lt;br/&gt;
	at org.apache.derbyTesting.functionTests.util.PrivilegedFileOpsForTests.copy(PrivilegedFileOpsForTests.java:233)&lt;br/&gt;
	at org.apache.derbyTesting.junit.BaseTestCase.runBare(BaseTestCase.java:132)&lt;/p&gt;


&lt;p&gt;I&apos;ll attach the full error stack trace, and the console output of the suite run.&lt;/p&gt;

</comment>
                            <comment id="13213982" author="myrna" created="Wed, 22 Feb 2012 20:35:42 +0000"  >&lt;p&gt;Uploading the output of the largedata test run which failed, and the error-stacktrace from the fail directory.&lt;br/&gt;
I&apos;ve modified the location of the test dir a bit. &lt;/p&gt;</comment>
                            <comment id="13214005" author="mikem" created="Wed, 22 Feb 2012 21:01:58 +0000"  >&lt;p&gt;I will look at the linux error.  I had run the test on windows and it was fine there.  First thing I will look at is the compress call.  It currently uses&lt;br/&gt;
the default which builds all indexes in parallel, which may require all files to be open at the same time.  Will see if sequential option still pops&lt;br/&gt;
the bug before the fix.  If I can&apos;t get  a fix soon, I will disable test on non-windows.&lt;/p&gt;</comment>
                            <comment id="13215293" author="myrna" created="Fri, 24 Feb 2012 01:19:20 +0000"  >&lt;p&gt;Oddly enough, after the latest change, I saw another failure with the largeDataTest suite:&lt;/p&gt;

&lt;p&gt;1) LobLimitsTestjava.sql.SQLException: Table/View &apos;BLOBTBL&apos; already exists in Schema &apos;APP&apos;.&lt;br/&gt;
	at org.apache.derby.client.am.SQLExceptionFactory40.getSQLException(Unknown Source)&lt;br/&gt;
	at org.apache.derby.client.am.SqlException.getSQLException(Unknown Source)&lt;br/&gt;
	at org.apache.derby.client.am.Statement.execute(Unknown Source)&lt;br/&gt;
	at org.apache.derbyTesting.functionTests.tests.largedata.LobLimitsTest.setupTables(LobLimitsTest.java:107)&lt;br/&gt;
	at org.apache.derbyTesting.functionTests.tests.largedata.LobLimitsTest$1.decorateSQL(LobLimitsTest.java:141)&lt;br/&gt;
	at org.apache.derbyTesting.junit.CleanDatabaseTestSetup.setUp(CleanDatabaseTestSetup.java:112)&lt;br/&gt;
	at junit.extensions.TestSetup$1.protect(TestSetup.java:20)&lt;br/&gt;
	at junit.extensions.TestSetup.run(TestSetup.java:25)&lt;br/&gt;
	at org.apache.derbyTesting.junit.BaseTestSetup.run(BaseTestSetup.java:57)&lt;br/&gt;
	at junit.extensions.TestDecorator.basicRun(TestDecorator.java:24)&lt;br/&gt;
	at junit.extensions.TestSetup$1.protect(TestSetup.java:21)&lt;br/&gt;
	at junit.extensions.TestSetup.run(TestSetup.java:25)&lt;br/&gt;
	at junit.extensions.TestDecorator.basicRun(TestDecorator.java:24)&lt;br/&gt;
	at junit.extensions.TestSetup$1.protect(TestSetup.java:21)&lt;br/&gt;
	at junit.extensions.TestSetup.run(TestSetup.java:25)&lt;br/&gt;
	at org.apache.derbyTesting.junit.BaseTestSetup.run(BaseTestSetup.java:57)&lt;br/&gt;
	at junit.extensions.TestDecorator.basicRun(TestDecorator.java:24)&lt;br/&gt;
	at junit.extensions.TestSetup$1.protect(TestSetup.java:21)&lt;br/&gt;
	at junit.extensions.TestSetup.run(TestSetup.java:25)&lt;br/&gt;
Caused by: org.apache.derby.client.am.SqlException: Table/View &apos;BLOBTBL&apos; already exists in Schema &apos;APP&apos;.&lt;br/&gt;
	at org.apache.derby.client.am.Statement.completeSqlca(Unknown Source)&lt;br/&gt;
	at org.apache.derby.client.am.Statement.completeExecuteImmediate(Unknown Source)&lt;br/&gt;
	at org.apache.derby.client.net.NetStatementReply.parseEXCSQLIMMreply(Unknown Source)&lt;br/&gt;
	at org.apache.derby.client.net.NetStatementReply.readExecuteImmediate(Unknown Source)&lt;br/&gt;
	at org.apache.derby.client.net.StatementReply.readExecuteImmediate(Unknown Source)&lt;br/&gt;
	at org.apache.derby.client.net.NetStatement.readExecuteImmediate_(Unknown Source)&lt;br/&gt;
	at org.apache.derby.client.am.Statement.readExecuteImmediate(Unknown Source)&lt;br/&gt;
	at org.apache.derby.client.am.Statement.flowExecute(Unknown Source)&lt;br/&gt;
	at org.apache.derby.client.am.Statement.executeX(Unknown Source)&lt;br/&gt;
	... 26 more&lt;/p&gt;
</comment>
                            <comment id="13217573" author="mmccawley" created="Mon, 27 Feb 2012 21:23:15 +0000"  >&lt;p&gt;I ran the patch on my databases in Ubuntu and did not have the same problem.&lt;/p&gt;</comment>
                            <comment id="13218174" author="myrna" created="Tue, 28 Feb 2012 13:41:41 +0000"  >&lt;p&gt;I&apos;ve seen this failure again, this time on windows. &lt;br/&gt;
It happened in the very last test: &lt;br/&gt;
    (emb)largedata.LobLimitsTest.test_05_ClobNegative used 707031 ms E &lt;br/&gt;
The Derby5624Test passed on this windows run. &lt;/p&gt;

&lt;p&gt;It appears to me that even without the new test the largeDataTest suite runs slower than before on Linux. &lt;br/&gt;
Perhaps this is related to changes made to the test harness recently? &lt;br/&gt;
Some changes went in just today that perhaps improve this - I&apos;ll rerun and report back. &lt;/p&gt;

&lt;p&gt;Unfortunately, when the suite passes on my Linux machines, it takes about 11 hours or longer, and fills up the disk to up to 99% at times. &lt;br/&gt;
I use a new test directory each night. So if there&apos;s a failure, things get progressively worse in subsequent tests because of left-over files filling up the disk. It&apos;s a balancing act between getting time to look at a failure and filling up the disk. &lt;/p&gt;</comment>
                            <comment id="13218233" author="kristwaa" created="Tue, 28 Feb 2012 14:45:46 +0000"  >&lt;p&gt;A change introduced by &lt;a href=&quot;https://issues.apache.org/jira/browse/DERBY-5614&quot; title=&quot;NullPointerException with INSERT INTO [global temporary table] SELECT ... FROM [VTI]&quot; class=&quot;issue-link&quot; data-issue-key=&quot;DERBY-5614&quot;&gt;&lt;del&gt;DERBY-5614&lt;/del&gt;&lt;/a&gt; may have cause the slowdown, but I don&apos;t see how it&apos;s related to the extra disk usage. The bug in SpawnedProcess would cause the VM to stay alive idling until the TimerTask got purged (even when cancelled), but the order of the tests matters. So it would only be relevant if a test using SpawnedProcess ran less than 45 minutes before the last test in the run finished.&lt;/p&gt;

&lt;p&gt;When I ran suites.All in parallell just now (4 runners), it took 1 hour and 6 minutes (including org.apache.derby.PackagePrivateTestSuite and org.apache.derbyTesting.functionTests.tests.memory._Suite). This was on Solaris 11 with JDK 7. Assuming all 4 runners are busy at all times that&apos;s a worst case time of around 4.5 hours - 11 hours definitely sounds like way too much (unless that machine is running a lot of other stuff at the same time).&lt;/p&gt;</comment>
                            <comment id="13218405" author="myrna" created="Tue, 28 Feb 2012 17:58:04 +0000"  >&lt;p&gt;Hi Kristian, thanks for your response...&lt;br/&gt;
I didn&apos;t mean to imply that the disk usage is related the changes for &lt;a href=&quot;https://issues.apache.org/jira/browse/DERBY-5614&quot; title=&quot;NullPointerException with INSERT INTO [global temporary table] SELECT ... FROM [VTI]&quot; class=&quot;issue-link&quot; data-issue-key=&quot;DERBY-5614&quot;&gt;&lt;del&gt;DERBY-5614&lt;/del&gt;&lt;/a&gt; or any other recent change, apologies for the confusion.&lt;/p&gt;

&lt;p&gt;In comments on this bug I was referring to the full largedata._Suite (suites.All only runs largedata.LobLimitsLiteTest).&lt;/p&gt;</comment>
                            <comment id="13218446" author="mikem" created="Tue, 28 Feb 2012 18:39:37 +0000"  >&lt;p&gt;i&apos;d appreciate it if someone good with junit could review my new test.  Most critical would be if it has the appropriate decorators.  I could move the new test to the end if we think it is affecting the other tests, or alter the test to drop the table sooner (I was assuming the table would get&lt;br/&gt;
dropped automatically).  The new test really should not be that much of a drain on disk space, compared to the rest of the tests as it mostly&lt;br/&gt;
adds 1000 small files.   &lt;/p&gt;

&lt;p&gt;I can also back out test on windows also if we think it is causing problems there.  &lt;/p&gt;</comment>
                            <comment id="13218457" author="myrna" created="Tue, 28 Feb 2012 18:44:26 +0000"  >&lt;p&gt;I think the fact that I saw the File already exists failure with both a run on windows and one on linux, while we have the test skipped on linux, proves that your test does &lt;b&gt;not&lt;/b&gt; cause any harm. Apologies for not making this clear.&lt;/p&gt;</comment>
                            <comment id="13218705" author="myrna" created="Tue, 28 Feb 2012 23:41:16 +0000"  >&lt;p&gt;I changed the maximum open file setting for one linux machine from 1024 to 2048, and then the Derby5624Test passed.&lt;/p&gt;</comment>
                            <comment id="13219356" author="myrna" created="Wed, 29 Feb 2012 17:45:35 +0000"  >&lt;p&gt;I reran the largedata._Suite in windows with ibm 1.6 (sr9 fp1), build sync-ed to revision 1294957, and this time, I did not see the &apos;table/view already exist&apos; failure - all pass. So, that really was unrelated. Also, my run on linux with ibm 1.6 sr9fp1(skipping Derby5624Test) passed cleanly again. So the only remaining concern is the running out of file descriptors on a default configured linux system (with max no. of open files set to 1024).&lt;/p&gt;</comment>
                            <comment id="13219742" author="dagw" created="Thu, 1 Mar 2012 02:31:17 +0000"  >&lt;p&gt;I tested Derby5624Test on Solaris 11 (standalone) and it ran OK there, so it could be included in the suite for that platform, I think. It took 317s on my box.&lt;/p&gt;</comment>
                            <comment id="13244435" author="mikem" created="Mon, 2 Apr 2012 19:45:00 +0100"  >&lt;p&gt;fixed in trunk and backported to 10.8, 10.7, 10.6, and 10.5.  &lt;/p&gt;

&lt;p&gt;	&lt;a href=&quot;https://issues.apache.org/jira/browse/DERBY-5624&quot; title=&quot;System can run out of stack space while processing DropOnCommit requests.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;DERBY-5624&quot;&gt;&lt;del&gt;DERBY-5624&lt;/del&gt;&lt;/a&gt; System can run out of stack space while processing DropOnCommit requests.&lt;/p&gt;

&lt;p&gt;Taking care of cleanup after a commit is handled by notifying all &quot;Observers&quot;&lt;br/&gt;
that an event has taken place that they might want to act on and cleanup. In&lt;br/&gt;
the added test case this is triggered by off line commit which effectively&lt;br/&gt;
drops and recreates the base table and all of its indexes after loading the&lt;br/&gt;
data into them.&lt;/p&gt;

&lt;p&gt;Sometimes these Observers may execute work which adds to the Observer queue,&lt;br/&gt;
and that queue can &quot;miss&quot; them in the first pass through.&lt;/p&gt;

&lt;p&gt;A previous fix for this problem added a recursive call to notifyObservers in&lt;br/&gt;
the place that could cause this addition of observers. This recursive call&lt;br/&gt;
was causing stack problems when the number of Observers became large. For&lt;br/&gt;
the checked in test case this was 1000 indexes on 1000 columns of the table.&lt;br/&gt;
For other users I believe the cause was a by product of sorts on large disk&lt;br/&gt;
based sorts for multi-gigabyte tables and indexes. 2 users were reporting&lt;br/&gt;
similar failed stacks for failing compresses of large tables, and one was&lt;br/&gt;
able to take this fix to their environment and then successfully run the&lt;br/&gt;
compress.&lt;/p&gt;

&lt;p&gt;The fix was to remove the recursion and instead loop at the outermost point&lt;br/&gt;
until there were no Observers.&lt;/p&gt;

&lt;p&gt;Adding the test to the largedata suite as it takes over 10 minutes to run&lt;br/&gt;
on my machine. &lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="12710549">DERBY-6556</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12515284" name="DERBY-5624.patch" size="2231" author="mikem" created="Mon, 20 Feb 2012 21:30:40 +0000"/>
                            <attachment id="12515646" name="error-stacktrace_mod.out" size="9795" author="myrna" created="Wed, 22 Feb 2012 20:35:42 +0000"/>
                            <attachment id="12515645" name="largedatasuite_mod.out" size="24631" author="myrna" created="Wed, 22 Feb 2012 20:35:42 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Tue, 21 Feb 2012 14:58:56 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>228623</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12310090" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Issue &amp; fix info</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10422"><![CDATA[High Value Fix]]></customfieldvalue>
    <customfieldvalue key="10102"><![CDATA[Patch Available]]></customfieldvalue>
    <customfieldvalue key="10424"><![CDATA[Repro attached]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hy087b:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>35147</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12310050" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Urgency</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10051"><![CDATA[Urgent]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>
</channel>
</rss>