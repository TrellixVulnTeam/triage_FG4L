<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 03:43:02 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/DERBY-2911/DERBY-2911.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[DERBY-2911] Implement a buffer manager using java.util.concurrent classes</title>
                <link>https://issues.apache.org/jira/browse/DERBY-2911</link>
                <project id="10594" key="DERBY">Derby</project>
                    <description>&lt;p&gt;There are indications that the buffer manager is a bottleneck for some types of multi-user load. For instance, Anders Morken wrote this in a comment on &lt;a href=&quot;https://issues.apache.org/jira/browse/DERBY-1704&quot; title=&quot;Allow more concurrency in the lock manager&quot; class=&quot;issue-link&quot; data-issue-key=&quot;DERBY-1704&quot;&gt;&lt;del&gt;DERBY-1704&lt;/del&gt;&lt;/a&gt;: &quot;With a separate table and index for each thread (to remove latch contention and lock waits from the equation) we (...) found that org.apache.derby.impl.services.cache.Clock.find()/release() caused about 5 times more contention than the synchronization in LockSet.lockObject() and LockSet.unlock(). That might be an indicator of where to apply the next push&quot;.&lt;/p&gt;

&lt;p&gt;It would be interesting to see the scalability and performance of a buffer manager which exploits the concurrency utilities added in Java SE 5.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12373154">DERBY-2911</key>
            <summary>Implement a buffer manager using java.util.concurrent classes</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="knutanders">Knut Anders Hatlen</assignee>
                                    <reporter username="knutanders">Knut Anders Hatlen</reporter>
                        <labels>
                    </labels>
                <created>Fri, 6 Jul 2007 10:51:48 +0100</created>
                <updated>Mon, 29 Jun 2009 15:07:10 +0100</updated>
                            <resolved>Tue, 1 Apr 2008 10:05:58 +0100</resolved>
                                    <version>10.4.1.3</version>
                                    <fixVersion>10.4.1.3</fixVersion>
                                    <component>Services</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>1</watches>
                                                                <comments>
                            <comment id="12523242" author="knutanders" created="Tue, 28 Aug 2007 14:57:08 +0100"  >&lt;p&gt;I am interested in working on this issue. What I would like to&lt;br/&gt;
achieve, is to make it possible for different threads to access the&lt;br/&gt;
buffer manager without blocking each other, as long as they request&lt;br/&gt;
different objects and the requested objects are present in the&lt;br/&gt;
cache. (That is, I think it is OK that accesses to the same object or&lt;br/&gt;
accesses that need to fetch an object into the cache might have to&lt;br/&gt;
wait for each other, as they do in the current buffer manager.)&lt;/p&gt;

&lt;p&gt;There are two ways to achieve this:&lt;/p&gt;

&lt;p&gt;  1) Rewrite the old buffer manager (Clock) so that it allows more&lt;br/&gt;
     concurrent access (possibly splitting the HashMap and changing&lt;br/&gt;
     the synchronization model for Clock/CachedItem)&lt;/p&gt;

&lt;p&gt;  2) Write a new buffer manager which uses the concurrency utilities&lt;br/&gt;
     in newer Java versions (ConcurrentHashMap, ReentrantLock and&lt;br/&gt;
     friends)&lt;/p&gt;

&lt;p&gt;I like option 2 best myself, mostly because it allows us to reuse the&lt;br/&gt;
wheel (concurrency utils) rather than reinventing it. The downside is&lt;br/&gt;
that the old implementation must be kept in the code as long as we&lt;br/&gt;
support JVMs without the concurrency utilities (JDK 1.4 and&lt;br/&gt;
Foundation). Because of the clearly defined interface (CacheManager)&lt;br/&gt;
for the buffer manager, adding an alternative implementation should be&lt;br/&gt;
transparent to the rest of the Derby code, though.&lt;/p&gt;

&lt;p&gt;If we decide to go for option 2, I will try to implement it&lt;br/&gt;
incrementally with these steps&lt;/p&gt;

&lt;p&gt;  1) Implement a buffer manager with no replacement policy (that is,&lt;br/&gt;
     it ignores the maximum size and never throws data out). After&lt;br/&gt;
     this step, the buffer manager should allow concurrent access for&lt;br/&gt;
     all threads that request different objects.&lt;/p&gt;

&lt;p&gt;  2) Implement the replacement policy. After this step, the buffer&lt;br/&gt;
     manager should be able to throw out objects that have not been&lt;br/&gt;
     used for some time, and thereby avoid growing bigger than the&lt;br/&gt;
     maximum size.&lt;/p&gt;

&lt;p&gt;  3) Enable the new buffer manager by default for JDK 1.5 and higher.&lt;/p&gt;

&lt;p&gt;In step 2, I think I will stick to the clock algorithm that we&lt;br/&gt;
currently use. Last year, a Google Summer of Code student investigated&lt;br/&gt;
different replacement algorithms for Derby. Although changing the&lt;br/&gt;
replacement algorithm is out of the scope of this issue, he suggested&lt;br/&gt;
some changes to make it easier to switch replacement algorithm. I will&lt;br/&gt;
see if I can get some ideas from his work.&lt;/p&gt;

&lt;p&gt;Comments to this plan would be appreciated.&lt;/p&gt;</comment>
                            <comment id="12523256" author="djd" created="Tue, 28 Aug 2007 16:17:49 +0100"  >&lt;p&gt;I&apos;d say option 2 is best.&lt;/p&gt;</comment>
                            <comment id="12523288" author="bryanpendleton" created="Tue, 28 Aug 2007 18:34:16 +0100"  >&lt;p&gt;Would a particular instantiation of the engine ever have more than&lt;br/&gt;
one buffer manager at a time? How (and why) would a user choose&lt;br/&gt;
one buffer manager implementation versus the other? Would it&lt;br/&gt;
be as simple as:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if this is JDK 1.5+, we always unconditionally use the new one&lt;/li&gt;
	&lt;li&gt;if this is JDK 1.4, we always unconditionally use the old one&lt;br/&gt;
Or is there some other reason that a user would want to override this?&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12523478" author="knutanders" created="Wed, 29 Aug 2007 07:48:08 +0100"  >&lt;p&gt;Thanks for your comments, Dan and Bryan.&lt;/p&gt;

&lt;p&gt;I was thinking about using Derby&apos;s module loader, which would basically do what Bryan says. With JDK 1.4 or J2ME, the old one is used, and with JDK 1.5+ the new one. It is possible to override the default by specifying -Dderby.module.cacheManager=org.apache.derby... on the command line, but only when using sane builds, I think. Except for the obvious case where there is a bug in the new implementation, I don&apos;t think users would want to override it. The one thing I can think of, is if the old implementation performs better for a certain load/configuration.&lt;/p&gt;</comment>
                            <comment id="12523549" author="knutanders" created="Wed, 29 Aug 2007 12:50:44 +0100"  >&lt;p&gt;I noticed that some of the methods in CacheFactory/CacheManager are not used. As a first step before starting on a new cache manager, I would like to remove those methods from the interfaces (but I will leave the actual implementations in Clock untouched for now). This way we can avoid having two implementations of something that&apos;s never used. We can always implement some of the methods later, if we ever need them, but I think it is better to wait until the need arises, rather than to guess now what we might need in the future.&lt;/p&gt;

&lt;p&gt;The methods that are not used, are:&lt;/p&gt;

&lt;p&gt;  CacheFactory.newSizedCacheManager()&lt;br/&gt;
  CacheManager.getMaximumSize()&lt;br/&gt;
  CacheManager.resize()&lt;br/&gt;
  CacheManager.containsKey()&lt;br/&gt;
  CacheManager.setUsed()&lt;br/&gt;
  CacheManager.getNumberInUse()&lt;br/&gt;
  CacheManager.getCacheStats()&lt;br/&gt;
  CacheManager.resetCacheStats()&lt;br/&gt;
  CacheManager.scan()&lt;/p&gt;

&lt;p&gt;getCacheStats() and resetCacheStats() are called by other methods (simple forwarding methods), but those methods are never called. The attached patch (d2911-unused) removes all the unused methods, including the methods that call getCacheStats() and resetCacheStats(). suites.All passed. I have also started a derbyall run which has not completed yet.&lt;/p&gt;</comment>
                            <comment id="12523782" author="knutanders" created="Thu, 30 Aug 2007 08:08:22 +0100"  >&lt;p&gt;Committed d2911-unused.diff with revision 571055.&lt;/p&gt;</comment>
                            <comment id="12524488" author="knutanders" created="Mon, 3 Sep 2007 11:06:09 +0100"  >&lt;p&gt;I have written a small test which can be used to measure performance improvements in this issue. It is a multi-threaded test which performs primary-key lookups. Each thread has its own table to work on to avoid contention on latches (which is a scalability issue separate from the one we&apos;re trying to solve in this JIRA issue).&lt;/p&gt;

&lt;p&gt;It&apos;s a standalone Java program for now since our JUnit framework doesn&apos;t support multi-threaded tests yet, as far as I can see. To run the test, use this command line:&lt;/p&gt;

&lt;p&gt;  java d2911perf &amp;lt;DB-URL&amp;gt; &amp;lt;THREADS&amp;gt; &amp;lt;WARMUP&amp;gt; &amp;lt;COLLECT&amp;gt;&lt;/p&gt;

&lt;p&gt;The test will have a warm-up phase of &amp;lt;WARMUP&amp;gt; seconds and collect results for &amp;lt;COLLECT&amp;gt; seconds.&lt;/p&gt;</comment>
                            <comment id="12524519" author="knutanders" created="Mon, 3 Sep 2007 15:07:35 +0100"  >&lt;p&gt;Attached is a partial implementation of a new buffer manager. It adds a class called ConcurrentCache, which implements the CacheManager interface and keeps the cached objects in a ConcurrentHashMap. It also adds ConcurrentCacheFactory, which creates instances of the new cache, and CacheEntry, which represents an entry in the cache and uses a ReentrantLock to protect its internal state from concurrent accesses.&lt;/p&gt;

&lt;p&gt;Currently, only basic operations like find, release, release, remove and clean have been implemented. There is no replacement algorithm, which basically means that the cache doesn&apos;t have a defined maximum size. I managed to run the attached performance test with the patch (manually edited modules.properties to load the new buffer manager). It showed promising results. With 100 threads on a machine with eight CPUs, the throughput was almost doubled. With fewer threads (or fewer CPUs) there was of course less improvement. I didn&apos;t notice any performance loss for the single-threaded case, though.&lt;/p&gt;</comment>
                            <comment id="12524541" author="bryanpendleton" created="Mon, 3 Sep 2007 16:21:27 +0100"  >&lt;p&gt;Is there something about our JUnit framework that inhibits multi-threaded tests?&lt;br/&gt;
I&apos;m not sure why JUnit would know or care how many threads you used.&lt;/p&gt;</comment>
                            <comment id="12524559" author="kristwaa" created="Mon, 3 Sep 2007 18:10:14 +0100"  >&lt;p&gt;I think the main issue is that the JUnit framework (for instance the runner) is not aware of threads spawned from the test method.&lt;br/&gt;
If you don&apos;t code the waiting for the spawned threads into your test method, JUnit will just complete and continue/exit - possibly terminating your threads if the test is the last test to be run. &lt;br/&gt;
Second, you don&apos;t have any reporting, monitoring or controlling capabilities for the threads.&lt;/p&gt;

&lt;p&gt;If you write your test method appropriately (some approaches used are Thread.join() or features from the java.util.concurrent package in Java SE 5.0), I think you would be okay. You would just not have any &quot;support functions&quot; from the framework, and would have to do everything yourself.&lt;br/&gt;
There are also a few libraries for writing multithreaded JUnit tests out there (GroboUtils, the TestDecorator approach, maybe more?).&lt;/p&gt;

&lt;p&gt;Other test frameworks, for instance TestNG, have the possibility to specify how many parallel threads that are running a test method (not to be mixed up with the support to run tests in parallel, taking things like dependencies into account). It is not quite clear to me how success/failure is measured, or how the timeout feature is implemented.&lt;/p&gt;</comment>
                            <comment id="12524690" author="knutanders" created="Tue, 4 Sep 2007 12:39:54 +0100"  >&lt;p&gt;I was primarily referring to the performance test framework (JDBCPerfTestCase) which makes it very easy to write single-threaded performance tests, but currently doesn&apos;t support writing multi-threaded tests. I&apos;m sure there&apos;s nothing that prevents us from writing multi-threaded tests provided that we address the challenges summarized by Kristian.&lt;/p&gt;</comment>
                            <comment id="12524706" author="knutanders" created="Tue, 4 Sep 2007 13:27:53 +0100"  >&lt;p&gt;Since the code added by the patch is disabled, there should be little risk of breaking something. I therefore committed it with revision 572645. We would need the classes and the stubs regardless of whether there are comments to the implementation, I think. And you should of course feel free to test it and comment on it.&lt;/p&gt;

&lt;p&gt;The simplest way to test it, is to edit the derby.module.cacheManager property in modules.property so that it points to ConcurrentCacheFactory instead of ClockFactory, and recompile. I made that change myself and ran the full JUnit test suite. There was a total of 19 test cases failing (17 errors + 2 failures). Most of them failed because CacheManager.values() is not implemented (used by a VTI). I&apos;ll investigate the other failures, but it seems at least some of them are caused by the lack of replacement policy so that objects are left in the cache when the test expects them to have been thrown out.&lt;/p&gt;</comment>
                            <comment id="12524745" author="knutanders" created="Tue, 4 Sep 2007 15:48:10 +0100"  >&lt;p&gt;The attached patch (d2911-2.diff) implements the values() method. With this patch, I only see 13 failures in the JUnit tests. All of the failures are in StatementPlanCacheTest and fail with &quot;expect plan to &lt;span class=&quot;error&quot;&gt;&amp;#91;be&amp;#93;&lt;/span&gt; thrown out&quot;. These failures are expected until the replacement policy is in place.&lt;/p&gt;

&lt;p&gt;Committed revision 572693.&lt;/p&gt;</comment>
                            <comment id="12524802" author="mkhettry" created="Tue, 4 Sep 2007 18:53:38 +0100"  >&lt;p&gt;One thought on the ConcurrentCache class. Since this is a Java 5 class,  is it possbie to use generics? In this case something like:&lt;/p&gt;

&lt;p&gt;ConcurrentCache&amp;lt;K&amp;gt; &lt;/p&gt;
{
  private final ConcurrentHashMap&amp;lt;K, CacheEntry&amp;gt; cache;
}

&lt;p&gt;I realize that most of (or all) the time, users of this class will be pre jdk 5 classes and this may not buy us all that much but it is still worth doing I think. &lt;/p&gt;</comment>
                            <comment id="12525012" author="knutanders" created="Wed, 5 Sep 2007 09:19:16 +0100"  >&lt;p&gt;Thanks Manish, that&apos;s a great suggestion, but since the objects in the cache need to implement the Cacheable interface and Cacheable.getIdentity() returns an Object, we&apos;re kind of locked into declaring the hash key as Object. We could work around that problem by casting the identity to the parametrized type K, like&lt;/p&gt;

&lt;p&gt;    K key = (K) cacheable.getIdentity();&lt;br/&gt;
    cache.remove(key);&lt;/p&gt;

&lt;p&gt;but that&apos;ll give us a compile-time warning about an unchecked cast. So until the interfaces that we need to work with are rewritten using generics, I think it&apos;s best not to parametrize the key type.&lt;/p&gt;</comment>
                            <comment id="12525094" author="knutanders" created="Wed, 5 Sep 2007 14:40:37 +0100"  >&lt;p&gt;Attached is a patch (d2911-3.diff) which implements the shutdown method (more code will probably have to go in there when we have a replacement policy and a background writer). The shutdown method ensures that new calls to find() and findCached() return null (and also create() since that&apos;s how it was implemented in Clock, although the CacheManager interface doesn&apos;t require that) by setting a volatile boolean flag. It also calls cleanAll() and ageOut() as demanded by CacheManager.shutdown()&apos;s javadoc (Clock actually calls ageOut() both before and after cleanAll(), but I fail to see why that should make any difference).&lt;/p&gt;</comment>
                            <comment id="12525096" author="knutanders" created="Wed, 5 Sep 2007 14:44:29 +0100"  >&lt;p&gt;Committed revision 572953.&lt;/p&gt;</comment>
                            <comment id="12525706" author="knutanders" created="Fri, 7 Sep 2007 14:19:54 +0100"  >&lt;p&gt;The next step is to implement the replacement algorithm. As I said&lt;br/&gt;
earlier, I will use the clock algorithm as the old buffer manager&lt;br/&gt;
does, but separate the implementation of the buffer manager and the&lt;br/&gt;
replacement algorithm to make it easier to switch to or experiment&lt;br/&gt;
with other algorithms.&lt;/p&gt;

&lt;p&gt;I was thinking that we could have an interface for the replacement&lt;br/&gt;
policy that would look similar to this:&lt;/p&gt;

&lt;p&gt;interface ReplacementPolicy {&lt;br/&gt;
  Callback insertEntry(CacheEntry e);&lt;br/&gt;
  interface Callback &lt;/p&gt;
{
    void access();
    void free();
  }
&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;So when ConcurrentCache needs to insert a new entry, it would call&lt;br/&gt;
ReplacementPolicy.insertEntry() which would find free space (or make&lt;br/&gt;
room) for the new entry. With the clock algorithm, this means moving&lt;br/&gt;
the clock handle until a not recently used entry is found, and&lt;br/&gt;
possibly cleaning dirty entries or increasing the size of the&lt;br/&gt;
clock. insertEntry() would return a callback object, which&lt;br/&gt;
ConcurrentCache could use to notify the replacement policy about&lt;br/&gt;
events. Callback.access() would be used to notify that someone&lt;br/&gt;
accessed the object, so that it for instance could be marked as&lt;br/&gt;
recently used in the clock, and Callback.free() would mark the entry&lt;br/&gt;
as unused/invalid and make it available for reuse.&lt;/p&gt;</comment>
                            <comment id="12526718" author="knutanders" created="Wed, 12 Sep 2007 10:12:46 +0100"  >&lt;p&gt;Some observations on the replacement algorithm in the current buffer&lt;br/&gt;
manager:&lt;/p&gt;

&lt;p&gt;1) If it is known that there is at least one invalid entry in the&lt;br/&gt;
cache, the normal clock rotation algorithm (which scans no more than&lt;br/&gt;
20% of the cache) is bypassed and a scan which may go through the&lt;br/&gt;
entire cache is performed instead. (The rationale is that we will&lt;br/&gt;
avoid growing the cache if we don&apos;t have to.) This is probably OK&lt;br/&gt;
since invalid objects should be rare (normally present only after DROP&lt;br/&gt;
TABLE or similar). However, because objects that are about to be&lt;br/&gt;
inserted into the cache are regarded as invalid but kept so that they&lt;br/&gt;
can&apos;t actually be reused by someone else yet, the old buffer manager&lt;br/&gt;
has suffered from starting these scans too frequently (see&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/DERBY-704&quot; title=&quot;Large page cache kills initial performance&quot; class=&quot;issue-link&quot; data-issue-key=&quot;DERBY-704&quot;&gt;&lt;del&gt;DERBY-704&lt;/del&gt;&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;The new buffer manager should try to avoid this problem by only&lt;br/&gt;
starting a scan for invalid objects if it knows that there are items&lt;br/&gt;
that have been invalidated by removal.&lt;/p&gt;

&lt;p&gt;2) When the clock is rotated and the hand sweeps over an unkept, not&lt;br/&gt;
recently used and dirty object, the global synchronization is dropped&lt;br/&gt;
and the object is cleaned. After cleaning the object, the global&lt;br/&gt;
synchronization is re-obtained, but then some other thread may have&lt;br/&gt;
moved the clock hand past the cleaned object while the first thread&lt;br/&gt;
didn&apos;t hold the global synchronization lock. In that case, the first&lt;br/&gt;
thread has cleaned an object but is not allowed to reuse its&lt;br/&gt;
entry. Perhaps it&apos;s just a theoretical problem, but this could mean&lt;br/&gt;
that some threads have to write multiple dirty pages to disk before&lt;br/&gt;
they are allowed to insert a new one into the cache.&lt;/p&gt;

&lt;p&gt;Since the new buffer manager uses synchronization with finer&lt;br/&gt;
granularity, it should avoid this problem by keeping the&lt;br/&gt;
synchronization lock while the object is being cleaned. Then it knows&lt;br/&gt;
that the entry can be reused as soon as the object has been cleaned.&lt;/p&gt;</comment>
                            <comment id="12527441" author="knutanders" created="Fri, 14 Sep 2007 11:13:30 +0100"  >&lt;p&gt;Patch d2911-entry-javadoc.diff extends the class javadoc for CacheEntry with descriptions of its different states. I also changed a note in the javadoc which said that one thread could only hold the lock on one CacheEntry at a time to prevent deadlocks. Since the replacement algorithm probably needs to hold the lock on two entries at a time (the entry that is about to be inserted and the entry that is to be evicted), I loosened the single-lock-per-thread requirement. Now the javadoc says that it is OK to hold the lock on two different CacheEntry objects, as long as the first entry to be locked is in the uninitialized state and the second entry is not in the uninitialized state. By enforcing a strict order for obtaining the locks, concurrent threads won&apos;t run into deadlocks with each other.&lt;/p&gt;</comment>
                            <comment id="12527445" author="knutanders" created="Fri, 14 Sep 2007 11:22:42 +0100"  >&lt;p&gt;Committed the javadoc patch with revision 575607.&lt;/p&gt;</comment>
                            <comment id="12528713" author="knutanders" created="Wed, 19 Sep 2007 10:50:13 +0100"  >&lt;p&gt;Attaching a patch (d2911-4.diff) which makes a couple of small changes to ConcurrentCache.java:&lt;/p&gt;

&lt;p&gt;  1) Make findFreeCacheable() take the CacheEntry as a parameter call setCacheable() on the entry instead of returning a Cacheable. When the replacement algorithm is implemented, findFreeCacheable() is going to need the CacheEntry reference in order to insert the entry into the replacement policy&apos;s internal data structure (e.g., clock).&lt;/p&gt;

&lt;p&gt;  2) Make removeEntry() take a key (object identity) instead of a CacheEntry so that it also can be used to remove entries from the cache when setIdentity/createIdentity failed.&lt;/p&gt;</comment>
                            <comment id="12528717" author="knutanders" created="Wed, 19 Sep 2007 10:51:59 +0100"  >&lt;p&gt;Reattaching with &quot;grant license...&quot;&lt;/p&gt;</comment>
                            <comment id="12528719" author="knutanders" created="Wed, 19 Sep 2007 10:59:10 +0100"  >&lt;p&gt;Committed d2911-4 with revision 577224.&lt;/p&gt;</comment>
                            <comment id="12528750" author="knutanders" created="Wed, 19 Sep 2007 14:00:37 +0100"  >&lt;p&gt;Some thoughts about replacement algorithm and synchronization:&lt;/p&gt;

&lt;p&gt;In the old cache manager, one basically had a global synchronization lock that every user of the cache needed to obtain before entering the manager. One advantage of that approach is that there can&apos;t be deadlocks as long as there&apos;s only one lock. With the more fine-grained synchronization in the new buffer manager, one need to be careful not to introduce the risk of deadlocks.&lt;/p&gt;

&lt;p&gt;The current implementation of ConcurrentCache should not be deadlock prone, since each thread never has locked more than one entry in the cache. When the replacement algorithm is implemented it will be more complex, as there will be other data structures that we need to lock.&lt;/p&gt;

&lt;p&gt;Take for instance insertion of a new item into the cache. Then we at least need to synchronize on (A) the entry to insert, (B) the clock data structure, and (C) the entry to evict from the cache. In a previous comment, I mentioned that it is OK to lock two entries if the first one is about to be inserted and the second one is already in the cache, so locking C while holding the lock on A is OK. But allowing C to be locked while holding the lock on both A and B, would be problematic. If someone calls CacheManager.remove(), we first need to lock the entry that is about to be removed, and then lock the clock data structure to actually remove the entry. This would mean that we obtain the synchronization locks in the order C-&amp;gt;B, which could potentially lead to deadlocks if the insertion of entries had the lock order A-&amp;gt;B-&amp;gt;C.&lt;/p&gt;

&lt;p&gt;To solve this, I think we would have to break up the A-&amp;gt;B-&amp;gt;C lock chain somehow. What comes to mind, is that we could unlock B before we lock C, and then reobtain the lock on B after we have locked C. That would leave an open window for others to modify the relationship between B and C for a short while, so we would have to revalidate that what we thought we knew is still true.&lt;/p&gt;

&lt;p&gt;So instead of doing this&lt;/p&gt;

&lt;p&gt;lock A (entry to insert)&lt;br/&gt;
--&amp;gt;lock B (clock)&lt;br/&gt;
----&amp;gt;fetch C (entry to evict) from B&lt;br/&gt;
----&amp;gt;lock C&lt;br/&gt;
------&amp;gt;reuse Cacheable from C in A&lt;br/&gt;
------&amp;gt;remove C from B&lt;br/&gt;
------&amp;gt;unlock C&lt;br/&gt;
----&amp;gt;insert A into B&lt;br/&gt;
----&amp;gt;unlock B&lt;br/&gt;
--&amp;gt;unlock A&lt;br/&gt;
return&lt;/p&gt;

&lt;p&gt;we could do this&lt;/p&gt;

&lt;p&gt;lock A (entry to insert)&lt;br/&gt;
--&amp;gt;lock B (clock)&lt;br/&gt;
----&amp;gt;fetch C (entry to evict)&lt;br/&gt;
----&amp;gt;unlock B&lt;br/&gt;
--&amp;gt;lock C&lt;br/&gt;
----&amp;gt;validate that no one else evicted C after we unlocked B (otherwise retry the preceding steps)&lt;br/&gt;
----&amp;gt;reuse Cacheable from C in A&lt;br/&gt;
----&amp;gt;lock B&lt;br/&gt;
------&amp;gt;remove C from B&lt;br/&gt;
------&amp;gt;unlock B&lt;br/&gt;
----&amp;gt;unlock C&lt;br/&gt;
--&amp;gt;unlock A&lt;br/&gt;
return&lt;/p&gt;

&lt;p&gt;This way we break down the A-&amp;gt;B-&amp;gt;C locking into the sequences A-&amp;gt;B and by A-&amp;gt;C-&amp;gt;B, none of which conflict with the B-&amp;gt;C locking required by CacheFactory.remove().&lt;/p&gt;</comment>
                            <comment id="12528768" author="dagw" created="Wed, 19 Sep 2007 14:57:00 +0100"  >&lt;p&gt; Makes sense to me. The last sentence should probably read:&lt;/p&gt;

&lt;p&gt;&quot;This way we break down the A-&amp;gt;B-&amp;gt;C locking into the sequences A-&amp;gt;B and by A-&amp;gt;C-&amp;gt;B, none of which conflict with the C-&amp;gt;B locking required by CacheFactory.remove().&quot; &lt;br/&gt;
i.e.&lt;br/&gt;
(C-&amp;gt;B instead of B-&amp;gt;C) ?&lt;/p&gt;</comment>
                            <comment id="12528771" author="knutanders" created="Wed, 19 Sep 2007 15:07:49 +0100"  >&lt;p&gt;Thanks Dag! You&apos;re quite right, it should have said C-&amp;gt;B.&lt;/p&gt;</comment>
                            <comment id="12529019" author="knutanders" created="Thu, 20 Sep 2007 09:05:08 +0100"  >&lt;p&gt;I manually enabled the new buffer manager in modules.properties and ran suites.All + derbyall. The number of test cases failing in suites.All is now 7, of which 6 are expected failures in StatementPlanCacheTest mentioned in a previous comment. The last JUnit failure is in ImportExportTest:client:&lt;/p&gt;

&lt;p&gt;1) ImportExportTest:clientjava.sql.SQLException: Limitation: Record cannot be updated or inserted due to lack of space on the page. Use the parameters derby.storage.pageSize and/or derby.storage.pageReservedSpace to work around this limitation.&lt;/p&gt;

&lt;p&gt;In derbyall, 8 tests failed:&lt;/p&gt;

&lt;p&gt;derbyall/derbyall.fail:unit/cacheService.unit&lt;br/&gt;
derbyall/storeall/storeall.fail:store/rollForwardRecovery.sql&lt;br/&gt;
derbyall/storeall/storeall.fail:store/backupRestore1.java&lt;br/&gt;
derbyall/storeall/storeall.fail:store/OnlineBackupTest1.java&lt;br/&gt;
derbyall/storeall/storeall.fail:unit/T_RawStoreFactory.unit&lt;br/&gt;
derbyall/storeall/storeall.fail:unit/T_b2i.unit&lt;br/&gt;
derbyall/storeall/storeall.fail:unit/recoveryTest.unit&lt;br/&gt;
derbyall/storeall/storeall.fail:store/RecoveryAfterBackup.java&lt;/p&gt;

&lt;p&gt;All the failures in derbyall seem to happen because entries are left in the cache in an invalid state if Cacheable.setIdentity() or Cacheable.createIdentity() throws a StandardException. I will fix this by removing the entries whose identity cannot be set/created.&lt;/p&gt;</comment>
                            <comment id="12529129" author="knutanders" created="Thu, 20 Sep 2007 16:02:00 +0100"  >&lt;p&gt;The attached patch (d2911-5.diff) fixes the problem with invalid entries being left in the cache if Cacheable.setIdentity()/createIdentity() fails. Most of the derbyall failures went away. I&apos;m rerunning the tests now to see if this change introduces any new failures.&lt;/p&gt;</comment>
                            <comment id="12529330" author="knutanders" created="Fri, 21 Sep 2007 08:55:32 +0100"  >&lt;p&gt;Committed d2911-5 with revision 578006.&lt;/p&gt;

&lt;p&gt;When I reran suites.All and derbyall with the new buffer manager enabled after applying this patch, I only saw the expected failures in suites.All and two failures in derbyall.&lt;/p&gt;

&lt;p&gt;To find out which of the failures that are to be expected when the buffer manager has no replacement algorithm, I ran the regression tests with the old buffer manager modified so that the replacement algorithm wasn&apos;t used (that is, I hard coded the Clock.maximumSize to Integer.MAX_VALUE). Then I saw the same failures in suites.All, but only one failure in derbyall. That leaves us with one &quot;unexplained&quot; error: unit/T_RawStoreFactory.unit which runs out of time.&lt;/p&gt;</comment>
                            <comment id="12529945" author="knutanders" created="Mon, 24 Sep 2007 20:05:27 +0100"  >&lt;p&gt;It seems like the failure in unit/T_RawStoreFactory.unit is to be expected too. Hard coding Clock.maximumSize to Integer.MAX_VALUE didn&apos;t disable all parts of the replacement algorithm. Clock.findFreeItem() would still reuse invalid entries instead of allocating new ones. When I also disabled the reuse of invalid entries, the test failed with Clock as well.&lt;/p&gt;</comment>
                            <comment id="12530696" author="knutanders" created="Thu, 27 Sep 2007 13:05:31 +0100"  >&lt;p&gt;Attaching a new patch (d2911-6.diff) which implements a replacement algorithm using the interface and synchronization model described in the previous comments. The main part of the patch is the new ClockPolicy class and its rotateClock() method. I have done my best to state all requirements about synchronization and lock order in comments, so I hope it is possible for others to understand the code...&lt;/p&gt;

&lt;p&gt;I manually edited modules.properties to enable the new buffer manager and ran the full regression test suite. suites.All ran successfully, whereas derbyall had one failure. The failure in derbyall was unit/T_RawStoreFactory.unit which has been mentioned before. This is an expected failure until reuse of invalid entries has been enabled for caches whose size is smaller than the maximum size.&lt;/p&gt;

&lt;p&gt;I have not run any performance tests on this last patch, but I will do so. The performance test attached to this issue doesn&apos;t test the replacement algorithm since it creates just a small database. I will therefore see if I can run some other tests with different buffer sizes and also test it with update load, for instance using the test client attached to &lt;a href=&quot;https://issues.apache.org/jira/browse/DERBY-1961&quot; title=&quot;Investigate resource usage for different types of load on an in-memory database&quot; class=&quot;issue-link&quot; data-issue-key=&quot;DERBY-1961&quot;&gt;&lt;del&gt;DERBY-1961&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;What&apos;s left to do:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;reuse Cacheable from invalid entries instead of growing the clock when size &amp;lt; maxSize (will fix the failure in unit/T_RawStoreFactory.unit)&lt;/li&gt;
	&lt;li&gt;implement a background cleaner&lt;/li&gt;
	&lt;li&gt;shrink the cache if it exceeds the maximum size&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12530942" author="knutanders" created="Fri, 28 Sep 2007 09:37:33 +0100"  >&lt;p&gt;I checked in the v6 patch with revision 580252. The new buffer manager is still disabled by default. To test it out, you need to edit modules.properties and replace ClockFactory with ConcurrentCacheFactory.&lt;/p&gt;

&lt;p&gt;I have run some performance tests with various types of load. The results look promising. I will come back with the details later.&lt;/p&gt;</comment>
                            <comment id="12531061" author="knutanders" created="Fri, 28 Sep 2007 16:54:48 +0100"  >&lt;p&gt;I mentioned that I ran some performance tests with the new buffer manager after patch #6. Here&apos;s a pdf (perftest6.pdf) with the details for those interested in graphs and numbers. I have used the test client attached to this issue and the one attached to &lt;a href=&quot;https://issues.apache.org/jira/browse/DERBY-1961&quot; title=&quot;Investigate resource usage for different types of load on an in-memory database&quot; class=&quot;issue-link&quot; data-issue-key=&quot;DERBY-1961&quot;&gt;&lt;del&gt;DERBY-1961&lt;/del&gt;&lt;/a&gt;, so there are numbers for single-record select operations, update operations and two-way joins. To test the replacement algorithm, I varied the page cache size from 40 up to 40000.&lt;/p&gt;

&lt;p&gt;The tests were run both on an AMD Opteron with two 2.4 GHz CPUs, and on a Sun Fire T2000, 1.0 GHz with 8 cores/32 hardware threads.&lt;/p&gt;

&lt;p&gt;The test client attached to this issue was used to show the scalability of the buffer manager. The &lt;a href=&quot;https://issues.apache.org/jira/browse/DERBY-1961&quot; title=&quot;Investigate resource usage for different types of load on an in-memory database&quot; class=&quot;issue-link&quot; data-issue-key=&quot;DERBY-1961&quot;&gt;&lt;del&gt;DERBY-1961&lt;/del&gt;&lt;/a&gt; client was used primarily to test the replacement algorithm, since that client scales poorly because of latch contention on the root node of the B-tree (all threads work on the same table).&lt;/p&gt;

&lt;p&gt;For the details, see the pdf, but I&apos;m pasting the conclusion here:&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;For most of the loads used in these tests, the new buffer manager performs as good as or better than the old buffer manager. There was one exception: single-record update on Opteron with page cache size 40000. That test was not conclusive, though, as the single run which had highest throughput in that configuration was the new buffer manager with 60 concurrent clients. Overall, the new buffer manager seems to increase the performance.&lt;/p&gt;

&lt;p&gt;Note that the new buffer manager does not yet implement a background cleaner, which is supposed to increase the likelihood of finding a clean page to evict and thereby making the page replacement algorithm more effective. It is not clear to me whether the existence of a background cleaner affects the performance of these tests.&lt;/p&gt;
&lt;hr /&gt;</comment>
                            <comment id="12531521" author="knutanders" created="Mon, 1 Oct 2007 15:35:39 +0100"  >&lt;p&gt;I don&apos;t understand why unit/T_RawStoreFactory.unit fails. Or to be more specific, I don&apos;t understand the connection between the failure and the buffer manager. The failure is caused by some pages not being freed (that is, marked as free in the alloc page) on a rollback to savepoint in T_RawStoreFactory.P042(). What&apos;s strange is that&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;with Clock (old buffer manager) it works fine&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;when the scan for invalid items to reuse in Clock.findFreeItem() is commented out, it fails&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;with unmodified Clock, and all test cases except P042() commented out, it fails&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;So it seems like the result of P042 is somehow dependent on the contents of the page cache (or perhaps the container cache?) when the test case starts, which is strange both because I thought the alloc page didn&apos;t care whether a page was in the cache or not, and because the test case creates a new container at the beginning so that none of the pages used in the test should be present in the cache anyway.&lt;/p&gt;</comment>
                            <comment id="12531802" author="knutanders" created="Tue, 2 Oct 2007 14:56:08 +0100"  >&lt;p&gt;Attaching a new patch (d2911-7.diff) which implements reuse of free holder objects if the maximum size of the cache has not been reached (it only touches one file - ClockPolicy.java). The patch fixes the failure in unit/T_RawStoreFactory.unit. I have also started the full regression test suite.&lt;/p&gt;

&lt;p&gt;The failure in T_RawStoreFactory is still a mystery to me. I ended up with scanning for free items backwards from the end of the clock. If the scan started from the beginning, the test would fail. I suspect that either the test does not test what it&apos;s supposed to test, or perhaps there is a bug somewhere, but since I see the same failure if I change the scan direction in the old buffer manager, I&apos;m confident that the buffer manager is not the problem.&lt;/p&gt;</comment>
                            <comment id="12534027" author="knutanders" created="Thu, 11 Oct 2007 13:40:26 +0100"  >&lt;p&gt;Now that the mystery with the failure in T_RawStoreFactory is solved (&lt;a href=&quot;https://issues.apache.org/jira/browse/DERBY-3099&quot; title=&quot;Possible bug in interaction with buffer manager causing pages not to be freed on rollback to savepoint&quot; class=&quot;issue-link&quot; data-issue-key=&quot;DERBY-3099&quot;&gt;&lt;del&gt;DERBY-3099&lt;/del&gt;&lt;/a&gt;), I&apos;ll update the d2911-7 patch so that it scans for unused entries from the beginning of the clock.&lt;/p&gt;</comment>
                            <comment id="12534293" author="knutanders" created="Fri, 12 Oct 2007 14:53:22 +0100"  >&lt;p&gt;The attached patch 7a replaces patch 7. It implements reuse of free entries when the cache is smaller than its max size. What&apos;s changed is that the scan for reusable entries is performed by rotateClock() instead of a separate method. This means that we will also try to reuse free entries when the cache is full and we know there is at least one free entry, even though we didn&apos;t find a free or evictable entry after scanning 20% of the cache.&lt;/p&gt;

&lt;p&gt;Derbyall and suites.All both ran cleanly with the new buffer manager enabled in modules.properties.&lt;/p&gt;</comment>
                            <comment id="12535162" author="knutanders" created="Tue, 16 Oct 2007 07:51:21 +0100"  >&lt;p&gt;Committed d2911-7a with revision 585060.&lt;/p&gt;</comment>
                            <comment id="12536700" author="knutanders" created="Mon, 22 Oct 2007 16:06:18 +0100"  >&lt;p&gt;I have not implemented a background cleaner for the new buffer manager yet. I will try to implement one that resembles the existing one as much as possible. My understanding is that the background cleaner can be invoked either to clean dirty pages in front of the clock hand, or to shrink the cache if it has grown bigger than its max size. The cleaner is invoked by rotateClock() when it detects that a page needs to be clean (but rotateClock() will clean that page itself), or when it detects that the cache is too large. It then scans the cache, starting right in front of the clock hand, until it finds a page to clean. During the scan, it evicts clean and not recently used pages if the cache needs to shrink. The scan stops when at least one of the following conditions are satisfied:&lt;/p&gt;

&lt;p&gt;  a) a page has been cleaned&lt;br/&gt;
  b) it has seen that 5% of the pages in the cache are clean&lt;br/&gt;
  c) it has visited 10% of the pages in the cache&lt;/p&gt;

&lt;p&gt;I&apos;m not sure whether the restriction imposed by condition a, that the background cleaner never cleans more than one page per invocation, is intended or not. Before I started reading the code, I expected the background cleaner to be more aggressive and clean at least a couple of pages before it went back to sleep. In fact, it looks like the intention at some point was that it should requeue itself after writing out the page, as seen by this return statement in Clock.performWork(boolean):&lt;/p&gt;

&lt;p&gt;		needService = true;&lt;br/&gt;
		return Serviceable.REQUEUE; // return is actually ignored.&lt;/p&gt;

&lt;p&gt;The reason why the return value is ignored, is that BasicDaemon never requeues a request when the Serviceable is a subscriber. This sounds reasonable, since a subscriber would normally be serviced again later anyway. The background cleaner is however subscribed in onDemandOnly mode, so it won&apos;t be serviced until it puts a new request in the queue itself.&lt;/p&gt;

&lt;p&gt;Does anyone have any details on how the background cleaner is intended to work?&lt;/p&gt;</comment>
                            <comment id="12539551" author="dyret" created="Fri, 2 Nov 2007 09:45:12 +0000"  >&lt;p&gt;Hi Knut, &lt;/p&gt;

&lt;p&gt;Your interpretation seems reasonable to me. Have you tried&lt;br/&gt;
lifting restriction a), so that the background cleaner is allowed&lt;br/&gt;
to clean more than one page? Is there a reason not to let the&lt;br/&gt;
background cleaner re-queue itself?&lt;/p&gt;</comment>
                            <comment id="12539575" author="knutanders" created="Fri, 2 Nov 2007 13:02:37 +0000"  >&lt;p&gt;&amp;gt; Have you tried lifting restriction a), so that the background cleaner is allowed to clean more than one page?&lt;/p&gt;

&lt;p&gt;I ran a small experiment with the test client attached to &lt;a href=&quot;https://issues.apache.org/jira/browse/DERBY-1961&quot; title=&quot;Investigate resource usage for different types of load on an in-memory database&quot; class=&quot;issue-link&quot; data-issue-key=&quot;DERBY-1961&quot;&gt;&lt;del&gt;DERBY-1961&lt;/del&gt;&lt;/a&gt; (single-record update transactions), but I couldn&apos;t see much difference between the runs which had a background cleaner and those which didn&apos;t have one. I also tried to make the background cleaner write 10 pages instead of 1, but that didn&apos;t help either. Now, the &lt;a href=&quot;https://issues.apache.org/jira/browse/DERBY-1961&quot; title=&quot;Investigate resource usage for different types of load on an in-memory database&quot; class=&quot;issue-link&quot; data-issue-key=&quot;DERBY-1961&quot;&gt;&lt;del&gt;DERBY-1961&lt;/del&gt;&lt;/a&gt; test client uses a relatively small DB (~10MB, I think), and since we don&apos;t sync after we write a page, I guess most writes only go to the file system cache and are fairly quick. I could try to rerun the experiment with a database that&apos;s much larger than the physical RAM and see if there&apos;s a noticeable difference then.&lt;/p&gt;

&lt;p&gt;&amp;gt; Is there a reason not to let the background cleaner re-queue itself?&lt;/p&gt;

&lt;p&gt;Only if the background cleaner slows things down, but then it would be better to disable it alltogether. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;I think I&apos;ll just try to make the new background cleaner behave the same way as the old one (or as close as possible) for now. If we find out later that the current behaviour is not optimal, it shouldn&apos;t take much work to change it.&lt;/p&gt;</comment>
                            <comment id="12540462" author="knutanders" created="Tue, 6 Nov 2007 15:48:54 +0000"  >&lt;p&gt;Now I have tested the buffer manager and background cleaner on a larger&lt;br/&gt;
database. I used the test client from &lt;a href=&quot;https://issues.apache.org/jira/browse/DERBY-1961&quot; title=&quot;Investigate resource usage for different types of load on an in-memory database&quot; class=&quot;issue-link&quot; data-issue-key=&quot;DERBY-1961&quot;&gt;&lt;del&gt;DERBY-1961&lt;/del&gt;&lt;/a&gt; and turned up the&lt;br/&gt;
table size so that the table took ~14GB and the index ~2GB. I ran the&lt;br/&gt;
test on a dual-CPU AMD Opteron with 2 GB RAM, two SCSI disks (one for&lt;br/&gt;
log and one for data) with the write cache disabled. &lt;/p&gt;

&lt;p&gt;With single-record select operations (random primary key lookups)&lt;br/&gt;
there was no difference between the two buffer managers, and there was&lt;br/&gt;
no difference between running with or without the background&lt;br/&gt;
writer. Both observations were as expected (there should be no&lt;br/&gt;
difference between the two buffer managers since the performance was&lt;br/&gt;
disk bound and they use the same replacement algorithm, and the&lt;br/&gt;
background cleaner shouldn&apos;t have any effect on read-only load). The&lt;br/&gt;
tests were repeated with page cache size 1000 (4MB), 10000 (40MB) and&lt;br/&gt;
100000 (400MB), with from 1 up to 100 concurrent connections.&lt;/p&gt;

&lt;p&gt;Running with single-record update load (random primary key lookup +&lt;br/&gt;
update of string column not part of the primary key), there wasn&apos;t any&lt;br/&gt;
observable effect of the background cleaner on that load&lt;br/&gt;
either. However, when comparing the two buffer manager against each&lt;br/&gt;
other, it seemed that Clock consistently had 3-5% higher throughput&lt;br/&gt;
than ConcurrentCache when the number of concurrent connections&lt;br/&gt;
exceeded 5-10. These results were seen both when the page cache size&lt;br/&gt;
was 4MB and when it was 40MB. When it was 400MB, there was no&lt;br/&gt;
observable difference.&lt;/p&gt;

&lt;p&gt;So for some reason, it seems like the old buffer manager works better&lt;br/&gt;
when there&apos;s a combination of high eviction rate and many dirty pages&lt;br/&gt;
(and the db working set is so large that I/O operations normally go to&lt;br/&gt;
disk rather than FS buffer). I found that observation a little&lt;br/&gt;
strange, since the performance for this kind of load should be almost&lt;br/&gt;
exclusively dependent on the page replacement algorithm, which is&lt;br/&gt;
supposed to be identical in the two buffer managers. The only thing I&lt;br/&gt;
know is different, is what I mentioned in one of my previous comments:&lt;/p&gt;

&lt;p&gt;&amp;gt; 2) When the clock is rotated and the hand sweeps over an unkept, not&lt;br/&gt;
&amp;gt; recently used and dirty object, the global synchronization is&lt;br/&gt;
&amp;gt; dropped and the object is cleaned. After cleaning the object, the&lt;br/&gt;
&amp;gt; global synchronization is re-obtained, but then some other thread&lt;br/&gt;
&amp;gt; may have moved the clock hand past the cleaned object while the&lt;br/&gt;
&amp;gt; first thread didn&apos;t hold the global synchronization lock. In that&lt;br/&gt;
&amp;gt; case, the first thread has cleaned an object but is not allowed to&lt;br/&gt;
&amp;gt; reuse its entry. Perhaps it&apos;s just a theoretical problem, but this&lt;br/&gt;
&amp;gt; could mean that some threads have to write multiple dirty pages to&lt;br/&gt;
&amp;gt; disk before they are allowed to insert a new one into the cache.&lt;br/&gt;
&amp;gt;&lt;br/&gt;
&amp;gt; Since the new buffer manager uses synchronization with finer&lt;br/&gt;
&amp;gt; granularity, it should avoid this problem by keeping the&lt;br/&gt;
&amp;gt; synchronization lock while the object is being cleaned. Then it&lt;br/&gt;
&amp;gt; knows that the entry can be reused as soon as the object has been&lt;br/&gt;
&amp;gt; cleaned.&lt;/p&gt;

&lt;p&gt;So it seems we are hitting the behaviour described above. Because the&lt;br/&gt;
writes are likely to go to disk (not only to the file system cache), it&lt;br/&gt;
is very likely that the clock hand is moved past the page being&lt;br/&gt;
written, so that the cleaned page is not evicted, and the thread that&lt;br/&gt;
cleaned it has to keep searching for an evictable page.&lt;/p&gt;

&lt;p&gt;To verify that this was the cause of the difference in performance, I&lt;br/&gt;
changed ConcurrentCache/ClockPolicy so that it would always continue&lt;br/&gt;
the search after it had cleaned a page (that is, the user thread would&lt;br/&gt;
clean the page and wait until it had been cleaned, but it would not&lt;br/&gt;
evict the page). With this change, the performance of ConcurrentCache&lt;br/&gt;
matched Clock in all the above mentioned tests, with no observable&lt;br/&gt;
difference. Intuitively, I would say that the change would make the&lt;br/&gt;
user threads perform unnecessary work (cleaning pages that they&lt;br/&gt;
wouldn&apos;t use, at least not until the clock hand has rotated another&lt;br/&gt;
full round). However, it seems that there&apos;s some pipe-lining effect or&lt;br/&gt;
something that makes it more efficient.&lt;/p&gt;

&lt;p&gt;I found this observation quite interesting. It&apos;s kind of like the user&lt;br/&gt;
threads are working as background cleaners for each other. I&apos;ll do&lt;br/&gt;
some more experimenting and see if I can trick the background cleaner&lt;br/&gt;
into doing the same work. I was thinking that instead of forcing the&lt;br/&gt;
user thread to wait for the page to be cleaned and then continue, it&lt;br/&gt;
could delegate the cleaning to the background cleaner since they won&apos;t&lt;br/&gt;
use the page anyway and shouldn&apos;t have to wait.&lt;/p&gt;</comment>
                            <comment id="12541049" author="knutanders" created="Thu, 8 Nov 2007 13:43:25 +0000"  >&lt;p&gt;I have experimented a little more with the mentioned update load on a&lt;br/&gt;
large (~16GB) database. What I tried, was to change the rotateClock()&lt;br/&gt;
method in the new buffer manager so that when it found a dirty and&lt;br/&gt;
not-recently used page, it put the page in a queue which the&lt;br/&gt;
background writer would process, and go to the next page without&lt;br/&gt;
waiting for the previous page to be written. If the queue grew larger&lt;br/&gt;
than 1/10 of the maximum cache size (because the background thread&lt;br/&gt;
couldn&apos;t clean the pages fast enough to keep up with the user&lt;br/&gt;
threads), the user thread would however clean the page itself instead&lt;br/&gt;
of putting the page in the queue.&lt;/p&gt;

&lt;p&gt;The attached patch (cleaner.diff) shows this approach. (Note that this&lt;br/&gt;
patch is only for testing. It doesn&apos;t actually use Derby&apos;s daemon&lt;br/&gt;
thread, but a separate thread that just sits and listens to an&lt;br/&gt;
ArrayBlockingQueue.) I tested it with the update load and page cache&lt;br/&gt;
size 1000, 10000 and 100000, and it seemed to have as good as or&lt;br/&gt;
better throughput than the old buffer manager in all of these&lt;br/&gt;
tests. (Graphs showing the results are contained in the cleaner.tar&lt;br/&gt;
archive.)&lt;/p&gt;

&lt;p&gt;To me this sounds like a good way to use the background thread, as it&lt;br/&gt;
offloads the user threads so that they don&apos;t need to wait for&lt;br/&gt;
potentially slow I/O operations. One might of course say that it&apos;s not&lt;br/&gt;
ideal, as it cleans pages right behind the clock hand instead of right&lt;br/&gt;
in front of it. However, the pages that are cleaned have not been used&lt;br/&gt;
during the last full round on the clock, so it is less likely that&lt;br/&gt;
they will be used on the next round as well, and therefore they&apos;ll&lt;br/&gt;
probably stay eligible for eviction. Also, if the background cleaner&lt;br/&gt;
works on pages right in front of the hand, it is quite likely that if&lt;br/&gt;
it finds a page to write, the user threads will catch up with it and&lt;br/&gt;
pass it while it&apos;s performing the I/O operation, so that it&lt;br/&gt;
effectively ends up cleaning behind the clock hand anyway.&lt;/p&gt;

&lt;p&gt;If there are no objections or other good ideas, I will try to use the&lt;br/&gt;
approach in cleaner.diff and integrate it with Derby&apos;s own background&lt;br/&gt;
thread.&lt;/p&gt;</comment>
                            <comment id="12541262" author="jorgenlo" created="Fri, 9 Nov 2007 09:03:58 +0000"  >&lt;p&gt;Hi Knut,&lt;/p&gt;

&lt;p&gt;Thanks for providing these nice graphs; they make it much easier to understand the effects of the new buffer manager. The results are dramatic in many of the illustrated cases!&lt;/p&gt;

&lt;p&gt;Intuitively, I would say that your plan of using a more aggressive background cleaner sounds good since user threads are not using the pages they have cleaned (although I don&apos;t understand why not using that page is better). The graphs also clearly indicate that this approach is better than letting the user thread do the job.&lt;/p&gt;

&lt;p&gt;One question: what happens if the clock rotates too fast and reaches a synchronized (due to I/O) page? Will it have to wait for the synchronization to be lifted, or can the clock jump to the next page somehow? Since the background cleaner is used to write most of the pages, it (the background cleaner) could potentially be far behind the clock if I understand this correctly. If the clock is unnecessarily blocked by I/O it&apos;s game over &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;On a side note - why are performance tests in Derby performed with back-to-back transactions rather than using a more realistic (i.e., Poisson distributed) load? Poisson is normally used to simulate the transaction arrival rate from many independent clients &lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;. In your case, I think the background cleaner could perform even better with this (more realistic) workload since it would be scheduled in the short periods with slightly fewer active transactions.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; &lt;a href=&quot;http://en.wikipedia.org/wiki/Poisson_process&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Poisson_process&lt;/a&gt;&lt;/p&gt;
</comment>
                            <comment id="12541291" author="knutanders" created="Fri, 9 Nov 2007 11:09:56 +0000"  >&lt;p&gt;Hi J&#248;rgen,&lt;/p&gt;

&lt;p&gt;Thanks for taking the time to comment on this issue.&lt;/p&gt;

&lt;p&gt;&amp;gt; One question: what happens if the clock rotates too fast and reaches a synchronized (due to I/O) page? Will it have to wait for the synchronization to be lifted, or can the clock jump to the next page somehow?&lt;/p&gt;

&lt;p&gt;It will have to wait, and that may quite likely have happened in the runs with a small page cache. It would indeed be interesting to see how the performance is affected if we allowed the clock to jump to the next page in that case. By the way, that&apos;s what the old buffer manager does, but for reasons mentioned in a previous comment (simplicity of implementation and less likelihood of blocking because of finer sync granularity) ConcurrentCache doesn&apos;t. Based on the results for update load on large db, I think it&apos;s a good idea to run some tests to see if that was a good design decision. It shouldn&apos;t be too hard to make that change.&lt;/p&gt;

&lt;p&gt;&amp;gt; why are performance tests in Derby performed with back-to-back transactions rather than using a more realistic (i.e., Poisson distributed) load?&lt;/p&gt;

&lt;p&gt;No particular reason, I think. And I think it&apos;s a very good idea to try with a Poisson distribution or similar, especially to test the background cleaner. If you know any open source test clients that could be used, I&apos;d be more than happy to try them out. Otherwise, I may see if I can write one myself.&lt;/p&gt;</comment>
                            <comment id="12541304" author="jorgenlo" created="Fri, 9 Nov 2007 12:38:37 +0000"  >&lt;p&gt;Could this be used?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://commons.apache.org/sandbox/performance/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://commons.apache.org/sandbox/performance/&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="12541729" author="knutanders" created="Mon, 12 Nov 2007 10:22:06 +0000"  >&lt;p&gt;&amp;gt; One question: what happens if the clock rotates too fast and reaches&lt;br/&gt;
&amp;gt; a synchronized (due to I/O) page? Will it have to wait for the&lt;br/&gt;
&amp;gt; synchronization to be lifted, or can the clock jump to the next page&lt;br/&gt;
&amp;gt; somehow? Since the background cleaner is used to write most of the&lt;br/&gt;
&amp;gt; pages, it (the background cleaner) could potentially be far behind&lt;br/&gt;
&amp;gt; the clock if I understand this correctly. If the clock is&lt;br/&gt;
&amp;gt; unnecessarily blocked by I/O it&apos;s game over &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;I made a change so that instead of cleaning the page while holding the&lt;br/&gt;
synchronization on the cache entry, the keep count was incremented (to&lt;br/&gt;
ensure that the page wouldn&apos;t get evicted) and the synchronization was&lt;br/&gt;
dropped. This didn&apos;t change anything, though. And thinking more about&lt;br/&gt;
it, that makes sense, since if the clock rotates too fast, there won&apos;t&lt;br/&gt;
be evictable pages and it&apos;s game over anyway...&lt;/p&gt;

&lt;p&gt;That said, I think it&apos;s a good idea to drop the synchronization while&lt;br/&gt;
cleaning since it will increase the concurrency (although not by much,&lt;br/&gt;
since the synchronization is much more fine-grained than in Clock). I&lt;br/&gt;
think I&apos;ll keep it as it is for now and do a cleanup of it later.&lt;/p&gt;

&lt;p&gt;Another potential concurrency improvement for the replacement&lt;br/&gt;
algorithm is to protect the clock with a&lt;br/&gt;
java.util.concurrent.locks.ReadWriteLock instead of synchronization on&lt;br/&gt;
the clock. Then we don&apos;t have to lock the clock structure exclusively&lt;br/&gt;
unless the clock needs to grow or shrink.&lt;/p&gt;</comment>
                            <comment id="12541737" author="knutanders" created="Mon, 12 Nov 2007 10:54:51 +0000"  >&lt;p&gt;&amp;gt; Intuitively, I would say that your plan of using a more aggressive&lt;br/&gt;
&amp;gt; background cleaner sounds good since user threads are not using the&lt;br/&gt;
&amp;gt; pages they have cleaned (although I don&apos;t understand why not using&lt;br/&gt;
&amp;gt; that page is better).&lt;/p&gt;

&lt;p&gt;I also have a hard time understanding why not using the page after&lt;br/&gt;
writing it out is better, but I think it may be a result of a&lt;br/&gt;
double-buffering effect together with the file system cache. When a&lt;br/&gt;
page is not necessarily evicted after it has been cleaned (the&lt;br/&gt;
behaviour of Clock in trunk), a user thread may clean multiple pages&lt;br/&gt;
before it is able to evict a page. That could lead to more clustered&lt;br/&gt;
writes and, since the writes go via the fs cache and not directly to&lt;br/&gt;
the disk, it might increase the probability of the fs cache being able&lt;br/&gt;
to clean more pages in one single disk operation.&lt;/p&gt;

&lt;p&gt;To test this, I reran the tests with direct I/O enabled on the data&lt;br/&gt;
disk (which disables the fs cache). I still saw that all approaches&lt;br/&gt;
which frequently ended up not evicting the pages they had cleaned had&lt;br/&gt;
about the same performance as Clock in trunk. What had changed was&lt;br/&gt;
that evicting the pages that had been cleaned, seemed to be slightly&lt;br/&gt;
better than not evicting them.&lt;/p&gt;

&lt;p&gt;Since not evicting the pages is what most closely resembles the&lt;br/&gt;
behaviour of the current buffer manager, and it sounds more likely&lt;br/&gt;
that users have the fs cache enabled than disabled, I think I&apos;ll go&lt;br/&gt;
for that solution. Also, the more aggressive background cleaner seems&lt;br/&gt;
to perform better then the old one, so I&apos;ll try to come up with a&lt;br/&gt;
patch which combines those two approaches.&lt;/p&gt;</comment>
                            <comment id="12543103" author="knutanders" created="Fri, 16 Nov 2007 15:22:55 +0000"  >&lt;p&gt;Attaching a new patch (derby-2911-8.diff) which implements the&lt;br/&gt;
background cleaner as discussed earlier.&lt;/p&gt;

&lt;p&gt;The patch adds a class called BackgroundCleaner. This class implements&lt;br/&gt;
the Serviceable interface and subscribes to the cache manager&apos;s daemon&lt;br/&gt;
service. When a user thread finds an object that needs to be cleaned,&lt;br/&gt;
it gives it to BackgroundCleaner, which cleans the object in the&lt;br/&gt;
daemon service&apos;s thread instead of the user thread. BackgroundCleaner&lt;br/&gt;
keeps the objects it needs to clean in a queue with a maximum size of&lt;br/&gt;
1/10 of the cache size. (There is no particular reason for choosing&lt;br/&gt;
exactly 1/10, it just sounded not too big and not too small. Anyway,&lt;br/&gt;
it&apos;s basically just an array pointing to objects already in the cache,&lt;br/&gt;
so it&apos;s memory footprint will be small compared to the rest of the&lt;br/&gt;
cache.) If the queue is full when the background cleaner receives a&lt;br/&gt;
request from a user thread, the user thread will have to clean the&lt;br/&gt;
object itself.&lt;/p&gt;

&lt;p&gt;I also incorporated J&#248;rgen&apos;s suggestion about releasing&lt;br/&gt;
synchronization lock on the entry while it is being cleaned. This also&lt;br/&gt;
led to a small change in how the checkpoint would write the&lt;br/&gt;
pages. Basically, when the checkpoint or the replacement algorithm&lt;br/&gt;
cleans an object, it will increase the entry&apos;s keep count (without&lt;br/&gt;
marking it as recently used) to prevent eviction, and release the&lt;br/&gt;
synchronization lock on the entry. When the sync lock has been&lt;br/&gt;
released, the object is written, and finally the sync lock is&lt;br/&gt;
re-obtained so that the keep count can be decremented.&lt;/p&gt;

&lt;p&gt;The javadoc comment for Clock.useDaemonService() says the following&lt;br/&gt;
about multi-threading: &quot;MT - synchronization provided by caller&quot;. The&lt;br/&gt;
interface (CacheManager) does not say anything about this, but I added&lt;br/&gt;
a similar comment in the interface javadoc, and used that guarantee to&lt;br/&gt;
avoid the need for synchronization when accessing the reference&lt;br/&gt;
assigned in ConcurrentCache.useDaemonService(). (Currently, this&lt;br/&gt;
method is only used when the database is booted, so I believe the&lt;br/&gt;
comment in Clock is correct.)&lt;/p&gt;

&lt;p&gt;Derbyall and suites.All ran cleanly on Solaris 10 and Sun JDK 6 when I&lt;br/&gt;
manually enabled the new buffer manager in modules.properties. As for&lt;br/&gt;
the previous patches, this patch does not affect any code used by&lt;br/&gt;
default in Derby.&lt;/p&gt;</comment>
                            <comment id="12543105" author="knutanders" created="Fri, 16 Nov 2007 15:29:13 +0000"  >&lt;p&gt;&amp;gt; Could this be used?&lt;br/&gt;
&amp;gt; &lt;a href=&quot;http://commons.apache.org/sandbox/performance/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://commons.apache.org/sandbox/performance/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Thanks J&#248;rgen, that looks very useful. It turned out that it didn&apos;t take too many code lines to make the existing test generate Poisson distributed load, so I wrote a small test client myself. I&apos;ll come back with the code and the numbers later.&lt;/p&gt;</comment>
                            <comment id="12543514" author="knutanders" created="Mon, 19 Nov 2007 10:29:56 +0000"  >&lt;p&gt;poisson_patch8.tar contains the code I used to run Poisson distributed single-record updates on a 16GB database. It also contains graphs showing the average response times with derby.storage.pageCacheSize 1000 and 10000, comparing trunk and ConcurrentCache with patch #8. As predicted by J&#248;rgen, this test shows more clearly the advantages of the new background cleaner. It seems clear that the old buffer manager collapse (that is, its response times grow quickly) at a significantly lower number of requests per second with this load.&lt;/p&gt;</comment>
                            <comment id="12543524" author="knutanders" created="Mon, 19 Nov 2007 11:14:08 +0000"  >&lt;p&gt;Committed patch #8 with revision 596265.&lt;/p&gt;

&lt;p&gt;Now the only thing that is not implemented, is shrinking the clock when it has exceeded its maximum size (this happens if no evictable items are found by rotateClock()). The old buffer manager would delegate the shrinking to its daemon service from rotateClock() if it had a daemon service. If it didn&apos;t have a daemon service, it would do the work itself from release(). I think we should try the same approach for ConcurrentCache.&lt;/p&gt;</comment>
                            <comment id="12547783" author="knutanders" created="Mon, 3 Dec 2007 10:33:25 +0000"  >&lt;p&gt;Attaching a new patch (d2911-9) which implements shrinking of the&lt;br/&gt;
clock when it has exceeded the maximum size. The shrinking is&lt;br/&gt;
performed in the background cleaner if the cache has one, and in the&lt;br/&gt;
user thread otherwise.&lt;/p&gt;

&lt;p&gt;Also, Clock has a method called trimToSize() which tries to remove as&lt;br/&gt;
many invalid entries as possible (even if the cache is smaller than&lt;br/&gt;
the maximum size). It is called from ageOut(), discard() and&lt;br/&gt;
performWork(), but it doesn&apos;t do anything unless all of these&lt;br/&gt;
conditions are satisfied:&lt;/p&gt;

&lt;p&gt;  1) the method must have been called at least as many times as 1/8 of&lt;br/&gt;
  the number of elements in the cache&lt;/p&gt;

&lt;p&gt;  2) the cache must not contain less than 32 elements&lt;/p&gt;

&lt;p&gt;  3) more than 25% of the elements in the cache must be invalid&lt;/p&gt;

&lt;p&gt;This doesn&apos;t happen very often (according to the code coverage report at&lt;br/&gt;
&lt;a href=&quot;http://people.apache.org/~fuzzylogic/codecoverage/529822/_files/21f.html#25&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://people.apache.org/~fuzzylogic/codecoverage/529822/_files/21f.html#25&lt;/a&gt;&lt;br/&gt;
it doesn&apos;t happen at all in our test runs), so I&apos;m not sure it adds&lt;br/&gt;
enough value to justify the extra complexity. However, to keep things&lt;br/&gt;
as close to the old implementation as possible, I added such a method&lt;br/&gt;
to the new buffer manager too. There is one small difference, though:&lt;br/&gt;
The old buffer manager would first move all invalid elements to the&lt;br/&gt;
end of the ArrayList, before the elements were removed from the&lt;br/&gt;
list. While it is possibly more efficient, this operation is performed&lt;br/&gt;
so infrequently that it&apos;s hardly worth the effort to optimize&lt;br/&gt;
it. Also, the old buffer manager would lock the entire cache while&lt;br/&gt;
doing this, so it knew that the elements were still invalid after it&lt;br/&gt;
had moved them to the end of the list. The new buffer manager only&lt;br/&gt;
locks one element at a time, so it doesn&apos;t know that the elements will&lt;br/&gt;
stay invalid after they have been moved to the end of the&lt;br/&gt;
list. Therefore, I chose to implement the method as a simple scan&lt;br/&gt;
which removed invalid entries as it found them.&lt;/p&gt;


&lt;p&gt;I successfully ran derbyall and suites.All with the new buffer manager&lt;br/&gt;
enabled in modules.properties.&lt;/p&gt;</comment>
                            <comment id="12547789" author="knutanders" created="Mon, 3 Dec 2007 10:53:49 +0000"  >&lt;p&gt;I think there is another issue similar to the one J&#248;rgen commented on&lt;br/&gt;
about holding the lock on an entry while writing it to disk.&lt;/p&gt;

&lt;p&gt;When an entry is fetched into the cache (either with create() or&lt;br/&gt;
find()), the old buffer manager would release the lock on the cache&lt;br/&gt;
before running createIdentity()/setIdentity(). I assumed it released&lt;br/&gt;
the lock because it didn&apos;t want to block the entire cache while&lt;br/&gt;
performing what could be time consuming operations. I have now found&lt;br/&gt;
another reason for releasing the lock: When a new container is&lt;br/&gt;
created, the call to createIdentity() will look at data in other&lt;br/&gt;
containers and therefore reenter the container cache. If we hold the&lt;br/&gt;
lock on the entry that is being created, we can possibly run into a&lt;br/&gt;
deadlock when the container cache is reentered. Will try to come up&lt;br/&gt;
with a fix.&lt;/p&gt;</comment>
                            <comment id="12548976" author="knutanders" created="Thu, 6 Dec 2007 10:14:46 +0000"  >&lt;p&gt;Committed d2911-9.diff with revision 601680.&lt;/p&gt;</comment>
                            <comment id="12550447" author="knutanders" created="Tue, 11 Dec 2007 13:57:09 +0000"  >&lt;p&gt;Attaching a new patch (d2911-10). This patch addresses the potential deadlock issue if user code (i.e., Cacheable.setIdentity() or Cacheable.createIdentity()) reenters the cache. It makes the callers of setIdentity()/createIdentity() release the ReentrantLock on the cache entry before calling one of those methods. Other threads that want to access the object in that entry therefore have to wait until the identity has been set, whereas threads scanning the cache (e.g., checkpoint or clock rotation) skip the entry instead of waiting since they just see an invalid/kept entry.&lt;/p&gt;

&lt;p&gt;All the regression tests passed when I enabled ConcurrentCache in modules.properties.&lt;/p&gt;</comment>
                            <comment id="12550855" author="knutanders" created="Wed, 12 Dec 2007 07:01:29 +0000"  >&lt;p&gt;Committed d2911-10.diff with revision 603491.&lt;/p&gt;</comment>
                            <comment id="12551749" author="knutanders" created="Fri, 14 Dec 2007 10:37:39 +0000"  >&lt;p&gt;Attaching a new patch which (d2911-11) which makes the new buffer manager use the initialSize parameter in CacheFactory.newCacheManager(). It only uses the parameter to specify the initial capacity of the ConcurrentHashMap in ConcurrentCache and the ArrayList in ClockPolicy. The interface actually says that we should create enough holder objects to hold that number of objects, but that&apos;s not what the old buffer manager does. I have logged an issue &lt;a href=&quot;https://issues.apache.org/jira/browse/DERBY-3275&quot; title=&quot;Mismatch between comments and actual use of initialSize parameter in CacheFactory and Clock&quot; class=&quot;issue-link&quot; data-issue-key=&quot;DERBY-3275&quot;&gt;&lt;del&gt;DERBY-3275&lt;/del&gt;&lt;/a&gt; for making the code and the comments match.&lt;/p&gt;</comment>
                            <comment id="12551766" author="knutanders" created="Fri, 14 Dec 2007 11:14:25 +0000"  >&lt;p&gt;Committed d2911-11.diff with revision 604153.&lt;/p&gt;

&lt;p&gt;I think that means that all the functionality of the old buffer manager is also implemented in the new buffer manager.&lt;/p&gt;

&lt;p&gt;In an earlier comment, I mentioned that we might see more concurrency when the eviction frequency is high if we replace the synchronization  lock on the ArrayList in ClockPolicy with a ReadWriteLock. I made quick patch and ran a couple of tests, but I couldn&apos;t see any significant improvement, so I&apos;ll just leave it as it is for now. If it later turns out to be a bottleneck, it&apos;ll be very simple to change it.&lt;/p&gt;

&lt;p&gt;I plan to rerun the tests I ran earlier on different hardware and with different page cache sizes to see if we still have the performance gains we had then, and also to see if there is some load/configuration where the performance is worse with the new buffer manager. I will post the results once I have them.&lt;/p&gt;

&lt;p&gt;I would appreciate it if someone would review the code. The new buffer manager does not share any code with the old buffer manager, and it is completely contained in these files in the impl/services/cache directory:&lt;/p&gt;

&lt;p&gt;  ConcurrentCacheFactory.java - implementation of the CacheFactory interface and contains only a single factory method to create a  ConcurrentCache instance&lt;/p&gt;

&lt;p&gt;  ConcurrentCache.java - the CacheManager implementation. This class contains the code needed to insert, find, release and remove items in the cache, and it is build around a ConcurrentHashMap&lt;/p&gt;

&lt;p&gt;  CacheEntry.java - wrapper/holder object for objects in the cache. This object contains some state for the entry (keep count, validity, etc), and code which enables fine-grained synchronization (ReentrantLock on entry level instead of synchronization on cache level as in the old buffer manager)&lt;/p&gt;

&lt;p&gt;  ReplacementPolicy.java/ClockPolicy.java - interface and implementation of the replacement algorithm, that is the algorithm that is used to evict objects from the cache to make room for new objects when the cache is full&lt;/p&gt;

&lt;p&gt;  BackgroundCleaner.java - a Serviceable that is used by the cache manager to perform asynchronous I/O or other background tasks&lt;/p&gt;</comment>
                            <comment id="12558644" author="knutanders" created="Mon, 14 Jan 2008 16:00:53 +0000"  >&lt;p&gt;The attached patch merges the test clients used previously in this issue and places them under org.apache.derbyTesting.perf.clients. With the class Runner in that package, one can start each of the tests from the command line and configure which type of load generator (back-to-back or Poisson) to use.&lt;/p&gt;

&lt;p&gt;I am currently running tests which compare the performance of ConcurrentCache and Clock on one dual-CPU machine and on one 8-CPU machine. I&apos;ll post the results once I have them.&lt;/p&gt;</comment>
                            <comment id="12564805" author="knutanders" created="Fri, 1 Feb 2008 15:51:27 +0000"  >&lt;p&gt;I finally got around to look at the results from the tests I had running. Sorry for the delay.&lt;/p&gt;

&lt;p&gt;Attached is a diff (perftest2.diff) with an updated version of the performance tests that I used (the load that were supposed to resemble the d2911perf class attached to this issue, was changed so that it matched d2911perf more closely, and this change also simplified the code a bit), and a pdf (perftest.pdf) which summarizes the findings. Nothing new there, actually, the results were pretty consistent with the results that have been posted here earlier. The new buffer manager performs significantly better than the old one when there&apos;s no other contention point (like the log device or the root node in the B-tree) and there are multiple CPUs/cores, and a little better than or equal to the old buffer manager in cases where there is another contention point.&lt;/p&gt;</comment>
                            <comment id="12566586" author="knutanders" created="Thu, 7 Feb 2008 13:28:23 +0000"  >&lt;p&gt;Checked in the performance tests with revision 619404.&lt;/p&gt;</comment>
                            <comment id="12571358" author="oysteing" created="Fri, 22 Feb 2008 11:05:02 +0000"  >&lt;p&gt;I have looked at the code introduced in this JIRA for a new cache&lt;br/&gt;
manager, and think it looks very good.  I have only found one small issue&lt;br/&gt;
related to correctness (4e), but I have some minor comments/questions on&lt;br/&gt;
the organization of the code:&lt;/p&gt;

&lt;p&gt;1. ConcurrentCache:&lt;/p&gt;

&lt;p&gt;   a. What is the significant difference between removeEntry and&lt;br/&gt;
      evictEntry?  At first, I thought it was that evictEntry set the&lt;br/&gt;
      Cacheable to null, but then I discovered that removeEntry does&lt;br/&gt;
      the same through its call to CacheEntry#free.  So I guess the&lt;br/&gt;
      only difference is that removeEntry will include a call to&lt;br/&gt;
      ReplacementPolicy#Callback#free. I am not sure I understand why&lt;br/&gt;
      this should be avoided for evictEntry.  If both methods are&lt;br/&gt;
      needed, the first sentences of their javadoc should be different&lt;br/&gt;
      in order to be able to make a distinction when reading the&lt;br/&gt;
      javadoc summary.&lt;/p&gt;

&lt;p&gt;   b. To me, the organization of find and create methods for entries&lt;br/&gt;
      and cacheables are a bit confusing.  It seems to me that it would&lt;br/&gt;
      be easier to understand if it was based on basic methods for&lt;br/&gt;
      get/find and create, that just did what it said it did, and then&lt;br/&gt;
      created the more advanced operations like&lt;br/&gt;
      findAndCreateIfNotFound on top of that.  Is this done for&lt;br/&gt;
      efficiency reasons?  Especially, the findOrCreateObject method&lt;br/&gt;
      is a bit confusing since it uses a parameter to change behavior&lt;br/&gt;
      from findAndCreateIfNotFound to createAndFlagErrorIfFound.&lt;br/&gt;
      Would it be possible to create two separate methods here?  At&lt;br/&gt;
      least, I think the javadoc for the create parameter needs to&lt;br/&gt;
      describe exactly what will occur.   &lt;/p&gt;

&lt;p&gt;   c. findOrCreateObject: The create parameter will determine whether&lt;br/&gt;
      setIdentify and not createIdentify will be called when an object&lt;br/&gt;
      is not found in the cache.  Is this difference just an issue of&lt;br/&gt;
      whether createParameter has been provided, or is there something&lt;br/&gt;
      more that distinguishes these to cases?&lt;/p&gt;

&lt;p&gt;   d. findCached/release: Comments states that one &quot;don&apos;t need to call&lt;br/&gt;
      getEntry()&quot;, but I guess the points is that it would be harmful&lt;br/&gt;
      to call getEntry?  &lt;/p&gt;

&lt;p&gt;2. CacheEntry:&lt;/p&gt;

&lt;p&gt;   a. lockWhenIdentityIsSet: I do not feel the name really describes&lt;br/&gt;
      what the method is doing.  lockAndBlockUntilIdentityIsSet would&lt;br/&gt;
      be more descriptive.  Maybe you can come up with an even better&lt;br/&gt;
      (and shorter) name.&lt;/p&gt;

&lt;p&gt;3. ReplacementPolicy:&lt;/p&gt;

&lt;p&gt;   a. The Callback returned by insertEntry seems currently not to be&lt;br/&gt;
      in use.  In what situations may it be used?&lt;/p&gt;

&lt;p&gt;4. ClockPolicy:&lt;/p&gt;

&lt;p&gt;   a. grow: Instead of requiring that the caller synchronize, why not&lt;br/&gt;
      synchronize within the method?  (Like is done for several other&lt;br/&gt;
      methods, eg., moveHand)&lt;/p&gt;

&lt;p&gt;   b. rotateClock:  The concept of a small cache (&amp;lt;20 entries) is that&lt;br/&gt;
      relevant for any currently used caches in Derby?  It seems a bit&lt;br/&gt;
      strange to check 38 items when there are 19 entries, but only 4&lt;br/&gt;
      items if there are 20 entries.  But maybe it is not that&lt;br/&gt;
      relevant with respect to the cache sizes used in Derby?&lt;/p&gt;

&lt;p&gt;   c. rotateClock:  I think it would be good if some of the discussion&lt;br/&gt;
      in the JIRA about why it is not using the entries it has&lt;br/&gt;
      cleaned, would be reflected in comments.&lt;/p&gt;

&lt;p&gt;   d. shrinkMe: javadoc for return is incomplete&lt;/p&gt;

&lt;p&gt;   e. shrinkMe: I think there is an off-by-one error for the call on&lt;br/&gt;
      clock.get.  If pos==size-1 when entering the synchronized block,&lt;br/&gt;
      one will do get(size) which I guess would give&lt;br/&gt;
      ArrayIndexOutOfBoundsException. Also, the first element of clock&lt;br/&gt;
      will never be inspected since index will never be 0.&lt;/p&gt;

&lt;p&gt;   f. shrinkMe: The code to determine whether an item could be evicted&lt;br/&gt;
      is the same as for rotateClock.  Could this code be refactored&lt;br/&gt;
      into a common method?  (isEvictable() or something like that)&lt;/p&gt;

&lt;p&gt;   g. trimMe: This method contains a lot of heuristics that I guess&lt;br/&gt;
      you have inherited from the old clock implementation.  If you&lt;br/&gt;
      have got any insight into why the particular values are chosen,&lt;br/&gt;
      if would be good if you could comment on that.  (I notice that&lt;br/&gt;
      the criteria for being a small cache is not the same here as in&lt;br/&gt;
      rotateClock.)&lt;/p&gt;

&lt;p&gt;   h. Maybe some of the numeric constants used for heuristics in this&lt;br/&gt;
      class should be defined as named constants?&lt;/p&gt;


&lt;p&gt;5. BackgroundCleaner: &lt;/p&gt;

&lt;p&gt;   a. I am not sure the statement &quot;used by ConcurrentCache so that it&lt;br/&gt;
      doesn&apos;t have to wait for clean operations to finish&quot; covers the&lt;br/&gt;
      purpose.  Clean operations on ConcurrentCache does not use the&lt;br/&gt;
      background cleaner.  It is find operations on ConcurrentCache&lt;br/&gt;
      that will invoke the cleaner.  In addition, the background&lt;br/&gt;
      cleaner does not help a given find operation, it just makes&lt;br/&gt;
      future operation more likely to find an item that can be reused.&lt;/p&gt;

&lt;p&gt;   b. serviceImmediately:  As far as I can tell this method is not&lt;br/&gt;
      relevant since it is not used by BasicDaemon.  Maybe that should&lt;br/&gt;
      be indicated?&lt;/p&gt;</comment>
                            <comment id="12571390" author="knutanders" created="Fri, 22 Feb 2008 12:57:46 +0000"  >&lt;p&gt;Thanks for reviewing the code, &#216;ystein! Those are very good comments and questions. I&apos;ll study them more in detail and try to come up with answers over the weekend. &lt;/p&gt;</comment>
                            <comment id="12572924" author="knutanders" created="Wed, 27 Feb 2008 15:00:11 +0000"  >&lt;p&gt;I&apos;ve gone through the comments and questions. Please find my&lt;br/&gt;
responses below. Regarding the correctness error (4e), I think the&lt;br/&gt;
code is actually correct, but it could have been clearer.&lt;/p&gt;

&lt;p&gt;The new buffer manager isn&apos;t enabled in the build yet. Should the&lt;br/&gt;
issues mentioned by &#216;ystein be resolved first, or would it be OK to&lt;br/&gt;
enable it first and do the reorganization and improvements later?&lt;/p&gt;

&lt;p&gt;Responses to &#216;ystein&apos;s questions/comments:&lt;/p&gt;

&lt;p&gt;1. ConcurrentCache&lt;/p&gt;

&lt;p&gt;a) Difference between evictEntry() and removeEntry().&lt;/p&gt;

&lt;p&gt;I think you are right that the only significant difference is that&lt;br/&gt;
removeEntry() calls free() whereas evictEntry() doesn&apos;t. Calling&lt;br/&gt;
free() from evictEntry() wouldn&apos;t be correct in the current code,&lt;br/&gt;
though. free() is called to notify the replacement policy that the&lt;br/&gt;
Cacheable contained in the entry can be reused. When evictEntry() is&lt;br/&gt;
called, we&apos;re either in the process of reusing the Cacheable (in which&lt;br/&gt;
case we don&apos;t want to first mark it as reusable so that others can&lt;br/&gt;
grab it, and then try to reobtain it and possibly reuse it), or we&apos;re&lt;br/&gt;
shrinking the cache (in which case making it reusable would mean that&lt;br/&gt;
the cache doesn&apos;t shrink).&lt;/p&gt;

&lt;p&gt;I&apos;ll see if I can find a clever way to merge the two methods, or at&lt;br/&gt;
least improve the comments and perhaps the names of the methods.&lt;/p&gt;

&lt;p&gt;b) Organization of find and create methods.&lt;/p&gt;

&lt;p&gt;I guess it is possible to split them into two or three basic methods&lt;br/&gt;
and build the public methods on top of them. However, I&apos;m not sure&lt;br/&gt;
it&apos;s a good idea to remove the create flag. findOrCreateObject() is a&lt;br/&gt;
fairly large method, and the check of the create flag is a very small&lt;br/&gt;
part of it. If the create flag were removed, it would mean that we&lt;br/&gt;
have to duplicate a most of the logic in that method. But if we split&lt;br/&gt;
the logic into more basic methods, it will hopefully be easier to&lt;br/&gt;
understand how the create flag affects each of the basic methods.&lt;/p&gt;

&lt;p&gt;c) findOrCreateObject(): create vs createParameter&lt;/p&gt;

&lt;p&gt;create can be true even if createParameter is null, so I don&apos;t think&lt;br/&gt;
create can be skipped. The distinction between setIdentity() and&lt;br/&gt;
createIdentity() comes from the Cacheable interface and can&apos;t be&lt;br/&gt;
changed without modifying the existing buffer manager and the store as&lt;br/&gt;
well.&lt;/p&gt;

&lt;p&gt;d) findCached()/release(): get() vs getEntry()&lt;/p&gt;

&lt;p&gt;You&apos;re right, it is not harmful to call getEntry() in terms of&lt;br/&gt;
correctness, but it&apos;s slightly more expensive. I will update the&lt;br/&gt;
comments to make this clearer.&lt;/p&gt;

&lt;p&gt;2. CacheEntry&lt;/p&gt;

&lt;p&gt;a) Misleading name: lockWhenIdentityIsSet()&lt;/p&gt;

&lt;p&gt;Your suggestion lockAndBlockUntilIdentityIsSet() does match more&lt;br/&gt;
closely the calls in the method (lock() + await()), but since await()&lt;br/&gt;
will release the lock until it wakes up, I think the original name&lt;br/&gt;
describes better how the locking and blocking is perceived from&lt;br/&gt;
outside the method. Perhaps waitUntilIdentityIsSetAndLock() is a&lt;br/&gt;
better name? Of course, better comments in the method would also help.&lt;/p&gt;

&lt;p&gt;3. ReplacementPolicy&lt;/p&gt;

&lt;p&gt;a) Return value from insertEntry() isn&apos;t used.&lt;/p&gt;

&lt;p&gt;The return value isn&apos;t needed since the replacement policy will link&lt;br/&gt;
the CacheEntry and the Callback internally by calling&lt;br/&gt;
CacheEntry.setCallback(). Will change the method so that it&apos;s void.&lt;/p&gt;

&lt;p&gt;4. ClockPolicy&lt;/p&gt;

&lt;p&gt;a) Why not synchronize inside grow()?&lt;/p&gt;

&lt;p&gt;One of the callers needs the synchronization for more than just the&lt;br/&gt;
call to grow() and therefore needs the synchronized block anyway, so I&lt;br/&gt;
felt it was more consistent if all (both) callers used an explicit&lt;br/&gt;
synchronization block. (It also avoided double synchronization for&lt;br/&gt;
that caller.) The other methods you mention are supposed to be atomic,&lt;br/&gt;
and are not intended to be called from within a larger block&lt;br/&gt;
synchronized on the same object.&lt;/p&gt;

&lt;p&gt;b) Handling of small cache sizes in rotateClock().&lt;/p&gt;

&lt;p&gt;I agree that this is a bit strange. The logic is copied from&lt;br/&gt;
Clock.rotateClock(), by the way. I believe you are correct that it&apos;s&lt;br/&gt;
not relevant for the (default) cache sizes currently used (none is&lt;br/&gt;
smaller than 32). Perhaps it would be better to simplify it and just&lt;br/&gt;
say itemsToCheck = Math.max(20, ...) or something (the number 20 is&lt;br/&gt;
arbitrary).&lt;/p&gt;

&lt;p&gt;c) Add more comments about why cleaned entries are not reused.&lt;/p&gt;

&lt;p&gt;Will update the comments with more details from the JIRA discussion.&lt;/p&gt;

&lt;p&gt;d) Incomplete @return tag for shrinkMe().&lt;/p&gt;

&lt;p&gt;Thanks, fixed in revision 631225.&lt;/p&gt;

&lt;p&gt;e) Off-by-one error in shrinkMe().&lt;/p&gt;

&lt;p&gt;I don&apos;t think there is an off-by-one error, since we use the old value&lt;br/&gt;
of pos, not the one that has been increased by one. I guess it&apos;s a bit&lt;br/&gt;
confusing to use the postfix increment operator and store away the old&lt;br/&gt;
value on the same line in the code. Perhaps it would be clearer if the&lt;br/&gt;
code were reordered from&lt;/p&gt;

&lt;p&gt;                index = pos++;&lt;br/&gt;
                h = clock.get(index);&lt;/p&gt;

&lt;p&gt;to&lt;/p&gt;

&lt;p&gt;                h = clock.get(pos);&lt;br/&gt;
                index = pos;&lt;br/&gt;
                pos++;&lt;/p&gt;

&lt;p&gt;Or perhaps I misunderstood what you meant?&lt;/p&gt;

&lt;p&gt;f) Factor out common code for evicting entries in rotateClock() and&lt;br/&gt;
shrinkMe().&lt;/p&gt;

&lt;p&gt;Will do that.&lt;/p&gt;

&lt;p&gt;g) Comment on the heuristics in trimMe().&lt;/p&gt;

&lt;p&gt;As you quite correctly guessed, the heuristics have been taken from&lt;br/&gt;
the old clock implementation (Clock.trimToSize()), and I don&apos;t know&lt;br/&gt;
why they were chosen.&lt;/p&gt;

&lt;p&gt;What strikes me with the heuristics, is that they seem to do their&lt;br/&gt;
best to prevent this method from ever doing anything (which can also&lt;br/&gt;
be seen in the code coverage reports). Also, I don&apos;t see very much&lt;br/&gt;
value in shrinking a cache that already is smaller than the max&lt;br/&gt;
size. If someone needs it, the correct way would be to reduce the max&lt;br/&gt;
size of the cache, I think. Perhaps it would be better just to remove&lt;br/&gt;
it? Seems like it&apos;s much complexity for little added value.&lt;/p&gt;

&lt;p&gt;h) Naming constants used in heuristic&lt;/p&gt;

&lt;p&gt;Probably a good idea. Will do that.&lt;/p&gt;

&lt;p&gt;5. BackgroundCleaner&lt;/p&gt;

&lt;p&gt;a) Misleading class javadoc&lt;/p&gt;

&lt;p&gt;Will fix that comment.&lt;/p&gt;

&lt;p&gt;b) Comment that serviceImmediately() isn&apos;t used by BasicDaemon&lt;/p&gt;

&lt;p&gt;Could do that. Now, BackgroundCleaner doesn&apos;t know that it&apos;s being&lt;br/&gt;
serviced by a BasicDaemon, so I&apos;m not sure whether or not it&apos;s&lt;br/&gt;
BackgroundCleaner&apos;s concern that the method is not being used by&lt;br/&gt;
BasicDaemon. If another implementation of DaemonService is used&lt;br/&gt;
(that&apos;s not controlled by the cache implementation), the returned&lt;br/&gt;
value may become relevant.&lt;/p&gt;</comment>
                            <comment id="12573178" author="knutanders" created="Thu, 28 Feb 2008 08:15:57 +0000"  >&lt;p&gt;Here&apos;s the patch (d2911-enable.diff) that&apos;s needed if we want to enable the new buffer manager for Java version 1.5 and higher. It only changes a couple of lines in modules.properties to make sure the correct classes are loaded when the database is booted. It doesn&apos;t change anything for 1.4 or Java ME, so they&apos;ll still use the old buffer manager.&lt;/p&gt;</comment>
                            <comment id="12573179" author="oysteing" created="Thu, 28 Feb 2008 08:32:59 +0000"  >&lt;p&gt;I agree that we should just go ahead and enable the buffer manager.  The issues I raised can be addressed later.&lt;/p&gt;</comment>
                            <comment id="12573230" author="knutanders" created="Thu, 28 Feb 2008 11:17:45 +0000"  >&lt;p&gt;Thanks &#216;ystein. I checked in d2911-enable.diff with revision 631930. I have tested it with derbyall and suites.All on 1.6, and with suites.All on 1.4.2 and 1.5 (Sun&apos;s JVMs on Solaris 10).&lt;/p&gt;</comment>
                            <comment id="12576992" author="knutanders" created="Mon, 10 Mar 2008 14:03:11 +0000"  >&lt;p&gt;I improved some comments in revision 635556 to address 1a and 1d.&lt;/p&gt;

&lt;p&gt;1b and 1c were addressed in revision 635183 as part of &lt;a href=&quot;https://issues.apache.org/jira/browse/DERBY-3493&quot; title=&quot;stress.multi times out waiting on testers with blocked testers waiting on the same statement&quot; class=&quot;issue-link&quot; data-issue-key=&quot;DERBY-3493&quot;&gt;&lt;del&gt;DERBY-3493&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="12577030" author="knutanders" created="Mon, 10 Mar 2008 15:21:06 +0000"  >&lt;p&gt;Improved some more comments in revision 635577. This should address 4c, 5a and 5b.&lt;/p&gt;</comment>
                            <comment id="12577441" author="knutanders" created="Tue, 11 Mar 2008 13:28:14 +0000"  >&lt;p&gt;Attaching d2911-12.diff which addresses &#216;ystein&apos;s comments 3a, 4a (indirectly, since that code was removed), 4b and 4f. It makes ReplacementPolicy.insertEntry() void since the return value is not used, it simplifies the handling of small caches in ClockPolicy.rotateClock(), and it factors out common code in ClockPolicy.rotateClock() and ClockPolicy.shrinkMe(). This patch is not supposed to change the behaviour in any way.&lt;/p&gt;

&lt;p&gt;suites.All ran cleanly. I have also started derbyall and I will report back if it fails.&lt;/p&gt;</comment>
                            <comment id="12577460" author="knutanders" created="Tue, 11 Mar 2008 14:29:43 +0000"  >&lt;p&gt;All tests passed.&lt;/p&gt;</comment>
                            <comment id="12577755" author="knutanders" created="Wed, 12 Mar 2008 08:25:02 +0000"  >&lt;p&gt;Committed revision 636247.&lt;/p&gt;</comment>
                            <comment id="12577861" author="knutanders" created="Wed, 12 Mar 2008 14:42:45 +0000"  >&lt;p&gt;Attaching a patch (d2911-13) which addresses 2a and 4h. I have started the regression tests suite.&lt;/p&gt;

&lt;p&gt;2a: Instead of inventing a new name for lockWhenIdentityIsSet(), I created a new method waitUntilIdentityIsSet() and let the callers first invoke lock() and then waitUntilIdentityIsSet(). I think that makes the code clearer.&lt;/p&gt;

&lt;p&gt;4h: I created named constants for the numeric constants in rotateClock() and shrinkMe(). I didn&apos;t make any changes to trimMe(), since I&apos;m wondering if it&apos;s best just to remove it.&lt;/p&gt;

&lt;p&gt;After this patch, I think it&apos;s only 4g (explain the heuristics in ClockPolicy.trimMe()) that hasn&apos;t been addressed. I believe that trimMe() in reality is dead code (supported by the fact that its predecessor Clock.trimToSize() is always a no-op in the regression tests according to the test coverage reports), and a better and more reliable solution for the problem it tries to solve, is to reduce the cache size. Unless someone comes up with a situation where Clock.trimToSize() and ClockPolicy.trimMe() provide valuable functionality, I&apos;m inclined to remove the latter.&lt;/p&gt;</comment>
                            <comment id="12578189" author="knutanders" created="Thu, 13 Mar 2008 08:48:47 +0000"  >&lt;p&gt;Committed revision 636670.&lt;/p&gt;</comment>
                            <comment id="12582651" author="knutanders" created="Thu, 27 Mar 2008 13:55:52 +0000"  >&lt;p&gt;Attaching two new patches for review:&lt;/p&gt;

&lt;p&gt;d2911-14.diff - rewrites the part of ClockPolicy.shrinkMe() that &#216;ystein mentioned in his comment 4e. It is not supposed to change the behaviour, only make the relationship between the variables pos and index clearer.&lt;/p&gt;

&lt;p&gt;d2911-15.diff - removes the method ClockPolicy.trimeMe() since it&apos;s only adding complexity and doesn&apos;t have any real value (I think).&lt;/p&gt;

&lt;p&gt;Derbyall and suites.All ran cleanly (except ManagementMBeanTest which has failed for some time in the nightly regression tests too).&lt;/p&gt;</comment>
                            <comment id="12583453" author="knutanders" created="Sun, 30 Mar 2008 16:04:05 +0100"  >&lt;p&gt;Committed d2911-14.diff with revision 642752.&lt;br/&gt;
Committed d2911-15.diff with revision 642755.&lt;/p&gt;

&lt;p&gt;I think all comments have been addressed now, and I&apos;m not planning any more patches on this issue. Some of the patches have only been committed to trunk (revisions 635577, 636247, 636670, 642752 and 642755). None of them needs to go to the 10.4 branch, but unless there are objections, I&apos;ll merge them to minimize the difference between trunk and the branch (which makes it easier to map stack traces from bug reports against 10.4 to the code in trunk).&lt;/p&gt;</comment>
                            <comment id="12583468" author="kristwaa" created="Sun, 30 Mar 2008 18:35:13 +0100"  >&lt;p&gt;I think it is a good idea to merge the changes to the 10.4 branch before the release candidate is produced.&lt;br/&gt;
+1 from me.&lt;/p&gt;</comment>
                            <comment id="12584067" author="knutanders" created="Tue, 1 Apr 2008 10:05:58 +0100"  >&lt;p&gt;Merged the latest changes (rev. 635556, 635577, 636247, 636670, 642752, 642755) to 10.4 and committed revision 643327.&lt;/p&gt;

&lt;p&gt;Marking the issue as resolved (with fix version 10.4.0.0 since almost all the code went in before the 10.4 branch was cut).&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12389818">DERBY-3479</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12325504">DERBY-698</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12379282">DERBY-3092</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12369173" name="cleaner.diff" size="3538" author="knutanders" created="Thu, 8 Nov 2007 13:43:25 +0000"/>
                            <attachment id="12369172" name="cleaner.tar" size="16384" author="knutanders" created="Thu, 8 Nov 2007 13:43:25 +0000"/>
                            <attachment id="12365012" name="d2911-1.diff" size="25254" author="knutanders" created="Mon, 3 Sep 2007 15:07:35 +0100"/>
                            <attachment id="12365013" name="d2911-1.stat" size="293" author="knutanders" created="Mon, 3 Sep 2007 15:07:35 +0100"/>
                            <attachment id="12371430" name="d2911-10.diff" size="16036" author="knutanders" created="Tue, 11 Dec 2007 13:57:09 +0000"/>
                            <attachment id="12371431" name="d2911-10.stat" size="222" author="knutanders" created="Tue, 11 Dec 2007 13:57:09 +0000"/>
                            <attachment id="12371661" name="d2911-11.diff" size="3621" author="knutanders" created="Fri, 14 Dec 2007 10:37:39 +0000"/>
                            <attachment id="12377613" name="d2911-12.diff" size="10823" author="knutanders" created="Tue, 11 Mar 2008 13:28:13 +0000"/>
                            <attachment id="12377698" name="d2911-13.diff" size="6578" author="knutanders" created="Wed, 12 Mar 2008 14:42:45 +0000"/>
                            <attachment id="12378718" name="d2911-14.diff" size="1688" author="knutanders" created="Thu, 27 Mar 2008 13:55:52 +0000"/>
                            <attachment id="12378719" name="d2911-15.diff" size="8884" author="knutanders" created="Thu, 27 Mar 2008 13:55:52 +0000"/>
                            <attachment id="12365085" name="d2911-2.diff" size="1607" author="knutanders" created="Tue, 4 Sep 2007 15:48:10 +0100"/>
                            <attachment id="12365173" name="d2911-3.diff" size="2312" author="knutanders" created="Wed, 5 Sep 2007 14:40:37 +0100"/>
                            <attachment id="12366171" name="d2911-4.diff" size="4978" author="knutanders" created="Wed, 19 Sep 2007 10:51:59 +0100"/>
                            <attachment id="12366292" name="d2911-5.diff" size="4917" author="knutanders" created="Thu, 20 Sep 2007 16:01:58 +0100"/>
                            <attachment id="12366658" name="d2911-6.diff" size="24846" author="knutanders" created="Thu, 27 Sep 2007 13:05:30 +0100"/>
                            <attachment id="12366659" name="d2911-6.stat" size="445" author="knutanders" created="Thu, 27 Sep 2007 13:05:31 +0100"/>
                            <attachment id="12366922" name="d2911-7.diff" size="5633" author="knutanders" created="Tue, 2 Oct 2007 14:56:08 +0100"/>
                            <attachment id="12367637" name="d2911-7a.diff" size="6960" author="knutanders" created="Fri, 12 Oct 2007 14:53:21 +0100"/>
                            <attachment id="12370817" name="d2911-9.diff" size="20798" author="knutanders" created="Mon, 3 Dec 2007 10:33:24 +0000"/>
                            <attachment id="12370818" name="d2911-9.stat" size="308" author="knutanders" created="Mon, 3 Dec 2007 10:33:24 +0000"/>
                            <attachment id="12376701" name="d2911-enable.diff" size="982" author="knutanders" created="Thu, 28 Feb 2008 08:15:57 +0000"/>
                            <attachment id="12365837" name="d2911-entry-javadoc.diff" size="2835" author="knutanders" created="Fri, 14 Sep 2007 11:13:30 +0100"/>
                            <attachment id="12364760" name="d2911-unused.diff" size="10143" author="knutanders" created="Wed, 29 Aug 2007 12:50:44 +0100"/>
                            <attachment id="12364761" name="d2911-unused.stat" size="667" author="knutanders" created="Wed, 29 Aug 2007 12:50:44 +0100"/>
                            <attachment id="12365001" name="d2911perf.java" size="3357" author="knutanders" created="Mon, 3 Sep 2007 11:06:09 +0100"/>
                            <attachment id="12369667" name="derby-2911-8.diff" size="18412" author="knutanders" created="Fri, 16 Nov 2007 15:22:54 +0000"/>
                            <attachment id="12369668" name="derby-2911-8.stat" size="435" author="knutanders" created="Fri, 16 Nov 2007 15:22:55 +0000"/>
                            <attachment id="12373096" name="perftest.diff" size="48527" author="knutanders" created="Mon, 14 Jan 2008 16:00:43 +0000"/>
                            <attachment id="12374562" name="perftest.pdf" size="76194" author="knutanders" created="Fri, 1 Feb 2008 15:51:27 +0000"/>
                            <attachment id="12373097" name="perftest.stat" size="1006" author="knutanders" created="Mon, 14 Jan 2008 16:00:47 +0000"/>
                            <attachment id="12374561" name="perftest2.diff" size="47826" author="knutanders" created="Fri, 1 Feb 2008 15:51:27 +0000"/>
                            <attachment id="12366751" name="perftest6.pdf" size="91464" author="knutanders" created="Fri, 28 Sep 2007 16:54:47 +0100"/>
                            <attachment id="12369774" name="poisson_patch8.tar" size="19968" author="knutanders" created="Mon, 19 Nov 2007 10:29:56 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>34.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310200" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Bug behavior facts</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10362"><![CDATA[Performance]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Tue, 28 Aug 2007 15:17:49 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>30650</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hy0wpr:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>39118</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                            </customfields>
    </item>
</channel>
</rss>