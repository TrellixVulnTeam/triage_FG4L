<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 03:08:50 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/DERBY-96/DERBY-96.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[DERBY-96] partial log record writes that occur because of out-of order writes need to be handled by recovery.</title>
                <link>https://issues.apache.org/jira/browse/DERBY-96</link>
                <project id="10594" key="DERBY">Derby</project>
                    <description>&lt;p&gt;Incomplete log record write that occurs because of&lt;br/&gt;
an out of order partial writes gets recognized as complete during&lt;br/&gt;
recovery if the first sector and last sector happens to get written.&lt;br/&gt;
 Current system recognizes incompletely written log records by checking&lt;br/&gt;
the length of the record that is stored in the beginning and end.&lt;br/&gt;
 Format the log records are written to disk is:&lt;/p&gt;

&lt;p&gt;  &lt;ins&gt;---------&lt;del&gt;&lt;/ins&gt;&lt;/del&gt;-----------&lt;del&gt;&lt;ins&gt;&lt;/del&gt;-----------------&lt;/ins&gt;&lt;/p&gt;

&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; length     &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  LOG RECORD &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;    length   &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;  &lt;ins&gt;---------&lt;del&gt;&lt;/ins&gt;&lt;/del&gt;-----------&lt;del&gt;&lt;ins&gt;&lt;/del&gt;-----------------&lt;/ins&gt;&lt;/p&gt;


&lt;p&gt;This mechanism works fine if sectors are written in sequential manner or&lt;br/&gt;
log record size is less than 2 sectors. I  believe on SCSI types disks&lt;br/&gt;
order is not necessarily sequential, SCSI disk drives may sometimes do a&lt;br/&gt;
reordering of the sectors to optimize the performance.  If a log record&lt;br/&gt;
that spans multiple disk sectors is being written to SCISI type of&lt;br/&gt;
devices,  it is possible that first and last sector written before the&lt;br/&gt;
crash; If this occurs recovery system will incorrectly  interpret the&lt;br/&gt;
log records was completely written and replay the record. This could&lt;br/&gt;
lead to recovery errors or data corruption.&lt;br/&gt;
-&lt;/p&gt;


&lt;p&gt;This problem also will not occur if a disk drive has write cache with a&lt;br/&gt;
battery backup which will make sure I/O request will complete.&lt;/p&gt;</description>
                <environment></environment>
        <key id="28905">DERBY-96</key>
            <summary>partial log record writes that occur because of out-of order writes need to be handled by recovery.</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="tsuresh">Suresh Thalamati</assignee>
                                    <reporter username="tsuresh">Suresh Thalamati</reporter>
                        <labels>
                    </labels>
                <created>Thu, 9 Dec 2004 20:11:51 +0000</created>
                <updated>Wed, 1 Jul 2009 01:34:34 +0100</updated>
                            <resolved>Sat, 28 May 2005 10:07:35 +0100</resolved>
                                    <version>10.0.2.1</version>
                                    <fixVersion>10.1.1.0</fixVersion>
                                    <component>Store</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="56482" author="tsuresh" created="Thu, 9 Dec 2004 23:35:59 +0000"  >&lt;p&gt;Some thoughts on how this problem could be solved:&lt;/p&gt;

&lt;p&gt;To identify the partial writes, some form of checksum has to be added to the log data written to the file. On recovery using the checksum information partial written log records could be identified and thrown away.  Checksum information has to be included &lt;br/&gt;
with the log data before it is written to the disk. Now the issue is when do we calculate the checksum and write to the disk. &lt;/p&gt;

&lt;p&gt;Following are some logical points when the checksum can be calculated and written along with log informaton:&lt;/p&gt;

&lt;p&gt;1)Calculate the checksum for each log records and store the information as part of log record data structure.  Disadvantage of this approach, storing checksum with each log records could be expensive with respect to the amount of space and time spent to calculate. &lt;/p&gt;

&lt;p&gt;2)Calculate checksum for group of log records in the log buffers before writing the buffer to the disk and also write an addition log records that will have the checksum information and the length of the data. This log records (LogCheckSum) will be prefixes to the log buffer. The reason  checksum log records are to be written in the beginning  is it is easier to find to how much data has to be read during recovery to verify the checksum. &lt;/p&gt;

&lt;p&gt;    Log data is written only when log buffers is full or make sure WAL protocol is not violated.  Size of the data that is part of the checksum can potentially be 32K or whatever log buffer size is. Overhead with this approach is less compared to the first approach. &lt;/p&gt;


&lt;p&gt;3)Block-based log i/0: Idea is to group log record data into 4k/8K pages with a checksum on each page. During recovery checksum will be recalculated for  each &lt;br/&gt;
and match it one on the disk, if checksum does not match it is possibly as partial write. &lt;/p&gt;

&lt;p&gt;This approach is liked to have more overhead compared to the second one.  But this approach also has the benefit of making log writes aligned.  Not sure yet whether there is any performance by doing so. (Please see aligned Vs Non-Aligned e-mail thread on derby list). &lt;/p&gt;

&lt;p&gt;I should also bring to the attention this approach will likely require more changes  than 1 &amp;amp; 2 , reasons for that are :&lt;/p&gt;

&lt;p&gt;a)Current system assumed LSN to file offset. If  the data is written in page format , that will no longer be true. &lt;br/&gt;
b)To strict to WAL protocol , it may be required that an unfilled page needs to be written.  If this unfilled page happened to have a COMMITTED log records it can not be simply rewritten; If the rewrite is incomplete log records with committed information will be thrown away.  To avoid this issue,  log pages can not be written , which could lead to of unused space in the log file or implement safe-write mechanism(ping-pong algorithm). &lt;/p&gt;


&lt;p&gt;Upgrade: &lt;br/&gt;
Irrespective of what approach is used to solve this problem, I believe new type of information (checksum) has to be written to the disk, which will not be understood by Old versions.  &lt;/p&gt;


&lt;p&gt;Any comments/suggestion ?&lt;/p&gt;


&lt;p&gt;-suresh&lt;/p&gt;</comment>
                            <comment id="59041" author="tsuresh" created="Sat, 12 Feb 2005 04:12:53 +0000"  >
&lt;p&gt;Conclusion was to solve this problem by writing a checksum log record before writing  the log buffer and verify the checksum&lt;br/&gt;
during recovery. &lt;/p&gt;


&lt;p&gt;I don&apos;t know how to link derby dev list e-mail to zira. just&lt;br/&gt;
doing  copy/paste of comments from e-mail list. &lt;br/&gt;
Mike Matrigali wrote:&lt;/p&gt;


&lt;p&gt;&amp;gt;&amp;gt;I think that some fix to this issue should be implemented for the next&lt;br/&gt;
&amp;gt;&amp;gt;release.  The order of my preference is #2, #1, #3.&lt;br/&gt;
&amp;gt;&amp;gt;  &lt;br/&gt;
&amp;gt;&amp;gt;&lt;/p&gt;


&lt;p&gt;I believe option #2 (checksuming log recods in the log buffers before&lt;br/&gt;
writing to the disk)  is a good fix for this problem.&lt;br/&gt;
If there are no objectiions to this approach,  I will start to work on&lt;br/&gt;
this.&lt;/p&gt;


&lt;p&gt;-suresht&lt;/p&gt;



&lt;p&gt;&amp;gt;&amp;gt;I think that the option #2 can be implemented in the logging system and&lt;br/&gt;
&amp;gt;&amp;gt;require very little if no changes to the rest of the system processing&lt;br/&gt;
&amp;gt;&amp;gt;of log records.  Log record offsets remain efficient, ie. they can use&lt;br/&gt;
&amp;gt;&amp;gt;LSN&apos;s directly.  Only the boot time recovery code need look for the&lt;br/&gt;
&amp;gt;&amp;gt;new log record and do the work to verify checksums, online abort is&lt;br/&gt;
&amp;gt;&amp;gt;unaffected.&lt;br/&gt;
&amp;gt;&amp;gt;&lt;br/&gt;
&amp;gt;&amp;gt;I would like to see some performance numbers on the checksum overhead&lt;br/&gt;
&amp;gt;&amp;gt;and if it is measurable then maybe some discussion on checksum choice.&lt;br/&gt;
&amp;gt;&amp;gt;An obvious first choice would seem to be the standard java provided one&lt;br/&gt;
&amp;gt;&amp;gt;used on the data pages.  If I had it to do over, I would probably have&lt;br/&gt;
&amp;gt;&amp;gt;used a different approach on the data pages.  The point of the checksum&lt;br/&gt;
&amp;gt;&amp;gt;on the data page is not to catch data sector write errors, the system&lt;br/&gt;
&amp;gt;&amp;gt;expects the device to catch those, the only point is to catch&lt;br/&gt;
&amp;gt;&amp;gt;inconsistent sector writes (ie. 1st and 2nd 512 byte sector but not&lt;br/&gt;
&amp;gt;&amp;gt;3rd and 4th), for this the current checksum is overkill.  For this one&lt;br/&gt;
&amp;gt;&amp;gt;need not checksum every byte on the page,&lt;br/&gt;
&amp;gt;&amp;gt;one can guarantee a consistent write with 1 bit per sector in the page.&lt;br/&gt;
&amp;gt;&amp;gt;&lt;br/&gt;
&amp;gt;&amp;gt;In the future we may want to revisit #3 if it looks like the stream log&lt;br/&gt;
&amp;gt;&amp;gt;is an I/O bottleneck which can&apos;t be addressed by striping or some other&lt;br/&gt;
&amp;gt;&amp;gt;hardware help like smart caching controllers.  I see it as a performance&lt;br/&gt;
&amp;gt;&amp;gt;project rather than a correctness project.  It also is a lot more work&lt;br/&gt;
&amp;gt;&amp;gt;and risk.  Note that this could be a good project for someone wanting to&lt;br/&gt;
&amp;gt;&amp;gt;do some research in this area as it is implemented as a derby module&lt;br/&gt;
&amp;gt;&amp;gt;where an alternate implementation could be dropped in if available.&lt;br/&gt;
&amp;gt;&amp;gt;&lt;br/&gt;
&amp;gt;&amp;gt;While I believe that we should address this issue, I should also note&lt;br/&gt;
&amp;gt;&amp;gt;that in all my time working on cloudscape/derby I have never received a&lt;br/&gt;
&amp;gt;&amp;gt;problem database (in that time any log related error would have come&lt;br/&gt;
&amp;gt;&amp;gt;through me), that resulted from this out of order/imcomplete log&lt;br/&gt;
&amp;gt;&amp;gt;write issue - this of course does not mean it has not happened just that&lt;br/&gt;
&amp;gt;&amp;gt;it was not reported to us and/or did not affect the database in a&lt;br/&gt;
&amp;gt;&amp;gt;noticable way.  We have actually never seen an out of order write from&lt;br/&gt;
&amp;gt;&amp;gt;the data pages also - we have seen a few checksum errors but all of&lt;br/&gt;
&amp;gt;&amp;gt;those were caused by a bad disk.&lt;br/&gt;
&amp;gt;&amp;gt;&lt;br/&gt;
&amp;gt;&amp;gt;On the upgrade issue, it may be time to start an upgrade thread.  Here&lt;br/&gt;
&amp;gt;&amp;gt;are just some thoughts.  If doing option #2, it would be nice if the&lt;br/&gt;
&amp;gt;&amp;gt;new code could still read the old log files and then optionally&lt;br/&gt;
&amp;gt;&amp;gt;write the new log record or not.  Then if users wanted to run a&lt;br/&gt;
&amp;gt;&amp;gt;release in a &quot;soft&quot; upgrade mode where they needed to be able to&lt;br/&gt;
&amp;gt;&amp;gt;go back to the old software they could - they just would not get&lt;br/&gt;
&amp;gt;&amp;gt;this fix.  On a &quot;hard&quot; upgrade the software should continue to read&lt;br/&gt;
&amp;gt;&amp;gt;the old log files as they are currently formatted, and for any new&lt;br/&gt;
&amp;gt;&amp;gt;log files it should begin writing the new log record.  Once the new&lt;br/&gt;
&amp;gt;&amp;gt;log record make&apos;s it way into the log file accessing the db with the&lt;br/&gt;
&amp;gt;&amp;gt;old software is unsupported (it will throw an error as it won&apos;t know&lt;br/&gt;
&amp;gt;&amp;gt;what to do with the new log record).&lt;/p&gt;</comment>
                            <comment id="66496" author="tsuresh" created="Sat, 28 May 2005 10:07:35 +0100"  >&lt;p&gt;Following changes fixed this problem:&lt;br/&gt;
r178494 :&lt;br/&gt;
small fix to make sure that log buffers are switched are correctly when the are&lt;br/&gt;
full, when the log checksum  feature is disabled due to a  soft upgrade.&lt;/p&gt;

&lt;p&gt;r169737:&lt;br/&gt;
some  functional tests to test the transaction log checksum feature.Log corruption is simulated using a proxy storage factory that allows corruption of the log write request before being writing to the disk.  CorruptDiskStorage fact&lt;br/&gt;
ory by default forwards all the request to the underlying disk storage factory  except when corruption flags are  enabled.&lt;/p&gt;

&lt;p&gt;Recovery tests need to  boot the same database many times and have to use the different Subprotocol to enable the corruption instead of the default protocol. This seems to be&lt;br/&gt;
possible only  by adding  a new tests suite in the current test frame work.  Add ed a new suite&lt;br/&gt;
called &quot;storerecovery&quot; , may be all future recovery tests can be added to this suite.&lt;/p&gt;

&lt;p&gt;r164994:&lt;br/&gt;
changes to make softupgrade correctly with the  transaction log checksum feature in 10.1 Added  checkVersion() method  to log factory it&lt;br/&gt;
self,  becuase that is where the  version numbers are read  from  from the log control file , but did not export the call it to  the rawstore factory  as it is not needed now.  (This can be done easlily when there is a  need for upgrade&lt;br/&gt;
checks  in the other store modules..)&lt;/p&gt;

&lt;p&gt;r159651:&lt;br/&gt;
This is a  patch towards implementing checksum support for transaction log to handle out of order incomplete log writes during recovery.  This patch is based on writing a checksum log record that contain checksum information for&lt;br/&gt;
a group of log records in the log buffers. &lt;/p&gt;

&lt;p&gt;Changes in this patch addresses writing checksum information to the transaction log before the log records are written and verifying the log at recovery time using the checksum information on the disk.&lt;/p&gt;

&lt;p&gt;Writing Checksum Log Records:&lt;br/&gt;
Checksum log record contains checksum Algorithm, size of  the data  and the checksum value.&lt;br/&gt;
Added a new class to implement this log operation.&lt;/p&gt;

&lt;p&gt;The checksum Log record is placed before the actual log data the checksum record represents. This is done by reserving the space in the log buffers and in the log file then writing into reserved buffer space the checksum log record&lt;br/&gt;
whenever buffer is full or it need to be written because of a flush request due to a commit. Incase of a large log records that does not fit into a single log buffer, the log records are written directly to the log file, in this case&lt;br/&gt;
checksum log record represents only one log record and it is written to the log file before writing the large log record directly into the log file.&lt;/p&gt;

&lt;p&gt;In the current system the log group information is encrypted when a database is encrypted. There is no facility to identify that a log record is checksum log record without decrypting the log record. Checksum Log Record is also encrypted to work correctly with the rest of the system.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>29391</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hy0szb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>38513</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                            </customfields>
    </item>
</channel>
</rss>