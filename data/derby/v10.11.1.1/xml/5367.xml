<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 03:36:06 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/DERBY-5367/DERBY-5367.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[DERBY-5367] Stale data retrieved when using new collation=TERRITORY_BASED:PRIMARY feature</title>
                <link>https://issues.apache.org/jira/browse/DERBY-5367</link>
                <project id="10594" key="DERBY">Derby</project>
                    <description>&lt;p&gt;Our product recently upgraded to version 10.8.1.2 in order to take advantage of the new &apos;case-insensitive&apos; mode offered by Derby in the form of the &quot;collation=TERRITORY_BASED:PRIMARY&quot; connection parameter.&lt;/p&gt;

&lt;p&gt;Unfortunately, we have run into an issue whereby stale data appears to be retrieved from an index, even though the data in the table itself has changed.&lt;/p&gt;

&lt;p&gt;You can see this issue in the IJ session below.  The database in question was created using this Java parameter when invoking IJ:&lt;/p&gt;

&lt;p&gt;-Dij.database=jdbc:derby:test;create=true;collation=TERRITORY_BASED:PRIMARY&lt;/p&gt;

&lt;p&gt;Here is the IJ session:&lt;/p&gt;

&lt;p&gt;CONNECTION0* - 	jdbc:derby:test&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;= current connection&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;ij&amp;gt; CREATE TABLE tag (&lt;br/&gt;
    tag_id INTEGER GENERATED BY DEFAULT AS IDENTITY NOT NULL,&lt;br/&gt;
    tag VARCHAR(255) NOT NULL,&lt;br/&gt;
    CONSTRAINT tag_pk PRIMARY KEY (tag_id),&lt;br/&gt;
    CONSTRAINT tag_tag_unique UNIQUE (tag)&lt;br/&gt;
);&lt;br/&gt;
0 rows inserted/updated/deleted&lt;/p&gt;

&lt;p&gt;ij&amp;gt; &amp;#8211; first insert a value &apos;Test&apos;, note the upper-case &apos;T&apos; in &apos;Test&apos;&lt;br/&gt;
ij&amp;gt; INSERT INTO tag (tag) VALUES (&apos;Test&apos;);&lt;br/&gt;
1 row inserted/updated/deleted&lt;/p&gt;

&lt;p&gt;ij&amp;gt; SELECT * FROM tag;&lt;br/&gt;
TAG_ID     |TAG                                                                                                                             &lt;br/&gt;
--------------------------------------------------------------------------------------------------------------------------------------------&lt;br/&gt;
1          |Test                                                                                                                            &lt;/p&gt;

&lt;p&gt;1 row selected&lt;/p&gt;

&lt;p&gt;ij&amp;gt; &amp;#8211; Now delete the row&lt;br/&gt;
ij&amp;gt; DELETE FROM tag WHERE tag=&apos;Test&apos;;&lt;br/&gt;
1 row inserted/updated/deleted&lt;/p&gt;

&lt;p&gt;ij&amp;gt; &amp;#8211; You could run another SELECT here to verify it is gone, but it is.&lt;/p&gt;

&lt;p&gt;ij&amp;gt; &amp;#8211; Now insert a new value &apos;test&apos;, note the lower-case &apos;t&apos; in &apos;test&apos;&lt;br/&gt;
ij&amp;gt; INSERT INTO tag (tag) VALUES (&apos;test&apos;);&lt;br/&gt;
1 row inserted/updated/deleted&lt;/p&gt;

&lt;p&gt;ij&amp;gt; &amp;#8211; Now verify that the table contains only the lower-case version: &apos;test&apos;&lt;br/&gt;
ij&amp;gt; SELECT * FROM tag;&lt;br/&gt;
TAG_ID     |TAG                                                                                                                             &lt;br/&gt;
--------------------------------------------------------------------------------------------------------------------------------------------&lt;br/&gt;
2          |test                                                                                                                            &lt;/p&gt;

&lt;p&gt;1 row selected&lt;/p&gt;

&lt;p&gt;ij&amp;gt; &amp;#8211; Now, here is the bug.&lt;br/&gt;
ij&amp;gt; SELECT tag FROM tag;&lt;br/&gt;
TAG                                                                                                                             &lt;br/&gt;
--------------------------------------------------------------------------------------------------------------------------------&lt;br/&gt;
Test                                                                                                                            &lt;/p&gt;

&lt;p&gt;1 row selected&lt;br/&gt;
ij&amp;gt; &lt;/p&gt;

&lt;p&gt;Note in the last SELECT we specify the &apos;tag&apos; column specifically.  When we &apos;SELECT *&apos;, Derby performs a table-scan and the result is correct.  However, when we &apos;SELECT tag&apos;, Derby appears to use the index created for the &apos;tag_tag_unique&apos; unique constraint.  As an optimization Derby, like many databases, will use values directly from the index in the case where the index covers all requested columns.&lt;/p&gt;

&lt;p&gt;The bigger question is, why doesn&apos;t the DELETE action cause the entry in the tag_tag_unique index to be deleted?  Is this a further optimization?  If so, it is imperative that the index at least be updated when the new value is inserted.&lt;/p&gt;

&lt;p&gt;This is rather a severe bug for us that causes stale data to be returned.&lt;/p&gt;</description>
                <environment>Mac OS X, Windows</environment>
        <key id="12516862">DERBY-5367</key>
            <summary>Stale data retrieved when using new collation=TERRITORY_BASED:PRIMARY feature</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.png">Critical</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="kristwaa">Kristian Waagan</assignee>
                                    <reporter username="brettw">Brett Wooldridge</reporter>
                        <labels>
                    </labels>
                <created>Tue, 2 Aug 2011 06:45:09 +0100</created>
                <updated>Mon, 17 Jun 2013 10:19:15 +0100</updated>
                            <resolved>Sat, 24 Sep 2011 15:24:26 +0100</resolved>
                                    <version>10.8.1.2</version>
                    <version>10.9.1.0</version>
                                    <fixVersion>10.8.2.2</fixVersion>
                    <fixVersion>10.9.1.0</fixVersion>
                                    <component>Store</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>2</watches>
                                                                <comments>
                            <comment id="13083007" author="kristwaa" created="Thu, 11 Aug 2011 09:24:17 +0100"  >&lt;p&gt;Verified the repro on trunk (Solaris 11 Express).&lt;br/&gt;
Reset the urgency flag, that&apos;s for the release manager to specify.&lt;/p&gt;

&lt;p&gt;Compressing the table makes the problem go away - this further strengthen the suspicion about a corrupted index.&lt;br/&gt;
The bug is also seen when issuing a delete without a where-clause.&lt;/p&gt;</comment>
                            <comment id="13083033" author="kristwaa" created="Thu, 11 Aug 2011 10:34:11 +0100"  >&lt;p&gt;Turns out the problem here is the insert into the index (see BTreeController.doIns).&lt;br/&gt;
There is optimization code in there to undelete a deleted row in a unique index if the &quot;same&quot; row is inserted again before the row has been purged. This assumes that the fields that are part of the key are equal, but this assumption may be broken when using collations.&lt;/p&gt;

&lt;p&gt;The insert code is smart enough to update the row location of the index row, but it doesn&apos;t understand that the value stored in the base row has changed.&lt;/p&gt;

&lt;p&gt;I&apos;m not very familiar with this code, so I&apos;d like to get some feedback from more knowledgeable people.&lt;br/&gt;
To me these possible solutions come to mind:&lt;br/&gt;
 a) Deoptimize insert - always insert a new index row.&lt;br/&gt;
 b) Make conglomerate code aware of collations and take appropriate actions after comparing the new and the old value(s)&lt;/p&gt;

&lt;p&gt;I don&apos;t know what consequences the proposed solutions bring to the table.&lt;br/&gt;
A quick and dirty fix of the code (where I updated the key field) made the repro pass.&lt;/p&gt;</comment>
                            <comment id="13083165" author="dagw" created="Thu, 11 Aug 2011 16:28:03 +0100"  >&lt;p&gt;I think we would want to avoid turning off the optimization if we don&apos;t have to. &lt;br/&gt;
But is there a way to know what collations are &quot;safe&quot;? I think our default collation uses binary UTF comparisons, so those should be ok, cf. &lt;a href=&quot;http://db.apache.org/derby/docs/10.8/devguide/cdevcollation.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://db.apache.org/derby/docs/10.8/devguide/cdevcollation.html&lt;/a&gt; . Perhaps you can determine if non-default collation is active and only deoptimize (or do an extra compare to determine if de-opt&apos;ing needed) for those cases.&lt;/p&gt;</comment>
                            <comment id="13084133" author="kristwaa" created="Fri, 12 Aug 2011 14:56:59 +0100"  >&lt;p&gt;Attaching patch 1a, which replaces/updates the whole row if a deleted row is reused and collations are enabled.&lt;/p&gt;

&lt;p&gt;Patch ready for discussion and review.&lt;br/&gt;
It passed the regression tests and the bug demonstrated by the repro is addressed. I don&apos;t really expect to commit this one, but it demonstrates one possible solution.&lt;/p&gt;</comment>
                            <comment id="13084804" author="brettw" created="Sun, 14 Aug 2011 09:23:46 +0100"  >&lt;p&gt;Thanks Kristian.&lt;/p&gt;

&lt;p&gt;I have reviewed and tested the patch, and can&apos;t find any issues with it.  It seems like the minimal patch to existing code that would fix the bug.&lt;/p&gt;</comment>
                            <comment id="13085073" author="kristwaa" created="Mon, 15 Aug 2011 14:14:24 +0100"  >&lt;p&gt;Thanks for testing and reviewing the patch, Brett.&lt;br/&gt;
I will work hard to have a fix committed in time for the upcoming 10.8.2-release, but I&apos;d still like to get a comment from one of the store experts.&lt;br/&gt;
The store is tricky, and maybe there are other places in the code that isn&apos;t prepared for collations.&lt;/p&gt;

&lt;p&gt;Attaching patch 1b, with the following changes:&lt;br/&gt;
 o moved the check for collated types inside the sp.resultExact block. A further change may be to remove the instance variables altogether.&lt;br/&gt;
 o added a regression test in CollationTest2.&lt;/p&gt;

&lt;p&gt;There might be room for improvement in the wording (both comments and method-names), since my vocabulary related to collations is rather thin &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; Also, I believe we are applying a collation order even when there is no territory specified, so DVD.isCollatedType is maybe confusing?&lt;/p&gt;

&lt;p&gt;Another question:&lt;br/&gt;
The block above the one where I applied the fix deals with the case where also the row location is the same.&lt;br/&gt;
When does this case happen, and is it guaranteed that the row values are correct even when a (non-default) collation is being used?&lt;/p&gt;</comment>
                            <comment id="13092984" author="mikem" created="Mon, 29 Aug 2011 18:21:32 +0100"  >&lt;p&gt;I have not looked at patch yet.  First some answers to previous questions.  &lt;/p&gt;

&lt;p&gt;As to the following:&lt;br/&gt;
a) Deoptimize insert - always insert a new index row. &lt;/p&gt;

&lt;p&gt;This would be a bad idea.  Algorithms in the btree depend on every leaf row in the tree being unique, whether the rows are deleted or not.   There are 2 cases&lt;br/&gt;
to consider: unique and non-unique btrees.  In the case of non-unique btrees each leaf row is made unique by including the trailing row location as part of the whole key.  In the case of unique btrees the keys should be unique not including the row location, and thus the search algo&apos;s don&apos;t include it as part of the key.&lt;/p&gt;

&lt;p&gt;The kinds of things that depend on this may include the following:&lt;br/&gt;
o the various binary searches on the leafs assume a single unique final destination, if the code allows for multiple duplicate leafs then I think you might randomly end up in the middle of a list of keys depending on binary search, and the code does not expect to get to a destination and &quot;backup&quot; to beginning of&lt;br/&gt;
a list. &lt;br/&gt;
o the check for inserting a duplicate error assumes the binary search will get to exactly the ONE row that matches depending on unique vs non-unique&lt;br/&gt;
parameters.  I think bugs will arise if there are more than one matching row.  &lt;/p&gt;

&lt;p&gt;As to following question:&lt;br/&gt;
&amp;gt;The block above the one where I applied the fix deals with the case where also the row location is the same.&lt;br/&gt;
&amp;gt;When does this case happen, and is it guaranteed that the row values are correct even when a (non-default) collation is being used? &lt;br/&gt;
Logically the code should handle this as Derby theoretically allows for reusable record ids.  In practice heap pages are marked to not allow them, so &lt;br/&gt;
i don;t think this is a normal event.  It might happen as part of particular paths through crash recovery where we do logical undo vs. physical undo on&lt;br/&gt;
btree pages. &lt;/p&gt;

&lt;p&gt;I am not sure about the question about non-default collation.  At high level I believe it should work and the store should just count on the result &lt;br/&gt;
of the collation code doing the compare.  If it says 2 different things are the same then it should just treat them as the same.&lt;/p&gt;

&lt;p&gt;Next I will look at patch.  Sorry about delay - have been out and away from list for awhile.&lt;/p&gt;</comment>
                            <comment id="13093016" author="mikem" created="Mon, 29 Aug 2011 18:54:13 +0100"  >&lt;p&gt;I looked at the patch, and I am leaning toward it is not worth putting in the special case code for collation.  I think it would be better if you just got rid of the optimization of only updating the row location, and just do the whole row update in the case of matching row always.  I think this path is not a very normal path and better to make the code &lt;br/&gt;
simple and not make the fast insert path for all inserts deal with checking for collation or not.   Extra benefit is we will get better testing with one code path&lt;br/&gt;
and can optimize later if we need to.&lt;/p&gt;

&lt;p&gt;Do leave a comment why you are not doing the optimization, as it will seem strange without at least a reference to collation and why 2 things that compare the same might not actually be same value.  &lt;/p&gt;

&lt;p&gt;I do agree that in general the fix is the right one, in that you need to update the whole row rather than part in the collation case.&lt;/p&gt;

&lt;p&gt;Either as part of this fix or log another jira I do believe the other block that handles all fields being the same also should be fixed.  I posted a guess at those code paths, but to be sure probably easiest is to add a stack trace dump to standard out and run the full test suite.  Again I think it would be reasonable to &lt;br/&gt;
just remove the optimization always and do a full update with a comment.&lt;/p&gt;</comment>
                            <comment id="13093057" author="mikem" created="Mon, 29 Aug 2011 19:25:16 +0100"  >&lt;p&gt;a comment on the caching of the work to determine if any collation fields exist.  The current patch optimizes this on a per open conglomerate basis.&lt;br/&gt;
Absolute performance would be best if users did all their inserts in one open, but many do not.  A better place to optimize this would be at compile&lt;br/&gt;
time, this is the usual place to push these kinds of optimizations in Derby.  I have not done work in this area for quite awhile, but the DynamicCompiledOpenConglomInfo of OpenBTree is the place to do such compile time optimizations.  If changes are done in this area it is important to bump the version number to invalidate existing compiled stored plans.&lt;/p&gt;

&lt;p&gt;As I suggested I think it would be best to first check in a simple unoptimized fix, and then if necessary do the optimization based on measured&lt;br/&gt;
perf diffs in the future. &lt;/p&gt;</comment>
                            <comment id="13095531" author="kristwaa" created="Thu, 1 Sep 2011 19:54:51 +0100"  >&lt;p&gt;Thanks, Mike.&lt;/p&gt;

&lt;p&gt;First, I wrote a very simple micro-benchmark. It ran an insert followed by a delete on a table with a primary key in a loop.&lt;br/&gt;
With 30 seconds warm up, 60 seconds steady state and the average of three runs per codebase I ended up with a performance degradation of around 3.6% (insane build, using in-memory back end). I&apos;ll run this with the disk-based back end later, but first I have another issue to deal with - so these number may be invalid after all...&lt;/p&gt;

&lt;p&gt;With the simplest fix possible, which is to replace the updateFieldAtSlot with updateAtSlot, I&apos;m getting a total of 33 errors in suites.All. I didn&apos;t see these when I tested the first patch, probably because the failing tests aren&apos;t using collations so the new code was never run in these tests. Here&apos;s what the core exception looks like, which can be reproduced by running lang.AlterTableTest (need only dropColumn + for instance testJira3175):&lt;br/&gt;
java.lang.ArrayIndexOutOfBoundsException: -1&lt;br/&gt;
        at org.apache.derby.impl.store.raw.data.BasePage.getHeaderAtSlot(BasePage.java:1881)&lt;br/&gt;
        at org.apache.derby.impl.store.raw.data.StoredPage.storeRecordForUpdate(StoredPage.java:7316)&lt;br/&gt;
        at org.apache.derby.impl.store.raw.data.StoredPage.storeRecord(StoredPage.java:7185)&lt;br/&gt;
        at org.apache.derby.impl.store.raw.data.UpdateOperation.undoMe(UpdateOperation.java:201)&lt;br/&gt;
        at org.apache.derby.impl.store.raw.data.PhysicalUndoOperation.doMe(PhysicalUndoOperation.java:147)&lt;br/&gt;
        at org.apache.derby.impl.store.raw.log.FileLogger.logAndUndo(FileLogger.java:533)&lt;br/&gt;
        at org.apache.derby.impl.store.raw.xact.Xact.logAndUndo(Xact.java:374)&lt;br/&gt;
        at org.apache.derby.impl.store.raw.log.FileLogger.undo(FileLogger.java:1015)&lt;br/&gt;
        at org.apache.derby.impl.store.raw.xact.Xact.abort(Xact.java:952)&lt;br/&gt;
        at org.apache.derby.impl.store.access.RAMTransaction.abort(RAMTransaction.java:1985)&lt;br/&gt;
        at org.apache.derby.impl.sql.conn.GenericLanguageConnectionContext.doRollback(GenericLanguageConnectionContext.java:1767)&lt;br/&gt;
        at org.apache.derby.impl.sql.conn.GenericLanguageConnectionContext.userRollback(GenericLanguageConnectionContext.java:1675)&lt;br/&gt;
        at org.apache.derby.impl.jdbc.TransactionResourceImpl.rollback(TransactionResourceImpl.java:245)&lt;br/&gt;
        at org.apache.derby.impl.jdbc.EmbedConnection.rollback(EmbedConnection.java:1838)&lt;br/&gt;
        at org.apache.derbyTesting.junit.JDBC.cleanup(JDBC.java:224)&lt;br/&gt;
        at org.apache.derbyTesting.junit.BaseJDBCTestCase.tearDown(BaseJDBCTestCase.java:408)&lt;/p&gt;

&lt;p&gt;I do see that there are some differences related to undo-logic for the two method calls, but that may very well be a red herring. The problem does seem to be related to logging and transactions.&lt;br/&gt;
So, there&apos;s obviously something I don&apos;t understand here... I&apos;ll continue digging tomorrow - any hints would be helpful &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13095533" author="kristwaa" created="Thu, 1 Sep 2011 19:55:49 +0100"  >&lt;p&gt;Attaching patch 2a, which produces the errors mentioned in the above comment.&lt;br/&gt;
Not for commit.&lt;/p&gt;</comment>
                            <comment id="13100322" author="kristwaa" created="Thu, 8 Sep 2011 14:53:13 +0100"  >&lt;p&gt;Note to self that the new test should specify the English locale (there may not be a collation for the locale specified/used by the user).&lt;/p&gt;</comment>
                            <comment id="13100597" author="mikem" created="Thu, 8 Sep 2011 20:11:35 +0100"  >&lt;p&gt;not sure what is going on.   Did you run the tests in SANE mode at all?  That -1 sometimes is indicative of some sort of page corruption with something overwriting something else.  Sometimes the SANE code will give a different more obvious error.  &lt;/p&gt;</comment>
                            <comment id="13100606" author="mikem" created="Thu, 8 Sep 2011 20:18:07 +0100"  >&lt;p&gt;on the benchmark, i assume that was your original patch.  3.6% seems high to me.  I understand that using in memory this is likely the worst case.&lt;/p&gt;</comment>
                            <comment id="13100632" author="mikem" created="Thu, 8 Sep 2011 20:41:08 +0100"  >&lt;p&gt;I think the problem is that updateAtSlot interface can not be used in btree&apos;s in general.   It only supports physical undo which means that the row must be in exactly the same spot on the page as it was originally.  It is used in a couple of places in the btree code on the control row (ie. the row that is always in slot 0).  But otherwise rows move around in btrees and that is the purpose of the logical undo logic.  Updates of rows in btrees are always done as deletes followed by inserts.  But that does not help in this special case.  The updateFieldAtSlot interface does support logical undo.  &lt;/p&gt;

&lt;p&gt;Off hand seems like we could:&lt;br/&gt;
1) write a updateAtSlot that does logical undo.  Before this it was not needed because an update of btree row would never &quot;know&quot; that it was going back to the same slot, so it was always cleaner to just do the delete and insert.  I would be willing to help with this, let me know.  &lt;/p&gt;

&lt;p&gt;2) a little ugly but might be interesting as a quick fix to see if it works is that you could do updateFieldAtSlot for each field in the row.&lt;/p&gt;

&lt;p&gt;I will think about this over night and see if I can come up with anything simpler.  &lt;/p&gt;</comment>
                            <comment id="13100662" author="kristwaa" created="Thu, 8 Sep 2011 21:17:44 +0100"  >&lt;p&gt;Thanks for that information, Mike!&lt;/p&gt;

&lt;p&gt;I have written a patch for option 2 already, and suites.All ran without failure (sane build). I&apos;ll run derbyall too, and run a first simple performance experiment. I suppose the performance degradation will become worse with the number of key fields.&lt;/p&gt;

&lt;p&gt;Another option would be to combine 2 with an improved version of what I did in my first patch - only update the full row if the data types in the key are collation sensitive.&lt;/p&gt;

&lt;p&gt;I have no experience in the area of option 1, so I will probably be unable to get something working before Monday...&lt;/p&gt;

&lt;p&gt;FYI, I saw the -1 with a sane build. The record isn&apos;t found and the store (I don&apos;t remember the method name) returns -1 as the slot.&lt;/p&gt;</comment>
                            <comment id="13100709" author="kristwaa" created="Thu, 8 Sep 2011 22:25:13 +0100"  >&lt;p&gt;derbyall passed too.&lt;br/&gt;
Initial perf numbers seems to be comparable with the previous ones when using the in-memory back end.&lt;br/&gt;
I&apos;ll start some longer runs tomorrow with the on-disk back end and upload the test code.&lt;/p&gt;</comment>
                            <comment id="13100756" author="mikem" created="Thu, 8 Sep 2011 23:35:20 +0100"  >&lt;p&gt;For benchmarks it seems like we would like to know results for 2 different types of benchmarks.  The worst case for this code would be primary key insert and delete repeated using the same key.  I think that is what you are doing.  Then also just a normal mix of insert/delete.  The question I would be trying to understand is it worth the overhead to the &quot;normal&quot; path to know if we can then optimize for the worst case path.  Basically 3.6% degredation for that worst case which I don&apos;t think is a normal usage pattern may be ok.  What I don&apos;t want to see is&lt;br/&gt;
slowdown is &quot;normal&quot; case which I think we might have seen with patch 1 that did work in normal path to optimize the worst case path.&lt;/p&gt;

&lt;p&gt;I think option 2 is a reasonable approach for a bug fix to existing codelines.  Seems safe to me, and a good first step.  And doing it not &lt;br/&gt;
special cased gets us full testing that caught the previous problem with the patch.&lt;/p&gt;

&lt;p&gt;With more thought I don&apos;t think we should do option 1.  I think it might make sense to log a future project for the trunk to look into just going ahead a purging the deleted row and then retrying the insert.  Whie we have the latch we may just want to purge all committed deleted rows&lt;br/&gt;
in a similar fashion to what we do previous to splitting the page.&lt;br/&gt;
The problem is that purge is tricky in that it has to be committed while holding the latch on the page, you can&apos;t just call it in-line in the current&lt;br/&gt;
code.  And you have to make sure that if you fork a transaction that parent is not conflicting with child to do the purge, otherwise might loop&lt;br/&gt;
forever.  The purge will also address the RESOLVE in the code right now (since that comment was written HeapRowLocations are now variable in size based on CompressedNumber) .  And with the specific issue that is causing this issue it is also&lt;br/&gt;
possible that updating &quot;equal&quot; fields might need more space given that different characters and chacter sequences can be &quot;equal&quot;.  So the&lt;br/&gt;
problem is even possibly worse for the collation problem.&lt;/p&gt;</comment>
                            <comment id="13101316" author="kristwaa" created="Fri, 9 Sep 2011 17:39:52 +0100"  >&lt;p&gt;Attaching patch 3a, which I&apos;m running tests with.&lt;/p&gt;

&lt;p&gt;Can you specify in some more detail what you mean with a normal mix of insert/delete?&lt;/p&gt;

&lt;p&gt;I have started the first batch of tests for the simple case - insert and delete the same key over and over again.&lt;/p&gt;</comment>
                            <comment id="13102566" author="kristwaa" created="Mon, 12 Sep 2011 12:41:00 +0100"  >&lt;p&gt;I have run two simple benchmarks. The first one I cannot post the source code for, but it populates the database and then it starts deleting and inserting records. Here I saw around 7% performance decrease with the proposed fix.&lt;/p&gt;

&lt;p&gt;I&apos;ll attach the source for the other benchmark. It is very simple, and only inserts and deletes the same record over and over again. The insert and delete is done as one transaction.&lt;br/&gt;
I ran with 60 s warmup and 300 s steady-state, ten runs per configuration. Unless stated otherwise, all settings are default. I just rounded the final numbers.&lt;br/&gt;
Explanation for the &quot;bad&quot; runs follows further down.&lt;/p&gt;

&lt;p&gt;&amp;#8212; single column key&lt;br/&gt;
clean, auto on: 637 tps&lt;br/&gt;
d5367, auto on: 557 tps&lt;br/&gt;
==&amp;gt; -12,5%&lt;/p&gt;

&lt;p&gt;clean, auto off: 3560 tps (5 &quot;bad&quot; runs)&lt;br/&gt;
d5367, auto off: 2874 tps (3 &quot;bad&quot; runs)&lt;br/&gt;
==&amp;gt; -19.5%&lt;br/&gt;
(commit interval 5k)&lt;/p&gt;

&lt;p&gt;&amp;#8212; composite key, three columns&lt;br/&gt;
clean, auto on: 551 tps&lt;br/&gt;
d5367, auto on: 382 tps&lt;br/&gt;
==&amp;gt; -30%&lt;/p&gt;

&lt;p&gt;clean, auto off: 2799 tps (1 &quot;bad&quot; runs)&lt;br/&gt;
d5367, auto off: 1671 tps (0 &quot;bad&quot; runs)&lt;br/&gt;
==&amp;gt; -40%&lt;/p&gt;

&lt;p&gt;&amp;#8212; max checkpoint interval and log switch interval (single column key)&lt;br/&gt;
clean, auto off: 6272 tps (3 &quot;bad&quot; runs)&lt;br/&gt;
d5367, auto off: 4908 tps (0 &quot;bad&quot; runs)&lt;br/&gt;
==&amp;gt; -21,5%&lt;/p&gt;

&lt;p&gt;I tested the checkpoint interval and log switch settings because I saw runs with huge performance drops with the default settings. I wasn&apos;t able to get rid of these entirely. What I saw was runs with auto-commit off whose performance dropped roughly to the performance of the runs with auto-commit on. I haven&apos;t investigated further, but I&apos;d like to know if the drops are limited duration events, or if the performance drop is permanent.&lt;br/&gt;
The numbers above are for worst case scenarios.&lt;/p&gt;

&lt;p&gt;I&apos;m currently testing an alternative patch to see how performance is affected when we optimize to avoid updating all fields when there are no collated types in the conglomerate. This has the drawback that concerned Mike - there will be multiple code paths and the new code executed for collated types only will be a lot less tested.&lt;br/&gt;
This can be optimized further for composite keys, but that part is not high priority for me right now.&lt;/p&gt;</comment>
                            <comment id="13102991" author="kristwaa" created="Mon, 12 Sep 2011 21:14:07 +0100"  >&lt;p&gt;Attaching patch 4a, which takes another approach when it comes to optimizing the code. The old/current code will be run on indexes which don&apos;t have collated types, whereas the for indexes with collated types all fields will be updated (one by one).&lt;/p&gt;

&lt;p&gt;Hopefully, one should see identical performance in the normal case with this patch.&lt;/p&gt;

&lt;p&gt;With collated types performance will drop as indicated in the performance results I posted above (worst case). It might be possible to trade CPU for IO, i.e. to compare the values before deciding to update the field. One can also avoid updating fields that aren&apos;t collated (only relevant for composite keys).&lt;/p&gt;

&lt;p&gt;Patch ready for review.&lt;/p&gt;</comment>
                            <comment id="13103485" author="kristwaa" created="Tue, 13 Sep 2011 10:18:25 +0100"  >&lt;p&gt;Brett, do you have an idea of how often the insert-delete-insert pattern is executed in your application(s)? (on tables with an index, with or without collation)&lt;/p&gt;</comment>
                            <comment id="13108026" author="kristwaa" created="Mon, 19 Sep 2011 18:52:58 +0100"  >&lt;p&gt;Mike, what&apos;s your take on a fix along the lines of patch 4a?&lt;br/&gt;
I&apos;d have to run the same sets of benchmarks on non-collated database, first but if those results are looking okay I lean towards a commit.&lt;/p&gt;

&lt;p&gt;I don&apos;t have a good sense of how often this code will be run, so it&apos;s hard for me to say if we should just not optimize this at all. Judging by the performance numbers only, I would definitely do the optimization.&lt;/p&gt;

&lt;p&gt;I want to see a fix for this issue shortly, such that people affected can run off the head of 10.8 with their own build if they want to do so (with the risks associated of course).&lt;/p&gt;</comment>
                            <comment id="13108310" author="brettw" created="Tue, 20 Sep 2011 02:22:44 +0100"  >&lt;p&gt;Kristian,&lt;/p&gt;

&lt;p&gt;With respect to this particular case (collation), the insert-delete-insert pattern is strictly based on user input.  How this was discovered was that a user created a &quot;tag&quot; on an entity in our system (eg. &apos;cisco&apos;), realized they had mistyped, deleted &apos;cisco&apos;, and added &apos;Cisco&apos;.  This resulted in the lower-case &apos;cisco&apos; coming back (because of this bug).  So to answer you question, with respect to this bug, I would classify it as &quot;low frequency&quot;.&lt;/p&gt;

&lt;p&gt;However, obviously, insert-delete-insert patterns are fairly common in general.  Any system that collects information in an automated fashion (news feeds, network data, seismic data, etc.) might delete a bunch of old rows and insert new rows with greater frequency.&lt;/p&gt;

&lt;p&gt;To tell you the truth, because of &lt;a href=&quot;https://issues.apache.org/jira/browse/DERBY-4279&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/DERBY-4279&lt;/a&gt;, I&apos;ve abandoned this &quot;pattern&quot; almost completely in Derby.  Whenever I need to delete a bunch of rows (&apos;bunch&apos; being tens of) and insert a new batch in the same transaction, because of 4279, I now have to instead mark rows deleted (with an UPDATE statement) and then insert new rows.  Deletion of marked rows is handled later via a scheduled deletion job.&lt;/p&gt;

&lt;p&gt;That of course is an aside (4279), but I&apos;m surprised you can actually run your benchmark without running into it.&lt;/p&gt;</comment>
                            <comment id="13109000" author="dagw" created="Tue, 20 Sep 2011 22:11:12 +0100"  >&lt;p&gt;If I understand this correctly the optimization is the old code (already well tested) which would still apply for non-collated dbs. So, the new code would only apply for the (already buggy) collated code path. If so, it seems justified to me to have the optimization. &lt;/p&gt;</comment>
                            <comment id="13112159" author="mikem" created="Wed, 21 Sep 2011 23:08:46 +0100"  >&lt;p&gt;I have reviewed the most recent patch.  Here are comments, none of which I would say should hold up a commit:&lt;/p&gt;

&lt;p&gt;o I much prefer the current optimization as now it is a compile time decision.  I still would prefer if non-collated db&apos;s did not have to march through&lt;br/&gt;
   all the collumns and calculate if the row has collated columns on every compile.  I might have pushed the work up to language by adding an&lt;br/&gt;
   argument to OpenConglomerateScratchSpace to pass in hasCollatedTypes.  That would make most sense if the calling level has a better&lt;br/&gt;
   understanding of the collation state of the DB and/or table and could avoid the calculation.&lt;/p&gt;

&lt;p&gt;o I believe you said it, but just want to verify that you ran at least one set of tests where all tests went through the proposed collation code path just&lt;br/&gt;
   to make sure it was tested once.  My guess is that the new codepath you are adding is probably not tested at all, without adding some new tests.&lt;br/&gt;
   Seems like you should at least add some version of the test case from the bug report.&lt;/p&gt;

&lt;p&gt;o I think the following is also a bug for same reason as you are working on - but not sure how to force the code path, but happy to have it as a separate JIRA - let me know if I should report it:&lt;br/&gt;
 if (this.getConglomerate().nKeyFields ==&lt;br/&gt;
     this.getConglomerate().nUniqueColumns)&lt;/p&gt;
 {
     // The row that we found deleted is exactly the new ro
     targetleaf.page.deleteAtSlot(
         insert_slot, false, this.btree_undo);

     break;
 }

&lt;p&gt;o longer term I think that we should not be doing the update at all, and instead doing purges.  I think that will solve some of the possible out of space issues.  But I think there are self deadlocking issues there so I am in favor of your approach especially for back porting to existing releases.  &lt;/p&gt;</comment>
                            <comment id="13112604" author="kristwaa" created="Thu, 22 Sep 2011 15:09:55 +0100"  >&lt;p&gt;Mike,&lt;/p&gt;

&lt;p&gt;My replies below, to your comments above.&lt;/p&gt;

&lt;p&gt; o I&apos;ve changed this a bit again, see patch 4b.&lt;br/&gt;
   I now detect if there are columns with other collations than UCS BASIC&lt;br/&gt;
   when the conglomerate is created or opened. By tacking on to&lt;br/&gt;
   ConglomerateUtil.readCollationIdArray this comes almost for free (need&lt;br/&gt;
   one more boolean field in the conglom classes though), and we just have&lt;br/&gt;
   to iterate through the ids once upon creation.&lt;br/&gt;
   Derby always has to do an if on hasCollatedTypes(). This could maybe be&lt;br/&gt;
   made final with some refactoring (not sure if that would matter though).&lt;/p&gt;

&lt;p&gt; o Yes, the new test I wrote, based on the bug report, will be included.&lt;br/&gt;
   It is now part of patch 4b.&lt;br/&gt;
   There are no other tests stressing this piece of code in suites.All.&lt;/p&gt;

&lt;p&gt; o Please report this. It looked suspicious to me the first time I saw it,&lt;br/&gt;
   but I don&apos;t have the knowledge to add any more context.&lt;br/&gt;
   FYI, the test jdbciapi.SURQueryMixTest triggers that piece of code.&lt;br/&gt;
   (SUR = scrollable updatable resultset)&lt;/p&gt;

&lt;p&gt; o Noted. It would be good to log a JIRA for this if not already done.&lt;/p&gt;

&lt;p&gt;Now that it looks like there might be another RC, I&apos;d like to get a fix for&lt;br/&gt;
this issue included. Unfortunately, I&apos;ll be away next week. I&apos;ll commit this&lt;br/&gt;
tomorrow if I don&apos;t get any pushback before then.&lt;br/&gt;
If I commit and things go awry, back out the fix (it will be a single commit only).&lt;/p&gt;</comment>
                            <comment id="13112991" author="kristwaa" created="Thu, 22 Sep 2011 23:53:05 +0100"  >&lt;p&gt;Attaching patch 4c. The only difference from 4b is an extra assert in OpenConglomerateScratchSpace.&lt;/p&gt;</comment>
                            <comment id="13112996" author="kristwaa" created="Thu, 22 Sep 2011 23:55:34 +0100"  >&lt;p&gt;Committed patch 4c to trunk with revision 1174436.&lt;/p&gt;</comment>
                            <comment id="13113232" author="kristwaa" created="Fri, 23 Sep 2011 08:53:40 +0100"  >&lt;p&gt;Tests ran cleanly on 10.8 (with the fixed merged from trunk), with the exception of this error in testConcurrentInvalidation(org.apache.derbyTesting.functionTests.tests.lang.TruncateTableTest):&lt;br/&gt;
Helper thread failed&lt;br/&gt;
...&lt;br/&gt;
Fri Sep 23 01:19:36 CEST 2011 Thread&lt;span class=&quot;error&quot;&gt;&amp;#91;DRDAConnThread_77,5,derby.daemons&amp;#93;&lt;/span&gt; (XID = 24553), (SESSIONID = 31), (DATABASE = dbsqlauth), (DRDAID = &#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;.&#65533;&#65533;-808957766332849398&lt;/p&gt;
{3}
&lt;p&gt;), Failed Statement is: select * from d4275&lt;br/&gt;
ERROR XSAI2: The conglomerate (4,448) requested does not exist.&lt;br/&gt;
        at org.apache.derby.iapi.error.StandardException.newException(StandardException.java:286)&lt;br/&gt;
        at org.apache.derby.impl.store.access.heap.HeapConglomerateFactory.readConglomerate(HeapConglomerateFactory.java:254)&lt;br/&gt;
        at org.apache.derby.impl.store.access.RAMAccessManager.conglomCacheFind(RAMAccessManager.java:482)&lt;br/&gt;
        at org.apache.derby.impl.store.access.RAMTransaction.findExistingConglomerate(RAMTransaction.java:394)&lt;br/&gt;
        at org.apache.derby.impl.store.access.RAMTransaction.getDynamicCompiledConglomInfo(RAMTransaction.java:692)&lt;br/&gt;
        at org.apache.derby.impl.sql.execute.TableScanResultSet.openCore(TableScanResultSet.java:245)&lt;br/&gt;
        at org.apache.derby.impl.sql.execute.BulkTableScanResultSet.openCore(BulkTableScanResultSet.java:248)&lt;br/&gt;
        at org.apache.derby.impl.sql.execute.BasicNoPutResultSetImpl.open(BasicNoPutResultSetImpl.java:255)&lt;br/&gt;
        at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(GenericPreparedStatement.java:436)&lt;br/&gt;
        at org.apache.derby.impl.sql.GenericPreparedStatement.execute(GenericPreparedStatement.java:317)&lt;br/&gt;
        at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(EmbedStatement.java:1242)&lt;br/&gt;
        at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeStatement(EmbedPreparedStatement.java:1686)&lt;br/&gt;
        at org.apache.derby.impl.jdbc.EmbedPreparedStatement.execute(EmbedPreparedStatement.java:1341)&lt;br/&gt;
        at org.apache.derby.impl.drda.DRDAStatement.execute(DRDAStatement.java:706)&lt;br/&gt;
        at org.apache.derby.impl.drda.DRDAConnThread.processCommands(DRDAConnThread.java:866)&lt;br/&gt;
        at org.apache.derby.impl.drda.DRDAConnThread.run(DRDAConnThread.java:295)&lt;br/&gt;
Cleanup action completed&lt;/p&gt;


&lt;p&gt;I plan to backport the fix on Saturday.&lt;/p&gt;</comment>
                            <comment id="13113813" author="mikem" created="Fri, 23 Sep 2011 23:28:24 +0100"  >&lt;p&gt;i reviewed the latest patch and it looks good to me.  I agree that overhead should be basically &quot;free&quot; now with the code working off the&lt;br/&gt;
stored conglomerate info and not having to recalculate it.&lt;/p&gt;

&lt;p&gt;It is great that your test case both tries the newly created conglomerate and shuts down the database and restarts testing the read conglomerate&lt;br/&gt;
from disk case.&lt;/p&gt;
</comment>
                            <comment id="13113985" author="kristwaa" created="Sat, 24 Sep 2011 15:24:26 +0100"  >&lt;p&gt;Committed to 10.8 with revision 1175171 (the fix went in with revision 1174436 on trunk).&lt;/p&gt;

&lt;p&gt;I don&apos;t expect more work on this issue now, but there seems to be related work in this area of Derby.&lt;/p&gt;</comment>
                            <comment id="13685174" author="knutanders" created="Mon, 17 Jun 2013 10:19:15 +0100"  >&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;bulk update&amp;#93;&lt;/span&gt; Close all resolved issues that haven&apos;t been updated for more than one year.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12490246" name="derby-5367-1a-update_row_fully.diff" size="4512" author="kristwaa" created="Fri, 12 Aug 2011 14:56:59 +0100"/>
                            <attachment id="12490427" name="derby-5367-1b-update_row_fully.diff" size="8023" author="kristwaa" created="Mon, 15 Aug 2011 14:14:24 +0100"/>
                            <attachment id="12490428" name="derby-5367-1b-update_row_fully.stat" size="307" author="kristwaa" created="Mon, 15 Aug 2011 14:14:24 +0100"/>
                            <attachment id="12492647" name="derby-5367-2a-minimal_fix.diff" size="1796" author="kristwaa" created="Thu, 1 Sep 2011 19:55:49 +0100"/>
                            <attachment id="12493805" name="derby-5367-3a-update_field_by_field_preview.diff" size="1320" author="kristwaa" created="Fri, 9 Sep 2011 17:39:52 +0100"/>
                            <attachment id="12494109" name="derby-5367-4a-fix_with_optimization_improved.diff" size="5557" author="kristwaa" created="Mon, 12 Sep 2011 21:14:07 +0100"/>
                            <attachment id="12496117" name="derby-5367-4b-fix_with_optimization_improved.diff" size="21882" author="kristwaa" created="Thu, 22 Sep 2011 15:09:55 +0100"/>
                            <attachment id="12496193" name="derby-5367-4c-fix_with_optimization_improved.diff" size="22291" author="kristwaa" created="Thu, 22 Sep 2011 23:53:05 +0100"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>8.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310200" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Bug behavior facts</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10364"><![CDATA[Data corruption]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 11 Aug 2011 08:24:17 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>24799</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hy0f9b:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>36290</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                            </customfields>
    </item>
</channel>
</rss>