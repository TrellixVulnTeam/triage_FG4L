<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 03:52:50 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/DERBY-2254/DERBY-2254.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[DERBY-2254] Assert during log file switch: log file position exceeded max log file size</title>
                <link>https://issues.apache.org/jira/browse/DERBY-2254</link>
                <project id="10594" key="DERBY">Derby</project>
                    <description>&lt;p&gt;When running simple tpc-b like transactions against a embedded Derby based on a SANE build of trunk the following assertion occurs for the background thread and all user threads:&lt;/p&gt;

&lt;p&gt;   org.apache.derby.shared.common.sanity.AssertFailure: ASSERT FAILED log file position exceeded max log file size&lt;/p&gt;

&lt;p&gt;This seems to occur during a switch to a new log file.&lt;/p&gt;

&lt;p&gt;derby.log contains the following call stack for the background thread:&lt;/p&gt;

&lt;p&gt;Exception trace: &lt;br/&gt;
org.apache.derby.shared.common.sanity.AssertFailure: ASSERT FAILED log file position exceeded max log file size&lt;br/&gt;
	at org.apache.derby.shared.common.sanity.SanityManager.ASSERT(SanityManager.java:120)&lt;br/&gt;
	at org.apache.derby.impl.store.raw.log.LogCounter.makeLogInstantAsLong(LogCounter.java:120)&lt;br/&gt;
	at org.apache.derby.impl.store.raw.log.LogToFile.switchLogFile(LogToFile.java:1900)&lt;br/&gt;
	at org.apache.derby.impl.store.raw.log.LogToFile.appendLogRecord(LogToFile.java:3530)&lt;br/&gt;
	at org.apache.derby.impl.store.raw.log.FileLogger.logAndDo(FileLogger.java:345)&lt;br/&gt;
	at org.apache.derby.impl.store.raw.xact.Xact.logAndDo(Xact.java:1185)&lt;br/&gt;
	at org.apache.derby.impl.store.raw.log.LogToFile.checkpointWithTran(LogToFile.java:1540)&lt;br/&gt;
	at org.apache.derby.impl.store.raw.log.LogToFile.checkpoint(LogToFile.java:1357)&lt;br/&gt;
	at org.apache.derby.impl.store.raw.RawStore.checkpoint(RawStore.java:439)&lt;br/&gt;
	at org.apache.derby.impl.store.raw.log.LogToFile.performWork(LogToFile.java:3416)&lt;br/&gt;
	at org.apache.derby.impl.services.daemon.BasicDaemon.serviceClient(BasicDaemon.java:331)&lt;br/&gt;
	at org.apache.derby.impl.services.daemon.BasicDaemon.work(BasicDaemon.java:668)&lt;br/&gt;
	at org.apache.derby.impl.services.daemon.BasicDaemon.run(BasicDaemon.java:394)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:619)&lt;br/&gt;
2007-01-17 23:09:48.638 GMT Thread&lt;span class=&quot;error&quot;&gt;&amp;#91;derby.rawStoreDaemon,5,derby.daemons&amp;#93;&lt;/span&gt; Cleanup action starting&lt;br/&gt;
org.apache.derby.shared.common.sanity.AssertFailure: ASSERT FAILED log file position exceeded max log file size&lt;br/&gt;
	at org.apache.derby.shared.common.sanity.SanityManager.ASSERT(SanityManager.java:120)&lt;br/&gt;
	at org.apache.derby.impl.store.raw.log.LogCounter.makeLogInstantAsLong(LogCounter.java:120)&lt;br/&gt;
	at org.apache.derby.impl.store.raw.log.LogToFile.switchLogFile(LogToFile.java:1900)&lt;br/&gt;
	at org.apache.derby.impl.store.raw.log.LogToFile.appendLogRecord(LogToFile.java:3530)&lt;br/&gt;
	at org.apache.derby.impl.store.raw.log.FileLogger.logAndDo(FileLogger.java:345)&lt;br/&gt;
	at org.apache.derby.impl.store.raw.xact.Xact.logAndDo(Xact.java:1185)&lt;br/&gt;
	at org.apache.derby.impl.store.raw.log.LogToFile.checkpointWithTran(LogToFile.java:1540)&lt;br/&gt;
	at org.apache.derby.impl.store.raw.log.LogToFile.checkpoint(LogToFile.java:1357)&lt;br/&gt;
	at org.apache.derby.impl.store.raw.RawStore.checkpoint(RawStore.java:439)&lt;br/&gt;
	at org.apache.derby.impl.store.raw.log.LogToFile.performWork(LogToFile.java:3416)&lt;br/&gt;
	at org.apache.derby.impl.services.daemon.BasicDaemon.serviceClient(BasicDaemon.java:331)&lt;br/&gt;
	at org.apache.derby.impl.services.daemon.BasicDaemon.work(BasicDaemon.java:668)&lt;br/&gt;
	at org.apache.derby.impl.services.daemon.BasicDaemon.run(BasicDaemon.java:394)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:619)&lt;br/&gt;
Cleanup action completed&lt;/p&gt;

&lt;p&gt;For my user threads the call stack is similar:&lt;/p&gt;

&lt;p&gt;Database Class Loader started - derby.database.classpath=&apos;&apos;&lt;br/&gt;
2007-01-17 23:09:36.401 GMT Thread&lt;span class=&quot;error&quot;&gt;&amp;#91;Thread-51,5,main&amp;#93;&lt;/span&gt; (XID = 12632406), (SESSIONID = 51), (DATABASE = /export/home/tmp/derby-db), (DRDAID = null), Cleanup action starting&lt;br/&gt;
2007-01-17 23:09:36.401 GMT Thread&lt;span class=&quot;error&quot;&gt;&amp;#91;Thread-51,5,main&amp;#93;&lt;/span&gt; (XID = 12632406), (SESSIONID = 51), (DATABASE = /export/home/tmp/derby-db), (DRDAID = null), Failed Statement is: UPDATE accounts SET abal = abal + ? WHERE aid = ? AND bid = ?&lt;br/&gt;
org.apache.derby.shared.common.sanity.AssertFailure: ASSERT FAILED log file position exceeded max log file size&lt;br/&gt;
	at org.apache.derby.shared.common.sanity.SanityManager.ASSERT(SanityManager.java:120)&lt;br/&gt;
	at org.apache.derby.impl.store.raw.log.LogCounter.makeLogInstantAsLong(LogCounter.java:120)&lt;br/&gt;
	at org.apache.derby.impl.store.raw.log.LogToFile.switchLogFile(LogToFile.java:1900)&lt;br/&gt;
	at org.apache.derby.impl.store.raw.log.LogToFile.appendLogRecord(LogToFile.java:3530)&lt;br/&gt;
	at org.apache.derby.impl.store.raw.log.FileLogger.logAndDo(FileLogger.java:345)&lt;br/&gt;
	at org.apache.derby.impl.store.raw.xact.Xact.logAndDo(Xact.java:1185)&lt;br/&gt;
	at org.apache.derby.impl.store.raw.data.LoggableActions.doAction(LoggableActions.java:221)&lt;br/&gt;
	at org.apache.derby.impl.store.raw.data.LoggableActions.actionUpdate(LoggableActions.java:85)&lt;br/&gt;
	at org.apache.derby.impl.store.raw.data.StoredPage.doUpdateAtSlot(StoredPage.java:8521)&lt;br/&gt;
	at org.apache.derby.impl.store.raw.data.BasePage.updateAtSlot(BasePage.java:1108)&lt;br/&gt;
	at org.apache.derby.impl.store.access.conglomerate.GenericConglomerateController.replace(GenericConglomerateController.java:479)&lt;br/&gt;
	at org.apache.derby.impl.sql.execute.RowChangerImpl.updateRow(RowChangerImpl.java:523)&lt;br/&gt;
	at org.apache.derby.impl.sql.execute.UpdateResultSet.collectAffectedRows(UpdateResultSet.java:566)&lt;br/&gt;
	at org.apache.derby.impl.sql.execute.UpdateResultSet.open(UpdateResultSet.java:260)&lt;br/&gt;
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(GenericPreparedStatement.java:358)&lt;br/&gt;
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(EmbedStatement.java:1182)&lt;br/&gt;
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeStatement(EmbedPreparedStatement.java:1652)&lt;br/&gt;
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeUpdate(EmbedPreparedStatement.java:299)&lt;br/&gt;
	at com.sun.derby.perf.clients.tpcb.DBConnection.performTransaction(DBConnection.java:595)&lt;br/&gt;
	at com.sun.derby.perf.clients.tpcb.Client.run(Client.java:218)&lt;/p&gt;

&lt;p&gt;After this it seems like no user threads are able to connect to the database (not surpricing).&lt;/p&gt;

&lt;p&gt;This happend using a SANE build and I am unsure by just looking at the assertion in the code whether this situation would have been fatal also if I had been using an INSANE build.&lt;/p&gt;</description>
                <environment>Solaris 10, Java SE 6 build 104 </environment>
        <key id="12360809">DERBY-2254</key>
            <summary>Assert during log file switch: log file position exceeded max log file size</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="rhillegas">Rick Hillegas</assignee>
                                    <reporter username="olav">Olav Sandstaa</reporter>
                        <labels>
                            <label>derby_triage10_5_2</label>
                    </labels>
                <created>Thu, 18 Jan 2007 20:21:18 +0000</created>
                <updated>Mon, 17 Jun 2013 10:19:20 +0100</updated>
                            <resolved>Wed, 21 Mar 2012 00:03:36 +0000</resolved>
                                    <version>10.5.3.0</version>
                                    <fixVersion>10.5.3.2</fixVersion>
                    <fixVersion>10.6.2.4</fixVersion>
                    <fixVersion>10.7.1.4</fixVersion>
                    <fixVersion>10.8.2.2</fixVersion>
                    <fixVersion>10.9.1.0</fixVersion>
                                    <component>Store</component>
                        <due></due>
                            <votes>2</votes>
                                    <watches>5</watches>
                                                                <comments>
                            <comment id="12465881" author="olav" created="Thu, 18 Jan 2007 21:51:08 +0000"  >&lt;p&gt;The crash happened when running simple transactions (3 updates, 1&lt;br/&gt;
insert, 1 select) against a database with 10 GB user data (total data&lt;br/&gt;
volume 17 GB). Derby was configured with a 18 GB database buffer. The&lt;br/&gt;
crash occured about 2 hours after re-starting an existing&lt;br/&gt;
database. The load consisted of 64 clients running transactions&lt;br/&gt;
back-to-back. Since the database was &quot;just&quot; started most of the&lt;br/&gt;
transactions had to read data from the disk, thus there were about 60&lt;br/&gt;
concurrent read requests for getting data into the database buffer. At&lt;br/&gt;
the same time the single back-ground thread was having a hard time to&lt;br/&gt;
keep up writing out dirty data to the disk. At this time there were&lt;br/&gt;
plenty of free room in the page cache so no data blocks were &quot;evicted&quot;&lt;br/&gt;
out to disk by the user threads.&lt;/p&gt;

&lt;p&gt;Since the background thread were not able to flush data pages fast&lt;br/&gt;
enough it was also unable to reclaim log files. So the amount of log&lt;br/&gt;
kept growing. &lt;/p&gt;

&lt;p&gt;One thing I do not understand is that most of the log files have their&lt;br/&gt;
normal size while some are very close to the max log file size. Here&lt;br/&gt;
is a list of the files in the log directory after the crash:&lt;/p&gt;


&lt;p&gt;&lt;del&gt;rw-r&lt;/del&gt;----   1 olav       48 Jan 17 22:09 log.ctrl&lt;br/&gt;
&lt;del&gt;rw-r&lt;/del&gt;----   1 olav       48 Jan 17 22:09 logmirror.ctrl&lt;br/&gt;
&lt;del&gt;rw-r&lt;/del&gt;----   1 olav  1060411 Jan 17 22:09 log1181.dat&lt;br/&gt;
&lt;del&gt;rw-r&lt;/del&gt;----   1 olav  1067038 Jan 17 22:09 log1182.dat&lt;br/&gt;
&lt;del&gt;rw-r&lt;/del&gt;----   1 olav  1049488 Jan 17 22:09 log1183.dat&lt;br/&gt;
&lt;del&gt;rw-r&lt;/del&gt;----   1 olav  1050176 Jan 17 22:09 log1184.dat&lt;br/&gt;
&lt;del&gt;rw-r&lt;/del&gt;----   1 olav  1050758 Jan 17 22:10 log1185.dat&lt;br/&gt;
&lt;del&gt;rw-r&lt;/del&gt;----   1 olav  1050583 Jan 17 22:10 log1186.dat&lt;br/&gt;
&lt;del&gt;rw-r&lt;/del&gt;----   1 olav  1050178 Jan 17 22:10 log1187.dat&lt;br/&gt;
&lt;del&gt;rw-r&lt;/del&gt;----   1 olav  1054465 Jan 17 22:10 log1188.dat&lt;br/&gt;
&lt;del&gt;rw-r&lt;/del&gt;----   1 olav  1051730 Jan 17 22:11 log1189.dat&lt;br/&gt;
&lt;del&gt;rw-r&lt;/del&gt;----   1 olav  1052601 Jan 17 22:11 log1190.dat&lt;br/&gt;
&lt;del&gt;rw-r&lt;/del&gt;----   1 olav  268435436 Jan 17 22:55 log1191.dat&lt;br/&gt;
&lt;del&gt;rw-r&lt;/del&gt;----   1 olav  1048576 Jan 17 22:55 log1192.dat&lt;br/&gt;
&lt;del&gt;rw-r&lt;/del&gt;----   1 olav  1048576 Jan 17 22:55 log1193.dat&lt;br/&gt;
&lt;del&gt;rw-r&lt;/del&gt;----   1 olav  1048576 Jan 17 22:55 log1194.dat&lt;br/&gt;
&lt;del&gt;rw-r&lt;/del&gt;----   1 olav  1048576 Jan 17 22:55 log1195.dat&lt;br/&gt;
&lt;del&gt;rw-r&lt;/del&gt;----   1 olav  1048576 Jan 17 22:55 log1196.dat&lt;br/&gt;
&lt;del&gt;rw-r&lt;/del&gt;----   1 olav  1048576 Jan 17 22:55 log1197.dat&lt;br/&gt;
&lt;del&gt;rw-r&lt;/del&gt;----   1 olav  268435434 Jan 17 23:23 log1198.dat&lt;br/&gt;
&lt;del&gt;rw-r&lt;/del&gt;----   1 olav  1048576 Jan 17 23:23 log1199.dat&lt;br/&gt;
&lt;del&gt;rw-r&lt;/del&gt;----   1 olav  1048576 Jan 17 23:23 log1200.dat&lt;br/&gt;
&lt;del&gt;rw-r&lt;/del&gt;----   1 olav  1048576 Jan 17 23:23 log1201.dat&lt;br/&gt;
&lt;del&gt;rw-r&lt;/del&gt;----   1 olav  1048576 Jan 17 23:23 log1202.dat&lt;br/&gt;
&lt;del&gt;rw-r&lt;/del&gt;----   1 olav  268435429 Jan 17 23:46 log1203.dat&lt;br/&gt;
&lt;del&gt;rw-r&lt;/del&gt;----   1 olav  1048576 Jan 17 23:46 log1204.dat&lt;br/&gt;
&lt;del&gt;rw-r&lt;/del&gt;----   1 olav  1048576 Jan 17 23:46 log1205.dat&lt;br/&gt;
&lt;del&gt;rw-r&lt;/del&gt;----   1 olav  1048576 Jan 17 23:46 log1206.dat&lt;br/&gt;
&lt;del&gt;rw-r&lt;/del&gt;----   1 olav  1048576 Jan 17 23:46 log1207.dat&lt;br/&gt;
&lt;del&gt;rw-r&lt;/del&gt;----   1 olav  1048576 Jan 17 23:46 log1208.dat&lt;br/&gt;
&lt;del&gt;rw-r&lt;/del&gt;----   1 olav  1048576 Jan 17 23:46 log1209.dat&lt;br/&gt;
&lt;del&gt;rw-r&lt;/del&gt;----   1 olav  1048576 Jan 17 23:46 log1210.dat&lt;br/&gt;
&lt;del&gt;rw-r&lt;/del&gt;----   1 olav  268435456 Jan 18 00:09 log1211.dat&lt;br/&gt;
&lt;del&gt;rw-r&lt;/del&gt;----   1 olav        0 Jan 18 00:09 log1212.dat&lt;/p&gt;


&lt;p&gt;In org.apache.derby.impl.store.raw.log.LogCounter the following constant is defined&lt;/p&gt;

&lt;p&gt;	// reserve top 4 bits in log file size for future use&lt;br/&gt;
	public static final long MAX_LOGFILE_SIZE	    = (long)0x0FFFFFFFL; // 268435455&lt;/p&gt;

&lt;p&gt;and the assert is defined as:&lt;/p&gt;

&lt;p&gt;        SanityManager.ASSERT(filepos &amp;lt; MAX_LOGFILE_SIZE,&lt;br/&gt;
                             &quot;log file position exceeded max log file size&quot;);&lt;/p&gt;

&lt;p&gt;Comparing the constant MAX_LOGFILE_SIZE (268435455) with the length of&lt;br/&gt;
the second last file above (268435456) it actually seems like this log&lt;br/&gt;
file has become a byte too long.&lt;/p&gt;</comment>
                            <comment id="12465896" author="mikem" created="Thu, 18 Jan 2007 22:35:49 +0000"  >&lt;p&gt;This is very interesting, you are definitely delving into areas never before tested.  I don&apos;t think we have ever even had a machine with more than 2GB of memory.  18gb is definitely going to stress the system.  &lt;/p&gt;

&lt;p&gt;Here the issues I think you are running into, it may be better to get them into separate issues for tracking:&lt;br/&gt;
1) my best guess is that the act of switching the log file at 1 gig has become stuck behind finishing the checkpoint.  I don&apos;t &lt;br/&gt;
     believe there is a design issue that we have to wait for checkpoint to switch.  My first guess is that out single background&lt;br/&gt;
     daemon is stuck doing  a checkpoint and can&apos;t service the switch request - but I have not checked the code.   For these&lt;br/&gt;
     multi-processor systems a single daemon not doing anything async is probably not going to cut it.&lt;br/&gt;
2) The math for largest file number looks like it has a bug somewhere.  I don&apos;t think we ever really expected a file to grow to&lt;br/&gt;
      this size, mostly it was there to make sure someone didn&apos;t set the log file size to greater than it.   A test should be written&lt;br/&gt;
      and the code fixed to properly handle forcing a switch, or waiting for a switch if you get to this size.   My guess is this has&lt;br/&gt;
      not been tested before.  I am not sure how easy such a test will be, other than a stress test like yours - though the real&lt;br/&gt;
       fix is probably to fix 1 and avoid 2 all together.&lt;br/&gt;
3) ASSERT messages weren&apos;t very helpful, I&apos;ll update them so at least we get the real info out  when you run into this.&lt;/p&gt;</comment>
                            <comment id="12465897" author="mikem" created="Thu, 18 Jan 2007 22:37:40 +0000"  >&lt;p&gt;Also wanted to note that this max limit is really a hard limit as the log address can only properly address files in this range.  Once you go over that size we can&apos;t properly address records in that file.&lt;/p&gt;</comment>
                            <comment id="12465900" author="mikem" created="Thu, 18 Jan 2007 22:40:56 +0000"  >&lt;p&gt;It would also be good from a performance standpoint to minimize the amount of data written to a log file over it&apos;s preallocated size, as on some systems those writes cost twice as much as writes to  the preallocated size.  &lt;/p&gt;</comment>
                            <comment id="12466475" author="olav" created="Mon, 22 Jan 2007 15:10:49 +0000"  >&lt;p&gt;Mike, thank you for commenting on this and for explaining what you&lt;br/&gt;
think are the issues behind this problem. I agree that the best&lt;br/&gt;
solution seems be to fix this so that the background thread can do&lt;br/&gt;
log file switching without waiting for the checkpoing to complete.&lt;/p&gt;

&lt;p&gt;Some questions somebody maybe know the answer to:&lt;/p&gt;

&lt;p&gt;  -What is the reason for most of the log files having the default&lt;br/&gt;
   size of 1 MB but a few (4) log files grow to 268 kB before&lt;br/&gt;
   switching?&lt;/p&gt;

&lt;p&gt;  -The crash occurs after having produced about 1 GB of log. Is there&lt;br/&gt;
   a hard limit on 1 GB before the checkpoint must be completed?&lt;/p&gt;

&lt;p&gt;This crash happened during some experimental testing where I want to&lt;br/&gt;
study data rates to and from the disks during high load. I can do this&lt;br/&gt;
with a much smaller database buffer so being able to run with a 18 GB&lt;br/&gt;
database buffer is not an urgent issue for me right now.&lt;/p&gt;</comment>
                            <comment id="12466490" author="olav" created="Mon, 22 Jan 2007 15:54:57 +0000"  >&lt;p&gt;Just for the record: I tried to restart the database to see if it was able to recover. Not surpricingly, after about three hours into the recovery, it  was aborted with the same assert and the following call stack:&lt;/p&gt;

&lt;p&gt;Exception trace:&lt;br/&gt;
org.apache.derby.shared.common.sanity.AssertFailure: ASSERT FAILED log file position exceeded max log file size&lt;br/&gt;
        at org.apache.derby.shared.common.sanity.SanityManager.ASSERT(SanityManager.java:120)&lt;br/&gt;
        at org.apache.derby.impl.store.raw.log.LogCounter.makeLogInstantAsLong(LogCounter.java:120)&lt;br/&gt;
        at org.apache.derby.impl.store.raw.log.Scan.getNextRecordForward(Scan.java:973)&lt;br/&gt;
        at org.apache.derby.impl.store.raw.log.Scan.getNextRecord(Scan.java:206)        at org.apache.derby.impl.store.raw.log.FileLogger.redo(FileLogger.java:1176)&lt;br/&gt;
        at org.apache.derby.impl.store.raw.log.LogToFile.recover(LogToFile.java:811)&lt;br/&gt;
        at org.apache.derby.impl.store.raw.RawStore.boot(RawStore.java:308)&lt;br/&gt;
        at org.apache.derby.impl.services.monitor.BaseMonitor.boot(BaseMonitor.java:1994)&lt;br/&gt;
        at org.apache.derby.impl.services.monitor.TopService.bootModule(TopService.java:291)&lt;br/&gt;
        at org.apache.derby.impl.services.monitor.BaseMonitor.startModule(BaseMonitor.java:546)&lt;br/&gt;
        at org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Monitor.java:419)&lt;br/&gt;
        at org.apache.derby.impl.store.access.RAMAccessManager.boot(RAMAccessManager.java:988)&lt;br/&gt;
        at org.apache.derby.impl.services.monitor.BaseMonitor.boot(BaseMonitor.java:1994)&lt;br/&gt;
        at org.apache.derby.impl.services.monitor.TopService.bootModule(TopService.java:291)&lt;br/&gt;
        at org.apache.derby.impl.services.monitor.BaseMonitor.startModule(BaseMonitor.java:546)&lt;br/&gt;
        at org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Monitor.java:419)&lt;br/&gt;
        at org.apache.derby.impl.db.BasicDatabase.bootStore(BasicDatabase.java:746)&lt;br/&gt;
        at org.apache.derby.impl.db.BasicDatabase.boot(BasicDatabase.java:182)&lt;br/&gt;
        at org.apache.derby.impl.services.monitor.BaseMonitor.boot(BaseMonitor.java:1994)&lt;br/&gt;
        at org.apache.derby.impl.services.monitor.TopService.bootModule(TopService.java:291)&lt;br/&gt;
        at org.apache.derby.impl.services.monitor.BaseMonitor.bootService(BaseMonitor.java:1829)&lt;br/&gt;
        at org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(BaseMonitor.java:1695)&lt;br/&gt;
        at org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(BaseMonitor.java:1575)&lt;br/&gt;
        at org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(BaseMonitor.java:994)&lt;br/&gt;
        at org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Monitor.java:542)&lt;br/&gt;
        at org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(EmbedConnection.java:1633)&lt;br/&gt;
        at org.apache.derby.impl.jdbc.EmbedConnection.&amp;lt;init&amp;gt;(EmbedConnection.java:223)&lt;br/&gt;
        at org.apache.derby.impl.jdbc.EmbedConnection30.&amp;lt;init&amp;gt;(EmbedConnection30.java:73)&lt;br/&gt;
        at org.apache.derby.impl.jdbc.EmbedConnection40.&amp;lt;init&amp;gt;(EmbedConnection40.java:54)&lt;br/&gt;
        at org.apache.derby.jdbc.Driver40.getNewEmbedConnection(Driver40.java:64)&lt;br/&gt;
        at org.apache.derby.jdbc.InternalDriver.connect(InternalDriver.java:210)&lt;br/&gt;
        at org.apache.derby.jdbc.AutoloadedDriver.connect(AutoloadedDriver.java:117)&lt;br/&gt;
        at java.sql.DriverManager.getConnection(DriverManager.java:582)&lt;br/&gt;
        at java.sql.DriverManager.getConnection(DriverManager.java:185)&lt;br/&gt;
        at com.sun.derby.perf.clients.tpcb.DBConnection.loadDriver(DBConnection.java:140)&lt;br/&gt;
        at com.sun.derby.perf.clients.tpcb.DBConnection.&amp;lt;init&amp;gt;(DBConnection.java:49)&lt;br/&gt;
        at com.sun.derby.perf.clients.tpcb.TPCBClient.run(TPCBClient.java:132)&lt;br/&gt;
        at com.sun.derby.perf.clients.tpcb.TPCBClient.main(TPCBClient.java:381)&lt;/p&gt;</comment>
                            <comment id="12555737" author="quartz12h" created="Fri, 4 Jan 2008 00:21:33 +0000"  >&lt;p&gt;A few details:&lt;br/&gt;
-I have this state generated from a linux machine (not OS specific).&lt;br/&gt;
-I use the derbynet NetworkServerControl, not the embeded driver.&lt;br/&gt;
-I also saw the error while starting up on windows with embeded driver over the copied DB.&lt;br/&gt;
-I am on 10.2.2.0.&lt;/p&gt;

&lt;p&gt;I deleted the log file and the DB works. Of course I expect data loss but&lt;br/&gt;
I wanted to know that only the log file seems affected.&lt;/p&gt;</comment>
                            <comment id="12556731" author="mikem" created="Mon, 7 Jan 2008 22:03:45 +0000"  >&lt;p&gt;Deleting the log file is the easiest way to guarantee corruption in the derby database.  It is impossible to know what kind of &lt;br/&gt;
corruption might have happened but all the following are possible, because derby rely&apos;s on the info in the log file to maintain&lt;br/&gt;
consistent data.  Because of in memory database caching the database files on disk may be severely out of date/inconsistent without proper recovery processing using the log files.  All the following corruptions are possible when one deletes one or more&lt;br/&gt;
log files:&lt;br/&gt;
transaction inconsistent data between tables - many of these inconsistencies can&apos;t be determined by derby&lt;br/&gt;
inconsistent data between base tables and indexes - running the consistency checker does a good job of checking for this&lt;br/&gt;
inconsistent data in long rows - a long row may stretch across many pages and the links depend on tranactional consistency if this&lt;br/&gt;
     is broken a number of runtime errors may occur&lt;br/&gt;
inconsistent data in long columns - similar to long row a single column may be linked across many pages and this consistency is&lt;br/&gt;
    maintained by the transaction log.&lt;/p&gt;

&lt;p&gt;If it is ever necessary to remove the log because of some derby bug the only safe thing to do is to go back to your more recent &lt;br/&gt;
backup.  If you don&apos;t have one, what I usually tell users is to assume the db they are accessing is corrupt but accessible.  The only&lt;br/&gt;
safe thing at that point is to data mine the existing db to copy whatever data looks good out to a file and then load it into a newly&lt;br/&gt;
created db.  Otherwise one will never know if future errors you encounter in the db are because of this failed recovery.&lt;/p&gt;</comment>
                            <comment id="12556927" author="quartz12h" created="Tue, 8 Jan 2008 16:14:07 +0000"  >&lt;p&gt;Because  stored procedure call SYSCS_UTIL.SYSCS_COMPRESS_TABLE(.....)&lt;br/&gt;
cannot run concurrently with inserts (see #&lt;a href=&quot;https://issues.apache.org/jira/browse/DERBY-1068&quot; title=&quot;change of store contract: online compress operations should not share any locks with user transactions&quot; class=&quot;issue-link&quot; data-issue-key=&quot;DERBY-1068&quot;&gt;DERBY-1068&lt;/a&gt;)&lt;br/&gt;
our log file grows like crazy. I suppose either inserts goes really slow, or to a halt and backlog in the log file.&lt;br/&gt;
Eventually, it crashes. Dead for good. Irrecoverable.&lt;br/&gt;
We can&apos;t wait for a fix...&lt;/p&gt;</comment>
                            <comment id="12728114" author="knutanders" created="Tue, 7 Jul 2009 15:08:38 +0100"  >&lt;p&gt;Triaged for 10.5.2.&lt;/p&gt;</comment>
                            <comment id="12745751" author="trejkaz" created="Fri, 21 Aug 2009 01:49:55 +0100"  >&lt;p&gt;We have one user experiencing this issue in production.  We don&apos;t do any crazy stuff like compressing tables or checkpointing, but we do add lots of rows quickly and I don&apos;t know, maybe once the computer gets sufficiently fast, this issue surfaces.&lt;/p&gt;

&lt;p&gt;The machine our user are using is a monster with 32GB of RAM, although we allocate less than 2GB for each individual process which each have their own database.&lt;/p&gt;

&lt;p&gt;What is the underlying cause here?  I/O not being fast enough to update the database so it has to write to the log instead?  Is there any way to avoid it at all?&lt;/p&gt;</comment>
                            <comment id="13014798" author="rhillegas" created="Fri, 1 Apr 2011 20:18:48 +0100"  >&lt;p&gt;Re-categorizing this as a data corruption since the workaround of deleting the log file is scary.&lt;/p&gt;</comment>
                            <comment id="13025793" author="rhillegas" created="Wed, 27 Apr 2011 14:57:37 +0100"  >&lt;p&gt;Linking to &lt;a href=&quot;https://issues.apache.org/jira/browse/DERBY-2286&quot; title=&quot;Exception NoSpaceOnPage does not provide SQLState or exception text&quot; class=&quot;issue-link&quot; data-issue-key=&quot;DERBY-2286&quot;&gt;&lt;del&gt;DERBY-2286&lt;/del&gt;&lt;/a&gt;, which has a repro program which can be used to trigger this bug.&lt;/p&gt;</comment>
                            <comment id="13031330" author="rhillegas" created="Tue, 10 May 2011 20:37:42 +0100"  >&lt;p&gt;Attaching a repro which Knut created. This consists of the repro attached to &lt;a href=&quot;https://issues.apache.org/jira/browse/DERBY-2286&quot; title=&quot;Exception NoSpaceOnPage does not provide SQLState or exception text&quot; class=&quot;issue-link&quot; data-issue-key=&quot;DERBY-2286&quot;&gt;&lt;del&gt;DERBY-2286&lt;/del&gt;&lt;/a&gt;, which Knut discovered pops this bug if run against an in-memory database often enough. This repro creates an in-memory database and runs 4 update-intensive threads against it. If the bug doesn&apos;t occur within 30 seconds, the experiment is repeated. You may need to edit the run script to customize the location of your Java 6 jdk and your derby.jar.&lt;/p&gt;

&lt;p&gt;To run the repro, just unzip it and type&lt;/p&gt;

&lt;p&gt;./run&lt;/p&gt;

&lt;p&gt;Running this on a Solaris machine with a lot of processors, I am able to reproduce this bug sometimes after a couple iterations, usually in under 200 iterations, and always in under 300 iterations.&lt;/p&gt;</comment>
                            <comment id="13031690" author="rhillegas" created="Wed, 11 May 2011 14:22:55 +0100"  >&lt;p&gt;Attaching derby-2254-01-ab-accountForMoreOverhead.diff, a candidate fix for this bug. After applying this patch, I can&apos;t reproduce the bug using the attached repro. I have run the repro for more than 1500 iterations.&lt;/p&gt;

&lt;p&gt;This is my understanding of what is going on here:&lt;/p&gt;

&lt;p&gt;A) The original assertion is supposed to check that you are not creating an illegal log instant. A log instant consists of a log file number and an offset into that file. A log instant is illegal if the file number or offset are too big. In this particular case, the assertion fails because the offset is too big.&lt;/p&gt;

&lt;p&gt;B) The offset is the actual end of the current log file. That is, the log file has already spilled past its legal end.&lt;/p&gt;

&lt;p&gt;C) The assertion is triggered after the log writer notices that the log file needs to be switched. The log writer ought to detect this condition and switch the log file BEFORE writing a log record which spills over the end. Unfortunately, the log writer only notices this condition AFTER a log record has been written which straddles the legal end of file.&lt;/p&gt;

&lt;p&gt;D) We check for whether we should switch by asking whether the remaining bytes we need to write will spill past the legal end of the file. To compute the number of bytes we expect to write, we add the length of the log wrapper overhead to the length of the current log record. However, it turns out that may need to write more bytes than that. The calculation does not include an extra four bytes needed to write a terminating 0 to the log. The calculation also does not include the bytes needed to write a checksum record.&lt;/p&gt;

&lt;p&gt;E) If we are already very close to the legal end of the log file, then we could spill past the end after writing the wrapper, the log record, the terminating 0, and the checksum record. I suppose that we get here if the checkpointing thread can&apos;t get in often enough to switch log files before we encroach on the end of the current file.&lt;/p&gt;

&lt;p&gt;F) The fix is to require that the log accomodate that extra overhead (the terminating 0 and the checksum record). Now we force a log switch if there is not enough room to write the current log record, its wrapper, the terminating 0, and the checksum record.&lt;/p&gt;


&lt;p&gt;I have made 3 changes:&lt;/p&gt;

&lt;p&gt;1) The actual change to LogToFile to increase the size of the overhead space we require to avoid a log switch.&lt;/p&gt;

&lt;p&gt;2) All of the places in LogToFile which update the remembered endPostion have been reworked to call a brief method which sanity checks whether we have spilled past the end of the file already. I put in this instrumentation in order to find where the extra bytes were being written. I think it is a useful sanity check but I can remove it if people think that it will slow down log processing significantly.&lt;/p&gt;

&lt;p&gt;3) I added a little more information to the original assertion and its friends in LogCounter.&lt;/p&gt;


&lt;p&gt;Touches the following files:&lt;/p&gt;

&lt;p&gt;----------&lt;/p&gt;

&lt;p&gt;M      java/engine/org/apache/derby/impl/store/raw/log/LogAccessFile.java&lt;/p&gt;

&lt;p&gt;Exposed the length of the checksum record so that LogToFile can accurately calculate its overhead.&lt;/p&gt;

&lt;p&gt;----------&lt;/p&gt;

&lt;p&gt;M      java/engine/org/apache/derby/impl/store/raw/log/LogCounter.java&lt;/p&gt;

&lt;p&gt;Items (1) and (2) as described above.&lt;/p&gt;

&lt;p&gt;----------&lt;/p&gt;

&lt;p&gt;M      java/engine/org/apache/derby/impl/store/raw/log/LogToFile.java&lt;/p&gt;

&lt;p&gt;Item (3) above.&lt;/p&gt;</comment>
                            <comment id="13031875" author="dagw" created="Wed, 11 May 2011 18:32:23 +0100"  >&lt;p&gt;Great catch, Rick! Look safe to me, in that the added code could only make the log switch happen earlier. I believe its probably fine to keep the procedure for setting the end position, but I&apos;d suggest running some performance tests so we&apos;d know how much the impact is. Found this double assignment, which is probably a typo (harmless):&lt;/p&gt;

&lt;p&gt;&amp;gt; setEndPosition( endPosition = newLog.getFilePointer() );&lt;/p&gt;
</comment>
                            <comment id="13031987" author="rhillegas" created="Wed, 11 May 2011 20:31:49 +0100"  >&lt;p&gt;Thanks for the quick review, Dag. Attaching derby-2254-01-ac-accountForMoreOverhead.diff, which makes the change you recommended. The full regression tests ran cleanly for me.&lt;/p&gt;

&lt;p&gt;Any suggestions about a test I should run to measure whether the changes affect performance?&lt;/p&gt;

&lt;p&gt;Thanks,&lt;br/&gt;
-Rick&lt;/p&gt;</comment>
                            <comment id="13032109" author="knutanders" created="Wed, 11 May 2011 23:04:21 +0100"  >&lt;p&gt;I wouldn&apos;t worry too much about the performance impact of the patch. I&apos;d trust the runtime compiler to do the right thing and inline the setter method. And even if it doesn&apos;t, it looks like the new code only adds two calls to the setter method per log record, and that cost should be negligible compared to the cost of writing the log record to disk. I&apos;m not sure we have any good tests for this. We have the bank_tx load and the sr_update load in org.apache.derbyTesting.perf.clients.Runner that will produce a lot of log records, but you&apos;ll probably have to run with derby.system.durability=test or with the in-memory backend so that the changes aren&apos;t overshadowed by the tests waiting for the log being flushed to disk on commit.&lt;/p&gt;</comment>
                            <comment id="13032146" author="dagw" created="Wed, 11 May 2011 23:47:06 +0100"  >&lt;p&gt;I thought of the inlining possibility after I posted my comment, too. It should be easy for the compiler to inline this one, and in insane mode it would not even be a trade-off between speed and code volume (single assignment being cheaper than a procedure call). So, if you are so inclined, feel free to commit as is.&lt;/p&gt;
</comment>
                            <comment id="13032577" author="rhillegas" created="Thu, 12 May 2011 19:48:52 +0100"  >&lt;p&gt;Thanks Knut and Dag. I have committed derby-2254-01-ac-accountForMoreOverhead.diff to trunk at subversion revision 1102417.&lt;/p&gt;</comment>
                            <comment id="13032583" author="rhillegas" created="Thu, 12 May 2011 19:58:29 +0100"  >&lt;p&gt;Ported 1102417 from trunk to 10.8 branch at subversion revision 1102421.&lt;/p&gt;</comment>
                            <comment id="13222584" author="rhillegas" created="Mon, 5 Mar 2012 20:18:46 +0000"  >&lt;p&gt;I think that it should be safe to backport this fix to earlier branches.&lt;/p&gt;</comment>
                            <comment id="13229389" author="kmarsden" created="Wed, 14 Mar 2012 17:19:49 +0000"  >&lt;p&gt;Reopen for 10.5 backport consideration. If working on the backport for this issue. Temporarily assign yourself and add a comment that you are working on it. When finished, reresolve with the new fix versions or label backport_reject_10_x as appropriate.&lt;/p&gt;</comment>
                            <comment id="13231758" author="mikem" created="Sat, 17 Mar 2012 00:36:15 +0000"  >&lt;p&gt;Assign temporarily for backport.&lt;/p&gt;</comment>
                            <comment id="13233961" author="mikem" created="Tue, 20 Mar 2012 23:59:43 +0000"  >&lt;p&gt;backported to 10.5, 10.6 and 10.7.  restoring orignal owner, don&apos;t plan on backporting any farther back now.&lt;/p&gt;</comment>
                            <comment id="13685205" author="knutanders" created="Mon, 17 Jun 2013 10:19:20 +0100"  >&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;bulk update&amp;#93;&lt;/span&gt; Close all resolved issues that haven&apos;t been updated for more than one year.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12361813">DERBY-2286</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="12310040">
                    <name>Required</name>
                                                                <inwardlinks description="is required by">
                                        <issuelink>
            <issuekey id="12546430">DERBY-5654</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12478725" name="d2254.zip" size="2249" author="rhillegas" created="Tue, 10 May 2011 20:37:42 +0100"/>
                            <attachment id="12478816" name="derby-2254-01-ab-accountForMoreOverhead.diff" size="7718" author="rhillegas" created="Wed, 11 May 2011 14:22:55 +0100"/>
                            <attachment id="12478867" name="derby-2254-01-ac-accountForMoreOverhead.diff" size="7704" author="rhillegas" created="Wed, 11 May 2011 20:31:49 +0100"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310200" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Bug behavior facts</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10364"><![CDATA[Data corruption]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 18 Jan 2007 22:35:49 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>22968</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12310090" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Issue &amp; fix info</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10102"><![CDATA[Patch Available]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hy0clb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>35858</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12310050" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Urgency</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10052"><![CDATA[Normal]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>
</channel>
</rss>