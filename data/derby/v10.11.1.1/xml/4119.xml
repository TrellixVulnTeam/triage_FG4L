<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 03:41:13 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/DERBY-4119/DERBY-4119.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[DERBY-4119] Compress on a large table fails with IllegalArgumentException - Illegal Capacity</title>
                <link>https://issues.apache.org/jira/browse/DERBY-4119</link>
                <project id="10594" key="DERBY">Derby</project>
                    <description>&lt;p&gt;When compressing a large table, Derby failed with the following exception:&lt;br/&gt;
IllegalArgumentException; Illegal Capacity: -X&lt;/p&gt;

&lt;p&gt;I was able to access the database afterwards, but haven&apos;t yet checked if all the data is still available.&lt;br/&gt;
The compress was started with CALL SYSCS_UTIL.SYSCS_COMPRESS_TABLE(&apos;schema&apos;, &apos;table&apos;, 1) from ij.&lt;/p&gt;

&lt;p&gt;The data in the table was inserted with 25 concurrent threads. This seems to cause excessive table growth, as the data inserted should weigh in at around 2 GB. The table size after the insert is ten times bigger, 20 GB.&lt;br/&gt;
I have been able to generate the table and do a compress earlier, but then I have been using fewer insert threads.&lt;br/&gt;
I have also been able to successfully compress the table when retrying after the failure occurred (shut down the database, then booted again and compressed).&lt;/p&gt;

&lt;p&gt;I&apos;m trying to reproduce, and will post more information (like the stack trace) later.&lt;br/&gt;
So far my attempts at reproducing has failed. Normally the data is generated and the compress is started without shutting down the database. My attempts this far has consisted of doing compress on the existing database (where the failure was first seen).&lt;/p&gt;</description>
                <environment></environment>
        <key id="12419466">DERBY-4119</key>
            <summary>Compress on a large table fails with IllegalArgumentException - Illegal Capacity</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="knutanders">Knut Anders Hatlen</assignee>
                                    <reporter username="kristwaa">Kristian Waagan</reporter>
                        <labels>
                    </labels>
                <created>Thu, 26 Mar 2009 08:59:24 +0000</created>
                <updated>Tue, 16 Jun 2009 10:18:56 +0100</updated>
                            <resolved>Wed, 1 Apr 2009 08:56:38 +0100</resolved>
                                                    <fixVersion>10.5.1.1</fixVersion>
                    <fixVersion>10.6.1.0</fixVersion>
                                    <component>Store</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>1</watches>
                                                                <comments>
                            <comment id="12689415" author="knutanders" created="Thu, 26 Mar 2009 10:09:41 +0000"  >&lt;p&gt;One likely suspect is this vector allocation in impl.store.access.sort.MergeSort:&lt;/p&gt;

&lt;p&gt;			leftovers = new Vector(mergeRuns.size() - maxMergeRuns);&lt;/p&gt;

&lt;p&gt;Now, this is protected by &quot;while (mergeRuns.size() &amp;gt; maxMergeRuns)&quot;, so one should assume that the capacity argument is always positive. However, an integer overflow may make it negative if maxMergeRuns is negative. maxMergeRuns should of course never be negative, but I think there is a possibility that it can end up negative. Its value is taken from SortBuffer.capacity() which returns NodeAllocator.maxSize - 1. NodeAllocator has this method which changes the value of maxSize:&lt;/p&gt;

&lt;p&gt;	/**&lt;br/&gt;
	Expand the node allocator&apos;s capacity by certain percent.&lt;br/&gt;
	**/&lt;br/&gt;
	public void grow(int percent)&lt;/p&gt;
	{
		if (percent &amp;gt; 0)		// cannot shrink
			maxSize = maxSize * (100+percent)/100;
	}

&lt;p&gt;This method is always called with percent=100, and it will suffer from an integer overflow when the original maxSize is greater than ~11000000 because (maxSize * 200) will be greater than Integer.MAX_VALUE. It should be easy to change this calculation so that intermediate results are less likely to overflow.&lt;/p&gt;</comment>
                            <comment id="12689416" author="knutanders" created="Thu, 26 Mar 2009 10:13:44 +0000"  >&lt;p&gt;If you manage to reproduce it, you could see if this patch makes any difference.&lt;/p&gt;</comment>
                            <comment id="12689664" author="hartsookl" created="Thu, 26 Mar 2009 21:11:52 +0000"  >&lt;p&gt;We&apos;ve seen this problem in 10.4.2.0 when creating indexes and/or reestablishing foreign key constraints on large tables (in excess of some tens of millions of rows). A modification similar to the one in the diff seems to have solved the problem we were seeing. &lt;/p&gt;</comment>
                            <comment id="12689682" author="kristwaa" created="Thu, 26 Mar 2009 21:38:01 +0000"  >&lt;p&gt;Thanks for the feedback, Larry!&lt;/p&gt;

&lt;p&gt;I expect we&apos;ll investigate further and have a fix ready for the (expected) next 10.5 release candidate.&lt;br/&gt;
How easily can you reproduce the bug?&lt;br/&gt;
Can you post the values for the page cache size, and the minimum and maximum heap set for Java?&lt;/p&gt;

&lt;p&gt;I&apos;m having problems reproducing the bug with the application used when I saw the bug, and due to the size of the data it takes a while to run it through.&lt;/p&gt;</comment>
                            <comment id="12689993" author="hartsookl" created="Fri, 27 Mar 2009 18:11:26 +0000"  >&lt;p&gt;Hi Kristian,&lt;/p&gt;

&lt;p&gt;We have a lot of trouble reproducing it, too. In fact, we&apos;ve only seen it a customer sites and haven&apos;t been able to construct a simple test case. What I&apos;ve noticed is that, when the problem occurs, we are usually able to shutdown the database, restart it and run the alter table statement successfully. We run Derby in embedded mode and often have multiple different databases open simultaneously.&lt;/p&gt;

&lt;p&gt;Here are the stats you requested:&lt;br/&gt;
    page cache size 6250&lt;br/&gt;
    page size is 16384&lt;br/&gt;
   JVM max heap is anywhere from 8 GB to 20 GB.&lt;/p&gt;

&lt;p&gt;The most recent times I&apos;ve seen the error has been at a customer site. They were running RedHat on a server with, I think, 64 GB RAM and 4 dual-core CPUs. The JVM was configured with a max heap of 20 GB&lt;/p&gt;</comment>
                            <comment id="12693711" author="kristwaa" created="Mon, 30 Mar 2009 11:03:41 +0100"  >&lt;p&gt;I managed to reproduce the error and get the stack trace, but this time it happened during the creation of an index. Larry reported to have seen the error during index creation.&lt;br/&gt;
It happens in the same code area as suggested by Knut Anders. I also observed the error once during compress in a different run, but a reboot of the database had overwritten the log before I could look at it.&lt;/p&gt;

&lt;p&gt;I believe Knut&apos;s suggested fix will allow the vector to grow until it reaches Integer.MAX_VALUE, given there is enough heap memory to do so.&lt;br/&gt;
I have only tested the calculation itself, as each of my test runs with the repro takes many hours.&lt;br/&gt;
If we get this into the next release candidate, I can do some more test runs with it.&lt;/p&gt;


&lt;p&gt;2009-03-30 01:23:21.340 GMT Thread&lt;span class=&quot;error&quot;&gt;&amp;#91;DRDAConnThread_8,5,main&amp;#93;&lt;/span&gt; (XID = 12059858), (SESSIONID = 421), (DATABASE = db), (DRDAID = NF000001.F2EB-4196790664386933061&lt;/p&gt;
{211}
&lt;p&gt;), Failed Statement is: CREATE INDEX my_index ON my_table(my_col)&lt;br/&gt;
java.lang.IllegalArgumentException: Illegal Capacity: -18547753&lt;br/&gt;
        at java.util.Vector.&amp;lt;init&amp;gt;(Vector.java:109)&lt;br/&gt;
        at java.util.Vector.&amp;lt;init&amp;gt;(Vector.java:124)&lt;br/&gt;
        at org.apache.derby.impl.store.access.sort.MergeSort.multiStageMerge(Unknown Source)&lt;br/&gt;
        at org.apache.derby.impl.store.access.sort.MergeSort.openSortRowSource(Unknown Source)&lt;br/&gt;
        at org.apache.derby.impl.store.access.RAMTransaction.openSortRowSource(Unknown Source)&lt;br/&gt;
        at org.apache.derby.impl.sql.execute.CreateIndexConstantAction.loadSorter(Unknown Source)&lt;br/&gt;
        at org.apache.derby.impl.sql.execute.CreateIndexConstantAction.executeConstantAction(Unknown Source)&lt;br/&gt;
        at org.apache.derby.impl.sql.execute.MiscResultSet.open(Unknown Source)&lt;br/&gt;
        at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)&lt;br/&gt;
        at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)&lt;br/&gt;
        at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)&lt;br/&gt;
        at org.apache.derby.impl.jdbc.EmbedStatement.execute(Unknown Source)&lt;br/&gt;
        at org.apache.derby.impl.jdbc.EmbedStatement.executeUpdate(Unknown Source)&lt;br/&gt;
        at org.apache.derby.impl.drda.DRDAConnThread.parseEXCSQLIMM(Unknown Source)&lt;br/&gt;
        at org.apache.derby.impl.drda.DRDAConnThread.processCommands(Unknown Source)&lt;br/&gt;
        at org.apache.derby.impl.drda.DRDAConnThread.run(Unknown Source)&lt;br/&gt;
Cleanup action completed&lt;/p&gt;</comment>
                            <comment id="12693799" author="knutanders" created="Mon, 30 Mar 2009 16:19:45 +0100"  >&lt;p&gt;Thanks for testing. I&apos;ll run the regression tests and check in a fix if they pass.&lt;/p&gt;

&lt;p&gt;The suggested fix only reduces the chance of an overflow caused by an overflow in the intermediate result. It does not prevent an overflow if the final result is to large. This new patch (overflow2.diff) also addresses that problem by calculating the new max size as a long, and set it to Integer.MAX_VALUE if the value doesn&apos;t fit in an int. A similar fix is applied to the newNode() method because it may currently allocate an array larger than maxSize and therefore has the possibility to overflow even if we have maxSize under control.&lt;/p&gt;</comment>
                            <comment id="12693805" author="knutanders" created="Mon, 30 Mar 2009 16:33:01 +0100"  >&lt;p&gt;All the regression tests passed with the overflow2 patch, and it&apos;s ready for review. Thanks.&lt;/p&gt;</comment>
                            <comment id="12694007" author="kristwaa" created="Tue, 31 Mar 2009 09:06:35 +0100"  >&lt;p&gt;Regarding your first fix, I thought Derby ended up casting a float to int, which would result in Integer.MAX_VALUE if the float was bigger than that.&lt;br/&gt;
In any case, the way you solved it in the second patch is more readable.&lt;/p&gt;

&lt;p&gt;When it comes to the second patch, I haven&apos;t had the time to review it, but I do have one comment.&lt;br/&gt;
Does the JVM really handle a Vector of size Integer.MAX_VALUE? At least it used to be Integer.MAX_VALUE - X.&lt;br/&gt;
I tested quickly with a byte array, and for various JVMs I found X to be 0, 18 and 39. Would it make sense to subtract a small value from Integer.MAX_VALUE?&lt;/p&gt;

&lt;p&gt;This is an edge case, as you need quite a few gigs of heap to support an Object array with a capacity close to Integer.MAX_VALUE, but many machines these days do have enough memory for this. It is not clear to me what it takes for Derby to actually grow the vector to such sizes though.&lt;/p&gt;</comment>
                            <comment id="12694033" author="knutanders" created="Tue, 31 Mar 2009 10:26:44 +0100"  >&lt;p&gt;Thanks for looking at the patch.&lt;/p&gt;

&lt;p&gt;I think you&apos;re right that the float-&amp;gt;int cast in grow() in the first patch did the right thing, so the extra changes to that method in the second patch should not change the result in any way.&lt;/p&gt;

&lt;p&gt;Good point that some JVMs don&apos;t allow vectors/arrays whose size is close to Integer.MAX_VALUE. Instead of guessing the largest supported array size, perhaps we could catch OutOfMemoryError and stop growing if that happens. The old code did this:&lt;/p&gt;

&lt;p&gt;			// Attempt to allocate a new array.  If the allocation&lt;br/&gt;
			// fails, tell the caller that there are no more&lt;br/&gt;
			// nodes available.&lt;br/&gt;
			Node[] newArray = new Node&lt;span class=&quot;error&quot;&gt;&amp;#91;array.length * GROWTH_MULTIPLIER&amp;#93;&lt;/span&gt;;&lt;br/&gt;
			if (newArray == null)&lt;br/&gt;
				return null;&lt;/p&gt;

&lt;p&gt;The checking for newArray==null didn&apos;t make any sense, since the array allocation is not allowed to return null. My guess is that catching an OutOfMemoryError here, and returning null, would do what was originally intended. And it would also solve the problem with JVMs that don&apos;t support that large arrays, as it seems like an OutOfMemoryError is what&apos;s being thrown in this situation.&lt;/p&gt;</comment>
                            <comment id="12694035" author="knutanders" created="Tue, 31 Mar 2009 10:33:20 +0100"  >&lt;p&gt;Attaching a new patch which catches OutOfMemoryError and returns null if the allocation fails, instead of checking that the return value from the allocation was null.&lt;/p&gt;</comment>
                            <comment id="12694043" author="kristwaa" created="Tue, 31 Mar 2009 11:03:19 +0100"  >&lt;p&gt;My only comment to catching OOME instead of guessing the largest supported array size, is that for JVMs that don&apos;t support arrays of size Integer.MAX_VALUE you may end up with a lot smaller array, right?&lt;br/&gt;
Again, I haven&apos;t checked the code, but I assume this will lead to more stages in the sort, and not failing to do the sort itself?&lt;br/&gt;
If so, the approach sounds good to me.&lt;/p&gt;

&lt;p&gt;Another nit is to add a comment where you catch the OOME, stating it was done to resolve the issue that some (older?) JVMs don&apos;t support arrays of size Integer.MAX_VALUE (or just point to &lt;a href=&quot;https://issues.apache.org/jira/browse/DERBY-4119&quot; title=&quot;Compress on a large table fails with IllegalArgumentException - Illegal Capacity&quot; class=&quot;issue-link&quot; data-issue-key=&quot;DERBY-4119&quot;&gt;&lt;del&gt;DERBY-4119&lt;/del&gt;&lt;/a&gt;).&lt;br/&gt;
Code catching OOME tends to raise suspicion &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="12694084" author="knutanders" created="Tue, 31 Mar 2009 13:36:57 +0100"  >&lt;p&gt;Thanks again for looking at the patch.&lt;/p&gt;

&lt;p&gt;It&apos;s true that catching the OOME means that we end up with an array that&apos;s a lot smaller than Integer.MAX_VALUE (it doesn&apos;t have to be a lot smaller, but with the current value of GROWTH_MULTIPLIER (2) it will be about half the size). But even if it is smaller than ideal, it will still be better than the current situation, where it&apos;ll crash. And it will also fix problems that may arise because the amount of available memory has become smaller than it was when we checked it in MergeInserter.insert() and decided to increase the maximum size (which was the problem the original check for newArray==null was supposed to fix).&lt;/p&gt;

&lt;p&gt;If we had a way to find the exact maximum array size supported by the JVM, I agree that we should use that limit instead of MAX_VALUE, but it doesn&apos;t sound that appealing to guess a limit that may be suboptimal for JVMs without the limitation, and possibly too large for JVMs we haven&apos;t tested. If it turns out to be a problem, we could improve this further at a later point by successively trying to allocate a slightly smaller array on OOME until we are able to allocate it. Hopefully most JVMs will have removed this limitation before it becomes common for Derby to run into it.&lt;/p&gt;

&lt;p&gt;As far as I can see, your assumption is correct that failing to increase the size of the array will lead to more stages in the sort, and not cause the sort to fail.&lt;/p&gt;</comment>
                            <comment id="12694089" author="knutanders" created="Tue, 31 Mar 2009 13:50:56 +0100"  >&lt;p&gt;Committed overflow4.diff to trunk with revision 760422.&lt;br/&gt;
I also plan to backport it to 10.5.&lt;/p&gt;</comment>
                            <comment id="12694112" author="kristwaa" created="Tue, 31 Mar 2009 14:30:36 +0100"  >&lt;p&gt;Thanks for following up, Knut Anders &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;I have started a test-run for trunk, and will do at least a few runs.&lt;br/&gt;
When RC2 is out, I&apos;ll do some testing of that one too.&lt;/p&gt;</comment>
                            <comment id="12694416" author="knutanders" created="Wed, 1 Apr 2009 08:56:38 +0100"  >&lt;p&gt;Committed to 10.5 with revision 760809.&lt;/p&gt;</comment>
                            <comment id="12720002" author="kristwaa" created="Tue, 16 Jun 2009 10:18:56 +0100"  >&lt;p&gt;I reran the application/workload where I saw the reported issue. I haven&apos;t observed the problem after the patch went in, so I&apos;m closing the issue.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12403696" name="overflow.diff" size="586" author="knutanders" created="Thu, 26 Mar 2009 10:13:44 +0000"/>
                            <attachment id="12404146" name="overflow2.diff" size="1885" author="knutanders" created="Mon, 30 Mar 2009 16:19:45 +0100"/>
                            <attachment id="12404217" name="overflow3.diff" size="1943" author="knutanders" created="Tue, 31 Mar 2009 10:33:20 +0100"/>
                            <attachment id="12404237" name="overflow4.diff" size="2396" author="knutanders" created="Tue, 31 Mar 2009 13:50:56 +0100"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>4.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 26 Mar 2009 10:09:41 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>24036</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hy0wwv:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>39150</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                            </customfields>
    </item>
</channel>
</rss>