<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sat May 16 23:28:34 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/BOOKKEEPER-246/BOOKKEEPER-246.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[BOOKKEEPER-246] Recording of underreplication of ledger entries</title>
                <link>https://issues.apache.org/jira/browse/BOOKKEEPER-246</link>
                <project id="12311293" key="BOOKKEEPER">Bookkeeper</project>
                    <description>&lt;p&gt;This JIRA is to decide how to record that entries in a ledger are underreplicated. &lt;/p&gt;

&lt;p&gt;I think there is a common understanding (correct me if im wrong), that rereplication can be broken into two logically distinct phases. A) Detection of entry underreplication &amp;amp; B) Rereplication. &lt;/p&gt;

&lt;p&gt;This subtask is to handle the interaction between these two stages. Stage B needs to know what to rereplicate; how should Stage A inform it?&lt;/p&gt;</description>
                <environment></environment>
        <key id="12554512">BOOKKEEPER-246</key>
            <summary>Recording of underreplication of ledger entries</summary>
                <type id="7" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/subtask_alternate.png">Sub-task</type>
                            <parent id="12553925">BOOKKEEPER-237</parent>
                                    <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="ikelly">Ivan Kelly</assignee>
                                    <reporter username="ikelly">Ivan Kelly</reporter>
                        <labels>
                    </labels>
                <created>Wed, 9 May 2012 16:08:49 +0100</created>
                <updated>Thu, 2 May 2013 03:29:53 +0100</updated>
                            <resolved>Tue, 14 Aug 2012 10:42:08 +0100</resolved>
                                                    <fixVersion>4.2.0</fixVersion>
                                    <component>bookkeeper-auto-recovery</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                <comments>
                            <comment id="13271478" author="ikelly" created="Wed, 9 May 2012 16:13:01 +0100"  >&lt;p&gt;Regardless of how we implement this, we should have a clean interface to the rereplication store, so that it&apos;s not tied necessarily to using ZooKeeper. For this reason I&apos;m making the JIRA depend on &lt;a href=&quot;https://issues.apache.org/jira/browse/BOOKKEEPER-203&quot; title=&quot;improve ledger manager interface to remove zookeeper dependency on metadata operations.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;BOOKKEEPER-203&quot;&gt;&lt;del&gt;BOOKKEEPER-203&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="13399515" author="ikelly" created="Fri, 22 Jun 2012 19:52:58 +0100"  >&lt;p&gt;The patch requires &lt;a href=&quot;https://issues.apache.org/jira/browse/BOOKKEEPER-303&quot; title=&quot;LedgerMetadata should serialized using protobufs&quot; class=&quot;issue-link&quot; data-issue-key=&quot;BOOKKEEPER-303&quot;&gt;&lt;del&gt;BOOKKEEPER-303&lt;/del&gt;&lt;/a&gt; &amp;amp; &lt;a href=&quot;https://issues.apache.org/jira/browse/BOOKKEEPER-2&quot; title=&quot;bookkeeper does not put enough meta-data in to do recovery properly&quot; class=&quot;issue-link&quot; data-issue-key=&quot;BOOKKEEPER-2&quot;&gt;&lt;del&gt;BOOKKEEPER-2&lt;/del&gt;&lt;/a&gt;, in that order.&lt;/p&gt;</comment>
                            <comment id="13400527" author="umamaheswararao" created="Mon, 25 Jun 2012 16:21:26 +0100"  >&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;@Override
+    public void markLedgerComplete(long ledgerId) throws ReplicationException.UnavailableException {
+        try {
+            zkc.delete(getUrLedgerZnode(ledgerId), -1);
+        } catch (KeeperException.NoNodeException nne) {
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;  This delete call can fail with NodeNotEMpty exception as, lock file   will present inside that.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&lt;br/&gt;
I was written most of this node fetching code in Replication wroker. I like this abstraction. Implemented the Lock in separate class and make use of it. Already filed a JIRA &lt;a href=&quot;https://issues.apache.org/jira/browse/BOOKKEEPER-305&quot; title=&quot;Provide distributed lock implementation which will be used by Replication worker while replicating fragments.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;BOOKKEEPER-305&quot;&gt;&lt;del&gt;BOOKKEEPER-305&lt;/del&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&lt;p&gt;Also other case while deleting ledger node is&lt;br/&gt;
When other peers trying to aquire the lock for same node also, they will create one lock file, at the same time if the current node trying to delete the ledgerNode, then also NotEmpty exception can come. On exception may be better to call recursive delete api from ZKUtils?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&lt;br/&gt;
As per my understanding Rakesh is trying to update the multiple bookie failure to the same ledger under replicated Znode with some data, ex: BKIP:port1, BKIP:port2. SO, that current replication should check the version number for deleting the ledger znode. otherwise we may end up deleting that znode right?&lt;/li&gt;
&lt;/ul&gt;




&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;One question:&lt;br/&gt;
markLedgerUnderreplicated api will be used by Auditor.&lt;br/&gt;
getLedgerToRereplicate &amp;amp; markLedgerComplete apis will be used by ReplicationWorker right?&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13400727" author="ikelly" created="Mon, 25 Jun 2012 18:55:00 +0100"  >&lt;blockquote&gt;
&lt;p&gt;This delete call can fail with NodeNotEMpty exception as, lock file will present inside that.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The locks are created under the urLockPath, not urLedgerPath. This has two advantages. 1. we can just delete the ledger znode without worrying. 2. the ledger znodes can be moved to another metadata store, without affecting the locks. This could be important for &lt;a href=&quot;https://issues.apache.org/jira/browse/BOOKKEEPER-181&quot; title=&quot;Scale hedwig&quot; class=&quot;issue-link&quot; data-issue-key=&quot;BOOKKEEPER-181&quot;&gt;&lt;del&gt;BOOKKEEPER-181&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I was written most of this node fetching code in Replication wroker. I like this abstraction. Implemented the Lock in separate class and make use of it. Already filed a JIRA &lt;a href=&quot;https://issues.apache.org/jira/browse/BOOKKEEPER-305&quot; title=&quot;Provide distributed lock implementation which will be used by Replication worker while replicating fragments.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;BOOKKEEPER-305&quot;&gt;&lt;del&gt;BOOKKEEPER-305&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Do you have a patch for &lt;a href=&quot;https://issues.apache.org/jira/browse/BOOKKEEPER-305&quot; title=&quot;Provide distributed lock implementation which will be used by Replication worker while replicating fragments.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;BOOKKEEPER-305&quot;&gt;&lt;del&gt;BOOKKEEPER-305&lt;/del&gt;&lt;/a&gt;?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;As per my understanding Rakesh is trying to update the multiple bookie failure to the same ledger under replicated Znode with some data, ex: BKIP:port1, BKIP:port2. SO, that current replication should check the version number for deleting the ledger znode. otherwise we may end up deleting that znode right?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;This is something I hadn&apos;t considered. This can be implemented within the LedgerUnderreplicationManager without needing to change the API. Instead of storing just the lock znode in the heldLock map, I can store the stat also. Then when I try to markLedgerComplete, i can pass the stat.getVersion() to the delete to make sure if another bookie has failed, another recover will take place.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;markLedgerUnderreplicated api will be used by Auditor.&lt;br/&gt;
getLedgerToRereplicate &amp;amp; markLedgerComplete apis will be used by ReplicationWorker right?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Exactly.&lt;/p&gt;</comment>
                            <comment id="13400881" author="ikelly" created="Mon, 25 Jun 2012 21:52:08 +0100"  >&lt;p&gt;Addresses Uma&apos;s comments. The interface now takes a replica name when marking a ledger as underreplicated. The list of missing replicas is stored in the znode, and if a replica of a ledger fails, while another replica is being recovered, the ledger will not be marked as complete until it has been checked again.&lt;/p&gt;</comment>
                            <comment id="13401168" author="umamaheswararao" created="Tue, 26 Jun 2012 05:46:07 +0100"  >&lt;p&gt;Thanks a lot Ivan for the update on the patch!&lt;/p&gt;

&lt;p&gt;One doubt:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
heldLocks.put(ledgerId, &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Lock(lockPath, stat.getVersion()));
+                        &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; ledgerId;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I remember, we have decided that One Bookie will replicate one ledger at a time right?&lt;/p&gt;

&lt;p&gt;Then when this map will contain more than one element?&lt;/p&gt;


&lt;p&gt;One point I wanted to mention is that: for example if one of the ledger fragment contains the current Bookie in his ensemble, then we should not take the replication work for that fragment. Worker will continue with other ledger fragments and finally it will not delete ledger node but will release the lock. That means current Bookie is no more eligible for replicating that ledger. That is the reason I thought to keep the colected childrens in Q. At this situation I will enqueue this ledger inti Q again, so that, for sure I will visit this ledger only after completing the remainig ledgers in Q. (Infuture we may can build priority Q.....example: if ledger is having only one replica that should have high priority...etc)&lt;/p&gt;

&lt;p&gt;basic ReplicationWorker loop I have currently:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; (&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;) {
            DistributedLock lock = &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;;
            &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
                &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; ledger = underReplicatedLedgersQ.take();
                &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; ledgerToReplicate = underReplicatedPath + &lt;span class=&quot;code-quote&quot;&gt;&quot;/&quot;&lt;/span&gt; + ledger;
                lock = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; DistributedLock(zkc, ledgerToReplicate,
                        &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; RetryLockListenerImpl());
                &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; == lock.tryLock()) {
                    &lt;span class=&quot;code-keyword&quot;&gt;continue&lt;/span&gt;;
                }
                LOG
                        .info(&lt;span class=&quot;code-quote&quot;&gt;&quot;Aquired the lock &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; replicating the fragments of ledger : &quot;&lt;/span&gt;
                                + ledger);
                Stat uReplicatedZNodeStat = zkc
                        .exists(ledgerToReplicate, &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;);
                &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (uReplicatedZNodeStat == &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
                    &lt;span class=&quot;code-keyword&quot;&gt;continue&lt;/span&gt;; &lt;span class=&quot;code-comment&quot;&gt;// Is &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; correct? or will &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; exception?
&lt;/span&gt;                }
                &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; uReplicatedZNodeVersionNumber = uReplicatedZNodeStat
                        .getVersion();
                LedgerHandle ledgerHandle = getLedgerHandle(&lt;span class=&quot;code-object&quot;&gt;Long&lt;/span&gt;
                        .parseLong(ledger));

                CheckerCallback cb = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; CheckerCallback();
                &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (ledgerHandle != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
                    checker.checkLedger(ledgerHandle, cb);
                    Set&amp;lt;LedgerFragmentReplica&amp;gt; fragments = cb
                            .waitAndGetResult();

                    LOG.info(&lt;span class=&quot;code-quote&quot;&gt;&quot;Found  &quot;&lt;/span&gt; + fragments.size()
                            + &lt;span class=&quot;code-quote&quot;&gt;&quot; fragments &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; replication from ledger : &quot;&lt;/span&gt;
                            + ledger + &lt;span class=&quot;code-quote&quot;&gt;&quot;, fragments are = &quot;&lt;/span&gt; + fragments);
                    &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; isAllFragmentsReplicated = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;;
                    &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; isFargmentsArePartOfEnsemble = &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;;
                    &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (LedgerFragmentReplica lf : fragments) {

                        &lt;span class=&quot;code-comment&quot;&gt;// Target node already part of ensemble, let&apos;s pick it
&lt;/span&gt;                        &lt;span class=&quot;code-comment&quot;&gt;// by another Bookie &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; replication and &lt;span class=&quot;code-keyword&quot;&gt;continue&lt;/span&gt; with
&lt;/span&gt;                        &lt;span class=&quot;code-comment&quot;&gt;// other fragment.
&lt;/span&gt;                        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (lf.getEnsemble().contains(targetBookieAddress)) {
                            LOG
                                    .info(&lt;span class=&quot;code-quote&quot;&gt;&quot;Current Bookie &quot;&lt;/span&gt;
                                            + targetBookieAddress
                                            + &lt;span class=&quot;code-quote&quot;&gt;&quot; is  part of ensemble. So lets other bookies &quot;&lt;/span&gt;
                                            + &lt;span class=&quot;code-quote&quot;&gt;&quot;take care of replication &quot;&lt;/span&gt;
                                            + ledger);
                            isFargmentsArePartOfEnsemble = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;;
                            isAllFragmentsReplicated = &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;;
                            &lt;span class=&quot;code-keyword&quot;&gt;continue&lt;/span&gt;;
                        }

                        &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
                            isAllFragmentsReplicated = fragmentReplicator
                                    .replicate(lf, ledgerHandle,
                                            targetBookieAddress);
                        } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (Exception e) {
                            LOG.info(&lt;span class=&quot;code-quote&quot;&gt;&quot;Exception &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; replicating...&quot;&lt;/span&gt;, e);
                            isAllFragmentsReplicated = &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;;
                        }
                    }

                    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; == isAllFragmentsReplicated) {
                        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; == isFargmentsArePartOfEnsemble) {
                            &lt;span class=&quot;code-comment&quot;&gt;// Replication failed. Let&apos;s retry.
&lt;/span&gt;                            enqueue(ledger);
                        }
                        &lt;span class=&quot;code-keyword&quot;&gt;continue&lt;/span&gt;;
                    }
                }

                &lt;span class=&quot;code-comment&quot;&gt;// SuccessFully Replicated, Let&apos;s delete underReplicated Znode
&lt;/span&gt;                releaseLock(lock);
                LOG.info(&lt;span class=&quot;code-quote&quot;&gt;&quot;Released the lock &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; ledger : &quot;&lt;/span&gt; + ledger);

                &lt;span class=&quot;code-comment&quot;&gt;// Clean up
&lt;/span&gt;                &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; success = deleteUnderreplicatedZNode(ledgerToReplicate,
                        uReplicatedZNodeVersionNumber);
                &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; == success) {
                    &lt;span class=&quot;code-comment&quot;&gt;// Retry &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; replication as there may be version
&lt;/span&gt;                    &lt;span class=&quot;code-comment&quot;&gt;// mismatch &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; deleting node. Simply add into Queue?
&lt;/span&gt;                    LOG.info(&lt;span class=&quot;code-quote&quot;&gt;&quot;Underreplicated ledger node deletion failed.&quot;&lt;/span&gt;
                            + &lt;span class=&quot;code-quote&quot;&gt;&quot; Will retry replication &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; ledger : &quot;&lt;/span&gt; + ledger);
                    enqueue(ledger);
                    &lt;span class=&quot;code-keyword&quot;&gt;continue&lt;/span&gt;;
                }
                LOG.info(&lt;span class=&quot;code-quote&quot;&gt;&quot;Completed replication of ledger : &quot;&lt;/span&gt; + ledger);

            } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (InterruptedException e) {
                &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.currentThread().interrupt();
                LOG.error(&lt;span class=&quot;code-quote&quot;&gt;&quot;Interrupted &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; replicating ledger fragments,&quot;&lt;/span&gt;
                        + &lt;span class=&quot;code-quote&quot;&gt;&quot; stopping ReplicationWorker&quot;&lt;/span&gt;, e);
            } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (Throwable e) {
                LOG.error(&lt;span class=&quot;code-quote&quot;&gt;&quot;Fatal exception &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; replicating ledgers,&quot;&lt;/span&gt;
                        + &lt;span class=&quot;code-quote&quot;&gt;&quot; stopping ReplicationWorker&quot;&lt;/span&gt;, e);
            } &lt;span class=&quot;code-keyword&quot;&gt;finally&lt;/span&gt; {
                &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt; != lock &amp;amp;&amp;amp; lock.hasLock()) {
                    &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
                        releaseLock(lock);
                    } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (InterruptedException e) {
                        &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.currentThread().interrupt();
                        e.printStackTrace();
                    } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (KeeperException e) {
                        e.printStackTrace();
                    }
                    LOG.info(&lt;span class=&quot;code-quote&quot;&gt;&quot;Released the lock &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; ledger : &quot;&lt;/span&gt; + lock);
                }
            }
        }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13401203" author="rakeshr" created="Tue, 26 Jun 2012 07:55:58 +0100"  >&lt;p&gt;Hi Ivan, patch looks great. I have few suggestions and thoughts. Also, I think need to rebase the Auditor &lt;a href=&quot;https://issues.apache.org/jira/browse/BOOKKEEPER-272&quot; title=&quot;Provide automatic mechanism to know bookie failures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;BOOKKEEPER-272&quot;&gt;&lt;del&gt;BOOKKEEPER-272&lt;/del&gt;&lt;/a&gt; patch accordingly.&lt;/p&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Can we have factory for the LedgerUnderreplicationManager?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;We are creating underreplica znode like &quot;%s/urL%08d&quot;&lt;br/&gt;
urL00000009&lt;br/&gt;
But LedgerManager is creating znode like &quot;%010d&quot;&lt;br/&gt;
L_0000000009&lt;br/&gt;
I feel would be good to follow same pattern for maintainability. &lt;br/&gt;
(Presently there is no functional problem as replication logic is separate.)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;In the getLedgerToRereplicate(), changedLatch.countDown() should be only on EventType.NodeChildrenChanged like as follows, just to minimize the chance of unwanted exceptions.
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void process(WatchedEvent e) {
   &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (event.getType() == EventType.NodeChildrenChanged)
       changedLatch.countDown();
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;It would be good to have few info or debug logs for debuggability. I have seen many places we are silently continuing without logs&lt;br/&gt;
markLedgerUnderreplicated&lt;br/&gt;
markLedgerComplete&lt;br/&gt;
getLedgerToRereplicate&lt;br/&gt;
releaseLedger&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Also, one good pattern of handling the InterruptedException.&lt;br/&gt;
When you catch and swallow InterruptedException, you should call Thread.currentThread().interrupt() afterward, so that the interrupt status isn&apos;t lost
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
} &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (InterruptedException ie) {
    &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.currentThread().interrupt()
    &lt;span class=&quot;code-comment&quot;&gt;// logging or &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; exception back
&lt;/span&gt;}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;getLedgerToRereplicate:
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
List&amp;lt;&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;&amp;gt; children = zkc.getChildren(urLedgerPath, w);
zkc.getChildren(urLockPath, w);

Collections.shuffle(children);
&lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; (children.size() &amp;gt; 0) {
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;   Instead can we filter the children which doesn&apos;t have any lock, like:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
List&amp;lt;&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;&amp;gt; children = zkc.getChildren(urLedgerPath, w);
List&amp;lt;&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;&amp;gt; lockedChildren = zkc.getChildren(urLockPath, w);
Collection&amp;lt;&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;&amp;gt; nolockChildren = CollectionUtils.subtract(children, lockedChildren);

&lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; (nolockChildren.size() &amp;gt; 0) {
    Collections.shuffle(nolockChildren);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;-Rakesh&lt;/p&gt;</comment>
                            <comment id="13406420" author="ikelly" created="Wed, 4 Jul 2012 11:21:58 +0100"  >&lt;p&gt;Addressed Rakesh&apos;s comments. I didn&apos;t add a factory interface, because it&apos;s not necessary yet. We don&apos;t know if there will be different implementations of LedgerUnderreplicationManager. Having the interface makes it easy to add one later, but adding a factory doesn&apos;t give us any benefit right now. &lt;/p&gt;

&lt;p&gt;I used a slightly different algo for getLedgerToReplicate, to avoid pulling in guava just yet, though we will probably end up pulling it in anyhow.&lt;/p&gt;</comment>
                            <comment id="13406423" author="ikelly" created="Wed, 4 Jul 2012 11:24:00 +0100"  >&lt;p&gt;Also, this patch applies on top of (trunk+&lt;a href=&quot;https://issues.apache.org/jira/browse/BOOKKEEPER-317&quot; title=&quot;Exceptions for replication&quot; class=&quot;issue-link&quot; data-issue-key=&quot;BOOKKEEPER-317&quot;&gt;&lt;del&gt;BOOKKEEPER-317&lt;/del&gt;&lt;/a&gt;). It does &lt;b&gt;not&lt;/b&gt; require &lt;a href=&quot;https://issues.apache.org/jira/browse/BOOKKEEPER-2&quot; title=&quot;bookkeeper does not put enough meta-data in to do recovery properly&quot; class=&quot;issue-link&quot; data-issue-key=&quot;BOOKKEEPER-2&quot;&gt;&lt;del&gt;BOOKKEEPER-2&lt;/del&gt;&lt;/a&gt;. Which ever of &lt;a href=&quot;https://issues.apache.org/jira/browse/BOOKKEEPER-2&quot; title=&quot;bookkeeper does not put enough meta-data in to do recovery properly&quot; class=&quot;issue-link&quot; data-issue-key=&quot;BOOKKEEPER-2&quot;&gt;&lt;del&gt;BOOKKEEPER-2&lt;/del&gt;&lt;/a&gt; and this goes in first, the other will have to resolve a conflict on the protobuf file.&lt;/p&gt;</comment>
                            <comment id="13422672" author="fpj" created="Wed, 25 Jul 2012 23:13:23 +0100"  >&lt;p&gt;A few comments:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;The patch does not apply due to conflicts with recent commits;&lt;/li&gt;
	&lt;li&gt;The current patch seems to include the patch for &lt;a href=&quot;https://issues.apache.org/jira/browse/BOOKKEEPER-317&quot; title=&quot;Exceptions for replication&quot; class=&quot;issue-link&quot; data-issue-key=&quot;BOOKKEEPER-317&quot;&gt;&lt;del&gt;BOOKKEEPER-317&lt;/del&gt;&lt;/a&gt;, but the comment before this one says that it needs to be applied first. Might be a good idea to keep them separate.&lt;/li&gt;
	&lt;li&gt;Please in the class javadoc the znode layout the class ZkLedgerUnreplicationManager is using.&lt;/li&gt;
&lt;/ol&gt;
</comment>
                            <comment id="13422896" author="umamaheswararao" created="Thu, 26 Jul 2012 06:12:26 +0100"  >&lt;p&gt;I just had a quick look on the latest patch.&lt;/p&gt;

&lt;p&gt;some nits:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;ZkLedgerUnreplicationManager ---&amp;gt; ZkLedgerUnderreplicationManager ?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;int getLedgerZNodeVersion() 
{ return ledgerZNodeVersion; }
&lt;p&gt;  +    };&lt;br/&gt;
  Please remove &apos;;&apos;. It is not required.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Please give me some more time for clear review. Today I am little hurry as I will be travelling on this afternoon. I will provide my remainig comments mostly tomorrow or early next week.&lt;/p&gt;</comment>
                            <comment id="13422969" author="rakeshr" created="Thu, 26 Jul 2012 09:58:03 +0100"  >&lt;p&gt;@Ivan. Latest patch is looking nice. Just small comments which I missed:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;It would be good to heldLocks.remove(ledgerId); only after successful &apos;zkc.delete(l.getLockZNode(), -1)&apos;. This will reduce the chance of zk orphan locks ? Whats your opinion.
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void releaseLedger(&lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; ledgerId)
  &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
    Lock l = heldLocks.remove(ledgerId);
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (l != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
       zkc.delete(l.getLockZNode(), -1);
    }
    
  }&lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; block
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;I&apos;m having one general doubt, the way handling small zookeeper fluctutations. Say, ReplicationWorker has acquired the lock and finished rereplication. Assume ZK server got partitioned for a second, when it tries to either &lt;cite&gt;markLedgerComplete(ledgerId)&lt;/cite&gt; or &lt;cite&gt;releaseLedger(ledgerId)&lt;/cite&gt; and will get the KeeperException.&lt;br/&gt;
I think this is not very straightaway to this JIRA, but other modules like auditor and the replication workers are using these apis and will get these exceptions. I guess its basically a kind of cleanup.&lt;br/&gt;
Actually I&apos;d like to discuss any special handling of zk connectionloss KeeperException?&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13426631" author="hustlmsp" created="Wed, 1 Aug 2012 15:05:43 +0100"  >&lt;p&gt;I had several comments about this jira.&lt;/p&gt;

&lt;p&gt;1) the test case you added is for testing ZkLedgerUnderreplicationManager. As my understanding, we had a clean interface. So I think the tests are using this interface not depends on a specific implementation. so why not make the test generic at first time, so we don&apos;t need to change it in future.&lt;/p&gt;

&lt;p&gt;2) from the experiences on LedgerManager, putting too many entries into a single znode is not a good idea, so we had HierarchicalLedgerManager. for now, we are building a new zookeeper layout for under-replication ledgers, we should not let it to become a new problem. so we&apos;d consider hierarchical for under-replication ledgers, since we could not assume a too-big znode would not happen in reality. &lt;/p&gt;

&lt;p&gt;3) I am thinking is there any relationship between LedgerManagerFactory and LedgerUnderreplicationManager. since metadata could be store in other storage like HBase, it doesn&apos;t make sense if we store ledger metadata in HBase while putting underreplication ledgers in ZookKeeper. so is it possible to add an interface in LedgerManagerFactory to return its underreplication ledger manager.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;interface&lt;/span&gt; LedgerManagerFactory {

...

&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; LedgerUnderreplicationManager newLedgerUnderreplicationManager();

}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;so for HierarchicalLedgerManagerFactory and FlatLedgerManagerFactory, they could use same ZkLedgerUnderreplicationManager. &lt;/p&gt;

&lt;p&gt;for other implementation of LedgerManagerFactory, we force them to consider implementing their underreplication manager. of course, they could also leverage existed ZkUnderreplicationManager as its solution.&lt;/p&gt;</comment>
                            <comment id="13426686" author="umamaheswararao" created="Wed, 1 Aug 2012 16:09:50 +0100"  >&lt;p&gt;I have a few comments on API names in LedgerUnderreplicationManager.&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;markLedgerComplete --&amp;gt; markLedgerReplicated where as we already have an API markLedgerUnderreplicated.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;releaseLedger --&amp;gt; releaseLedgerFromRereplication where as we already have an API getLedgerToRereplicate.&lt;br/&gt;
  or we can just have like, getUnderReplicatedLedger, releaseUnderReplicatedLedger and mainly this apis deals with the aquiring locks on underReplicated ledgers.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;now we can remove ReplicationException from this patch as BK-317 committed.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;heldLocks.put(ledgerId, new Lock(lockPath, stat.getVersion()));&lt;br/&gt;
  Why we are trying maintaining the multiple locks. We have decided one Bookie will do only one ledger replication at a time right?&lt;br/&gt;
  So, in that case when we will have multiple heldlocks with diff ledger ids with one ReplicationWorker service?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;most of the code in testCompletion &amp;amp; testRelease is duplicated.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;It would be great if you add some javadoc in tests&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;can we please move getNewZooKeeper methods to ZookeeperUtil class and refactor the code in  ZookeeperUtil that has the similar logic already in startSever if you want to pass diff arguments etc.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Thanks,&lt;br/&gt;
Uma&lt;/p&gt;</comment>
                            <comment id="13428234" author="ikelly" created="Fri, 3 Aug 2012 18:06:10 +0100"  >&lt;p&gt;Addressed most comments in latest patch. Those I didn&apos;t, I&apos;ll respond to in next comment.&lt;/p&gt;</comment>
                            <comment id="13428238" author="ikelly" created="Fri, 3 Aug 2012 18:09:52 +0100"  >&lt;blockquote&gt;&lt;p&gt;&lt;br/&gt;
I&apos;m having one general doubt, the way handling small zookeeper fluctutations. Say, ReplicationWorker has acquired the lock and finished rereplication. Assume ZK server got partitioned for a second, when it tries to either markLedgerComplete(ledgerId) or releaseLedger(ledgerId) and will get the KeeperException.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The worse case scenario here will be another replicator comes along, and sees that the ledger is already fully replicated, so it does nothing.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;&lt;br/&gt;
It would be good to heldLocks.remove(ledgerId); only after successful &apos;zkc.delete(l.getLockZNode(), -1)&apos;. This will reduce the chance of zk orphan locks ? Whats your opinion. &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;If the zookeeper connection dies, the LedgerUnderreplicatedManager should be destroyed and a new on instantiated.&lt;/p&gt;</comment>
                            <comment id="13428240" author="ikelly" created="Fri, 3 Aug 2012 18:11:36 +0100"  >&lt;blockquote&gt;&lt;p&gt;most of the code in testCompletion &amp;amp; testRelease is duplicated.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;This is true, but trying to generalise it would probably end up adding more code and confuse the clarity of the code.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;heldLocks.put(ledgerId, new Lock(lockPath, stat.getVersion()));&lt;br/&gt;
Why we are trying maintaining the multiple locks. We have decided one Bookie will do only one ledger replication at a time right?&lt;br/&gt;
So, in that case when we will have multiple heldlocks with diff ledger ids with one ReplicationWorker service?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;For now, there&apos;s no harm in future proofing.&lt;/p&gt;</comment>
                            <comment id="13429031" author="rakeshr" created="Mon, 6 Aug 2012 09:43:45 +0100"  >&lt;blockquote&gt;&lt;p&gt;The worse case scenario here will be another replicator comes along, and sees that the ledger is already fully replicated, so it does nothing.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;There would be cases with partial replication: after his work, replicator will give chance to others by releasing the lock. Assume following is the ledger metadata.&lt;br/&gt;
0  BK1, BK2, BK3&lt;br/&gt;
10 BK1, BK4, BK3&lt;br/&gt;
Say BK1 shuts down, BK4 has acquired the lock and would able to replicate only first fragment as BK4 is already has one copy of second fragment. Now assume while releasing lock there is a slight zk fluctuation and got connection loss exception(but zk session is still alive). Since the BK4 lock exists, others couldn&apos;t acquire the lock. Here I feel, just recreation of the LedgerManager won&apos;t work fully, instead needs to either close zk session or force releasing the lock till session expiry(timeout).&lt;/p&gt;

&lt;p&gt;Actually, I&apos;m afraid of orphan locks that would create situations where holding locks infinitely. Also, LedgerUnderreplicationManager presently doesn&apos;t have any close apis and its taking zkclient externally?&lt;/p&gt;</comment>
                            <comment id="13432924" author="umamaheswararao" created="Mon, 13 Aug 2012 06:14:51 +0100"  >&lt;p&gt;Latest Patch looks great.&lt;/p&gt;

&lt;p&gt;Some comments regarding Rakesh points:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;LedgerUnderreplicationManager presently doesn&apos;t have any close apis and its taking zkclient externally?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;We should handle that zkc cleanups from outside of LedgerUnderreplicationManager as it is taking zkc from out side. &lt;/p&gt;

&lt;p&gt;I think what Rakesh pointing is,&lt;br/&gt;
on LedgerUnderreplicatedManager destroy, how we will do the cleanup?&lt;br/&gt;
cleaning the EPHEMERAL etc, as that nodes got created by LedgerUnderreplicatedManager.&lt;/p&gt;

&lt;p&gt;So, cleanUp API will be required inside LedgerUnderreplicatedManager to clean the EPHEMERAL nodes which are created by him? (this will be the case for graceful handling.)&lt;br/&gt;
On suddent crashes, anayway that nodes will get cleaned as process dies.&lt;/p&gt;</comment>
                            <comment id="13433066" author="ikelly" created="Mon, 13 Aug 2012 12:15:05 +0100"  >&lt;p&gt;@Rakesh&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Now assume while releasing lock there is a slight zk fluctuation and got connection loss exception&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I had a response to this, but the JIRA mess last week seems to have lost it. Basically, fluctuating connections cannot happen within a zookeeper session. If the connection goes down and a single request fails, all subsequent requests will fail.&lt;/p&gt;

&lt;p&gt;I&apos;ll add a close API.&lt;/p&gt;</comment>
                            <comment id="13433147" author="umamaheswararao" created="Mon, 13 Aug 2012 15:02:24 +0100"  >&lt;p&gt;One clarification, this close API will not solve Rakesh point right?&lt;br/&gt;
Becasue, real lock node presents but, it got removed by heldLocks map.&lt;/p&gt;

&lt;p&gt;But close is iterating over heldLocks and trying to call delete on them. Ingeneral this is fine. Once keeperException comes from release API, worker may try to close the replication manager. But that destroy will not really delete that lock node right? ( infact subsequent calls also may fail with KeeperException).&lt;/p&gt;

&lt;p&gt;In releaseUnderreplicatedLedger, how about delete first and in finally remove from the heldLocks map?&lt;/p&gt;

&lt;p&gt;Please correct me, If I miss something here.&lt;/p&gt;</comment>
                            <comment id="13433165" author="ikelly" created="Mon, 13 Aug 2012 15:22:26 +0100"  >&lt;p&gt;if zk.delete() fails, then that means the zookeeper session has died. The lock will disappear automatically when the server sees that the session is gone. Changing the order of the commands won&apos;t make any difference. If the delete fails and the lock is still in the map, a subsequent call won&apos;t be able to delete it as the zk handle will be broken at that stage anyhow.&lt;/p&gt;</comment>
                            <comment id="13433370" author="umamaheswararao" created="Mon, 13 Aug 2012 18:58:30 +0100"  >&lt;p&gt;Ivan, &lt;br/&gt;
But they are 2 different category of exceptions right?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Recoverable errors: the disconnected event, connection timed out, and the connection loss exception are examples of recoverable errors, they indicate a problem that happened, but the ZooKeeper handle is still valid and future operations will succeed once the ZooKeeper library can reestablish its connection to ZooKeeper.&lt;/p&gt;

&lt;p&gt;Fatal errors: the ZooKeeper handle has become invalid. This can be due to an explicit close, authentication errors, or session expiration. &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;connection loss is recoverable error right. We can reuse the zk handle as it is still valid. I am not sure, whether I am missing something from your point of view &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;.&lt;/p&gt;</comment>
                            <comment id="13433440" author="ikelly" created="Mon, 13 Aug 2012 20:23:53 +0100"  >&lt;p&gt;connection loss is only kinda recoverable with zookeeper and you have to jump through hoops to do it, to ensure that the new handle has the same session etc. Generally, when ZK loses the connection, you&apos;re session is dead. And this is fine for us. When we get an exception like this we should error out to the top level and basically restart the recovery worker, as these have very little state which isn&apos;t dependent on zookeeper.&lt;/p&gt;</comment>
                            <comment id="13433780" author="umamaheswararao" created="Tue, 14 Aug 2012 01:32:11 +0100"  >&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
When we get an exception like &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; we should error out to the top level and basically restart the recovery worker, as these have very little state which isn&apos;t dependent on zookeeper.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;ok Make sense to me, as Replication Worker is having little state. ( I am bit worried, for the small fluctuations, making replication worker process down. Anyway lets move this and we will see this point later once we integrate other stuffs).&lt;/p&gt;

&lt;p&gt;+1 for pushing this in.&lt;/p&gt;

&lt;p&gt;Please add others opinions.&lt;/p&gt;

&lt;p&gt;Thanks&lt;br/&gt;
Uma&lt;/p&gt;
</comment>
                            <comment id="13433845" author="hustlmsp" created="Tue, 14 Aug 2012 03:59:27 +0100"  >&lt;p&gt;the new patch looks good to me. +1&lt;/p&gt;</comment>
                            <comment id="13433908" author="rakeshr" created="Tue, 14 Aug 2012 06:35:49 +0100"  >&lt;blockquote&gt;&lt;p&gt;connection loss is only kinda recoverable with zookeeper and you have to jump through hoops to do it, to ensure that the new handle has the same session etc. Generally, when ZK loses the connection, you&apos;re session is dead. And this is fine for us. When we get an exception like this we should error out to the top level and basically restart the recovery worker, as these have very little state which isn&apos;t dependent on zookeeper.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree the other modules should look at these kinda exceptions. Yeah, ReplicationWorker &lt;a href=&quot;https://issues.apache.org/jira/browse/BOOKKEEPER-248&quot; title=&quot;Rereplicating of under replicated data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;BOOKKEEPER-248&quot;&gt;&lt;del&gt;BOOKKEEPER-248&lt;/del&gt;&lt;/a&gt; should consider these and have necessary cleanup logic.&lt;/p&gt;

&lt;p&gt;+1 looks good to me and ready to go.&lt;/p&gt;</comment>
                            <comment id="13434011" author="ikelly" created="Tue, 14 Aug 2012 10:08:45 +0100"  >&lt;p&gt;Great. I&apos;ll commit this now then.&lt;/p&gt;</comment>
                            <comment id="13434024" author="ikelly" created="Tue, 14 Aug 2012 10:42:08 +0100"  >&lt;p&gt;Committed r1372808. Thanks for reviewing guys.&lt;/p&gt;</comment>
                            <comment id="13434445" author="hudson" created="Tue, 14 Aug 2012 21:02:01 +0100"  >&lt;p&gt;Integrated in bookkeeper-trunk #647 (See &lt;a href=&quot;https://builds.apache.org/job/bookkeeper-trunk/647/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/bookkeeper-trunk/647/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/BOOKKEEPER-246&quot; title=&quot;Recording of underreplication of ledger entries&quot; class=&quot;issue-link&quot; data-issue-key=&quot;BOOKKEEPER-246&quot;&gt;&lt;del&gt;BOOKKEEPER-246&lt;/del&gt;&lt;/a&gt;: Recording of underreplication of ledger entries (ivank) (Revision 1372808)&lt;/p&gt;

&lt;p&gt;     Result = ABORTED&lt;br/&gt;
ivank : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/zookeeper/bookkeeper/trunk/CHANGES.txt&lt;/li&gt;
	&lt;li&gt;/zookeeper/bookkeeper/trunk/bookkeeper-server/src/main/java/org/apache/bookkeeper/meta/FlatLedgerManagerFactory.java&lt;/li&gt;
	&lt;li&gt;/zookeeper/bookkeeper/trunk/bookkeeper-server/src/main/java/org/apache/bookkeeper/meta/HierarchicalLedgerManagerFactory.java&lt;/li&gt;
	&lt;li&gt;/zookeeper/bookkeeper/trunk/bookkeeper-server/src/main/java/org/apache/bookkeeper/meta/LedgerManagerFactory.java&lt;/li&gt;
	&lt;li&gt;/zookeeper/bookkeeper/trunk/bookkeeper-server/src/main/java/org/apache/bookkeeper/meta/LedgerUnderreplicationManager.java&lt;/li&gt;
	&lt;li&gt;/zookeeper/bookkeeper/trunk/bookkeeper-server/src/main/java/org/apache/bookkeeper/meta/ZkLedgerUnderreplicationManager.java&lt;/li&gt;
	&lt;li&gt;/zookeeper/bookkeeper/trunk/bookkeeper-server/src/main/java/org/apache/bookkeeper/proto/DataFormats.java&lt;/li&gt;
	&lt;li&gt;/zookeeper/bookkeeper/trunk/bookkeeper-server/src/main/java/org/apache/bookkeeper/replication/ReplicationException.java&lt;/li&gt;
	&lt;li&gt;/zookeeper/bookkeeper/trunk/bookkeeper-server/src/main/proto/DataFormats.proto&lt;/li&gt;
	&lt;li&gt;/zookeeper/bookkeeper/trunk/bookkeeper-server/src/test/java/org/apache/bookkeeper/replication&lt;/li&gt;
	&lt;li&gt;/zookeeper/bookkeeper/trunk/bookkeeper-server/src/test/java/org/apache/bookkeeper/replication/TestLedgerUnderreplicationManager.java&lt;/li&gt;
	&lt;li&gt;/zookeeper/bookkeeper/trunk/bookkeeper-server/src/test/java/org/apache/bookkeeper/test/ZooKeeperUtil.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                                                <inwardlinks description="is blocked by">
                                        <issuelink>
            <issuekey id="12468954">BOOKKEEPER-2</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="12310040">
                    <name>Required</name>
                                            <outwardlinks description="requires">
                                        <issuelink>
            <issuekey id="12595818">BOOKKEEPER-317</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="12310051">
                    <name>Supercedes</name>
                                            <outwardlinks description="supercedes">
                                        <issuelink>
            <issuekey id="12595255">BOOKKEEPER-305</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="10001">
                    <name>dependent</name>
                                            <outwardlinks description="depends upon">
                                        <issuelink>
            <issuekey id="12549007">BOOKKEEPER-203</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12540652" name="BOOKKEEPER-246.diff" size="78376" author="ikelly" created="Mon, 13 Aug 2012 14:34:25 +0100"/>
                            <attachment id="12539066" name="BOOKKEEPER-246.diff" size="77396" author="ikelly" created="Fri, 3 Aug 2012 18:06:10 +0100"/>
                            <attachment id="12535068" name="BOOKKEEPER-246.diff" size="68085" author="ikelly" created="Wed, 4 Jul 2012 11:21:58 +0100"/>
                            <attachment id="12533375" name="BOOKKEEPER-246.diff" size="65037" author="ikelly" created="Mon, 25 Jun 2012 21:52:08 +0100"/>
                            <attachment id="12533088" name="BOOKKEEPER-246.diff" size="45412" author="ikelly" created="Fri, 22 Jun 2012 19:52:58 +0100"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>5.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Mon, 25 Jun 2012 15:21:26 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>238762</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hyn67j:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>169193</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                </customfields>
    </item>
</channel>
</rss>