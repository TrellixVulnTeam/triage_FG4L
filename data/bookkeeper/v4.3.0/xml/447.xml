<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sat May 16 23:26:21 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/BOOKKEEPER-447/BOOKKEEPER-447.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[BOOKKEEPER-447] Bookie can fail to recover if index pages flushed before ledger flush acknowledged</title>
                <link>https://issues.apache.org/jira/browse/BOOKKEEPER-447</link>
                <project id="12311293" key="BOOKKEEPER">Bookkeeper</project>
                    <description>&lt;p&gt;Bookie index page steal (LedgerCacheImpl::grabCleanPage) can cause index file to reflect unacknowledged entries (due to flushLedger). Suppose ledger and entry fail to flush due to Bookkeeper server crash, it will cause ledger recovery not able to use the bookie afterward, due to InterleavedStorageLedger::getEntry throws IOException.&lt;br/&gt;
If the ackSet bookies all experience this problem (DC environment), the ledger will not be able to recover.&lt;br/&gt;
The problem here essentially a violation of WAL. One reasonable fix is to track ledger flush progress (either per-ledger entry, or per-topic message). Do not flush index pages which tracks entries whose ledger (log) has not been flushed.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12613984">BOOKKEEPER-447</key>
            <summary>Bookie can fail to recover if index pages flushed before ledger flush acknowledged</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="ikelly">Ivan Kelly</assignee>
                                    <reporter username="yx3zhu@gmail.com">Yixue (Andrew) Zhu</reporter>
                        <labels>
                    </labels>
                <created>Tue, 30 Oct 2012 01:30:46 +0000</created>
                <updated>Wed, 13 Feb 2013 15:46:25 +0000</updated>
                            <resolved>Mon, 24 Dec 2012 05:23:50 +0000</resolved>
                                    <version>4.2.0</version>
                                    <fixVersion>4.2.0</fixVersion>
                    <fixVersion>4.1.1</fixVersion>
                                    <component>bookkeeper-server</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>7</watches>
                                                                <comments>
                            <comment id="13486590" author="yx3zhu@gmail.com" created="Tue, 30 Oct 2012 01:31:26 +0000"  >&lt;p&gt;Assign to Robin as he is adding progress counter as part of Group-commit work, which can be used to tweak the index flush.&lt;/p&gt;</comment>
                            <comment id="13486788" author="hustlmsp" created="Tue, 30 Oct 2012 10:43:11 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Bookie index page steal (LedgerCacheImpl::grabCleanPage) can cause index file to reflect unacknowledged entries (due to flushLedger). Suppose ledger and entry fail to flush due to Bookkeeper server crash, it will cause ledger recovery not able to use the bookie afterward, due to InterleavedStorageLedger::getEntry throws IOException.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If failed to flush entry log, the last mark will not be rolled. so the entries are still in journal, they would be replayed and added to new entry log files and update the ledger index. I assumed that it should not throw IOException when getEntry. Could you describe more about the case? is it easy to reproduce that?&lt;/p&gt;</comment>
                            <comment id="13486973" author="yx3zhu@gmail.com" created="Tue, 30 Oct 2012 16:10:14 +0000"  >&lt;p&gt;The recent journal is not flushed to begin with, they cannot possibly be replayed. In the mean time, index page stealth forced the index file be flushed. The index file contains unacknowledged entries.&lt;br/&gt;
Now, the client did not close the ledger, trying to recover, asking bookies for recent entry. The bookie looked at index file to get latest entry id, then look up entry log, which is not there, and throw IOException.&lt;br/&gt;
Client got bad response from bookie. If the AckSet bookies all have same issue, the client simply give up recovering the ledger.&lt;br/&gt;
All entries get lost, including those already acknowledged.&lt;/p&gt;</comment>
                            <comment id="13487046" author="fpj" created="Tue, 30 Oct 2012 17:41:39 +0000"  >&lt;p&gt;Great catch, guys! I also would like to know if you have a reliable way of reproducing it, but it seems plausible to me that it can happen. I can only see two ways around it:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;Tracking what has been flushed to the journal as you propose&lt;/li&gt;
	&lt;li&gt;Adding an entry to the index only after it has been flushed to the journal&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;I was wondering if you have thoughts on using the second.&lt;/p&gt;</comment>
                            <comment id="13487052" author="robindhamankar" created="Tue, 30 Oct 2012 17:45:30 +0000"  >&lt;p&gt;Since in the common case the journal would have been flushed before index entries are written out, it is better to follow the approach of tracking what has been flushed and if it hasn&apos;t (in the uncommon case) then waiting for it to be flushed.&lt;/p&gt;</comment>
                            <comment id="13487201" author="fpj" created="Tue, 30 Oct 2012 20:36:32 +0000"  >&lt;p&gt;Thanks for the feedback, Robin. Here is a another thought. In the case we end up writing to the ledger device before we write to the journal, as you report here, it is a problem if we have a pointer in the index of a ledger to an entry, but the entry doesn&apos;t exist. It shouldn&apos;t be a problem, however, to have an entry in the entry log, but not a pointer in the index. If you agree, then what if we simply flush the entry log before we flush the ledger cache in InterleavedLedgerStorage flush? we already write in the opposite order in InterleavedLedgerStorage.addEntry.&lt;/p&gt;
</comment>
                            <comment id="13487333" author="yx3zhu@gmail.com" created="Tue, 30 Oct 2012 22:56:54 +0000"  >&lt;p&gt;Flavio, the proposal of forcing entry log to be flushed before index would work, though the data is force flushed unnecessarily. Besides, when we optimize the entry log to cluster it by (ledger-id, entry-id), it will make things complicated than necessary.(&lt;a href=&quot;https://issues.apache.org/jira/browse/BOOKKEEPER-432&quot; title=&quot;Improve performance of entry log range read per ledger entries &quot; class=&quot;issue-link&quot; data-issue-key=&quot;BOOKKEEPER-432&quot;&gt;&lt;del&gt;BOOKKEEPER-432&lt;/del&gt;&lt;/a&gt;)&lt;/p&gt;</comment>
                            <comment id="13487381" author="hustlmsp" created="Tue, 30 Oct 2012 23:53:42 +0000"  >&lt;blockquote&gt;
&lt;p&gt;The recent journal is not flushed to begin with, they cannot possibly be replayed.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK. I got the issue. Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yx3zhu%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;yx3zhu@gmail.com&quot;&gt;Yixue (Andrew) Zhu&lt;/a&gt; for clarification.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If you agree, then what if we simply flush the entry log before we flush the ledger cache in InterleavedLedgerStorage flush?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=fpj&quot; class=&quot;user-hover&quot; rel=&quot;fpj&quot;&gt;Flavio Junqueira&lt;/a&gt; I think the problem here is ledger storage flushed before journal flushed. Although it could avoid IOException reading ledger index as your proposal to change the flush order in ledger storage, it volatiles the contract for a bookie server, who ack an entry after the entry has been committed to journal. I would prefer Robin&apos;s proposal.&lt;/p&gt;</comment>
                            <comment id="13487613" author="fpj" created="Wed, 31 Oct 2012 08:26:12 +0000"  >&lt;blockquote&gt;&lt;p&gt;the proposal of forcing entry log to be flushed before index would work, though the data is force flushed unnecessarily.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yx3zhu%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;yx3zhu@gmail.com&quot;&gt;Yixue (Andrew) Zhu&lt;/a&gt; We already force it to disk, so there is no extra penalty. Check InterleavedLedgerStore.flush()&lt;del&gt;&amp;gt;entryLogger.flush()&lt;/del&gt;&amp;gt;logChannel.flush(true)&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I think the problem here is ledger storage flushed before journal flushed.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hustlmsp&quot; class=&quot;user-hover&quot; rel=&quot;hustlmsp&quot;&gt;Sijie Guo&lt;/a&gt; Agreed, and my proposal does not prevent us from flushing to the ledger device before we do it to the journal, but it makes sure that if we do, we won&apos;t get the IOException. This change involves no more code and we only need to swap the order, it is very simple.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;it volatiles the contract for a bookie server, who ack an entry after the entry has been committed to journal.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It does not violate the contract because I&apos;m not suggesting the we ack after flushing to the ledger device. We keep acking only when it is persisted in the journal. &lt;/p&gt;

</comment>
                            <comment id="13487640" author="robindhamankar" created="Wed, 31 Oct 2012 09:47:27 +0000"  >&lt;p&gt;Flavio, the index is also flushed in LedgerCacheImpl#grabCleanPage. We dont want to flush all ledgers and all entry logs as in case of InterleavedLedgerStore#flush just to make a clean page available. In general since when we have to order I/Os, it is better to have the other writes depend on the journal write since we write much more frequently to the journal and also journal writes are sequential making the I/O cheaper that relying on writes to ledger entries.&lt;/p&gt;</comment>
                            <comment id="13487755" author="hustlmsp" created="Wed, 31 Oct 2012 13:19:14 +0000"  >&lt;p&gt;Revisited the steps of adding entry in bookie server:&lt;br/&gt;
1) add entry to ledger storage; (add to entry logger then update ledger index entry)&lt;br/&gt;
2) add entry to journal queue;&lt;br/&gt;
3) journal thread flushes journal queue to commit entry to disks.&lt;br/&gt;
4) in journal&apos;s adding callback, it respond to client.&lt;/p&gt;

&lt;p&gt;so the entry is available for read after step 1) even the entry is not committed to journal. This behavior is OK for BookKeeper since there was last confirmed hint guarantee in BookKeeper.&lt;/p&gt;

&lt;p&gt;But it was not so safe to make an entry to be available for read before committing to journal. Imaging that it was K/V storage (not bookkeeper), it first adding a key to memory for read then commit to journal for persistence. After the key is in memory, which is readable to client, client would read the value of key. But if crashed happend before committing to journal, the storage restarts and the key is gone. client would not read the key again, which causes inconsistent state.&lt;/p&gt;

&lt;p&gt;A better sequence for adding entry for a journal-based storage would be:&lt;br/&gt;
1) added to journal queue first&lt;br/&gt;
2) journal thread committed the add operation to journal&lt;br/&gt;
3) in the callback of adding entry to journal, it put addEntry operation in a writer thread&apos;s queue.&lt;br/&gt;
4) the write thread adds entry to ledger storage. &lt;br/&gt;
5) respond to client.&lt;/p&gt;

&lt;p&gt;In such sequence, we just make the entry available for read only after it was safely committed to disk. It would avoid inconsistent state as described above and also address this issue here.&lt;/p&gt;

&lt;p&gt;Performance consideration:&lt;/p&gt;

&lt;p&gt;the original steps: the latency of an addEntry operation would be (latency of adding entry to ledger storage) + (latency of committing entry to journal).&lt;/p&gt;

&lt;p&gt;the changed steps: the latency of an addEntry operation would be (latency of committing entry to journal) + (latency of adding entry to ledger storage). Since we don&apos;t add entry to ledger storage directly in the callback committing entry to journal, we just put the addEntry operation in a writer&apos;s thread (as improvement introduced in &lt;a href=&quot;https://issues.apache.org/jira/browse/BOOKKEEPER-429&quot; title=&quot;Provide separate read and write threads in the bookkeeper server&quot; class=&quot;issue-link&quot; data-issue-key=&quot;BOOKKEEPER-429&quot;&gt;&lt;del&gt;BOOKKEEPER-429&lt;/del&gt;&lt;/a&gt;), the latency of committing entry is still same as the original one. so the total latency of an addEntry operation remains same.&lt;/p&gt;

&lt;p&gt;Complexity:&lt;/p&gt;

&lt;p&gt;it just needs to change the order of adding entry, which doesn&apos;t introduce any other code. (I assumed that we would have a separated write thread and have a queue for those pending addEntry operations, which would be introduced in &lt;a href=&quot;https://issues.apache.org/jira/browse/BOOKKEEPER-429&quot; title=&quot;Provide separate read and write threads in the bookkeeper server&quot; class=&quot;issue-link&quot; data-issue-key=&quot;BOOKKEEPER-429&quot;&gt;&lt;del&gt;BOOKKEEPER-429&lt;/del&gt;&lt;/a&gt;). And the benefit of this change would make the behavior predicate even encountering crashes.&lt;/p&gt;</comment>
                            <comment id="13487782" author="fpj" created="Wed, 31 Oct 2012 13:57:57 +0000"  >&lt;p&gt;I don&apos;t think this change is necessary, Sijie, exactly because of the point you make about not violating any of our guarantees. It is true that the change is not complex, but we are now adding more threads to the story and the change is not strictly necessary. &lt;/p&gt;</comment>
                            <comment id="13487810" author="hustlmsp" created="Wed, 31 Oct 2012 14:17:58 +0000"  >&lt;p&gt;Flavio,&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;but we are now adding more threads to the story and the change is not strictly necessary.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;As my comment, I assumed that a write thread would be added in &lt;a href=&quot;https://issues.apache.org/jira/browse/BOOKKEEPER-429&quot; title=&quot;Provide separate read and write threads in the bookkeeper server&quot; class=&quot;issue-link&quot; data-issue-key=&quot;BOOKKEEPER-429&quot;&gt;&lt;del&gt;BOOKKEEPER-429&lt;/del&gt;&lt;/a&gt; to separated read/write threads. so no more threads added to the story. It would be a clear fix for this issue.&lt;/p&gt;
</comment>
                            <comment id="13487816" author="fpj" created="Wed, 31 Oct 2012 14:24:43 +0000"  >&lt;p&gt;In this case, I would feel more comfortable if we first work on the jiras that have been mentioned here (&lt;a href=&quot;https://issues.apache.org/jira/browse/BOOKKEEPER-429&quot; title=&quot;Provide separate read and write threads in the bookkeeper server&quot; class=&quot;issue-link&quot; data-issue-key=&quot;BOOKKEEPER-429&quot;&gt;&lt;del&gt;BOOKKEEPER-429&lt;/del&gt;&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/BOOKKEEPER-432&quot; title=&quot;Improve performance of entry log range read per ledger entries &quot; class=&quot;issue-link&quot; data-issue-key=&quot;BOOKKEEPER-432&quot;&gt;&lt;del&gt;BOOKKEEPER-432&lt;/del&gt;&lt;/a&gt;) before making further progress here. Shall we mark this jira as related those other two jiras? &lt;/p&gt;</comment>
                            <comment id="13487824" author="robindhamankar" created="Wed, 31 Oct 2012 14:42:00 +0000"  >&lt;p&gt;Sijie, Not reading data that has not been persisted can be achieved without having to delay inserting to the index or the log files. WAL enforcement would associate a monotonically increasing sequence number with each batch of queue entries that are written to the journal and use this timestamp to detect if all entries in an index page have already been persisted. The same check that is used before the index is persisted can be used when the index is read if we want to provide readers isolation from data that is persisted. In the common case, the journal flushes will be ahead of the subscriber consumption (read) so we will basically not introduce any overhead. &lt;/p&gt;

&lt;p&gt;Flavio, I dont think we want to couple these with 429 and 432. Those are performance optimizations, this is correctness.   &lt;/p&gt;</comment>
                            <comment id="13487888" author="fpj" created="Wed, 31 Oct 2012 15:58:52 +0000"  >&lt;blockquote&gt;&lt;p&gt;I dont think we want to couple these with 429 and 432. Those are performance optimizations, this is correctness. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;m also not a big fan of mixing up performance and correctness, but the discussions in this jira are referencing other changes, so I can&apos;t really say what the best way is with all these changes in flight.&lt;/p&gt;</comment>
                            <comment id="13488373" author="hustlmsp" created="Thu, 1 Nov 2012 00:21:27 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Not reading data that has not been persisted can be achieved without having to delay inserting to the index or the log files.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Changing the order of committing to journal and  adding to ledger storage doesn&apos;t affect the time that an entry to be readable from BookKeeper client. since there was a semantic guarantee in BookKeeper that a client would not read an entry before the entry has been Acked succeed. Ack means the entry should be committed to journal at least before responding to client.&lt;/p&gt;</comment>
                            <comment id="13488376" author="hustlmsp" created="Thu, 1 Nov 2012 00:24:23 +0000"  >&lt;p&gt;wrote a draft patch of the idea changing the order of adding entry to ledger storage and committing it to journal. there is no test case added yet to reproduce the issue here.&lt;/p&gt;</comment>
                            <comment id="13488391" author="yx3zhu@gmail.com" created="Thu, 1 Nov 2012 00:52:18 +0000"  >&lt;p&gt;Sijie,&lt;br/&gt;
The patch you have will ends up stall index/entry log caching and flushing, which could well be done in parallel.&lt;br/&gt;
Robin is already working on group commit of ledgers (in separat thread). The approach I described initially is classical database WAL technique.&lt;br/&gt;
I think it is best to coordinate with Robin on addressing this issue, if you are eager to work on it.&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13488426" author="hustlmsp" created="Thu, 1 Nov 2012 02:40:20 +0000"  >&lt;p&gt;Yixue, I would say sorry if it made you uncomfortable. I am pleased that we could have great improvement using group commit idea as you proposal. I attached a draft just to show my idea about this issue since we are discussing different opinions for it, and I don&apos;t change the patch to be available since it was still a draft to show the idea and there are still different opinions about this problem. I am not to interrupt Robin&apos;s work.&lt;/p&gt;</comment>
                            <comment id="13488430" author="yx3zhu@gmail.com" created="Thu, 1 Nov 2012 03:01:56 +0000"  >&lt;p&gt;Sijie,&lt;br/&gt;
NP. I did not mean to discourage discussion.&lt;/p&gt;</comment>
                            <comment id="13497285" author="ikelly" created="Wed, 14 Nov 2012 18:01:07 +0000"  >&lt;p&gt;I think the root problem here is that the entrylog must be flushed monolithically, while the index files are flushed individually. This means to clear up space for the index files, we need to flush the whole entrylog, or else get the problem described. This, at its core, is the same problem. Basically, if all entries are interleaved, then it&apos;s impossible for a bookie to flush all entries associated with an index page, without flushing everything around them.&lt;/p&gt;

&lt;p&gt;I had been thinking of a solution for &lt;a href=&quot;https://issues.apache.org/jira/browse/BOOKKEEPER-432&quot; title=&quot;Improve performance of entry log range read per ledger entries &quot; class=&quot;issue-link&quot; data-issue-key=&quot;BOOKKEEPER-432&quot;&gt;&lt;del&gt;BOOKKEEPER-432&lt;/del&gt;&lt;/a&gt;, which is somewhat similar to Aniruddha&apos;s.&lt;/p&gt;

&lt;p&gt;Basically, we have a SlabAllocator, which has blocks of memory, maybe 8k in size. Each ledger has two slabs, the entrylog and index slab. Entries for a ledger are written to the entrylog slab, and then the offset is written to the index slab. &lt;/p&gt;

&lt;p&gt;For a normal flush, we go through all ledgers, flush the entrylog slab (long sequential write), and then the index slab for each of them (using the offset from the entrylog flush to calculate the real offsets). &lt;/p&gt;

&lt;p&gt;For a &quot;reclaim me some memory&quot;, we can flush a single entrylog slab, and then the index slab. Of course, in implementation it would be more complex, but the basic idea is that, for a single ledger, the entrylog segment is independent until the point that it is on the disk.&lt;/p&gt;</comment>
                            <comment id="13498139" author="yx3zhu@gmail.com" created="Thu, 15 Nov 2012 17:06:00 +0000"  >&lt;p&gt;HI Ivan,&lt;/p&gt;

&lt;p&gt;I have a patch for &lt;a href=&quot;https://issues.apache.org/jira/browse/BOOKKEEPER-432&quot; title=&quot;Improve performance of entry log range read per ledger entries &quot; class=&quot;issue-link&quot; data-issue-key=&quot;BOOKKEEPER-432&quot;&gt;&lt;del&gt;BOOKKEEPER-432&lt;/del&gt;&lt;/a&gt; rolled out internally, will post it shortly.&lt;br/&gt;
After some benchmark, we decided not go with Aniruddha&apos;s approach.&lt;/p&gt;

&lt;p&gt;I will also creates a separate jira to eliminate/reduce on-disk index (my original proposal to &lt;a href=&quot;https://issues.apache.org/jira/browse/BOOKKEEPER-432&quot; title=&quot;Improve performance of entry log range read per ledger entries &quot; class=&quot;issue-link&quot; data-issue-key=&quot;BOOKKEEPER-432&quot;&gt;&lt;del&gt;BOOKKEEPER-432&lt;/del&gt;&lt;/a&gt;), as it hurt sync/write through put. Suppose there are thousands of ledgers per bookie, sync potentially needs to flush thousands of index files.&lt;/p&gt;

</comment>
                            <comment id="13498140" author="yx3zhu@gmail.com" created="Thu, 15 Nov 2012 17:07:16 +0000"  >&lt;p&gt;Assign myself, as we can separate group commit with this jira.&lt;/p&gt;</comment>
                            <comment id="13499013" author="ikelly" created="Fri, 16 Nov 2012 18:49:20 +0000"  >&lt;p&gt;By &lt;a href=&quot;https://issues.apache.org/jira/browse/BOOKKEEPER-432&quot; title=&quot;Improve performance of entry log range read per ledger entries &quot; class=&quot;issue-link&quot; data-issue-key=&quot;BOOKKEEPER-432&quot;&gt;&lt;del&gt;BOOKKEEPER-432&lt;/del&gt;&lt;/a&gt;, do you mean the skiplist approach specifically? Or simply sorting before flushing? I have a flight next week, so I have a plan to try out the slab based approach. I&apos;ll let you know what I find.&lt;/p&gt;

&lt;p&gt;I was thinking about this JIRA again last night. I think it can be very easily be solved by a Semaphore, and removing the explicit flush in #grabCleanPage(). &lt;/p&gt;

&lt;p&gt;The crux of the problem here is that we flush an index file before the entrylogger has flushed. The entrylogger should be flushing every 100ms. The flush mechanism is, flush the entrylog, and then flush all index page. If we cannot grab a free index, it means that index pages are not currently being flushed, i.e. the entrylog is flushing or we&apos;re waiting for the next flush (this is doubtful, if we&apos;re running out of pages, flushes are taking longer than 100ms, so once one flush ends another begins immediately).&lt;/p&gt;

&lt;p&gt;Now, it is only safe to grab a clean page once the entrylog is flushed. Once the entrylog has finished flushing, it will start flushing pages and freeing them. Therefore, to grab a clean page, we just need to wait for the current running flush to free one rather than flushing it outselves.&lt;/p&gt;

&lt;p&gt;What will happen from the client perspective is that messages will queue while we&apos;re waiting for a free page. But this is fine, as the system is over capacity, so queuing is inevitable.&lt;/p&gt;</comment>
                            <comment id="13499254" author="yx3zhu@gmail.com" created="Fri, 16 Nov 2012 23:52:43 +0000"  >&lt;p&gt;I was referring to first-cut of &lt;a href=&quot;https://issues.apache.org/jira/browse/BOOKKEEPER-432&quot; title=&quot;Improve performance of entry log range read per ledger entries &quot; class=&quot;issue-link&quot; data-issue-key=&quot;BOOKKEEPER-432&quot;&gt;&lt;del&gt;BOOKKEEPER-432&lt;/del&gt;&lt;/a&gt;, skipList is used as caching layer to sort entries, before they make it to entry log or index buffers.&lt;br/&gt;
We have run benchmark, it is better (read throughput) than the slab-based approach which Aniruddah experimented with. Some of the details of slab-based approach may be different though.&lt;/p&gt;

&lt;p&gt;When we experimented with thousands of active ledgers per hub, the sync thread takes quite a hit while flushing (thousands of files). I am not sure if it is good idea to peg the sync interval as 100ms. &lt;/p&gt;
</comment>
                            <comment id="13499391" author="ikelly" created="Sat, 17 Nov 2012 10:58:09 +0000"  >&lt;blockquote&gt;&lt;p&gt;I am not sure if it is good idea to peg the sync interval as 100ms. &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;During heavy load, the sync interval is irrelevant, it&apos;s syncing constantly.&lt;/p&gt;

&lt;p&gt;How does the skipList approach do on write throughput/latency?&lt;/p&gt;</comment>
                            <comment id="13501301" author="yx3zhu@gmail.com" created="Tue, 20 Nov 2012 17:06:18 +0000"  >&lt;p&gt;The skipList approach does not impact write throughput, which will be tracked separately:&lt;br/&gt;
The write operation tends to compete with read on disk I/O. The interval should be larger by default, with adaptiveness built in.&lt;br/&gt;
We are going to eliminate/reduce index files write separately, to measure improvement on the work throughput. &lt;/p&gt;</comment>
                            <comment id="13508301" author="ikelly" created="Sun, 2 Dec 2012 16:46:42 +0000"  >&lt;p&gt;Implemented semaphore solution and added test case which triggers the problem without the flush. I haven&apos;t checked how it affects performance, but I think it will improve performance if anything, as a flush initialiated by the LedgerCacheImpl would interfere with an SyncThread flush, causing more disk head movement.&lt;/p&gt;</comment>
                            <comment id="13508302" author="ikelly" created="Sun, 2 Dec 2012 16:47:16 +0000"  >&lt;p&gt;Will test performance monday, when I have access to a dedicated disk.&lt;/p&gt;</comment>
                            <comment id="13508765" author="ikelly" created="Mon, 3 Dec 2012 14:36:12 +0000"  >&lt;p&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/12555757/12555757_perf.png&quot; align=&quot;absmiddle&quot; border=&quot;0&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The benchmark was run with bkvhbase benchmark &lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;. Entry size was 100, each run was 5 minutes. For each number of ledgers I ran 3 times.&lt;/p&gt;

&lt;p&gt;Graph is quite bumpy, but it shows that, using the attached patch, performance is actually better for 1, 10 &amp;amp; 10000 ledgers, and a bit worse for 100 &amp;amp; 1000 ledgers. Previous tests have running against the complete bookie gives a max tpt of 108k&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt; so all these numbers are much better. &lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; &lt;a href=&quot;https://github.com/ivankelly/bkvhbase&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/ivankelly/bkvhbase&lt;/a&gt;&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt; Running against a complete bookie means having to write to WAL first. This will slow us down, as we lose a degree of batching&lt;/p&gt;</comment>
                            <comment id="13509293" author="fpj" created="Mon, 3 Dec 2012 22:59:25 +0000"  >&lt;p&gt;I have a few comments and questions here:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;I&apos;m not sure what this is doing:&lt;/li&gt;
&lt;/ul&gt;


&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+                        cleanPages.tryAcquire(100, TimeUnit.MILLISECONDS);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;if the 100 ms elapses and there has been no release, then it means that there is no clean page, no?&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Should we rename BookieTest to something less general, like BookieLedgerIndexTest?&lt;/li&gt;
	&lt;li&gt;Check the javadoc for testIndexPageEviction(), it has typos.&lt;/li&gt;
	&lt;li&gt;There is this comment in this same test:&lt;/li&gt;
&lt;/ul&gt;


&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;// don&apos;t start the bookie, this way sync thread wont run
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;but the code does start the bookie right after. Is this correct? Is it referring to second occurrence of &quot;new Bookie&quot;? &lt;/p&gt;</comment>
                            <comment id="13509430" author="hustlmsp" created="Tue, 4 Dec 2012 02:02:59 +0000"  >&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+                        cleanPages.tryAcquire(100, TimeUnit.MILLISECONDS);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Should this time interval align with sync thread time interval?&lt;/p&gt;</comment>
                            <comment id="13509833" author="ikelly" created="Tue, 4 Dec 2012 16:25:07 +0000"  >&lt;p&gt;Having looked at it again, the patch isn&apos;t safe. Theres no guarantee that the thread acquiring the semaphore will be the one which takes the freed page (it&apos;s guaranteed by being single threaded, but that should change). I&apos;ll need to take another look (it really should be as simple as using a semaphore, but LedgerCacheImpl is far more complex than it needs to be).&lt;/p&gt;</comment>
                            <comment id="13530089" author="ikelly" created="Wed, 12 Dec 2012 16:43:04 +0000"  >&lt;p&gt;New patch gets rid of the semaphore, and just uses notify()/wait(). The semaphore was unnecessary as LedgerCacheImpl does its own bookkeeping on which pages are clean and which are dirty.&lt;/p&gt;

&lt;p&gt;Basically, if there are no clean pages available, we wait for 100 ms, and then see if we can find any. It&apos;s kind of busy waiting, but the 100 ms will stop it going into a tight loop.&lt;/p&gt;</comment>
                            <comment id="13530721" author="hustlmsp" created="Thu, 13 Dec 2012 06:19:20 +0000"  >&lt;p&gt;Although this fix is quite simple and straightforward, I still could not be convinced using this solution. Because all writing requests are blocked until a flush is scheduled by SyncThread, so the latency heavily depends on the &lt;b&gt;flushInterval&lt;/b&gt;. Also, it just resolve the problem force flushing ledger when grabbing a clean page. but if sync thread happens to flush the ledger before journal flushes, same problem occurred. &lt;/p&gt;

&lt;p&gt;the right direction resolving this issue is that we should guarantee journal flushing before ledger index flushed. A possible way is to add a listener on Journal, when it flushed entires, it notified LedgerStorage which ledger is flushed. so LedgerStorage could know which ledger page it should flush, which page it should not flush.&lt;/p&gt;

&lt;p&gt;so the waiting time to grab a clean page is different for different solution, either depends on journal flushing latency or sync thread flush interval. journal flushing latency supposed to be very very low since it was used as a separated disk. but sync thread flush interval depends on user&apos;s setting and also it was a shared disk with read requests.&lt;/p&gt;
</comment>
                            <comment id="13530887" author="ikelly" created="Thu, 13 Dec 2012 10:40:49 +0000"  >
&lt;p&gt;I&apos;ve uploaded a small tweak to the patch, but the basic principle stays the same. &lt;/p&gt;

&lt;p&gt;The only time you will wait is if there are no clean pages available. There will only be no clean pages available if there is heavy write load on the server. In this case, the server can be in one of 2 states,  a) flushing the entrylog, and b) flushing the ledger. &lt;/p&gt;

&lt;p&gt;If the server is in a) it is not safe to flush the page, the corresponding entry log entries may not have been flushed, and also if you did flush you&apos;d be interrupting the sequential write.&lt;/p&gt;

&lt;p&gt;If the server is in b) then ledger pages are already being flushed, you will only wait until a single ledger page has been marked as clean to try and acquire one again.&lt;/p&gt;

&lt;p&gt;You could have a situation where the flushInterval is configured very high, and in that case it would be good to trigger a flush when capacity is low, but in that case, I think it would be better to trigger a full flush, entrylog and ledgers. I think taking Yixue&apos;s CacheCallback solution from &lt;a href=&quot;https://issues.apache.org/jira/browse/BOOKKEEPER-432&quot; title=&quot;Improve performance of entry log range read per ledger entries &quot; class=&quot;issue-link&quot; data-issue-key=&quot;BOOKKEEPER-432&quot;&gt;&lt;del&gt;BOOKKEEPER-432&lt;/del&gt;&lt;/a&gt; would fix that (basically to take the changes from Bookie.java). However, I&apos;m not sure this is necessary for 4.2.0. For 4.2.0 we need to prevent the flush occurring before the journal write. This patch does that. 4.3.0 will include major ledger storage changes anyhow, so it may be best to leave the CacheCallback until then.&lt;/p&gt;
</comment>
                            <comment id="13531304" author="hustlmsp" created="Thu, 13 Dec 2012 18:37:49 +0000"  >&lt;p&gt;CacheCallback is required for a skip list solution in &lt;a href=&quot;https://issues.apache.org/jira/browse/BOOKKEEPER-432&quot; title=&quot;Improve performance of entry log range read per ledger entries &quot; class=&quot;issue-link&quot; data-issue-key=&quot;BOOKKEEPER-432&quot;&gt;&lt;del&gt;BOOKKEEPER-432&lt;/del&gt;&lt;/a&gt;, because you had to flush the buffered entries to make spaces for newly added entries. You could use CacheCallback here to notify sync thread to force a full flush or just force flush entry logger, after that it is safe to force flush an ledger to steal its ledger cache page. I think it would be better than a busy-waiting, although it still not so perfect.&lt;/p&gt;
</comment>
                            <comment id="13531319" author="hustlmsp" created="Thu, 13 Dec 2012 18:46:18 +0000"  >&lt;blockquote&gt;
&lt;p&gt;For 4.2.0 we need to prevent the flush occurring before the journal write.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;One more point, I don&apos;t think you solution prevent flushing occurring before journal write. You just prevent flushing a ledger index before flushing entry logger, since flushing ledger storage and flushing journal are different threads. ledger storage had no idea whether its entry is committed to journal or not. &lt;/p&gt;

&lt;p&gt;The result of your fix is just guaranteeing an entry is persisted in ledger storage having its index entry and its data in entry log file, so we wouldn&apos;t got IOException when accessing it. It doesn&apos;t do as described.&lt;/p&gt;</comment>
                            <comment id="13531540" author="ikelly" created="Thu, 13 Dec 2012 21:37:46 +0000"  >&lt;p&gt;Actually, even requesting a flush of just the entrylog and then flushing the ledger page doesn&apos;t guarantee anything, as we don&apos;t use double buffering. It&apos;s possible that we could flush the entrylog, an entry is added to the ledger page, and then we flush the ledger page. Unless we put a big lock around everything which is ugly as hell and couples this stuff in a way I really dont like.&lt;/p&gt;

&lt;p&gt;How about another approach (not sure if it&apos;s been suggested before), that if we find an entry in the index and then we can&apos;t find it in the entrylog, we throw an exception the same way as if we never found it in the index?&lt;/p&gt;</comment>
                            <comment id="13532310" author="ikelly" created="Fri, 14 Dec 2012 14:07:42 +0000"  >&lt;p&gt;Adding patch containing solution where we just throw the NoEntryException if we get an IOException from the entrylogger trying to read an offset that doesn&apos;t exist.&lt;/p&gt;</comment>
                            <comment id="13532871" author="hustlmsp" created="Sat, 15 Dec 2012 01:35:31 +0000"  >&lt;p&gt;-1 for this patch. since I don&apos;t like the idea converting IOException to NoEntryException, actually IOException might means a valid add entry but it corrupts due to disk bad sectors. also, NoEntryException is a critical exception that we used for the termination condition for ledger recovery (see &lt;a href=&quot;https://issues.apache.org/jira/browse/BOOKKEEPER-365&quot; title=&quot;Ledger will never recover if one of the quorum bookie is down forever and others dont have entry&quot; class=&quot;issue-link&quot; data-issue-key=&quot;BOOKKEEPER-365&quot;&gt;&lt;del&gt;BOOKKEEPER-365&lt;/del&gt;&lt;/a&gt;). It would be better not to equal IOException to NoEntryException.&lt;/p&gt;</comment>
                            <comment id="13532874" author="hustlmsp" created="Sat, 15 Dec 2012 01:40:10 +0000"  >&lt;p&gt;Attach a patch to flush entry logger before flushing ledger index. Although it is not the best and graceful way to resolve it, it is better than previous patches.&lt;/p&gt;

&lt;p&gt;The patch also fixed Ivan&apos;s test case. &lt;/p&gt;</comment>
                            <comment id="13532975" author="hustlmsp" created="Sat, 15 Dec 2012 09:56:03 +0000"  >&lt;p&gt;a draft patch based on yixue and robin previous discussion&apos;s idea. It used a BitSet to track the sync status of its journal entries. only flush a dirty page when the BitSet is empty (all entries in this page are synced to journal).&lt;/p&gt;

&lt;p&gt;why BitSet? it is a bit trouble is although we adding entries in order, but this order is preserved by client not a bookie. the entries added in a bookie would be in any order due to retry and change ensemble logic. so it is not safe to use something like last entry id to track the progress of committing a ledger&apos;s entries to journal.&lt;/p&gt;

&lt;p&gt;Also this patch improved ledger flushing to prevent updating a ledger page when flushing it. updating a ledger page when flushing, which would cause unsynced journal entry&apos;s index is flushed. this is an already existed bug in current ledger flush, which would cause this issue even not force flush ledger when grabbing a clean page.&lt;/p&gt;

&lt;p&gt;this patch passed existed test cases. I haven&apos;t added test case for it. but I think we might need to cover more cases when adding tests, especially the issues I described in above paragraphs.&lt;/p&gt;

&lt;p&gt;BTW: there is an interesting project called hawtjournal. I followed its JournalListener idea in this patch.(&lt;a href=&quot;https://github.com/fusesource/hawtjournal/blob/master/src/main/java/org/fusesource/hawtjournal/api/JournalListener.java&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/fusesource/hawtjournal/blob/master/src/main/java/org/fusesource/hawtjournal/api/JournalListener.java&lt;/a&gt;)&lt;/p&gt;</comment>
                            <comment id="13532977" author="hustlmsp" created="Sat, 15 Dec 2012 10:05:37 +0000"  >&lt;p&gt;As I described in the previous comment, currently flushing page is not safe: sync thread flushed a page, other thread adds offset in the page. sync thread brings uncommitted entry (neither journal nor entry log) to index page. Any final solution is required to fix this potential issue, otherwise it is an unsafe solution.&lt;/p&gt;</comment>
                            <comment id="13532983" author="hadoopqa" created="Sat, 15 Dec 2012 10:22:33 +0000"  >&lt;p&gt;Testing JIRA &lt;a href=&quot;https://issues.apache.org/jira/browse/BOOKKEEPER-447&quot; title=&quot;Bookie can fail to recover if index pages flushed before ledger flush acknowledged&quot; class=&quot;issue-link&quot; data-issue-key=&quot;BOOKKEEPER-447&quot;&gt;&lt;del&gt;BOOKKEEPER-447&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;Patch &lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12561103/BOOKKEEPER-447_bitset.diff&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;BOOKKEEPER-447_bitset.diff&lt;/a&gt; downloaded at Sat Dec 15 10:01:27 UTC 2012&lt;/p&gt;

&lt;p&gt;----------------------------&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;green&quot;&gt;+1 PATCH_APPLIES&lt;/font&gt;&lt;br/&gt;
&lt;font color=&quot;green&quot;&gt;+1 CLEAN&lt;/font&gt;&lt;br/&gt;
&lt;font color=&quot;red&quot;&gt;-1 RAW_PATCH_ANALYSIS&lt;/font&gt;&lt;br/&gt;
.    &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; the patch does not introduce any @author tags&lt;br/&gt;
.    &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; the patch does not introduce any tabs&lt;br/&gt;
.    &lt;font color=&quot;red&quot;&gt;-1&lt;/font&gt; the patch contains 3 line(s) with trailing spaces&lt;br/&gt;
.    &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; the patch does not introduce any line longer than 120&lt;br/&gt;
.    &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; the patch does adds/modifies 1 testcase(s)&lt;br/&gt;
&lt;font color=&quot;green&quot;&gt;+1 RAT&lt;/font&gt;&lt;br/&gt;
.    &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; the patch does not seem to introduce new RAT warnings&lt;br/&gt;
&lt;font color=&quot;green&quot;&gt;+1 JAVADOC&lt;/font&gt;&lt;br/&gt;
.    &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; the patch does not seem to introduce new Javadoc warnings&lt;br/&gt;
&lt;font color=&quot;green&quot;&gt;+1 COMPILE&lt;/font&gt;&lt;br/&gt;
.    &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; HEAD compiles&lt;br/&gt;
.    &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; patch compiles&lt;br/&gt;
.    &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; the patch does not seem to introduce new javac warnings&lt;br/&gt;
&lt;font color=&quot;green&quot;&gt;+1 FINDBUGS&lt;/font&gt;&lt;br/&gt;
.    &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; the patch does not seem to introduce new Findbugs warnings&lt;br/&gt;
&lt;font color=&quot;red&quot;&gt;-1 TESTS&lt;/font&gt;&lt;br/&gt;
.    Tests run: 499&lt;br/&gt;
.    Tests failed: 0&lt;br/&gt;
.    Tests errors: 4&lt;/p&gt;

&lt;p&gt;.    The patch failed the following testcases:&lt;/p&gt;

&lt;p&gt;.      &lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;green&quot;&gt;+1 DISTRO&lt;/font&gt;&lt;br/&gt;
.    &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; distro tarball builds with the patch &lt;/p&gt;

&lt;p&gt;----------------------------&lt;br/&gt;
&lt;font color=&quot;red&quot;&gt;&lt;b&gt;-1 Overall result, please check the reported -1(s)&lt;/b&gt;&lt;/font&gt;&lt;/p&gt;


&lt;p&gt;The full output of the test-patch run is available at&lt;/p&gt;

&lt;p&gt;.   &lt;a href=&quot;https://builds.apache.org/job/bookkeeper-trunk-precommit-build/129/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/bookkeeper-trunk-precommit-build/129/&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13532996" author="ikelly" created="Sat, 15 Dec 2012 11:39:13 +0000"  >&lt;p&gt;I prefer the entrylog flush approach to the bitset approach, but I don&apos;t particularly like either as the both introduce a lot of coupling. At least with the entrylog approach, the coupling can be hidden behind the LedgerStorage interface and the Journal doesn&apos;t need to be touched. With the bitset approach you can&apos;t exercise the production LedgerStorage paths without having a journal running at the same time. The entrylog approach can be cleaned up to reduce coupling also, by adding EntryFlusher interface, I&apos;ll attach an patch for that.&lt;/p&gt;

&lt;p&gt;Regarding the IOException patch, to a client how is it any different if the bookie couldn&apos;t find a entry, or a bookie had an I/O error reading an entry? Does it change the behaviour the client will take?&lt;/p&gt;</comment>
                            <comment id="13532998" author="ikelly" created="Sat, 15 Dec 2012 11:47:10 +0000"  >&lt;p&gt;Actually, the entrylog approach won&apos;t work because of the lack of double buffering. Also, the bitset approach could lead to a situation where your spinning where pages are getting updated as fast as the journal is marking them as flushed.&lt;/p&gt;

&lt;p&gt;Really the solution for this is double buffering, which will come in with the skiplist approach.&lt;/p&gt;</comment>
                            <comment id="13533040" author="ikelly" created="Sat, 15 Dec 2012 14:51:50 +0000"  >&lt;p&gt;Going back to the IOException, we can even distinguish between disk corruptions and short reads at the EntryLogger level. EntryLogger#readEntry throws 4 IOExceptions apart from those which may be thrown from the file channel. 2 are for short data reads, and 2 are for the ledger id and entry id being incorrect. These could all be replaced with NoEntryException. &lt;/p&gt;</comment>
                            <comment id="13533101" author="ikelly" created="Sat, 15 Dec 2012 18:39:45 +0000"  >&lt;p&gt;Patch throws NoEntryException on short read from the entry log, IOException in all other cases. The entrylog doesn&apos;t prewrite, so this is safe.&lt;/p&gt;</comment>
                            <comment id="13533571" author="hustlmsp" created="Sun, 16 Dec 2012 23:46:45 +0000"  >&lt;p&gt;I still can&apos;t be convinced by NoEntryException solution. As NoEntryException is treated as the termination condition for ledger recovery. you might end up mixing IOException of a valid entry with NoSuchEntry. In honestly, it is difficult to say what caused short read.&lt;/p&gt;

&lt;p&gt;I am more comfortable on the solutions making data flushing only after journal committed, which is a more traditional way for a WAL implementation. Even using skipList solution, you still could not flush (maybe flush is not the best concept) skipList before journal commits, otherwise it still volatile WAL contract. If you worry about ledger storage and journal are too coupled in this way, why not use my first solution converting the order, committing journal first, adding entry to ledger storage later (&lt;a href=&quot;https://issues.apache.org/jira/browse/BOOKKEEPER-447?focusedCommentId=13487755&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13487755)?&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/BOOKKEEPER-447?focusedCommentId=13487755&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13487755)?&lt;/a&gt; I don&apos;t see any drawbacks of this solution so far for now and it would not make the code coupled.&lt;/p&gt;

&lt;p&gt;These are just my preferences. I have to say that I am fine with NoSuchEntry solution. But I would keep my +1 until I am convinced NoSuchEntryException is a safe way. Would like to see others&apos; opinions.&lt;/p&gt;</comment>
                            <comment id="13533776" author="ikelly" created="Mon, 17 Dec 2012 09:42:42 +0000"  >&lt;blockquote&gt;&lt;p&gt;I still can&apos;t be convinced by NoEntryException solution. As NoEntryException is treated as the termination condition for ledger recovery. you might end up mixing IOException of a valid entry with NoSuchEntry. In honestly, it is difficult to say what caused short read.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;In the latest patch, there are two conditions in which IOException now throws NoEntryException instead. The first one is where you try to read the entry length, and you don&apos;t get a full integer. The second one is where you try to read the entry and the number of bytes read is shorter than the number requests. Both these are consistent with the case where the entry simply hasn&apos;t been added to the entrylog. In the case of a bad segment, or FS corruption, the FileChannel#read itself would fail with a IOException, so we&apos;re not masking those errors. If the data on disk is corrupt, the check for the ledger id &amp;amp; entry id will fail with IOException so we&apos;re not masking that either. Finally, if the data is corrupt, the digest check will pick this up on the client side.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;&lt;br/&gt;
If you worry about ledger storage and journal are too coupled in this way, why not use my first solution converting the order, committing journal first, adding entry to ledger storage later&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I actually like this solution most of all, because it hits the core issue. We should benchmark to ensure it doesn&apos;t hurt performance. Also, I&apos;d make a few changes to the patch you provided. You only complete the client callback after adding to the ledger storage. I can see the reason for this (client will throttle if there&apos;s many outstanding ops), but this doesn&apos;t do anything for the case where many clients are writing a lot. It would be better to start queueing requests if the bookie is overwhelmed. This may cause some timeout errors, but this is good. It means the clients will start moving off the overwhelmed machine, to hopefully less loaded machines.&lt;/p&gt;

&lt;p&gt;At the moment, the ledger storage acts as the throttler effectively. If adds are going too fast, it will start flushing ledgers and waiting for new pages to become available. Changing the order removes that, so perhaps we should actually put a limit on the size of Journal#queue. Also, I would make the write thread a plain thread and use a queue to push requests to it. It would avoid the construction of a lot of Runnable objects.&lt;/p&gt;

&lt;p&gt;I&apos;m fine with either the exception based solution or the order swap solution. The first one is a smaller change, while the second could have larger side effects, so I&apos;m leaning towards the first, but I&apos;m willing to have my mind changed.&lt;/p&gt;</comment>
                            <comment id="13534666" author="hustlmsp" created="Tue, 18 Dec 2012 05:35:33 +0000"  >&lt;blockquote&gt;
&lt;p&gt;The second one is where you try to read the entry and the number of bytes read is shorter than the number requests.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;if the length field is corrupted with a larger number? maybe it is a corner case here. so If you are OK with allowing such case. I am fine with changing IOException to NoSuchEntryException, since it is indeed a minor change.&lt;/p&gt;</comment>
                            <comment id="13534819" author="ikelly" created="Tue, 18 Dec 2012 10:57:44 +0000"  >&lt;p&gt;I don&apos;t think there&apos;s much we can do about arbitrary bitflip failures like this except depend on the fact that we have a quorum which makes the probability of such a failure much much lower.&lt;/p&gt;</comment>
                            <comment id="13535660" author="hustlmsp" created="Wed, 19 Dec 2012 05:31:13 +0000"  >&lt;p&gt;I am fine with your explanation. but could you put some comments when throwing NoSuchEntry to list the concern about NoSuchEntry since it would be used as a condition for ledger recovery.&lt;/p&gt;

&lt;p&gt;besides that, your test code seems not right.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+        &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; i = 1; i &amp;lt;= numLedgers; i++) {
+            ByteBuffer packet = generateEntry(i, 1);
+            b.addEntry(packet, &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Bookie.NopWriteCallback(), &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;passwd&quot;&lt;/span&gt;.getBytes());
+        }
+
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+        &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; i = 0; i &amp;lt; numLedgers; i++) {
+            &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
+                b.readEntry(i, 1);
+            } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (Bookie.NoLedgerException nle) {
+                &lt;span class=&quot;code-comment&quot;&gt;// &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; is fine
&lt;/span&gt;+            } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (Bookie.NoEntryException nee) {
+                &lt;span class=&quot;code-comment&quot;&gt;// &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; is fine
&lt;/span&gt;+            } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (IOException ioe) {
+                LOG.info(&lt;span class=&quot;code-quote&quot;&gt;&quot;Shouldn&apos;t have received IOException&quot;&lt;/span&gt;, ioe);
+                fail(&lt;span class=&quot;code-quote&quot;&gt;&quot;Shouldn&apos;t &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; IOException, should say that entry is not found&quot;&lt;/span&gt;);
+            }
+        }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;you added ledger starting from 1 to numLedgers, but read ledgers starting from 0.. besides that, you should not catch NoLedgerException. because from the test, it supposed to not throw NoLedgerException.&lt;/p&gt;</comment>
                            <comment id="13536255" author="ikelly" created="Wed, 19 Dec 2012 18:32:41 +0000"  >&lt;p&gt;Bookie.NoLedgerException can occur if the ledger never writes an index file. Note that this is different to BKException.NoSuchLedgerException, which is only triggered by the ledger not existing in ZK.&lt;/p&gt;

&lt;p&gt;I&apos;ve addressed the remaining comments.&lt;/p&gt;</comment>
                            <comment id="13536312" author="hadoopqa" created="Wed, 19 Dec 2012 19:02:58 +0000"  >&lt;p&gt;Testing JIRA &lt;a href=&quot;https://issues.apache.org/jira/browse/BOOKKEEPER-447&quot; title=&quot;Bookie can fail to recover if index pages flushed before ledger flush acknowledged&quot; class=&quot;issue-link&quot; data-issue-key=&quot;BOOKKEEPER-447&quot;&gt;&lt;del&gt;BOOKKEEPER-447&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;Patch &lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12561759/0001-BOOKKEEPER-447-EntryLog-throws-NoSuchEntry-on-short-.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;0001-BOOKKEEPER-447-EntryLog-throws-NoSuchEntry-on-short-.patch&lt;/a&gt; downloaded at Wed Dec 19 18:41:25 UTC 2012&lt;/p&gt;

&lt;p&gt;----------------------------&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;green&quot;&gt;+1 PATCH_APPLIES&lt;/font&gt;&lt;br/&gt;
&lt;font color=&quot;green&quot;&gt;+1 CLEAN&lt;/font&gt;&lt;br/&gt;
&lt;font color=&quot;green&quot;&gt;+1 RAW_PATCH_ANALYSIS&lt;/font&gt;&lt;br/&gt;
.    &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; the patch does not introduce any @author tags&lt;br/&gt;
.    &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; the patch does not introduce any tabs&lt;br/&gt;
.    &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; the patch does not introduce any trailing spaces&lt;br/&gt;
.    &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; the patch does not introduce any line longer than 120&lt;br/&gt;
.    &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; the patch does adds/modifies 1 testcase(s)&lt;br/&gt;
&lt;font color=&quot;green&quot;&gt;+1 RAT&lt;/font&gt;&lt;br/&gt;
.    &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; the patch does not seem to introduce new RAT warnings&lt;br/&gt;
&lt;font color=&quot;green&quot;&gt;+1 JAVADOC&lt;/font&gt;&lt;br/&gt;
.    &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; the patch does not seem to introduce new Javadoc warnings&lt;br/&gt;
&lt;font color=&quot;green&quot;&gt;+1 COMPILE&lt;/font&gt;&lt;br/&gt;
.    &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; HEAD compiles&lt;br/&gt;
.    &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; patch compiles&lt;br/&gt;
.    &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; the patch does not seem to introduce new javac warnings&lt;br/&gt;
&lt;font color=&quot;green&quot;&gt;+1 FINDBUGS&lt;/font&gt;&lt;br/&gt;
.    &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; the patch does not seem to introduce new Findbugs warnings&lt;br/&gt;
&lt;font color=&quot;green&quot;&gt;+1 TESTS&lt;/font&gt;&lt;br/&gt;
.    Tests run: 506&lt;br/&gt;
&lt;font color=&quot;green&quot;&gt;+1 DISTRO&lt;/font&gt;&lt;br/&gt;
.    &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; distro tarball builds with the patch &lt;/p&gt;

&lt;p&gt;----------------------------&lt;br/&gt;
&lt;font color=&quot;green&quot;&gt;&lt;b&gt;+1 Overall result, good!, no -1s&lt;/b&gt;&lt;/font&gt;&lt;/p&gt;


&lt;p&gt;The full output of the test-patch run is available at&lt;/p&gt;

&lt;p&gt;.   &lt;a href=&quot;https://builds.apache.org/job/bookkeeper-trunk-precommit-build/158/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/bookkeeper-trunk-precommit-build/158/&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13536778" author="hustlmsp" created="Thu, 20 Dec 2012 04:57:52 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Bookie.NoLedgerException can occur if the ledger never writes an index file. Note that this is different to BKException.NoSuchLedgerException, which is only triggered by the ledger not existing in ZK.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;In the test case, you have limited pageLimit to 1. so &lt;span class=&quot;error&quot;&gt;&amp;#91;1 - (numLedgers - 1)&amp;#93;&lt;/span&gt; should be already flushed to index, only the last ledger doesn&apos;t flush. so you checked 1 to (numLedgers - 1), there should be no NoLedgerException. It is better to make it explicit not just catch all.&lt;/p&gt;</comment>
                            <comment id="13537100" author="ikelly" created="Thu, 20 Dec 2012 15:58:31 +0000"  >&lt;p&gt;test now explicit checks that NoLedgerException only comes for final ledger.&lt;/p&gt;</comment>
                            <comment id="13537143" author="hadoopqa" created="Thu, 20 Dec 2012 16:45:01 +0000"  >&lt;p&gt;Testing JIRA &lt;a href=&quot;https://issues.apache.org/jira/browse/BOOKKEEPER-447&quot; title=&quot;Bookie can fail to recover if index pages flushed before ledger flush acknowledged&quot; class=&quot;issue-link&quot; data-issue-key=&quot;BOOKKEEPER-447&quot;&gt;&lt;del&gt;BOOKKEEPER-447&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;Patch &lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12561910/0001-BOOKKEEPER-447-EntryLog-throws-NoSuchEntry-on-short-.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;0001-BOOKKEEPER-447-EntryLog-throws-NoSuchEntry-on-short-.patch&lt;/a&gt; downloaded at Thu Dec 20 16:23:33 UTC 2012&lt;/p&gt;

&lt;p&gt;----------------------------&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;green&quot;&gt;+1 PATCH_APPLIES&lt;/font&gt;&lt;br/&gt;
&lt;font color=&quot;green&quot;&gt;+1 CLEAN&lt;/font&gt;&lt;br/&gt;
&lt;font color=&quot;green&quot;&gt;+1 RAW_PATCH_ANALYSIS&lt;/font&gt;&lt;br/&gt;
.    &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; the patch does not introduce any @author tags&lt;br/&gt;
.    &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; the patch does not introduce any tabs&lt;br/&gt;
.    &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; the patch does not introduce any trailing spaces&lt;br/&gt;
.    &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; the patch does not introduce any line longer than 120&lt;br/&gt;
.    &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; the patch does adds/modifies 1 testcase(s)&lt;br/&gt;
&lt;font color=&quot;green&quot;&gt;+1 RAT&lt;/font&gt;&lt;br/&gt;
.    &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; the patch does not seem to introduce new RAT warnings&lt;br/&gt;
&lt;font color=&quot;green&quot;&gt;+1 JAVADOC&lt;/font&gt;&lt;br/&gt;
.    &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; the patch does not seem to introduce new Javadoc warnings&lt;br/&gt;
&lt;font color=&quot;green&quot;&gt;+1 COMPILE&lt;/font&gt;&lt;br/&gt;
.    &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; HEAD compiles&lt;br/&gt;
.    &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; patch compiles&lt;br/&gt;
.    &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; the patch does not seem to introduce new javac warnings&lt;br/&gt;
&lt;font color=&quot;green&quot;&gt;+1 FINDBUGS&lt;/font&gt;&lt;br/&gt;
.    &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; the patch does not seem to introduce new Findbugs warnings&lt;br/&gt;
&lt;font color=&quot;green&quot;&gt;+1 TESTS&lt;/font&gt;&lt;br/&gt;
.    Tests run: 506&lt;br/&gt;
&lt;font color=&quot;green&quot;&gt;+1 DISTRO&lt;/font&gt;&lt;br/&gt;
.    &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; distro tarball builds with the patch &lt;/p&gt;

&lt;p&gt;----------------------------&lt;br/&gt;
&lt;font color=&quot;green&quot;&gt;&lt;b&gt;+1 Overall result, good!, no -1s&lt;/b&gt;&lt;/font&gt;&lt;/p&gt;


&lt;p&gt;The full output of the test-patch run is available at&lt;/p&gt;

&lt;p&gt;.   &lt;a href=&quot;https://builds.apache.org/job/bookkeeper-trunk-precommit-build/162/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/bookkeeper-trunk-precommit-build/162/&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13539191" author="hustlmsp" created="Mon, 24 Dec 2012 05:07:51 +0000"  >&lt;p&gt;lgtm +1.&lt;/p&gt;</comment>
                            <comment id="13539194" author="hustlmsp" created="Mon, 24 Dec 2012 05:23:50 +0000"  >&lt;p&gt;committed as r1425588. thanks Ivan. thanks all guys involving the discussion. &lt;/p&gt;</comment>
                            <comment id="13539195" author="hustlmsp" created="Mon, 24 Dec 2012 05:26:22 +0000"  >&lt;p&gt;attach the new patch resolving the confliction when committing.&lt;/p&gt;</comment>
                            <comment id="13539226" author="hudson" created="Mon, 24 Dec 2012 09:40:22 +0000"  >&lt;p&gt;Integrated in bookkeeper-trunk2 #9 (See &lt;a href=&quot;https://builds.apache.org/job/bookkeeper-trunk2/9/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/bookkeeper-trunk2/9/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/BOOKKEEPER-447&quot; title=&quot;Bookie can fail to recover if index pages flushed before ledger flush acknowledged&quot; class=&quot;issue-link&quot; data-issue-key=&quot;BOOKKEEPER-447&quot;&gt;&lt;del&gt;BOOKKEEPER-447&lt;/del&gt;&lt;/a&gt;: Bookie can fail to recover if index pages flushed before ledger flush acknowledged (ivank via sijie) (Revision 1425588)&lt;/p&gt;

&lt;p&gt;     Result = SUCCESS&lt;br/&gt;
sijie : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/zookeeper/bookkeeper/trunk/CHANGES.txt&lt;/li&gt;
	&lt;li&gt;/zookeeper/bookkeeper/trunk/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/Bookie.java&lt;/li&gt;
	&lt;li&gt;/zookeeper/bookkeeper/trunk/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/EntryLogger.java&lt;/li&gt;
	&lt;li&gt;/zookeeper/bookkeeper/trunk/bookkeeper-server/src/test/java/org/apache/bookkeeper/bookie/LedgerCacheTest.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    </comments>
                    <attachments>
                            <attachment id="12561910" name="0001-BOOKKEEPER-447-EntryLog-throws-NoSuchEntry-on-short-.patch" size="9965" author="ikelly" created="Thu, 20 Dec 2012 15:58:31 +0000"/>
                            <attachment id="12561759" name="0001-BOOKKEEPER-447-EntryLog-throws-NoSuchEntry-on-short-.patch" size="9841" author="ikelly" created="Wed, 19 Dec 2012 18:31:15 +0000"/>
                            <attachment id="12561137" name="0001-BOOKKEEPER-447-EntryLog-throws-NoSuchEntry-on-short-.patch" size="9120" author="ikelly" created="Sat, 15 Dec 2012 18:39:45 +0000"/>
                            <attachment id="12560746" name="0001-BOOKKEEPER-447-LedgerCacheImpl-waits-on-lock-object-.patch" size="8875" author="ikelly" created="Thu, 13 Dec 2012 10:10:36 +0000"/>
                            <attachment id="12560591" name="0001-BOOKKEEPER-447-LedgerCacheImpl-waits-on-lock-object-.patch" size="8256" author="ikelly" created="Wed, 12 Dec 2012 16:43:04 +0000"/>
                            <attachment id="12555674" name="0001-BOOKKEEPER-447-LedgerCacheImpl-waits-on-semaphore-no.patch" size="9651" author="ikelly" created="Sun, 2 Dec 2012 16:46:42 +0000"/>
                            <attachment id="12560972" name="0001-BOOKKEEPER-447-Throw-NoSuchEntry-if-entry-is-not-fou.patch" size="7483" author="ikelly" created="Fri, 14 Dec 2012 14:07:42 +0000"/>
                            <attachment id="12562308" name="BOOKKEEPER-447.diff" size="8955" author="hustlmsp" created="Mon, 24 Dec 2012 05:26:22 +0000"/>
                            <attachment id="12551652" name="BOOKKEEPER-447.diff" size="7257" author="hustlmsp" created="Thu, 1 Nov 2012 00:24:23 +0000"/>
                            <attachment id="12561103" name="BOOKKEEPER-447_bitset.diff" size="28220" author="hustlmsp" created="Sat, 15 Dec 2012 09:56:03 +0000"/>
                            <attachment id="12561080" name="BOOKKEEPER-447_force_flush_entry_logger.patch" size="9144" author="hustlmsp" created="Sat, 15 Dec 2012 01:40:10 +0000"/>
                            <attachment id="12555757" name="perf.png" size="5380" author="ikelly" created="Mon, 3 Dec 2012 14:30:53 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>12.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Tue, 30 Oct 2012 10:43:11 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>252913</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hy74un:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>75596</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                </customfields>
    </item>
</channel>
</rss>