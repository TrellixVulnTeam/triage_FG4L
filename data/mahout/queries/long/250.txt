Wikipedia.org ships large bzip2 compressed archives hence it would make sense to be able to load the chunked XML into HDFS directly from the original file without having to uncompress a 25GB temporary file on the local hard drive. Reusing the Hadoop BZip2 codecs allows us to avoid having to introduce a new dependency.