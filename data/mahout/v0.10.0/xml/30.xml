<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 04:24:17 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/MAHOUT-30/MAHOUT-30.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[MAHOUT-30] dirichlet process implementation</title>
                <link>https://issues.apache.org/jira/browse/MAHOUT-30</link>
                <project id="12310751" key="MAHOUT">Mahout</project>
                    <description>
&lt;p&gt;Copied over from original issue:&lt;/p&gt;

&lt;p&gt;&amp;gt; Further extension can also be made by assuming an infinite mixture model. The implementation is only slightly more difficult and the result is a (nearly)&lt;br/&gt;
&amp;gt; non-parametric clustering algorithm.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12393413">MAHOUT-30</key>
            <summary>dirichlet process implementation</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="jeastman">Jeff Eastman</assignee>
                                    <reporter username="isabel">Isabel Drost-Fromm</reporter>
                        <labels>
                    </labels>
                <created>Tue, 8 Apr 2008 15:18:10 +0100</created>
                <updated>Sat, 21 May 2011 04:23:51 +0100</updated>
                            <resolved>Mon, 16 Mar 2009 00:19:52 +0000</resolved>
                                                    <fixVersion>0.3</fixVersion>
                                    <component>Clustering</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>1</watches>
                                                                <comments>
                            <comment id="12640687" author="jeastman" created="Fri, 17 Oct 2008 23:29:59 +0100"  >&lt;p&gt;Here&apos;s a work-in-progress Dirichlet Process Clustering algorithm that Ted Dunning has been coaching me to write. It originated from an R prototype which he wrote and which I translated into Java using our nascent Vector package. Then Ted took the Java and refactored it to introduce better abstractions, as the R implementation had no objects and my reverse-engineered abstractions were rather clunky. Finally, I got the new implementation working with a pluggable distributions framework based either upon commons-math (+ Ted&apos;s patches thereto) or the blog-0.2 framework (vanilla).&lt;/p&gt;

&lt;p&gt;I am posting this to the list in hopes of generating more interest from the larger Mahout community. It has taken me literally months to wrap my mind around this approach. Enjoy.&lt;/p&gt;

&lt;p&gt;To run this patch you will need to get the blog package at &lt;a href=&quot;http://people.csail.mit.edu/milch/blog&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://people.csail.mit.edu/milch/blog&lt;/a&gt;. Here is the beginning of the README file that came with the distribution:&lt;br/&gt;
=====&lt;br/&gt;
Bayesian Logic (BLOG) Inference Engine version 0.2&lt;/p&gt;

&lt;p&gt;Copyright (c) 2007, Massachusetts Institute of Technology&lt;br/&gt;
Copyright (c) 2005, 2006, Regents of the University of California&lt;br/&gt;
All rights reserved.  This software is distributed under the license &lt;br/&gt;
included in LICENSE.txt.&lt;/p&gt;

&lt;p&gt;Lead author: Brian Milch, milch@csail.mit.edu&lt;br/&gt;
Supervisors: Prof. Stuart Russell (Berkeley), Prof. Leslie Kaelbling (MIT)&lt;br/&gt;
Contributors: Bhaskara Marthi, Andrey Kolobov, David Sontag, Daniel L. Ong,&lt;br/&gt;
    Brendan Clark&lt;br/&gt;
=====&lt;/p&gt;

&lt;p&gt;Jeff&lt;/p&gt;</comment>
                            <comment id="12646790" author="jeastman" created="Wed, 12 Nov 2008 03:42:42 +0000"  >&lt;p&gt;I did some refactoring to better localize the major processing steps in DirichletCluster. The first refactoring was to create an iterate method that handles each iteration after the initialization phase initializes the models, Dirichlet and mixture. Then I factored out the three sub-processing steps: assignPointsToModels, recomputeModels and computeNewMixtureParameters and moved some of the instance variables to locals. Looking at the result it is starting to feel more like the other clustering algorithms so I&apos;m starting to think in terms of an M/R partitioning. &lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;In assignPointsToModels, we are iterating over all of the points, calculating their pdf with respect to the current clusters (models), applying the mixture probabilities and then using a multinomial sampling to assign them to a single cluster for this iteration. In a subsequent iteration we might assign them to different models, depending upon the probabilities.&lt;/li&gt;
	&lt;li&gt;In recomputeModels, we use the model assignments from this iteration to compute the posterior samples for each of the models. Then we generate a new model that best describes this sub-sample. The ah-ha for me here is that we create entirely new models in each iteration.&lt;/li&gt;
	&lt;li&gt;In iterate, we periodically output the models so we can gain whatever wisdom we wish from their parameters. I didn&apos;t factor that out but may later.&lt;/li&gt;
	&lt;li&gt;in computeNewMixtureParameters we use the same model assignments to compute a new alpha vector for the Dirichlet distribution and a new mixture vector too. Then we iterate over the points again, continually refining these values. The ah-ha for me here is we do not actually remember the model assignments. &lt;em&gt;I think, after the models have all been computed, we could run another pass over the points computing the pdf for each cluster as a probability vector. We do this in Kmeans too.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The first step seems very similar to the KmeansMapper: we read the Dirichlet, mixture and the current models into each mapper, then compute the multinomial and output the point keyed by its model index to the reducer. In a single reducer, all of the points are seen so we can calculate the posterior statistics if we switch the model to use a running sums (s0, s1, s2) algorithm for the std. Following this approach, it might be we could use a combiner to maintain the running sums (assuming Hadoop only runs it once for now) thus reducing the heavy load on the poor reducer. With a single reducer, I&apos;m pretty sure we can compute the new mixture and Dirichlet parameters. With multiple reducers I think those calculations would need help combing the reducer values in the driver.&lt;/p&gt;

&lt;p&gt;Ted, I know you&apos;ve thought about this a lot, do these steps sound reasonable?&lt;/p&gt;


&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeHeader panelHeader&quot; style=&quot;border-bottom-width: 1px;&quot;&gt;&lt;b&gt;DirichletCluster.java&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
 /**
   * Perform one iteration of the clustering process, updating the Dirichlet distribution
   * and returning the &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; mixtures
   * 
   * @param models a List&amp;lt;Model&amp;lt;Vector&amp;gt;&amp;gt; of models
   * @param dir a DirichletDistribution that will be modified in each iteration
   * @param mixture a Vector mixture 
   * @param clusterSamples a List&amp;lt;List&amp;lt;Model&amp;lt;Vector&amp;gt;&amp;gt;&amp;gt; that will be modified in each iteration
   * @param iteration the &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; iteration number
   * @&lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; a &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; mixture Vector
   */
  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; Vector iterate(List&amp;lt;Model&amp;lt;Vector&amp;gt;&amp;gt; models, DirichletDistribution dir,
      Vector mixture, List&amp;lt;List&amp;lt;Model&amp;lt;Vector&amp;gt;&amp;gt;&amp;gt; clusterSamples, &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; iteration) {
    &lt;span class=&quot;code-comment&quot;&gt;// z holds the assignment of cluster numbers to points &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; iteration
&lt;/span&gt;    Vector z = assignPointsToModels(models, mixture);
    models = recomputeModels(z);

    &lt;span class=&quot;code-comment&quot;&gt;// periodically add models to cluster samples after getting started
&lt;/span&gt;    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; ((iteration &amp;gt; burnin) &amp;amp;&amp;amp; (iteration % thin == 0))
      clusterSamples.add(models);

    &lt;span class=&quot;code-comment&quot;&gt;// compute and &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; the &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; mixture parameters (adjusting 
&lt;/span&gt;    &lt;span class=&quot;code-comment&quot;&gt;// the Dirichlet dir&apos;s alpha vector in the process
&lt;/span&gt;    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; computeNewMixtureParameters(dir, z);
  }

  /**
   * Assign all points to a model based upon the current mixture parameters and
   * the probability that each point is described by each model&apos;s pdf.
   * 
   * @param models the current List&amp;lt;Model&amp;lt;Vector&amp;gt;&amp;gt; of models
   * @param mixture the Vector of mixture probabilities
   * @&lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; z the Vector of assignment of points to models
   */
  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; Vector assignPointsToModels(List&amp;lt;Model&amp;lt;Vector&amp;gt;&amp;gt; models, Vector mixture) {
    &lt;span class=&quot;code-comment&quot;&gt;// z holds the assignment of cluster numbers to points &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; iteration
&lt;/span&gt;    Vector z = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; DenseVector(sampleData.size());

    Vector pi = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; DenseVector(maxClusters);
    &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; ix = 0; ix &amp;lt; sampleData.size(); i++) {
      Vector x = sampleData.get(ix);
      &lt;span class=&quot;code-comment&quot;&gt;// compute probabilities &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; each cluster, saving the largest
&lt;/span&gt;      &lt;span class=&quot;code-object&quot;&gt;double&lt;/span&gt; max = 0;
      &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; k = 0; k &amp;lt; maxClusters; k++) {
        &lt;span class=&quot;code-object&quot;&gt;double&lt;/span&gt; p = mixture.get(k) * models.get(k).pdf(x);
        pi.set(k, p);
        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (max &amp;lt; p)
          max = p;
      }
      &lt;span class=&quot;code-comment&quot;&gt;// normalize the probabilities by largest observed value
&lt;/span&gt;      pi.assign(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; TimesFunction(), 1.0 / max);
      &lt;span class=&quot;code-comment&quot;&gt;// then pick one cluster by sampling a multinomial based upon the probabilities
&lt;/span&gt;      &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; model = dist.rmultinom(pi);
      z.set(i, model);
    }
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; z;
  }

  /**
   * Recompute &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; models based upon the assignment of points to models and &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; them
   * 
   * @param z the Vector of assignment of points to models
   * @&lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; the List&amp;lt;Model&amp;lt;Vector&amp;gt;&amp;gt; of models
   */
  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; List&amp;lt;Model&amp;lt;Vector&amp;gt;&amp;gt; recomputeModels(Vector z) {
    List&amp;lt;Model&amp;lt;Vector&amp;gt;&amp;gt; newModels = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ArrayList&amp;lt;Model&amp;lt;Vector&amp;gt;&amp;gt;();
    &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; k = 0; k &amp;lt; maxClusters; k++) {
      &lt;span class=&quot;code-comment&quot;&gt;// collect all data assigned to each cluster
&lt;/span&gt;      List&amp;lt;Vector&amp;gt; data = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ArrayList&amp;lt;Vector&amp;gt;();
      &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; i = 0; i &amp;lt; sampleData.size(); i++)
        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (z.get(i) == k)
          data.add(sampleData.get(i));
      &lt;span class=&quot;code-comment&quot;&gt;// add a &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; posterior model &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; any data &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; use a prior model
&lt;/span&gt;      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (data.size() &amp;gt; 0)
        newModels.add(modelFactory.sampleFromPosterior(data));
      &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt;
        newModels.add(modelFactory.sampleFromPrior());
    }
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; newModels;
  }

  /**
   * Compute a &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; mixture based upon the Dirichlet distribution and the assignment of points to models.
   * 
   * @param dir the DirichletDistribution, which is modified during &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; process
   * @param z the Vector of assignment of points to models
   * @&lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; the &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; mixture Vector
   */
  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; Vector computeNewMixtureParameters(DirichletDistribution dir, Vector z) {
    Vector mixture;
    Vector a = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; DenseVector(maxClusters);
    a.assign(alpha_0 / maxClusters);
    &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; i = 0; i &amp;lt; z.size(); i++) {
      &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; z_i = (&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;) z.get(i);
      a.set(z_i, a.get(z_i) + 1);
    }
    dir.setAlpha(a);
    mixture = dir.sample();
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; mixture;
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12646967" author="tdunning" created="Wed, 12 Nov 2008 17:36:28 +0000"  >&lt;p&gt;Jeff,&lt;/p&gt;

&lt;p&gt;These look like really nice refactorings.  The process is nice and clear.&lt;/p&gt;

&lt;p&gt;The only key trick that may confuse people is that each step is a sampling.  Thus assignment to clusters does NOT assign to the best cluster, it picks a cluster at random, biased by the mixture parameters and model pdf&apos;s.  Likewise, model computation does NOT compute the best model, it samples from the distribution given by the data.  Same is true for the mixture parameters.&lt;/p&gt;

&lt;p&gt;Your code does this.  I just think that this is a hard point for people to understand in these techniques. &lt;/p&gt;</comment>
                            <comment id="12647186" author="jeastman" created="Thu, 13 Nov 2008 04:36:40 +0000"  >&lt;p&gt;I refactored again and was able eliminate materializing of the posterior &lt;tt&gt;data&lt;/tt&gt; sets by adding &lt;tt&gt;observe()&lt;/tt&gt; and &lt;tt&gt;computeParameters()&lt;/tt&gt; operations to &lt;tt&gt;Model&lt;/tt&gt;. The idea is that all models begin in their prior state and are asked to observe each sample that is assigned to them. Then, before &lt;tt&gt;pdf()&lt;/tt&gt; is called on them in the next iteration a call to &lt;tt&gt;computeParameters()&lt;/tt&gt; finalizes the parameters once and turns the model into a posterior model. I also compute &lt;tt&gt;counts&lt;/tt&gt; on the fly to eliminate materializing &lt;tt&gt;z&lt;/tt&gt; altogether. I hope I didn&apos;t throw the baby out with the bath water.&lt;/p&gt;

&lt;p&gt;Finally, I introduced a &lt;tt&gt;DirichletState&lt;/tt&gt; bean to hold the models, dirichlet distribution and the mixture, simplifying the arguments and, I think, fixing a bug in the earlier refactoring. The algorithm runs over 10,000 points and produces the following outputs (prior() indicates a model with no observations, n is the number of observations, m the mean and sd the std):&lt;/p&gt;

&lt;p&gt;Generating 4000 samples m=&lt;span class=&quot;error&quot;&gt;&amp;#91;1.0, 1.0&amp;#93;&lt;/span&gt; sd=3.0&lt;br/&gt;
Generating 3000 samples m=&lt;span class=&quot;error&quot;&gt;&amp;#91;1.0, 0.0&amp;#93;&lt;/span&gt; sd=0.1&lt;br/&gt;
Generating 3000 samples m=&lt;span class=&quot;error&quot;&gt;&amp;#91;0.0, 1.0&amp;#93;&lt;/span&gt; sd=0.1&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;sample&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;= [prior(), normal(n=6604 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;0.67, 0.63&amp;#93;&lt;/span&gt; sd=1.11), normal(n=86 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;0.77, 2.81&amp;#93;&lt;/span&gt; sd=2.15), prior(), normal(n=242 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;2.89, 1.67&amp;#93;&lt;/span&gt; sd=2.14), normal(n=2532 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;0.53, 0.55&amp;#93;&lt;/span&gt; sd=0.69), normal(n=339 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;0.99, 1.70&amp;#93;&lt;/span&gt; sd=2.18), normal(n=77 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;0.53, 0.47&amp;#93;&lt;/span&gt; sd=0.51), normal(n=119 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;0.36, 0.47&amp;#93;&lt;/span&gt; sd=2.85), normal(n=1 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;0.00, 0.00&amp;#93;&lt;/span&gt; sd=0.33)]&lt;/li&gt;
	&lt;li&gt;sample&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;= [prior(), normal(n=6626 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;0.62, 0.54&amp;#93;&lt;/span&gt; sd=0.91), normal(n=137 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;0.51, 2.99&amp;#93;&lt;/span&gt; sd=1.56), normal(n=2 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;0.57, 0.25&amp;#93;&lt;/span&gt; sd=0.70), normal(n=506 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;2.55, 0.93&amp;#93;&lt;/span&gt; sd=1.73), normal(n=1573 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;0.38, 0.60&amp;#93;&lt;/span&gt; sd=0.50), normal(n=848 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;0.81, 1.59&amp;#93;&lt;/span&gt; sd=2.11), normal(n=67 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;0.76, 0.31&amp;#93;&lt;/span&gt; sd=0.45), normal(n=240 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;0.73, 0.31&amp;#93;&lt;/span&gt; sd=2.24), normal(n=1 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;0.00, 0.00&amp;#93;&lt;/span&gt; sd=0.98)]&lt;/li&gt;
	&lt;li&gt;sample&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt;= [prior(), normal(n=5842 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;0.67, 0.39&amp;#93;&lt;/span&gt; sd=0.73), normal(n=157 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;0.73, 3.12&amp;#93;&lt;/span&gt; sd=1.14), prior(), normal(n=655 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;2.32, 0.64&amp;#93;&lt;/span&gt; sd=1.60), normal(n=1439 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;0.00, 1.00&amp;#93;&lt;/span&gt; sd=0.33), normal(n=1439 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;0.78, 1.53&amp;#93;&lt;/span&gt; sd=1.89), normal(n=66 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;0.96, -0.04&amp;#93;&lt;/span&gt; sd=0.24), normal(n=399 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;0.63, -0.03&amp;#93;&lt;/span&gt; sd=1.99), normal(n=3 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;-0.07, 0.76&amp;#93;&lt;/span&gt; sd=0.41)]&lt;/li&gt;
&lt;/ul&gt;


&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeHeader panelHeader&quot; style=&quot;border-bottom-width: 1px;&quot;&gt;&lt;b&gt;Model&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
/**
 * A model is a probability distribution over observed data points and allows 
 * the probability of any data point to be computed.
 */
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;interface&lt;/span&gt; Model&amp;lt;Observation&amp;gt; {
  
  /**
   * Observe the given observation, retaining information about it
   * 
   * @param x an Observation from the posterior
   */
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;abstract&lt;/span&gt; void observe(Observation x);
  
  /**
   * Compute a &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; set of posterior parameters based upon the Observations 
   * that have been observed since my creation
   */
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;abstract&lt;/span&gt; void computeParameters();

  /**
  * Return the probability that the observation is described by &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; model
  * 
  * @param x an Observation from the posterior
  * @&lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; the probability that x is in z
  */
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;abstract&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;double&lt;/span&gt; pdf(Observation x);
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeHeader panelHeader&quot; style=&quot;border-bottom-width: 1px;&quot;&gt;&lt;b&gt;DirichletCluster&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  /**
   * Initialize the variables and run the iterations to assign the sample data
   * points to a computed number of clusters
   *
   * @&lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; a List&amp;lt;List&amp;lt;Model&amp;lt;Observation&amp;gt;&amp;gt;&amp;gt; of the observed models
   */
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; List&amp;lt;List&amp;lt;Model&amp;lt;Observation&amp;gt;&amp;gt;&amp;gt; dirichletCluster() {
    DirichletState&amp;lt;Observation&amp;gt; state = initializeState();

    &lt;span class=&quot;code-comment&quot;&gt;// create a posterior sample list to collect results
&lt;/span&gt;    List&amp;lt;List&amp;lt;Model&amp;lt;Observation&amp;gt;&amp;gt;&amp;gt; clusterSamples = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ArrayList&amp;lt;List&amp;lt;Model&amp;lt;Observation&amp;gt;&amp;gt;&amp;gt;();

    &lt;span class=&quot;code-comment&quot;&gt;// now iterate
&lt;/span&gt;    &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; iteration = 0; iteration &amp;lt; maxIterations; iteration++)
      iterate(state, iteration, clusterSamples);

    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; clusterSamples;
  }

  /**
   * Initialize the state of the computation
   * 
   * @&lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; the DirichletState
   */
  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; DirichletState&amp;lt;Observation&amp;gt; initializeState() {
    &lt;span class=&quot;code-comment&quot;&gt;// get initial prior models
&lt;/span&gt;    List&amp;lt;Model&amp;lt;Observation&amp;gt;&amp;gt; models = createPriorModels();
    &lt;span class=&quot;code-comment&quot;&gt;// create the initial distribution.
&lt;/span&gt;    DirichletDistribution distribution = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; DirichletDistribution(maxClusters,
        alpha_0, dist);
    &lt;span class=&quot;code-comment&quot;&gt;// mixture parameters are sampled from the Dirichlet distribution. 
&lt;/span&gt;    Vector mixture = distribution.sample();
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; DirichletState&amp;lt;Observation&amp;gt;(models, distribution, mixture);
  }

  /**
   * Create a list of prior models
   * @&lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; the Observation
   */
  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; List&amp;lt;Model&amp;lt;Observation&amp;gt;&amp;gt; createPriorModels() {
    List&amp;lt;Model&amp;lt;Observation&amp;gt;&amp;gt; models = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ArrayList&amp;lt;Model&amp;lt;Observation&amp;gt;&amp;gt;();
    &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; k = 0; k &amp;lt; maxClusters; k++) {
      models.add(modelFactory.sampleFromPrior());
    }
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; models;
  }

  /**
   * Perform one iteration of the clustering process, updating the state &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; the next iteration
   * @param state the DirichletState&amp;lt;Observation&amp;gt; of &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; iteration
   * @param iteration the &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; iteration number
   * @param clusterSamples a List&amp;lt;List&amp;lt;Model&amp;lt;Observation&amp;gt;&amp;gt;&amp;gt; that will be modified in each iteration
   */
  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; void iterate(DirichletState&amp;lt;Observation&amp;gt; state, &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; iteration,
      List&amp;lt;List&amp;lt;Model&amp;lt;Observation&amp;gt;&amp;gt;&amp;gt; clusterSamples) {

    &lt;span class=&quot;code-comment&quot;&gt;// create &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; prior models
&lt;/span&gt;    List&amp;lt;Model&amp;lt;Observation&amp;gt;&amp;gt; newModels = createPriorModels();

    &lt;span class=&quot;code-comment&quot;&gt;// initialize vector of membership counts
&lt;/span&gt;    Vector counts = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; DenseVector(maxClusters);
    counts.assign(alpha_0 / maxClusters);

    &lt;span class=&quot;code-comment&quot;&gt;// iterate over the samples
&lt;/span&gt;    &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (Observation x : sampleData) {
      &lt;span class=&quot;code-comment&quot;&gt;// compute vector of probabilities x is described by each model
&lt;/span&gt;      Vector pi = computeProbabilities(state, x);
      &lt;span class=&quot;code-comment&quot;&gt;// then pick one cluster by sampling a Multinomial distribution based upon them
&lt;/span&gt;      &lt;span class=&quot;code-comment&quot;&gt;// see: http://en.wikipedia.org/wiki/Multinomial_distribution
&lt;/span&gt;      &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; model = dist.rmultinom(pi);
      &lt;span class=&quot;code-comment&quot;&gt;// ask the selected model to observe the datum
&lt;/span&gt;      newModels.get(model).observe(x);
      &lt;span class=&quot;code-comment&quot;&gt;// record counts &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; the model
&lt;/span&gt;      counts.set(model, counts.get(model) + 1);
    }

    &lt;span class=&quot;code-comment&quot;&gt;// compute &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; model parameters based upon observations
&lt;/span&gt;    &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (Model&amp;lt;Observation&amp;gt; m : newModels)
      m.computeParameters();

    &lt;span class=&quot;code-comment&quot;&gt;// update the state from the &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; models and counts
&lt;/span&gt;    state.distribution.setAlpha(counts);
    state.mixture = state.distribution.sample();
    state.models = newModels;

    &lt;span class=&quot;code-comment&quot;&gt;// periodically add models to cluster samples after getting started
&lt;/span&gt;    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; ((iteration &amp;gt; burnin) &amp;amp;&amp;amp; (iteration % thin == 0))
      clusterSamples.add(state.models);
  }

  /**
   * Compute a normalized vector of probabilities that x is described
   * by each model using the mixture and the model pdfs
   * 
   * @param state the DirichletState&amp;lt;Observation&amp;gt; of &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; iteration
   * @param x an Observation
   * @&lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; the Vector of probabilities
   */
  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; Vector computeProbabilities(DirichletState&amp;lt;Observation&amp;gt; state,
      Observation x) {
    Vector pi = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; DenseVector(maxClusters);
    &lt;span class=&quot;code-object&quot;&gt;double&lt;/span&gt; max = 0;
    &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; k = 0; k &amp;lt; maxClusters; k++) {
      &lt;span class=&quot;code-object&quot;&gt;double&lt;/span&gt; p = state.mixture.get(k) * state.models.get(k).pdf(x);
      pi.set(k, p);
      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (max &amp;lt; p)
        max = p;
    }
    &lt;span class=&quot;code-comment&quot;&gt;// normalize the probabilities by largest observed value
&lt;/span&gt;    pi.assign(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; TimesFunction(), 1.0 / max);
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; pi;
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12647877" author="jeastman" created="Sat, 15 Nov 2008 17:04:42 +0000"  >&lt;p&gt;This patch is a complete implementation of a non-M/R Dirichlet Process clustering algorithm. It has been refactored significantly from the above. I will describe these changes in a subsequent posting. I&apos;d like to commit this to trunk in time for 0.1 if people agree.&lt;/p&gt;</comment>
                            <comment id="12647880" author="jeastman" created="Sat, 15 Nov 2008 17:17:47 +0000"  >&lt;p&gt;The above patch makes several improvements to the above:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;refactored state updating and cluster sampling into DirichletState&lt;/li&gt;
	&lt;li&gt;refactored creating list of models into ModelDistribution&lt;/li&gt;
	&lt;li&gt;refactored state parameters from DirichletCluster to DirichletState&lt;/li&gt;
	&lt;li&gt;refactored count into the model&lt;/li&gt;
	&lt;li&gt;changed list&amp;lt;Model&amp;gt; to Model[]&lt;/li&gt;
	&lt;li&gt;added significance filtering to print out&lt;/li&gt;
	&lt;li&gt;increased number of iterations to 30 to demonstrate better convergence&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The algorithm now produces the following output when run over 10,000 points:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Using fixed random seed for repeatability.&lt;/li&gt;
	&lt;li&gt;testDirichletCluster10000&lt;/li&gt;
	&lt;li&gt;Generating 4000 samples m=&lt;span class=&quot;error&quot;&gt;&amp;#91;1.0, 1.0&amp;#93;&lt;/span&gt; sd=3.0&lt;/li&gt;
	&lt;li&gt;Generating 3000 samples m=&lt;span class=&quot;error&quot;&gt;&amp;#91;1.0, 0.0&amp;#93;&lt;/span&gt; sd=0.1&lt;/li&gt;
	&lt;li&gt;Generating 3000 samples m=&lt;span class=&quot;error&quot;&gt;&amp;#91;0.0, 1.0&amp;#93;&lt;/span&gt; sd=0.1&lt;/li&gt;
	&lt;li&gt;sample&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;= normal(n=4037 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;0.80, 0.73&amp;#93;&lt;/span&gt; sd=1.40), normal(n=3844 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;0.51, 0.51&amp;#93;&lt;/span&gt; sd=0.68), normal(n=1092 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;0.51, 0.47&amp;#93;&lt;/span&gt; sd=0.53), normal(n=794 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;1.26, 1.60&amp;#93;&lt;/span&gt; sd=2.22),&lt;/li&gt;
	&lt;li&gt;sample&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;= normal(n=4562 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;0.72, 0.68&amp;#93;&lt;/span&gt; sd=1.25), normal(n=2992 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;0.48, 0.52&amp;#93;&lt;/span&gt; sd=0.58), normal(n=1022 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;0.67, 0.31&amp;#93;&lt;/span&gt; sd=0.53), normal(n=1227 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;1.17, 1.41&amp;#93;&lt;/span&gt; sd=2.13),&lt;/li&gt;
	&lt;li&gt;sample&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt;= normal(n=4377 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;0.66, 0.61&amp;#93;&lt;/span&gt; sd=1.08), normal(n=2592 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;0.28, 0.71&amp;#93;&lt;/span&gt; sd=0.51), normal(n=1057 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;1.04, -0.06&amp;#93;&lt;/span&gt; sd=0.25), normal(n=1831 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;1.15, 1.26&amp;#93;&lt;/span&gt; sd=2.05),&lt;/li&gt;
	&lt;li&gt;sample&lt;span class=&quot;error&quot;&gt;&amp;#91;3&amp;#93;&lt;/span&gt;= normal(n=4302 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;0.74, 0.36&amp;#93;&lt;/span&gt; sd=0.80), normal(n=2075 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;-0.00, 1.01&amp;#93;&lt;/span&gt; sd=0.32), normal(n=793 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;1.04, -0.05&amp;#93;&lt;/span&gt; sd=0.20), normal(n=2694 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;1.04, 1.17&amp;#93;&lt;/span&gt; sd=1.93),&lt;/li&gt;
	&lt;li&gt;sample&lt;span class=&quot;error&quot;&gt;&amp;#91;4&amp;#93;&lt;/span&gt;= normal(n=3602 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;0.80, 0.21&amp;#93;&lt;/span&gt; sd=0.58), normal(n=1923 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;-0.05, 1.05&amp;#93;&lt;/span&gt; sd=0.26), normal(n=621 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;1.03, -0.06&amp;#93;&lt;/span&gt; sd=0.19), normal(n=3677 m=&lt;span class=&quot;error&quot;&gt;&amp;#91;0.94, 1.09&amp;#93;&lt;/span&gt; sd=1.77),&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12651384" author="jeastman" created="Thu, 27 Nov 2008 16:50:13 +0000"  >&lt;p&gt;This patch fixes a randomization problem caused by using two Random instances and adds several display tests that were used to generate the examples in the wiki.&lt;/p&gt;</comment>
                            <comment id="12659218" author="isabel" created="Fri, 26 Dec 2008 01:24:48 +0000"  >&lt;p&gt;First of all: Great work, Jeff. I finally found some time to have a closer look at the code. To me it already looks pretty clear and easy to understand. Some minor comments:&lt;/p&gt;

&lt;p&gt;I did not have a close look at the code for displaying the clustering process so far. If it is to be retained in the final version it might be a good idea to move that into its own package?&lt;/p&gt;

&lt;p&gt;I was wondering why you wrapped two math packages (blog and commons-math). Maybe it helps if you shortly name advantages and shortcomings of either? I was missing a pointer to Ted&apos;s patch to commons math. Maybe I just overlooked it?&lt;/p&gt;

&lt;p&gt;In the patch I am missing changes to the dependencies of the pom.xml. I guess you would check in the libraries the patch is depending on into our libs directory? On first sight the license of BLOG as well its dependencies seems fine, any one else verified this?&lt;/p&gt;

&lt;p&gt;Me personally, I would love to see the mathematical fomulae behind the implementation in the docs as well, maybe a pointer to a book chapter/ publication or other source that explains the algorithm in more detail.&lt;/p&gt;

&lt;p&gt;Looking through the code, I found it a little irritating to have classes ModelDistribution, NormalModelDistribution, and DirichletDistribution around. As the only method in the interface ModelDistribution is sampleFromPrior, it might be clearer if it were named ModelSampler? But maybe it is just the time of day I looked at it...&lt;/p&gt;
</comment>
                            <comment id="12659258" author="jeastman@windwardsolutions.com" created="Fri, 26 Dec 2008 18:52:46 +0000"  >&lt;p&gt;Hi Isabel,&lt;/p&gt;

&lt;p&gt;I&apos;m so happy you had time to look through the code. Getting it to this &lt;br/&gt;
point was a great ordeal for me as the math is complicated and I have no &lt;br/&gt;
formal statistics background. Ted&apos;s help was critical in getting me to &lt;br/&gt;
the tipping point where I now understand the implementation well enough &lt;br/&gt;
to make progress on my own. I&apos;m getting ready for a week vacation and &lt;br/&gt;
will not have email but would love to continue this dialog and am very &lt;br/&gt;
open to your suggestions below. See more comments therein.&lt;/p&gt;

&lt;p&gt;Jeff&lt;/p&gt;

&lt;p&gt;I was thinking of moving the display code into the examples directory.&lt;br/&gt;
I did that so Ted could use his favorite library but he has not been &lt;br/&gt;
pursuing it. I&apos;m happy with blog and, as commons does not have the &lt;br/&gt;
needed sampling methods without Ted&apos;s patches, suggest we could go with &lt;br/&gt;
blog. Removing the plugability would clean up the code some too.&lt;br/&gt;
? Does this relate to maven?&lt;br/&gt;
Boy, I would too. Especially if it was clear enough that I could &lt;br/&gt;
understand it &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
Some of those were terms Ted introduced from my original port of his R &lt;br/&gt;
example. I&apos;m not hung up but perhaps we should include him in the &lt;br/&gt;
discussion?&lt;/p&gt;
</comment>
                            <comment id="12659260" author="tdunning" created="Fri, 26 Dec 2008 19:51:52 +0000"  >
&lt;p&gt;Regarding the question of whether something should be called a Distribution or a Sampler, the mathematical terminology is that a distribution is something you can sample so the the Distribution terminology would be most compatible that way.  The fact that only one method is currently defined is likely a temporary thing ... other methods could well be required for later efforts.&lt;/p&gt;


&lt;p&gt;On Fri, Dec 26, 2008 at 10:53 AM, Jeff Eastman (JIRA) &amp;lt;jira@apache.org&amp;gt; wrote:&lt;/p&gt;

&lt;p&gt;    ... Some of those were terms Ted introduced from my original port of his R&lt;br/&gt;
    example. I&apos;m not hung up but perhaps we should include him in the&lt;br/&gt;
    discussion?&lt;/p&gt;</comment>
                            <comment id="12659273" author="tdunning" created="Fri, 26 Dec 2008 21:57:21 +0000"  >&lt;p&gt;One good reference is this relatively dense article by McCullagh and Yang:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://ba.stat.cmu.edu/journal/2008/vol03/issue01/yang.pdf&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://ba.stat.cmu.edu/journal/2008/vol03/issue01/yang.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;There is also a more approachable example in Chris Bishop&apos;s book on Machine Learning.  See &lt;a href=&quot;http://research.microsoft.com/en-us/um/people/cmbishop/PRML/index.htm&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://research.microsoft.com/en-us/um/people/cmbishop/PRML/index.htm&lt;/a&gt;.  I think that chapter 9 is where the example of clustering using a mixture model is found.&lt;/p&gt;

&lt;p&gt;The Neal and Blei references from the McCullagh and Yang paper are also good.  Zoubin Gharamani has some very nice tutorials out which describe why non-parametric Bayesian approaches to problems are very cool.  One is at &lt;a href=&quot;http://learning.eng.cam.ac.uk/zoubin/talks/uai05tutorial-b.pdf&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://learning.eng.cam.ac.uk/zoubin/talks/uai05tutorial-b.pdf&lt;/a&gt; but here are video versions about as well.&lt;/p&gt;</comment>
                            <comment id="12659282" author="isabel" created="Fri, 26 Dec 2008 22:43:07 +0000"  >&lt;p&gt;&amp;gt; Regarding the question of whether something should be called a Distribution or a Sampler, the mathematical terminology is that a distribution is &lt;br/&gt;
&amp;gt; something you can sample so the the Distribution terminology would be most compatible that way. The fact that only one method is currently &lt;br/&gt;
&amp;gt; defined is likely a temporary thing ... other methods could well be required for later efforts.&lt;/p&gt;

&lt;p&gt;I understand. I do not have any strong objections. I think, a short class comment in DirichletDistribution would already help to avoid at least my confusion. (Although not trying to understand code at 2a.m. local time might help as well... &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; )&lt;/p&gt;

&lt;p&gt;&amp;gt; I was thinking of moving the display code into the examples directory.&lt;/p&gt;

&lt;p&gt;Sounds like a great idea to me.&lt;/p&gt;

&lt;p&gt;&amp;gt; I did that so Ted could use his favorite library but he has not been pursuing it. I&apos;m happy with blog and, as commons does not have the &lt;br/&gt;
&amp;gt; needed sampling methods without Ted&apos;s patches, suggest we could go with blog. Removing the plugability would clean up the code some too.&lt;/p&gt;

&lt;p&gt;Do you know what the current status of the patches is? I must admit I have a slight preference for commons-math as well, in case they support what we need.&lt;/p&gt;</comment>
                            <comment id="12659283" author="isabel" created="Fri, 26 Dec 2008 22:46:26 +0000"  >&lt;p&gt;Ted: I am moving the references over into the wiki page. Hope that is fine with you?&lt;/p&gt;</comment>
                            <comment id="12659375" author="tdunning" created="Sat, 27 Dec 2008 20:06:16 +0000"  >
&lt;p&gt;That would be great.  The wiki is really a better place.&lt;/p&gt;</comment>
                            <comment id="12668280" author="jeastman" created="Thu, 29 Jan 2009 01:55:19 +0000"  >&lt;p&gt;This patch moves the display-related classes into the examples subtree and removes the dependency upon commons.math which needs some extra patches to the release and is TBD. It still depends upon blog-0.3 which I have only been able to find in source format. I will try to locate or build a suitable jar for that. I believe, since it is under BSD license we are ok but I&apos;d like confirmation. The blog site is at &lt;a href=&quot;http://people.csail.mit.edu/milch/blog/software.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://people.csail.mit.edu/milch/blog/software.html&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="12678806" author="jeastman" created="Wed, 4 Mar 2009 17:37:15 +0000"  >&lt;p&gt;This patch is completely self-contained as it depends only upon the uncommons.math package for its distributions. I think this is ready to commit to trunk, after the release of course. The next steps will be to implement a M/R algorithm using this basic approach.&lt;/p&gt;</comment>
                            <comment id="12678807" author="jeastman" created="Wed, 4 Mar 2009 17:38:53 +0000"  >&lt;p&gt;Here&apos;s the patch file &lt;/p&gt;</comment>
                            <comment id="12681358" author="jeastman" created="Thu, 12 Mar 2009 15:31:11 +0000"  >&lt;p&gt;This file contains a tar of a standalone Eclipse project named Dirichlet Cluster. The directory structure is self-contained and only depends upon the Mahout project. It uses the Gson beta1.3 jar file for serialization/deserialization and that is included. &lt;/p&gt;

&lt;p&gt;This version contains an initial Hadoop implementation that has been tested through 10 iterations. In each iteration, clusters are read from the previous iteration (in the state-i directories) and points are assigned to clusters in the Mapper. The reducer then observes all points for each cluster and computes new clusters which are output for the subsequent iteration. The unit test TestMapReduce.testDriverMRIterations() creates 400 data points and runs the MR Driver. Then it gathers all of the state files and summarizes them on the console.&lt;/p&gt;

&lt;p&gt;I noticed when I replaced the beta distribution code earlier that the clustering now tends to put everything into the same cluster. I&apos;m suspicious about the beta values that are being computed and need to investigate this further. &lt;/p&gt;

&lt;p&gt;I think this design will allow an arbitrary number of Mappers and Reducers up to the number of clusters. There is a stub Combiner class that is not currently used. I will continue to develop unit tests but I wanted to get this into view because it is a real first light MR implementation.&lt;/p&gt;

&lt;p&gt;Jeff&lt;/p&gt;</comment>
                            <comment id="12681590" author="jeastman" created="Fri, 13 Mar 2009 03:16:01 +0000"  >&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;fixed bug in rBeta where arguments to rGamma were backwards&lt;/li&gt;
	&lt;li&gt;fixed model std calculations to handle single sample case (s0=1 =&amp;gt; std==0.0 which made resulting pdf==NaN and horked all subsequent cluster assignments)&lt;/li&gt;
	&lt;li&gt;added cluster history display to examples with colors to illustrate cluster convergence&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This is behaving pretty nicely now&lt;/p&gt;</comment>
                            <comment id="12682064" author="jeastman" created="Sat, 14 Mar 2009 16:29:14 +0000"  >&lt;p&gt;    generateSamples(500, 0, 0, 0.5);&lt;br/&gt;
    generateSamples(500, 2, 0, 0.2);&lt;br/&gt;
    generateSamples(500, 0, 2, 0.3);&lt;br/&gt;
    generateSamples(500, 2, 2, 1);&lt;br/&gt;
    DirichletDriver.runJob(&quot;input&quot;, &quot;output&quot;,&lt;br/&gt;
        &quot;org.apache.mahout.clustering.dirichlet.SampledNormalDistribution&quot;, 20, 15, 1.0, 2);&lt;/p&gt;</comment>
                            <comment id="12682187" author="jeastman" created="Sun, 15 Mar 2009 22:51:12 +0000"  >&lt;p&gt;Final patch file is ready to commit. Need to add entry to pom for gson jar which is currently in core/lib.&lt;/p&gt;</comment>
                            <comment id="12682192" author="jeastman" created="Mon, 16 Mar 2009 00:19:52 +0000"  >&lt;p&gt;Committed revision 754797. Committed code was refactored slightly from last patch (models moved from test to core), and pom was updated to reflect Gson dependency.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310010">
                    <name>Incorporates</name>
                                                                <inwardlinks description="is part of">
                                        <issuelink>
            <issuekey id="12388537">MAHOUT-4</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12392386" name="MAHOUT-30.patch" size="33624" author="jeastman" created="Fri, 17 Oct 2008 23:29:59 +0100"/>
                            <attachment id="12393988" name="MAHOUT-30b.patch" size="46197" author="jeastman" created="Sat, 15 Nov 2008 17:04:41 +0000"/>
                            <attachment id="12394856" name="MAHOUT-30c.patch" size="73556" author="jeastman" created="Thu, 27 Nov 2008 16:50:13 +0000"/>
                            <attachment id="12398965" name="MAHOUT-30d.patch" size="68049" author="jeastman" created="Thu, 29 Jan 2009 01:55:19 +0000"/>
                            <attachment id="12401424" name="MAHOUT-30e.patch" size="71400" author="jeastman" created="Wed, 4 Mar 2009 17:38:53 +0000"/>
                            <attachment id="12402232" name="MAHOUT-30f.patch" size="146418" author="jeastman" created="Sun, 15 Mar 2009 22:51:12 +0000"/>
                            <attachment id="12402110" name="dirichlet-2.tar" size="706560" author="jeastman" created="Fri, 13 Mar 2009 03:16:01 +0000"/>
                            <attachment id="12402208" name="screenshot-1.jpg" size="152553" author="jeastman" created="Sat, 14 Mar 2009 16:29:14 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>8.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Fri, 17 Oct 2008 22:29:59 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>10036</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hxy7mv:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>23388</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                </customfields>
    </item>
</channel>
</rss>