<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 04:19:21 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/MAHOUT-796/MAHOUT-796.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[MAHOUT-796] Modified power iterations in existing SSVD code</title>
                <link>https://issues.apache.org/jira/browse/MAHOUT-796</link>
                <project id="12310751" key="MAHOUT">Mahout</project>
                    <description>&lt;p&gt;Nathan Halko contacted me and pointed out importance of availability of power iterations and their significant effect on accuracy of smaller eigenvalues and noise attenuation. &lt;/p&gt;

&lt;p&gt;Essentially, we would like to introduce yet another job parameter, q, that governs amount of optional power iterations. The suggestion how to modify the algorithm is outlined here : &lt;a href=&quot;https://github.com/dlyubimov/ssvd-lsi/wiki/Power-iterations-scratchpad&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/dlyubimov/ssvd-lsi/wiki/Power-iterations-scratchpad&lt;/a&gt; .&lt;/p&gt;

&lt;p&gt;Note that it is different from original power iterations formula in the paper in the sense that additional orthogonalization performed after each iteration. Nathan points out that that improves errors in smaller eigenvalues a lot (If i interpret it right). &lt;/p&gt;</description>
                <environment></environment>
        <key id="12520182">MAHOUT-796</key>
            <summary>Modified power iterations in existing SSVD code</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="dlyubimov">Dmitriy Lyubimov</assignee>
                                    <reporter username="dlyubimov">Dmitriy Lyubimov</reporter>
                        <labels>
                            <label>SSVD</label>
                    </labels>
                <created>Fri, 26 Aug 2011 00:38:37 +0100</created>
                <updated>Thu, 2 May 2013 03:29:43 +0100</updated>
                            <resolved>Wed, 7 Sep 2011 22:42:12 +0100</resolved>
                                    <version>0.5</version>
                                    <fixVersion>0.6</fixVersion>
                                    <component>Math</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="13091436" author="dlyubimov" created="Fri, 26 Aug 2011 00:52:57 +0100"  >&lt;p&gt;current code is essentially equivalent to running this mod with q=0.&lt;/p&gt;</comment>
                            <comment id="13091493" author="tdunning" created="Fri, 26 Aug 2011 02:16:55 +0100"  >&lt;p&gt;The outline that I have in &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-792&quot; title=&quot;Add new stochastic decomposition code&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-792&quot;&gt;&lt;del&gt;MAHOUT-792&lt;/del&gt;&lt;/a&gt; does exactly this without the iteration and has the framework&lt;br/&gt;
available to do the power iteration efficiently, including the out-of-core QR.&lt;/p&gt;

&lt;p&gt;For most recommendation applications, I would be surprised if iterations are important, but at&lt;br/&gt;
least one user of the current system was surprised when their test matrix with a very low condition&lt;br/&gt;
number didn&apos;t get good results with the current algorithm.  It would be nice to be able to say&lt;br/&gt;
&quot;turn the knob to get better results&quot;.&lt;/p&gt;</comment>
                            <comment id="13091509" author="dlyubimov" created="Fri, 26 Aug 2011 02:42:52 +0100"  >&lt;p&gt;if by low condition number you mean ratio of max to min singular value, then with slow decay i never got good results either. That&apos;s a sign of either highly noisy or ever randomly generated data. I think it would help if user came forward and explained his/her case, esp. given experimental nature of the code.&lt;/p&gt;</comment>
                            <comment id="13091515" author="tdunning" created="Fri, 26 Aug 2011 03:09:53 +0100"  >&lt;p&gt;Yes.  That is what I mean by condition number.  &lt;/p&gt;

&lt;p&gt;The only place that I have seen slow decay is with synthetically generated random data.  Even real random data has some interesting singular values.&lt;/p&gt;</comment>
                            <comment id="13091527" author="dlyubimov" created="Fri, 26 Aug 2011 03:32:20 +0100"  >&lt;blockquote&gt;&lt;p&gt;For most recommendation applications, I would be surprised if iterations are important,&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ok I have no first hand knowledge myself, Nathan mentioned it did appear quite important in his previous work. I hope he could comment himself on a nature of the data he was looking at.&lt;/p&gt;

&lt;p&gt;So are you encouraging to proceed, benchmark, compare? get more info? do it anyway? or drop it without comparisons?&lt;/p&gt;

</comment>
                            <comment id="13091528" author="nathanhalko" created="Fri, 26 Aug 2011 03:33:20 +0100"  >&lt;p&gt;The lower the condition number (or low signal to noise) the harder it is to extract the top k singular vectors because in a sense they are not that much more important than the other n-k.  We see pollution from the smaller n-k singular directions and that degrades our approximation of the top k space.  Power iterations (just a few) are extremely important to amplify the gap between important directions and the unimportant directions.  Instead of sampling matrix A, we sample matrix (AA*)^qA which has the same singular vectors but an exaggerated spectrum&lt;/p&gt;

&lt;p&gt;   sigma^&lt;/p&gt;
{2q+1}

&lt;p&gt;In infinite precision there would be no need to orthogonalize between iterations, only at the last step.  However, in finite precision, the small singular values can fall below machine precision when taken to the 2q+1st power and we won&apos;t be able to accurately recover them.  It also prevents overflow if your matrix has a very large sig_max.  It is mostly a precaution to keep from loosing information and for most cases could probably be skipped or done only intermittently.  If orthogonalization is a bottleneck we could consider not doing it.&lt;/p&gt;
</comment>
                            <comment id="13091529" author="tdunning" created="Fri, 26 Aug 2011 03:39:00 +0100"  >&lt;blockquote&gt;
&lt;p&gt;So are you encouraging to proceed, benchmark, compare? get more info? do it anyway? or drop it without comparisons?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I will proceed with my latest family of computations without power iterations.  Along the way, I will look for ways to do them efficiently.  It took me months of looking at other things for the coin to drop on the fact that we don&apos;t have to keep the Q around for the QR decompositions, however, so it may be a bit before I figure out how to implement the power iteration really efficiently.&lt;/p&gt;</comment>
                            <comment id="13091532" author="dlyubimov" created="Fri, 26 Aug 2011 03:46:04 +0100"  >&lt;p&gt;Orthogonalization, i wouldn&apos;t say it has been a bottleneck for me personally. I do orthogonalization in two MR steps, but those steps are also doing 2 multiplications. I am still looking to run an experiment with an input where it would be a practical problem (such as QR step taking &amp;gt; 10..15 min). It also so happened that my previous results were tainted by the fact that i ran without native libraries installed so decompression was chomping on my CPU too, so no clean data now. But it seems indeed like roughly 50% of everything else.&lt;/p&gt;

&lt;p&gt;but i am still waiting for run-a-terabyte-input opportunity.&lt;/p&gt;</comment>
                            <comment id="13091533" author="nathanhalko" created="Fri, 26 Aug 2011 03:47:21 +0100"  >&lt;p&gt;We did some work with facial recognition, computing &apos;eigenfaces&apos; and reported in the paper.  In this case there is only 2 orders of magnitude between the signal and the &apos;noise&apos;.  It shows a dramatic difference between the accuracy of one pass versus just one power iteration.  Note that after one power iteration, there is now 6 orders of magnitude separating signal and noise.  &lt;/p&gt;

&lt;p&gt;But this is only looking at approximation error ||A-UU*A||.  It could very well be the case in recommendation applications that this measure is not appropriate, I don&apos;t know.  But it is a very valuable option to have at one&apos;s disposal just in case.&lt;/p&gt;</comment>
                            <comment id="13091538" author="tdunning" created="Fri, 26 Aug 2011 04:08:46 +0100"  >&lt;p&gt;For the in-memory implementations, I think that this is a non-issue.  Power iteration should simply be implemented.  In that case, the original form using Y = (A&apos;A)^q A \Omega seems fine and I don&apos;t yet quite see how the iteration that Dmitriy proposes will get the right result.  Whichever method is used, it is a good thing to do.&lt;/p&gt;

&lt;p&gt;The problems that I see are for the out-of-core problems.  There, computing A&apos;A can often give pathologically bad results if the sparse pattern is highly skewed.  That approach also leads to significant fill-in which is not a good thing.  On the hand, multiplying A times anything too large to store in memory such as B typically is may be horribly bad as well. &lt;/p&gt;

&lt;p&gt;The orthogonalization is no big deal since it requires only a single pass through the data to accumulate the small matrix required for the Cholesky trick.&lt;/p&gt;</comment>
                            <comment id="13091545" author="nathanhalko" created="Fri, 26 Aug 2011 04:30:39 +0100"  >&lt;p&gt;I checked Dmitriy&apos;s scheme today and it makes sense.  It accumulates Q&apos; using the machinery already in place, QJob and BtJob&lt;/p&gt;

&lt;p&gt;QR = Y = A\Omega&lt;br/&gt;
B0 = Q&apos;A&lt;br/&gt;
B1 = (AB0&apos;)&apos;A = B0A&apos;A = (Q&apos;AA&apos;)A = (Q_new)&apos;A &lt;/p&gt;

&lt;p&gt;A&apos;A should never be computed, only Z = A&apos;AY where Y is dense and X=AY, Z=A&apos;X avoiding the problem of scarce overlap and fill in.&lt;/p&gt;
</comment>
                            <comment id="13091601" author="tdunning" created="Fri, 26 Aug 2011 06:43:10 +0100"  >&lt;p&gt;One problem here is that the Q&apos;s are large and potentially dense.  Thus, accumulating them is not a great idea.  That can be worked around in the single iteration because we can keep R in memory and can reconstruct chunks of Y given chunks of A.&lt;/p&gt;

&lt;p&gt;That trick becomes a bit more involved if we want to keep all of the Q&apos;s in such an implicit form.  Computing (AA&apos;)^q A\Omega is relatively simple as you point out if A&apos; is available as a linear operator, but I thought I understood that reorthogonalizing was a good idea.  What I don&apos;t see is how to re-orthogonalize without keeping very large matrices in memory or doing a dangerously dense operation.  Yet.&lt;/p&gt;


</comment>
                            <comment id="13092000" author="tdunning" created="Fri, 26 Aug 2011 21:23:48 +0100"  >&lt;p&gt;To clarify a bit about my worries, the issue that I see is that we could compute A\Omega in a single pass of A because we could effectively store all of \Omega in memory (i.e. just keep the seed and hash function).  This is nice because row-wise decomposition of A makes everything work nicely.&lt;/p&gt;

&lt;p&gt;In computing (AB&apos;) A out-of-core, however, we have a product of A by a tall skinny matrix B that requires roughly as much storage as A.  Each row-wise patch of A will have to be combined with each row-wise chunk of B&apos; (i.e. each column-wise chunk of B).  This means that we have to read B many times which leads to quadratic time.&lt;/p&gt;
</comment>
                            <comment id="13092179" author="nathanhalko" created="Sat, 27 Aug 2011 04:28:26 +0100"  >&lt;p&gt;The reorthogonalizations aren&apos;t essential and if its a barrier to power iterations we should forego them at the moment.  A quick and dirty trick to avoid even sweeping through A again is to neglect the cross terms in the product (AA&apos;)^qA\Omega and just use (A_iA_i&apos;)^qA_i\Omega. This could be extremely naive but I&apos;ve been getting some good results with it.  The accuracy typically falls about half way between single pass and full power iterations so it could be useful (although it could be dangerous as well).  &lt;/p&gt;

&lt;p&gt;sig_51  &amp;lt;-  optimal&lt;br/&gt;
   60.6531&lt;/p&gt;

&lt;p&gt;full power iters 1&lt;br/&gt;
   81.2668&lt;/p&gt;

&lt;p&gt;full power iters 4&lt;br/&gt;
   67.5545&lt;/p&gt;

&lt;p&gt;row-wise power iters 1&lt;br/&gt;
   89.4983&lt;/p&gt;

&lt;p&gt;row-wise power iters 4&lt;br/&gt;
   82.2247&lt;/p&gt;

&lt;p&gt;single pass&lt;br/&gt;
   92.8736&lt;/p&gt;

&lt;p&gt;norm A&lt;br/&gt;
  100.0000 &lt;/p&gt;

&lt;p&gt;The &apos;row-wise power iters&apos; being (A_iA_i&apos;)^qA_i\Omega.  &lt;/p&gt;
</comment>
                            <comment id="13092380" author="dlyubimov" created="Sat, 27 Aug 2011 21:54:43 +0100"  >&lt;p&gt;Hi, &lt;/p&gt;

&lt;p&gt;I put together B_i pipeline &lt;a href=&quot;https://github.com/dlyubimov/ssvd-lsi/wiki/Power-iterations-scratchpad&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/dlyubimov/ssvd-lsi/wiki/Power-iterations-scratchpad&lt;/a&gt;. It seems it is a pretty straightforward enhancement that falls back on a lot of existing stuff, with fundamental additions of AB&apos; multiplication and QR pushdown to reducer of the first job (instead of doing it in the mapper of the first job) &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;A quick and dirty trick to avoid even sweeping through A again is to neglect the cross terms in the product (AA&apos;)^qA\Omega and just use (A_iA_i&apos;)^qA_i\Omega. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think that even if that&apos;s less flops, it is still more difficult to implement than the full power iterations with reorthogonalization as you&apos;ve initially proposed. &lt;/p&gt;

&lt;p&gt;After all, IMO there&apos;s no big reason to be afraid of more work for as long as it brings more precision and we have a control over how much more work we want to do. &lt;/p&gt;

&lt;p&gt;I also can incorporate a Cholesky trick into B_0 pipeline at some point &amp;#8211; or just have it as an alternative flow controlled by a job parameter.&lt;/p&gt;</comment>
                            <comment id="13092385" author="dlyubimov" created="Sat, 27 Aug 2011 22:37:01 +0100"  >&lt;p&gt;PS it also just occurred to me that full B&apos; does not have to be ever written out either because BB&apos; can be accumulated in reducers of 2nd step. Then front end will just have to aggregate few triangular partial B&apos; products produced by however many reducers, directly (we don&apos;t save full BB&apos; since it&apos;s symmetrical, nor do we compute full outer products of BB&apos; there). That saves on full Bt I/O and avoids startup costs of BB&apos; job. &lt;/p&gt;

&lt;p&gt;Thus, full job is 2 MR passes with q=0, 4MR passes with q=1 and 6MR passes with q=2. If understand Nathan&apos;s point right, 3 orthogonalizations (which corresponds to q=2) is quite enough.&lt;/p&gt;

&lt;p&gt;V and U jobs are optional and running in parallel, so they can count for another iteration.&lt;/p&gt;

&lt;p&gt;so with q=2 (maximum case) and U,V output requested we end up with 7 &lt;b&gt;sequential&lt;/b&gt; MR iterations.&lt;/p&gt;</comment>
                            <comment id="13092386" author="dlyubimov" created="Sat, 27 Aug 2011 22:53:22 +0100"  >&lt;p&gt;And finally, i can&apos;t see a reason why we can&apos;t incorporate &quot;Cholesky trick&quot; either by substituting Y_1..Y_q = AB&apos; instead of A\Omega to compute B_i.&lt;/p&gt;

&lt;p&gt;In other words, MR operationally aside, if we assert that we have some function such that currently provides B_0=g(Y_0) where Y_0=A\Omega, then there&apos;s no reason to assume we can&apos;t use the same function g to compute B_i=g(Y_i) for as long as Y_i=AB&apos;_&lt;/p&gt;
{i-1}
&lt;p&gt;. also see my scratchpad for the same.&lt;/p&gt;</comment>
                            <comment id="13093388" author="dlyubimov" created="Tue, 30 Aug 2011 03:48:21 +0100"  >&lt;p&gt;I re-did dense test to construct 20,000x1,000 dense matrix (20 mln non-zero elements)  with random singular vectors &lt;/p&gt;

&lt;p&gt;The way i construct input is i generate random singular vector matrices and orthogonalize them using stable Gramm-Schmidt, multiply one of them (whatever is shorter) by Sigma and then produce row-wise surrogate input. &lt;/p&gt;

&lt;p&gt;For predefined singular values = 10,4,1,(0.1...), n=1000, m=20000, k=3, p=10 i get stochastic values &lt;br/&gt;
--SSVD solver singular values:&lt;br/&gt;
svs: 9.998401  3.998322  0.972622  0.100000  0.100000  0.100000  0.100000  0.100000  0.100000  0.100000  0.100000  0.100000  0.100000  &lt;/p&gt;


&lt;p&gt;so you see if the decay is good then precision loss with 1 pass in my case doesn&apos;t exceed 2.8% in the worst case (3rd value) and the time is quite good. (same brunch as for mahout-797 in my github). &lt;/p&gt;

&lt;p&gt;Keep in mind that this precision loss also includes loss generated during simulated input construction, it&apos;s not all the solver&apos;s.&lt;/p&gt;

&lt;p&gt;I also got rid of BBt job and fixed problems with sparse input on that branch.&lt;/p&gt;
</comment>
                            <comment id="13093389" author="dlyubimov" created="Tue, 30 Aug 2011 03:52:45 +0100"  >&lt;p&gt;PS i removed result comparison to Colt&apos;s SVD since it takes too much time to compute 20mln matrix for its in-core full rank SVD algorithm.&lt;/p&gt;</comment>
                            <comment id="13093393" author="dlyubimov" created="Tue, 30 Aug 2011 04:00:50 +0100"  >&lt;p&gt;linking to &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-790&quot; title=&quot;Redundancy in Matrix API, view or get?&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-790&quot;&gt;&lt;del&gt;MAHOUT-790&lt;/del&gt;&lt;/a&gt; as I have merged my branch on top of that ongoing work&lt;/p&gt;</comment>
                            <comment id="13094018" author="dlyubimov" created="Tue, 30 Aug 2011 20:19:43 +0100"  >&lt;p&gt;AB&apos; is a heavy multiplication of course. &lt;/p&gt;

&lt;p&gt;I don&apos;t want to use standard multiplication because&lt;/p&gt;

&lt;p&gt;&amp;#8211; i want to be doing more things in reducer &lt;br/&gt;
&amp;#8211; need custom grouping/sorting to ensure output is partitioned the same way as A splits &lt;/p&gt;

&lt;p&gt;At this point it seems that the best strategy is just to preload entire A block into memory as a (sparse) matrix and open B&apos; stream as a side file and hope it is not going to generate too much flood i/o. I don&apos;t know a workaround for it anyway since whatever blocking scheme is used, we need cartesian products from both matrix inputs and that will cause i/o and i don&apos;t think there&apos;s any clever collocation trick to be had there&lt;/p&gt;
</comment>
                            <comment id="13094041" author="tdunning" created="Tue, 30 Aug 2011 21:11:11 +0100"  >&lt;blockquote&gt;
&lt;p&gt;At this point it seems that the best strategy is just to preload entire A block into memory as a (sparse) matrix and open B&apos; stream as a side file and hope it is not going to generate too much flood i/o. I don&apos;t know a workaround for it anyway since whatever blocking scheme is used, we need cartesian products from both matrix inputs and that will cause i/o and i don&apos;t think there&apos;s any clever collocation trick to be had there&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Presumably there could be a role for the distributed cache here to make the I/O load more manageable.&lt;/p&gt;

&lt;p&gt;This is just the sort of thing that the MapR ability to control placement, mirroring and to read via NFS comes in really, really handy.  Can&apos;t really assume that for Mahout, though.&lt;/p&gt;</comment>
                            <comment id="13094114" author="dlyubimov" created="Tue, 30 Aug 2011 22:33:27 +0100"  >&lt;p&gt;AFAIK distributed cache would actually do the same except it would also store the file on disk. &lt;/p&gt;

&lt;p&gt;The disadvantage here is that we add disk i/o time to this. The advantage is that if we hit the same node with a mapper of the same task more than once, as far as i understand, they&apos;d have the entire B&apos; locally. That&apos;s an interesting idea, actually. But for big clusters where a job is unlikely to hit the same node with more than 1 task, this probably would actually be detrimental. Plus, if B is really big (somethin like 100Gb big) then we are requiring a lot of hdd from a node. &lt;/p&gt;

&lt;p&gt;Plus for jobs that use memory mapping or any sort of random access, distributed cache is the only option &amp;#8211; but we don&apos;t need that. &lt;/p&gt;

&lt;p&gt;Ok, let me make implementation that opens a stream first, just to prove/measure whatever we are improving, and later perhaps there&apos;s a good sense to add an option to use distributed cache for this. Maybe there will be another trick we don&apos;t see to streamline this, but i so far did not find any. So it will give us some time to think.&lt;/p&gt;
</comment>
                            <comment id="13095146" author="dlyubimov" created="Thu, 1 Sep 2011 07:59:59 +0100"  >&lt;p&gt;Ok first implementation with QR solvers is ready, added -q parameter. (all in git remote git@github.com:dlyubimov/mahout-commits branch &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-796&quot; title=&quot;Modified power iterations in existing SSVD code&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-796&quot;&gt;&lt;del&gt;MAHOUT-796&lt;/del&gt;&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;Did not have time to test distributed version and larger inputs. on a toy input 1000x2000, k=3, p=10 (optimal 10,4,1,(0.1) ): &lt;/p&gt;

&lt;p&gt;q=0: &lt;br/&gt;
--SSVD solver singular values:&lt;br/&gt;
svs: 9.998472  3.993542  0.990456  0.100000  0.100000  0.100000  0.100000  0.100000  0.100000  0.100000  0.100000  0.100000  0.100000  &lt;/p&gt;

&lt;p&gt;q=1: (+2 more sequential steps):&lt;br/&gt;
--SSVD solver singular values:&lt;br/&gt;
svs: 10.000000  4.000000  0.999999  0.100000  0.100000  0.100000  0.100000  0.100000  0.100000  0.100000  0.100000  0.100000  0.100000  &lt;/p&gt;

&lt;p&gt;So, much better (although much slower as well). &lt;br/&gt;
I of course understand that each run exhibit noise, so to prove it works better consistently i need to run more than just 2 attempts. But that&apos;s encouraging. it worked (and actually at first attempt)!&lt;/p&gt;

&lt;p&gt;I tried some optimization to handle sparse cases a little better as well, i guess it taxes densier cases a little bit.&lt;/p&gt;

&lt;p&gt;So this will be put on hold until i add Cholesky option and then i will have to return to this issue to enable the same schema but Y&apos;Y+ Cholesky path.&lt;/p&gt;

&lt;p&gt;I refactored QR steps into standalone OutputCollector implementations so that they can now be more easily be pipelined inside mappers and reducers so code is much more readable now. &lt;/p&gt;

&lt;p&gt;So after a few tests and final fixes i think it is a commit worthy but it has dependency on Ted&apos;s refactoring &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-790&quot; title=&quot;Redundancy in Matrix API, view or get?&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-790&quot;&gt;&lt;del&gt;MAHOUT-790&lt;/del&gt;&lt;/a&gt; pushed to trunk.&lt;/p&gt;
</comment>
                            <comment id="13095365" author="nathanhalko" created="Thu, 1 Sep 2011 16:53:32 +0100"  >&lt;p&gt;Wow those are great results Dmitriy!  q definitely adds power and reliability to the algorithm.&lt;/p&gt;</comment>
                            <comment id="13095764" author="dlyubimov" created="Fri, 2 Sep 2011 05:26:40 +0100"  >&lt;p&gt;Patch. Local solver tests pass. I also tested multiple splits on sufficiently larger inputs and sparse inputs with local MR. &lt;/p&gt;

&lt;p&gt;I still need to test with yet bigger file with multiple reducers since local MR does not support multiple reducers.&lt;/p&gt;

&lt;p&gt;What i noticed is that with just one additional power iteration with orthogonalization there&apos;s practically no need to run any oversampling (p). So yes power iteration runs more steps but runtime can be reduced significantly just because you don&apos;t need as wide projection anymore. small values are pretty good without much oversampling. &lt;/p&gt;

&lt;p&gt;Amazing.&lt;/p&gt;</comment>
                            <comment id="13095765" author="dlyubimov" created="Fri, 2 Sep 2011 05:27:27 +0100"  >&lt;p&gt;Patch attached. &lt;br/&gt;
I will commit after another round of MR in distributed mode. Hopefully will be done by Tue.&lt;/p&gt;</comment>
                            <comment id="13096252" author="dlyubimov" created="Fri, 2 Sep 2011 21:01:48 +0100"  >&lt;p&gt;Fixed CLI issues. &lt;/p&gt;

&lt;p&gt;Tested distributed MR on 1Gb input with multiple reducers q=1 ok.&lt;/p&gt;</comment>
                            <comment id="13096264" author="dlyubimov" created="Fri, 2 Sep 2011 21:07:26 +0100"  >&lt;p&gt;I am now convinced that all the inefficiency and cpu-bound behavior comes from the multiplications Q&apos;B, AB&apos;. Indeed, I generate outer products which i then output row by row and sum up in combiners and reducers. this creates tremendous pressure in terms of keys for spill and reduce sorts. Although it seems DRM.times(DRM) employs exactly the same problem. &lt;/p&gt;

&lt;p&gt;I think tremendous improvements would result if we output outer products of B&apos; or AB&apos; in vertical thin but long blocks. &lt;/p&gt;

&lt;p&gt;This seem like a trivial idea but I was preoccupied by &apos;proof of concept&apos; issues whereas matrix multiplication has been seen as algorithmically trivial. &lt;/p&gt;

&lt;p&gt;the command line i used: &lt;/p&gt;

&lt;p&gt;bin/mahout ssvd --input /tmp/DRM/* --output /tmp/SSVD1 --tempDir /tmp/SSVD-tmp -k 10 -p 3 -q 1 -r 10000 --reduceTasks 3 -Dmapred.child.java.opts=&apos;-Xmx500m&apos;&lt;/p&gt;

&lt;p&gt;For ABt job you need perhaps more than double the memory of the split in case of dense input because ABtJob is catering to sparse inputs first and loads A block column-wise using SequentialAccessSparseVectors. So for a split size of 64Mb standard -Xmx200m may not be enough, and it is definitely not enough for 128mb splits.&lt;/p&gt;

&lt;p&gt;note also that I ran on CDH3u0 without any Mahout recompilation with CDH3 binaries and it seems to work out of the box.&lt;/p&gt;</comment>
                            <comment id="13096574" author="dlyubimov" created="Sat, 3 Sep 2011 05:18:00 +0100"  >&lt;p&gt;rewrote AB&apos; and Bt jobs to do row-wise sparse blocking of outer product outputs from mapper and combiner. &lt;/p&gt;

&lt;p&gt;In 1G tests looks like a major improvment, about order of magnitude speed up for multiplication part and 4 times speed-up of a combo QR+multiplication (multiplication is still the bigger part). &lt;/p&gt;

&lt;p&gt;I think this is the final iteration on this issue, i will commit within couple of days.&lt;/p&gt;</comment>
                            <comment id="13096575" author="dlyubimov" created="Sat, 3 Sep 2011 05:28:16 +0100"  >&lt;p&gt;oops. test fix.&lt;/p&gt;</comment>
                            <comment id="13096598" author="dlyubimov" created="Sat, 3 Sep 2011 07:54:46 +0100"  >&lt;p&gt;also changed CLI a little bit: &lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;made A block height optional with default value of 10,000 which should be fine with most inputs on 64mb splits, ~200 eigen values and -Xmx500m in child processes.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;added -oh outer product  sparse row-wise block cardinality used by &apos;big&apos; multiplications Q&apos;A and AB&apos; with default value of 10,000. It may need increases with very sparse inputs in order to be more efficient for spill sorts (map-side combiners), but it would be always equally efficent in reduce-side sorts which was so far the longest running stuff in all there is (blocked multiplications are still most expensive; but they are closing in on QR expenses which does not use sorts).&lt;/li&gt;
&lt;/ul&gt;

</comment>
                            <comment id="13096622" author="hudson" created="Sat, 3 Sep 2011 10:32:58 +0100"  >&lt;p&gt;Integrated in Mahout-Quality #1017 (See &lt;a href=&quot;https://builds.apache.org/job/Mahout-Quality/1017/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/Mahout-Quality/1017/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-796&quot; title=&quot;Modified power iterations in existing SSVD code&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-796&quot;&gt;&lt;del&gt;MAHOUT-796&lt;/del&gt;&lt;/a&gt;: SSVD power iterations; performance patches.&lt;/p&gt;

&lt;p&gt;dlyubimov : &lt;a href=&quot;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1164806&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1164806&lt;/a&gt;&lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/mahout/trunk/core/src/main/java/org/apache/mahout/common/IOUtils.java&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/ABtJob.java&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/BBtJob.java&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/BtJob.java&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/GivensThinSolver.java&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/Omega.java&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/QJob.java&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/SSVDCli.java&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/SSVDPrototype.java&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/SSVDSolver.java&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/SparseRowBlockAccumulator.java&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/SparseRowBlockWritable.java&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/SplitPartitionedWritable.java&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/UpperTriangular.java&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/YtYJob.java&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/qr&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/qr/GivensThinSolver.java&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/qr/GrammSchmidt.java&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/qr/QRFirstStep.java&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/qr/QRLastStep.java&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/LocalSSVDSolverDenseTest.java&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/LocalSSVDSolverSparseSequentialTest.java&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/SSVDPrototypeTest.java&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/SSVDTestsHelper.java&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/src/conf/ssvd.props&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13099581" author="dlyubimov" created="Wed, 7 Sep 2011 22:42:12 +0100"  >&lt;p&gt;Resolving.&lt;/p&gt;

&lt;p&gt;i will add code doing the same with Cholesky trick as a part of &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-797&quot; title=&quot;MapReduce SSVD: provide alternative B-pipeline per B=R&amp;#39; ^{-1} Y&amp;#39;A&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-797&quot;&gt;&lt;del&gt;MAHOUT-797&lt;/del&gt;&lt;/a&gt; once it&apos;s implemented.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10001">
                    <name>dependent</name>
                                            <outwardlinks description="depends upon">
                                        <issuelink>
            <issuekey id="12519510">MAHOUT-790</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12492797" name="MAHOUT-796-2.patch" size="167309" author="dlyubimov" created="Fri, 2 Sep 2011 21:01:48 +0100"/>
                            <attachment id="12492872" name="MAHOUT-796-3.patch" size="179165" author="dlyubimov" created="Sat, 3 Sep 2011 05:18:00 +0100"/>
                            <attachment id="12492873" name="MAHOUT-796-4.patch" size="179220" author="dlyubimov" created="Sat, 3 Sep 2011 05:28:16 +0100"/>
                            <attachment id="12492713" name="MAHOUT-796.patch" size="165230" author="dlyubimov" created="Fri, 2 Sep 2011 05:26:40 +0100"/>
                            <attachment id="12493650" name="Power Iterations.pdf" size="262309" author="dlyubimov" created="Thu, 8 Sep 2011 20:34:08 +0100"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>5.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Fri, 26 Aug 2011 01:16:55 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9267</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hxy2xb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>22625</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                </customfields>
    </item>
</channel>
</rss>