<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 04:26:01 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/MAHOUT-220/MAHOUT-220.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[MAHOUT-220] Mahout Bayes Code cleanup</title>
                <link>https://issues.apache.org/jira/browse/MAHOUT-220</link>
                <project id="12310751" key="MAHOUT">Mahout</project>
                    <description>&lt;p&gt;Following isabel&apos;s checkstyle, I am adding a whole slew of code cleanup with the following exceptions&lt;br/&gt;
1.  Line length used is 120 instead of 80. &lt;br/&gt;
2.  static final log is kept as is. not LOG. &lt;/p&gt;</description>
                <environment></environment>
        <key id="12443201">MAHOUT-220</key>
            <summary>Mahout Bayes Code cleanup</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="robinanil">Robin Anil</assignee>
                                    <reporter username="robinanil">Robin Anil</reporter>
                        <labels>
                    </labels>
                <created>Sun, 13 Dec 2009 15:01:03 +0000</created>
                <updated>Sat, 21 May 2011 04:23:53 +0100</updated>
                            <resolved>Fri, 5 Feb 2010 09:38:30 +0000</resolved>
                                    <version>0.3</version>
                                    <fixVersion>0.3</fixVersion>
                                    <component>Classification</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="12789903" author="srowen" created="Sun, 13 Dec 2009 15:25:11 +0000"  >&lt;p&gt;I&apos;m all for it. Do you need someone to commit?&lt;/p&gt;</comment>
                            <comment id="12789907" author="robinanil" created="Sun, 13 Dec 2009 15:42:19 +0000"  >&lt;p&gt;Not yet. Still the mvn checkstyle:checkstyle is throwing some errors. Is the checkstyle old there?&lt;/p&gt;</comment>
                            <comment id="12790653" author="isabel" created="Tue, 15 Dec 2009 10:37:02 +0000"  >&lt;p&gt;Before reorganizing code - could someone who is more familiar with the specific rules of the code-style used at Lucene double-check the exact checkstyle rules used for site-generation? I reused the checkstyle configuration that was already in Mahout-trunk (relaxing some of its rules) but am in doubt whether it really reflects our rules.&lt;/p&gt;</comment>
                            <comment id="12794433" author="srowen" created="Thu, 24 Dec 2009 14:22:54 +0000"  >&lt;p&gt;Robin is this something I should commit or do you have an updated version?&lt;/p&gt;</comment>
                            <comment id="12794678" author="robinanil" created="Sun, 27 Dec 2009 10:30:08 +0000"  >&lt;p&gt;I will update a new patch. I am reverting all these changes. Will stick to 80 column format and the new lucene code formatter. Will start re-working from latest trunk&lt;/p&gt;</comment>
                            <comment id="12794960" author="robinanil" created="Mon, 28 Dec 2009 23:15:27 +0000"  >&lt;p&gt;This is the Formatted cleaned up mahout  bayes code based on the &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-233&quot; title=&quot;Modifying Mahout Check style to match our current coding style&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-233&quot;&gt;&lt;del&gt;MAHOUT-233&lt;/del&gt;&lt;/a&gt; checkstyle and Eclipse formatter&lt;/p&gt;</comment>
                            <comment id="12794976" author="tdunning" created="Tue, 29 Dec 2009 00:36:43 +0000"  >
&lt;p&gt;Robin,&lt;/p&gt;

&lt;p&gt;I was just looking at some of the code and was having a hard time understanding the way that the implementations of bayes.interfaces.DataSource were storing their information.  I also had trouble understand just what it was that was being stored.&lt;/p&gt;

&lt;p&gt;I think that a tiny amount of package or class level comments would clear that up enormously.&lt;/p&gt;

&lt;p&gt;My goal in reading the code was to understand how much my recent start on an sgd classifier could share with the already existing Naive bayes classifiers.  I mention that since it alwyas helps me write comments if I know what the question in the reader&apos;s mind is that I need to answer. &lt;/p&gt;</comment>
                            <comment id="12795050" author="robinanil" created="Tue, 29 Dec 2009 13:33:05 +0000"  >&lt;p&gt;Datastore is an interface which allows you pick a named vector or a named matrix and lookup the cell.  &lt;br/&gt;
For Bayes classifier, the entire code is based on tokens and not SparseVectors. The names of the matrix, the row and column are therefore string and the contract between the Algorithm and Datastore is decided per algo. for the Cbayes/Bayes algorithms, We have the HBaseBayesDatastore.java and InMemoryBayesDatastore.java. &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  &lt;span class=&quot;code-object&quot;&gt;double&lt;/span&gt; getWeight(&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; matrixName, &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; row, &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; column) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; InvalidDatastoreException;
  &lt;span class=&quot;code-object&quot;&gt;double&lt;/span&gt; getWeight(&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; vectorName, &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; index) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; InvalidDatastoreException;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For sgd algorithm. I suggest you define your own matrix names, row indices and column indices, which your algorithm and your datastore agree upon.&lt;/p&gt;

&lt;p&gt;I know it, this creates a limitation that you cant use integer based column and row names. Maybe we can parameterize it OR change Bayes package to use Vectors instead of the current string token based implementation. &lt;/p&gt;

&lt;p&gt;I am currenly writing a Map/reduce job to convert text documents to vectors without relying on Lucene. Once that is done, I will overhaul the classifier package to use SparseVectors. &lt;/p&gt;

&lt;p&gt;Before that I need to know if this Patch is ok. In terms of code style, I will then patch it and start with the enhancements. &lt;/p&gt;</comment>
                            <comment id="12795114" author="jake.mannix" created="Tue, 29 Dec 2009 19:18:21 +0000"  >&lt;p&gt;Robin,&lt;/p&gt;

&lt;p&gt;  To really be scalable here, I&apos;m down with the M/R approach for the classifiers.  The random-access nature of the current Datastore interface definitely seems limiting - even using HBase this way means we&apos;re making lots of remote calls, while a traditional hadoop job would do the nice &quot;put the coding where the data lives&quot; instead.&lt;/p&gt;

&lt;p&gt;Switching over to use SparseVectors and doing things sequentially over the data set stored in SequenceFile&apos;s of them seems definitely the way I&apos;d see this going.  Is that what your current hadoopified version of this do?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I am currenly writing a Map/reduce job to convert text documents to vectors without relying on Lucene.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;What is the way you&apos;re doing this?  Is this bag-of-words representation (what form of tf are you using?  how are you putting in idf if it&apos;s fully distributed?)?&lt;/p&gt;</comment>
                            <comment id="12795117" author="robinanil" created="Tue, 29 Dec 2009 19:35:53 +0000"  >&lt;p&gt;A Caching layer is implemented in HbaseDatastore, You can set the cache size. Take a look at &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-124&quot; title=&quot;Online Classification using HBase&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-124&quot;&gt;&lt;del&gt;MAHOUT-124&lt;/del&gt;&lt;/a&gt; for more details&lt;/p&gt;

&lt;p&gt;I am just porting the feature mapper and tfidf mapper from bayes classifier common over to make a the new text vectorizer. Take a look at them. Its a fully distributed way of doing tf.idf in 2 map/reduces. &lt;/p&gt;

&lt;p&gt;For the vector convertor&lt;br/&gt;
Here is the idea in Steps&lt;/p&gt;

&lt;p&gt;M/R1:  Count frequencies of words tokenized using configurable lucene Analyzer&lt;br/&gt;
SEQ1: read the frequency list, prune words less than minSupport and create the dictionary file(string =&amp;gt; long) and the frequency file (string=&amp;gt;long)&lt;br/&gt;
Do map/reduce in chunks by keeping a block of the dictionary file in memory. &lt;br/&gt;
   repeat- M/R2: Run over the input documents. replacing string with the integer id. and create (docid =&amp;gt; sparsevector). This sparsevector as weigths as TF. but its incomplete.&lt;br/&gt;
Now run a map reduce over the incomplete sparse vectors. Group by docid.In reducer, merge the sparse vectors. &lt;br/&gt;
Initial SparseVectors dataset is ready.&lt;/p&gt;

&lt;p&gt;function multiplyIDF()&lt;/p&gt;
{
M/R3: Calculate DF from the SparseVector dataset
M/R4: Run over the SparseVector TF dataset. and get IDF.
}


&lt;p&gt;This is the first plan. Atleast when i finish. Second is to convert the document into a stream of integers using the dictionary file. Then subsequent funcitons can run M/R jobs to calculate LLR and make bigrams. &lt;/p&gt;

&lt;p&gt;For this. The sparsevector merge MapReduce fucntion should be generic enough. &lt;/p&gt;



</comment>
                            <comment id="12795122" author="tdunning" created="Tue, 29 Dec 2009 19:48:56 +0000"  >
&lt;p&gt;Anil,&lt;/p&gt;

&lt;p&gt;See classifier.sgd.TermRandomizer (and implementations DenseRandomizer and BinaryRandomizer) for a term list to vector converter.  These are in the &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-228&quot; title=&quot;Need sequential logistic regression implementation using SGD techniques&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-228&quot;&gt;&lt;del&gt;MAHOUT-228&lt;/del&gt;&lt;/a&gt; patch.&lt;/p&gt;

&lt;p&gt;It has the virtue of converting term lists to vectors of fixed size.  It currently does not do term weighting, but that would be a very easy fix.  The approach is roughly along the lines of &lt;a href=&quot;http://arxiv.org/PS_cache/arxiv/pdf/0902/0902.2206v2.pdf&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://arxiv.org/PS_cache/arxiv/pdf/0902/0902.2206v2.pdf&lt;/a&gt; or the stochastic decomposition work.&lt;/p&gt;

&lt;p&gt;If you like these, we can promote them to a common area under classifier.&lt;/p&gt;</comment>
                            <comment id="12795124" author="jake.mannix" created="Tue, 29 Dec 2009 19:54:41 +0000"  >&lt;p&gt;Ted,&lt;/p&gt;

&lt;p&gt;  While I&apos;m totally down with using the randomizer / hashing techniques in places, I don&apos;t think we should totally wed ourselves to it either - having the option of using the &quot;real&quot; vector representation should probably be implemented to, as people understand it better, and it&apos;s pretty standard.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;If you like these, we can promote them to a common area under classifier.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;  They might belong in a more general place, actually.  If I&apos;m going to use some of this stuff in the decompositions (although I&apos;m not sure yet of the efficacy of the single hash for doing SVD), it should go somewhere in the math module.&lt;/p&gt;</comment>
                            <comment id="12795127" author="robinanil" created="Tue, 29 Dec 2009 20:00:09 +0000"  >&lt;p&gt;I am not very clear what is happening there when two words have the same hash?. Arent we loosing out on a lot of information ? The one i am proposing is going to do exact numbering of the features. &lt;/p&gt;

&lt;p&gt;One thing my method suffer from is addition of new data. It will take another couple of M/R to create the new dictionary file, while preserving the old ids. Its cumbersome but doable.&lt;br/&gt;
What is happening in a Randomizer approach. Since you are fixing the feature set size. The new hash ids will also change when that feature set size increase right?&lt;/p&gt;</comment>
                            <comment id="12795128" author="jake.mannix" created="Tue, 29 Dec 2009 20:00:58 +0000"  >&lt;p&gt;Anil,&lt;/p&gt;

&lt;p&gt;  Your map-reduces look great, that&apos;s the kind of thing I&apos;ve done for this as well.  Good stuff.  &lt;/p&gt;

&lt;p&gt;As for HBase and caching layers,  I&apos;d say it&apos;s still not fully scalable, as it&apos;s limited by whatever cache size you set, and your hit/miss ratio.  It seems the Datastore interface really is just a wrapper around Matrix and Vector, calling out to the entries.  Doing so in a random-access fashion seems like the reverse of the the way I&apos;d do it: pass the Algorithm &lt;b&gt;to&lt;/b&gt; the Datastore, and have the computations be done where the data lives (iterate over the Datastore internally, either in memory, or if it knows it&apos;s backed by mySQL, say, it can batch calls to the db, pulling chunks into memory, if it&apos;s HDFS-backed, then it can fire off a M/R job, etc...).&lt;/p&gt;</comment>
                            <comment id="12795131" author="jake.mannix" created="Tue, 29 Dec 2009 20:04:33 +0000"  >&lt;blockquote&gt;&lt;p&gt;I am not very clear what is happening there when two words have the same hash?. Arent we loosing out on a lot of information ?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;You can lose some information, sure, but there are &lt;b&gt;tons&lt;/b&gt; of words, and you don&apos;t lose much information.  It is a probabilistic technique though.&lt;/p&gt;

&lt;p&gt;Personally I prefer the mutli-hash approach, because at least there I really believe the projection is preserving distances properly.  In the single hash case, sometimes (ie for some single word documents, with different words), the collapse of distance is extreme (as Robin is alluding to).&lt;/p&gt;</comment>
                            <comment id="12795133" author="robinanil" created="Tue, 29 Dec 2009 20:14:41 +0000"  >&lt;p&gt;Anyways, I guess we are sounding like ML engineers here. This is a library, our job is to have options for people like us to debate over &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;. So lets agree upon a common mechanism. &lt;/p&gt;

&lt;p&gt;i.e Have different ways to create a term frequency vector. ie List&amp;lt;String&amp;gt; =&amp;gt; SparseVector from documents. &lt;/p&gt;

&lt;p&gt;Once the SparseVector is created. Use uniform M/R jobs to do things like tfidf weighting, log likelihood(although i think we need the orginal file to get the co-occurrence and not the SparseVector)&lt;/p&gt;

&lt;p&gt;Any ideas?&lt;/p&gt;



</comment>
                            <comment id="12795135" author="tdunning" created="Tue, 29 Dec 2009 20:20:20 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Robin: I am not very clear what is happening there when two words have the same hash?. Arent we loosing out on a lot of information ? The one i am proposing is going to do exact numbering of the features.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That is the point of the &quot;probes&quot; parameter.  That allows for multiple hashing as Jake is suggesting.  If you have, for example, 4 probes for each word, the chances of complete collision is minuscule and where there are collisions, the learning algorithm puts the weight on the non-colliding probes.&lt;/p&gt;

&lt;p&gt;The extreme case is the DenseRandomizer.  Every term gets spread out to every feature so you have collisions on every term on every feature.  Because of the random weighting, you preserve enough information to allow effective learning.&lt;/p&gt;

&lt;p&gt;See vowpal wabbit for a practical example.  They handle 10^12 (very) sparse features in memory and can learn at disk bandwidth in some applications.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Jake: They might belong in a more general place, actually. If I&apos;m going to use some of this stuff in the decompositions (although I&apos;m not sure yet of the efficacy of the single hash for doing SVD), it should go somewhere in the math module.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Should we generalize this concept to Vectorizer?  The dictionary approach can accept a previously computed dictionary (possibly augmenting it on the fly) and might be called a DictionaryVectorizer or WeightedDictionaryVectorizer.  At the level I have been working, the storage of the dictionary is an open question.  The randomizers could inherit from the same basic interface (or abstract class).&lt;/p&gt;</comment>
                            <comment id="12795136" author="tdunning" created="Tue, 29 Dec 2009 20:21:57 +0000"  >&lt;blockquote&gt;
&lt;p&gt;For sgd algorithm. I suggest you define your own matrix names, row indices and column indices, which your algorithm and your datastore agree upon.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That is fine if sgd is an island, but it plausibly should be able to output models to be used by the Bayes classifier in a map-reduce setting.  That requires some documentation of how DataStore is used by the Bayes models.&lt;/p&gt;</comment>
                            <comment id="12795137" author="jake.mannix" created="Tue, 29 Dec 2009 20:27:17 +0000"  >&lt;blockquote&gt;&lt;p&gt;The extreme case is the DenseRandomizer. Every term gets spread out to every feature so you have collisions on every term on every feature. Because of the random weighting, you preserve enough information to allow effective learning.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, this is the use case in the stochastic decomposition case, cool.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Should we generalize this concept to Vectorizer? The dictionary approach can accept a previously computed dictionary (possibly augmenting it on the fly) and might be called a DictionaryVectorizer or WeightedDictionaryVectorizer. At the level I have been working, the storage of the dictionary is an open question. The randomizers could inherit from the same basic interface (or abstract class).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Definitely.  &lt;/p&gt;</comment>
                            <comment id="12795139" author="robinanil" created="Tue, 29 Dec 2009 20:30:04 +0000"  >&lt;p&gt;The current Bayes implementation is an island. if you skim through the training mechanism. Its a very optimised. (with least map/reduces) and the kind of information I store in hbase and in memory is very specific to that paper. &lt;/p&gt;

&lt;p&gt;First there is the weight, which is a matrix of feature as row and label as column and cell as the weight.&lt;br/&gt;
Secondly, there is sum of cols and rows. put along with the weight matrix. &lt;br/&gt;
Then there are special rows containing, the theta normalizer and alpha smoothing value etc. &lt;/p&gt;

&lt;p&gt; You can see its not really doing bayes rule. it is reproducing the math of CBayes paper.  So I see noway of it direcly using the sgd model. &lt;/p&gt;

&lt;p&gt;We could have a Bayes Algo implementation specfic to the model you are training.  If thats ok?&lt;/p&gt;</comment>
                            <comment id="12795140" author="jake.mannix" created="Tue, 29 Dec 2009 20:31:49 +0000"  >&lt;blockquote&gt;&lt;p&gt;Robin:  This is a library, our job is to have options for people like us to debate over . So lets agree upon a common mechanism.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yep, agreed.  We need fully deterministic techniques as well as probabilistic ones (which will often scale better), and let people use what works for them and they are comfortable with.&lt;/p&gt;</comment>
                            <comment id="12795320" author="gsingers" created="Wed, 30 Dec 2009 14:21:35 +0000"  >&lt;p&gt;FWIW, I&apos;d say stuff that converts text, etc. to our internal representations belongs in the Utils module, where all the &quot;helper&quot; classes are.&lt;/p&gt;</comment>
                            <comment id="12795551" author="tdunning" created="Thu, 31 Dec 2009 06:16:55 +0000"  >&lt;blockquote&gt;
&lt;p&gt;FWIW, I&apos;d say stuff that converts text, etc. to our internal representations belongs in the Utils module, where all the &quot;helper&quot; classes are. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That is what I would have liked to do.  We have some problems with dependencies.&lt;/p&gt;

&lt;p&gt;Utils depends on core which depends on math.  But the classifier stuff depends on vectorization and is in core.  That means that either the classifier stuff has to be moved to utils (or a new module before utils) or vectorization moved down.  Math seems like a nice home for vectorization, but some kinds need random numbers and RandomUtils is in core.&lt;/p&gt;

&lt;p&gt;Bleah.&lt;/p&gt;

&lt;p&gt;My final answer is to put the vectorization in core, partly because I am too lazy to re-imagine the entire module structure.  This does raise the question of whether our modules are serving our needs or we are serving theirs.&lt;/p&gt;
</comment>
                            <comment id="12795554" author="jake.mannix" created="Thu, 31 Dec 2009 06:42:49 +0000"  >&lt;blockquote&gt;&lt;p&gt;This does raise the question of whether our modules are serving our needs or we are serving theirs.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Heh, good question.  I really like math &lt;b&gt;not&lt;/b&gt; depending on core, for lots of reasons (for example, other projects which want our math can import that without needing core, and once I figure out how to do &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-205&quot; title=&quot;Pull Writable (and anything else hadoop dependent) out of the matrix module&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-205&quot;&gt;&lt;del&gt;MAHOUT-205&lt;/del&gt;&lt;/a&gt; correctly, without needing hadoop at all, which is a big plus).&lt;/p&gt;

&lt;p&gt;Why does utils depend on core?  Why is utils in its own module anyways, instead of just being in core?  Is there anything which we imagine will depend on core but not need utils?  Does RandomUtils need to be in core, or can it get pushed down to math?&lt;/p&gt;</comment>
                            <comment id="12795603" author="gsingers" created="Thu, 31 Dec 2009 14:33:24 +0000"  >&lt;p&gt;Yeah, I don&apos;t think Utils should need to depend on core.  I would think it should be:&lt;/p&gt;

&lt;p&gt;Most things depend on Math (i.e. Vectors) including core and utils&lt;br/&gt;
Utils should be standalone tools for getting things into the appropriate mathematical representation which can be consumed by core, et. al.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Why is utils in its own module anyways, instead of just being in core?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I imagined utils to be the place where things that did useful ancillary tasks lived, such as converting ARFF to Mahout Vector or converting a Lucene index to Mahout Vector or converting a whole slew of raw text to Mahout Vector.  That way the core wouldn&apos;t need to be muddied by the various dependencies and could be as lightweight as possible.  So, overtime, Utils may grow to have a dep. on Tika for instance or a pipeline like OpenPipeline, but the core need not know anything about it.&lt;/p&gt;

&lt;p&gt;Bleah, indeed!&lt;/p&gt;</comment>
                            <comment id="12795921" author="srowen" created="Sat, 2 Jan 2010 21:16:32 +0000"  >&lt;p&gt;I think a utils module remains a good idea, or else core starts to depend on a whole bunch of stuff merely because of some tool code sitting around. It seems right to me.&lt;/p&gt;

&lt;p&gt;Math should also not depend on core.&lt;/p&gt;

&lt;p&gt;... but back to the issue at hand, literally, is this ready to commit? seems like this was not the original topic of the issue.&lt;/p&gt;</comment>
                            <comment id="12796490" author="robinanil" created="Tue, 5 Jan 2010 02:24:49 +0000"  >&lt;p&gt;I am ready to commit the first cut, before moving on to more cleanups.&lt;/p&gt;

&lt;p&gt;But this cleanup depends on the codestyle xml diff that I posted. Anyone care to take a look at that ?&lt;/p&gt;</comment>
                            <comment id="12803466" author="srowen" created="Thu, 21 Jan 2010 20:11:26 +0000"  >&lt;p&gt;I believe you&apos;re clear to commit that code style patch, and this, and close this up.&lt;/p&gt;</comment>
                            <comment id="12830026" author="robinanil" created="Fri, 5 Feb 2010 09:37:54 +0000"  >&lt;p&gt;Linked&lt;/p&gt;</comment>
                            <comment id="12830028" author="robinanil" created="Fri, 5 Feb 2010 09:38:30 +0000"  >&lt;p&gt;Committed. &lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                            <outwardlinks description="blocks">
                                        <issuelink>
            <issuekey id="12444260">MAHOUT-233</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12429039" name="MAHOUT-BAYES.patch" size="281286" author="robinanil" created="Mon, 28 Dec 2009 23:15:27 +0000"/>
                            <attachment id="12427855" name="MAHOUT-BAYES.patch" size="193566" author="robinanil" created="Sun, 13 Dec 2009 15:01:33 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Sun, 13 Dec 2009 15:25:11 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9845</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hxy6gv:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>23199</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                </customfields>
    </item>
</channel>
</rss>