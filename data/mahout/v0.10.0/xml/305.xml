<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 04:26:50 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/MAHOUT-305/MAHOUT-305.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[MAHOUT-305] Combine both cooccurrence-based CF M/R jobs</title>
                <link>https://issues.apache.org/jira/browse/MAHOUT-305</link>
                <project id="12310751" key="MAHOUT">Mahout</project>
                    <description>&lt;p&gt;We have two different but essentially identical MapReduce jobs to make recommendations based on item co-occurrence: org.apache.mahout.cf.taste.hadoop.&lt;/p&gt;
{item,cooccurrence}
&lt;p&gt;. They ought to be merged. Not sure exactly how to approach that but noting this in JIRA, per Ankur.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12456936">MAHOUT-305</key>
            <summary>Combine both cooccurrence-based CF M/R jobs</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="ankur">Ankur</assignee>
                                    <reporter username="srowen">Sean Owen</reporter>
                        <labels>
                    </labels>
                <created>Sun, 21 Feb 2010 13:28:07 +0000</created>
                <updated>Sun, 31 Oct 2010 15:49:08 +0000</updated>
                            <resolved>Wed, 28 Apr 2010 05:42:26 +0100</resolved>
                                    <version>0.2</version>
                                    <fixVersion>0.4</fixVersion>
                                    <component>Collaborative Filtering</component>
                        <due>Wed, 31 Mar 2010 00:00:00 +0000</due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="12836543" author="ankur" created="Mon, 22 Feb 2010 08:13:50 +0000"  >&lt;p&gt;Sean, Thanks for filing the jira. Nothing points from our discussion here.&lt;/p&gt;

&lt;p&gt;1. Need to decide on the dataset to run both the implementations on. I have netflix dataset in mind but a strange thing I observed during my tests with it is that there were 2 - 3 users who rated more than 10,000 movies! This seemed a little odd to me. Can you or some else who has had experience with the dataset validate my observation ?  &lt;/p&gt;

&lt;p&gt;2. Both the implementations need to run on dataset in the identical environment to gauge performance and accuracy. For accuracy I believe we need to do a Precision-Recall test. My understanding of it is that &lt;/p&gt;

&lt;p&gt;      a) Do a 80-20 split of the data (80% train and 20% test) with split happening on a timeline. &lt;br/&gt;
      b) Feed training data to the algorithm and generate recommendations for a subset of users from training data. &lt;br/&gt;
      c) Compare those recommendations with items actually present in the history of user in test data.&lt;br/&gt;
      d) Calculate precision = tp / (tp + fp) = (recommendations actually present in user&apos;s history) / (total items recommended)&lt;br/&gt;
      e) Calculate recall = tp / (tp + fn) =    (recommendations actually present in user&apos;s history) / (total items in user&apos;s history)&lt;br/&gt;
      f) Finally take a simple avg of both across all the users to get approx global precision/recall. &lt;/p&gt;

&lt;p&gt;please feel free to correct any of the step above if I misunderstood anything.&lt;/p&gt;</comment>
                            <comment id="12836666" author="ankur" created="Mon, 22 Feb 2010 15:30:27 +0000"  >&lt;p&gt;Hey Sean,&lt;br/&gt;
               Have you played with netflix dataset? Are there really user who have rated more than 10,000 movies? For PR test do we have something already that will work in this case or some coding is required ? Any other thoughts ?  &lt;/p&gt;</comment>
                            <comment id="12836689" author="srowen" created="Mon, 22 Feb 2010 16:07:12 +0000"  >&lt;p&gt;Yes there are some prolific users. I don&apos;t have anything ready-made for such a test; the existing eval framework won&apos;t work here. I think it would need a bit of coding to pull out some test data, run the job, compare the results.&lt;/p&gt;

&lt;p&gt;I have only one little tweak to make to the procedure you mention here. Really, we ought to pull out the most-preferred movies as test data. After all the recommendations will be for those movies that should be rated highly. We wouldn&apos;t want to punish the algorithm for failing to recommend something I have rated, but didn&apos;t like, over something I haven&apos;t rated but indeed would like.&lt;/p&gt;

&lt;p&gt;One very crude way to do this is remove all 5-star ratings in the data set, and see how many of those actually come back in the recommendations.&lt;/p&gt;</comment>
                            <comment id="12836725" author="ankur" created="Mon, 22 Feb 2010 17:02:55 +0000"  >&lt;p&gt;Typically when doing train-test data split, we divide the data on a timeline. So as a simple example if we have 10 days data then we would keep last 2 days data as test data and remaining as training data. If we remove all 5 star rating the crude way, we may not be able to ensure this condition, not a hard one but still a best practice AFAIK.  Also I am not sure if 5 star ratings would be 20 or even 10% of the total data.&lt;/p&gt;

&lt;p&gt;The crude way you mentioned is ok for a start but I am not sure if its a fair evaluation or not. Also with this we would effectively be calculating precision as&lt;br/&gt;
precision = (5 start recommendations actually present in user&apos;s history) / (total 5 star recommendations)&lt;br/&gt;
recall = (5 start recommendations actually present in user&apos;s history) / (total 5 start items in user&apos;s history)&lt;/p&gt;

&lt;p&gt;is that what you mean?&lt;/p&gt;</comment>
                            <comment id="12836733" author="srowen" created="Mon, 22 Feb 2010 17:20:33 +0000"  >&lt;p&gt;Say I&apos;ve made the following ratings:&lt;/p&gt;

&lt;p&gt;5 stars: Harry Potter&lt;br/&gt;
5 stars: Harry Potter 2&lt;br/&gt;
1 star: Maid in Manhattan&lt;/p&gt;

&lt;p&gt;Say I remove Maid in Manhattan as test data. I run recommendations and it recommends to me Harry Potter 3 (which presumably I would rate highly). The implementation would be penalized for not returning Maid in Manhattan, when that&apos;s surely not what it should have returned.&lt;/p&gt;

&lt;p&gt;Even if you take out only the most highly-rated movies as test data (this is what the existing CF precsion/recall evaluator does), this phenomenon can still occur: the recommender could return a movie that&apos;s better than anything you&apos;ve yet seen but that would be considered &apos;bad&apos; by this evaluation style. It&apos;s still not a fair test, but it&apos;s less un-fair.&lt;/p&gt;

&lt;p&gt;Yes you could take the 20% most-highly-rated movies from each user as test data if you like, not just 5-star.&lt;/p&gt;

&lt;p&gt;Say I ask for 10 recommendations. Precision @ 10 is the proportion of those 10 that were in the users&apos; history (top ratings). Recall @ 10 is the proportion of all top-rated items that appeared in those 10. I think this is a little different than what you&apos;re saying?&lt;/p&gt;</comment>
                            <comment id="12837123" author="ankur" created="Tue, 23 Feb 2010 07:34:52 +0000"  >&lt;p&gt;With co-occurrence analysis we are dropping ratings. So if there are a lot of people who watched &quot;Harry Potter&quot; also watched &quot;Maid in manhattan&quot; it will have a higher chance of getting recommended regardless of ratings.&lt;/p&gt;

&lt;p&gt;I am trying not be influenced too much by ratings as that is not the strength of this algorithm. Where it really shines is when you have lots and lots of sparse user click data where a click may be present or absent. Something like an online book store  or a shopping site. We are sticking with netflix as there is no such publicly available dataset AFAIK.    &lt;/p&gt;

&lt;p&gt;Ok so moving forward with the action plan, here is what I propose to do. Please feel free to suggest modifications.&lt;/p&gt;

&lt;p&gt;1. For each user take out the most recent movies that he has rated 3 or 4 or 5 as TEST data.  Use the remaining as TRAIN data. &lt;br/&gt;
2. Run both implementations in identical environment on test data and record runtimes and results&lt;br/&gt;
3. Join recommendation results with TEST data on &apos;user&apos; key and calculate precision recall.&lt;br/&gt;
4. Report average precision &amp;amp; recall.&lt;/p&gt;

&lt;p&gt;Ok so when separating top ratings as TEST data. For each user &lt;/p&gt;

&lt;p&gt;Precision @10 = (3,4,5 rating movies recommended &amp;amp; actually present ) / 10&lt;br/&gt;
Recall @ 10 =  (3,4,5 rating movies recommended &amp;amp; actually present ) / (all 3,4,5 movies seen by user)&lt;/p&gt;

&lt;p&gt;Hope this was more clear.&lt;/p&gt;</comment>
                            <comment id="12837156" author="srowen" created="Tue, 23 Feb 2010 09:07:32 +0000"  >&lt;p&gt;You don&apos;t entirely drop ratings, it&apos;s that they don&apos;t figure into the similarity metric, right? But yes, ratings are not relevant to the point I was trying to make. Regardless, Harry Potter 3 is a better recommendation.&lt;/p&gt;

&lt;p&gt;I really think you have to take out the highest-rated items or this is a fairly flawed test, for this reason. Does anyone else have experience in or thoughts on defining precision and recall in this context? 3,4,5 is arbitrary, just pick the top n, or top n%, I&apos;d imagine.&lt;/p&gt;</comment>
                            <comment id="12837192" author="ankur" created="Tue, 23 Feb 2010 11:03:33 +0000"  >&lt;p&gt;Just picking random N % data for each user calculating avg precision and recall across all users in test data  and then repeating the test K times to take average across all runs should be reasonably fair assessment IMHO.&lt;/p&gt;

&lt;p&gt;Mahouters your opinion here would be valuable.&lt;/p&gt;</comment>
                            <comment id="12837193" author="srowen" created="Tue, 23 Feb 2010 11:09:36 +0000"  >&lt;p&gt;I just don&apos;t think it can be random. it&apos;s like doing a PR test on search results and defining &quot;relevant&quot; documents as some randomly-chosen subset of all documents.&lt;/p&gt;</comment>
                            <comment id="12837198" author="ankur" created="Tue, 23 Feb 2010 11:31:50 +0000"  >&lt;p&gt;I am not proposing that we choose random subset over all movies.  Rather choose random N% movie ratings  from EACH user and use it as test data to get precision recall across this test set.  Also repeat this procedure X times to get a fair assessment. They seem to do it the same way - &lt;a href=&quot;http://www2007.org/papers/paper570.pdf&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www2007.org/papers/paper570.pdf&lt;/a&gt; &lt;/p&gt;</comment>
                            <comment id="12837203" author="srowen" created="Tue, 23 Feb 2010 11:37:11 +0000"  >&lt;p&gt;Yes I understand that, and it still doesn&apos;t change the issue here.&lt;/p&gt;

&lt;p&gt;The paper here deals with a data set with no ratings; picking any item as test data is as good as the next. This isn&apos;t the case when we have ratings, and we do.&lt;/p&gt;</comment>
                            <comment id="12837205" author="ankur" created="Tue, 23 Feb 2010 11:48:40 +0000"  >&lt;p&gt;Well! not factoring ratings in the similarity metric but having them influence the train/test data for evaluation doesn&apos;t sound fair to me. So I don&apos;t think both of us agree on the evaluation methodology.  &lt;/p&gt;</comment>
                            <comment id="12837209" author="srowen" created="Tue, 23 Feb 2010 11:56:28 +0000"  >&lt;p&gt;They don&apos;t influence the similarity metric but they do influence the estimated ratings and therefore recommendations. Are we talking about the same algorithm? The last step is to multiply the co-occurrence matrix by the user &lt;del&gt;rating&lt;/del&gt; vector.&lt;/p&gt;</comment>
                            <comment id="12837230" author="ankur" created="Tue, 23 Feb 2010 12:56:33 +0000"  >&lt;p&gt;&lt;b&gt;smile&lt;/b&gt; There we go. &lt;br/&gt;
Our last steps are essentially different. I don&apos;t do any multiplication, instead I just join (user, movie) on &apos;movie&apos;  with co-occurrence set followed by a group on &apos;user&apos; to calculate recommendations. I guess while joining I should multiply ratings with co-occurrence counts for better evaluation.&lt;/p&gt;

&lt;p&gt;Can you give a small illustrative example with dummy data to describe your last steps? &lt;/p&gt;</comment>
                            <comment id="12837245" author="srowen" created="Tue, 23 Feb 2010 14:11:46 +0000"  >&lt;p&gt;Yeah it&apos;s just a small generalization &amp;#8211; what I&apos;m up to reduces to the same thing if all ratings are 1. You can make it run faster if there are no ratings involved, I&apos;m sure.&lt;/p&gt;

&lt;p&gt;I&apos;m going to send you a draft of chapter 6 of MiA which has a complete writeup on this.&lt;/p&gt;</comment>
                            <comment id="12837377" author="tdunning" created="Tue, 23 Feb 2010 18:44:02 +0000"  >
&lt;p&gt;My own experience is that all that counts in recommendations is the probability of click (interest) on a set of recommendations.  As such, the best analog is probably precision at 10 or 20.  I don&apos;t think that recall at 10 or 20 makes any sense at all (with a depth limited situation like this, you have given up on recall and are only looking at precision).&lt;/p&gt;

&lt;p&gt;Ankur&apos;s suggestion about keeping the most recent 4&apos;s and 5&apos;s as test data seems right to me.  My only beefs are that you don&apos;t need recall@10 and what to do with the unrated items.  Presumably a new style algorithm could surface items that the user hadn&apos;t thought of, but really likes.  In practice, I think that counting unrated items in the results as misses isn&apos;t a big deal in the Netflix data.  In the real world where test data is more scarce, I would count unrated items as misses in off-line evaluation, but try to run as many alternatives as possible against live users.&lt;/p&gt;

</comment>
                            <comment id="12837378" author="tdunning" created="Tue, 23 Feb 2010 18:44:03 +0000"  >
&lt;p&gt;My own experience is that all that counts in recommendations is the probability of click (interest) on a set of recommendations.  As such, the best analog is probably precision at 10 or 20.  I don&apos;t think that recall at 10 or 20 makes any sense at all (with a depth limited situation like this, you have given up on recall and are only looking at precision).&lt;/p&gt;

&lt;p&gt;Ankur&apos;s suggestion about keeping the most recent 4&apos;s and 5&apos;s as test data seems right to me.  My only beefs are that you don&apos;t need recall@10 and what to do with the unrated items.  Presumably a new style algorithm could surface items that the user hadn&apos;t thought of, but really likes.  In practice, I think that counting unrated items in the results as misses isn&apos;t a big deal in the Netflix data.  In the real world where test data is more scarce, I would count unrated items as misses in off-line evaluation, but try to run as many alternatives as possible against live users.&lt;/p&gt;

</comment>
                            <comment id="12837517" author="srowen" created="Tue, 23 Feb 2010 23:35:47 +0000"  >&lt;p&gt;Agree, I do not see recall as useful, but hey, including it for completeness isn&apos;t a big deal.&lt;/p&gt;

&lt;p&gt;Yeah in this context there&apos;s no choice but to count unrated items as misses. My intuition based on limited experience is it is in fact an issue &amp;#8211; are the best items for a user typically found among their ratings in real-world data sets? I just can&apos;t imagine it&apos;s so for most users, who express few ratings.&lt;/p&gt;

&lt;p&gt;So I harp on picking the highest-rated items for exclusion, and based only on the criteria, in order to align best with the process being tested, because even then the test is fairly flawed, but at least minimally flawed.&lt;/p&gt;</comment>
                            <comment id="12837528" author="tdunning" created="Tue, 23 Feb 2010 23:52:50 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Yeah in this context there&apos;s no choice but to count unrated items as misses. My intuition based on limited experience is it is in fact an issue - are the best items for a user typically found among their ratings in real-world data sets? I just can&apos;t imagine it&apos;s so for most users, who express few ratings.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This suggests that mean reciprocal rank (MRR) of the top 5 or 10 highly rated items might be a useful measure.  Even if the top 10 has several unrated good choices, if the rated choices are all pretty high then you can have pretty good feelings even if they didn&apos;t quite make the top 10.&lt;/p&gt;</comment>
                            <comment id="12838044" author="jamborta" created="Wed, 24 Feb 2010 22:41:20 +0000"  >&lt;p&gt;I usually pick random N % data for each user as Ankur suggested. This would ensure that the recommender is not biased, and it doesn&apos;t really matter that non-relevant items are in this subset, since they needed to be ranked lower anyway. &lt;/p&gt;

&lt;p&gt;I think the way Sean implemented it is also pretty good, taking the top-n relevant items and evaluating on that data, but you have to build a new model for each user, which makes it impossible to use it on a big data set, especially with SVD.&lt;/p&gt;

&lt;p&gt;I agree that the other issue is how to deal with non-rated item. I personally just rank items that has known ratings, so that the relevance judgement is always known. but I&apos;ve been thinking to changing it to count unrated items as non-relevant. I think there are pros and cons either way.&lt;/p&gt;</comment>
                            <comment id="12860221" author="srowen" created="Fri, 23 Apr 2010 12:30:52 +0100"  >&lt;p&gt;First copying and pasting my comment from the mailing list:&lt;/p&gt;

&lt;p&gt;Ankur effectively raised issues about the performance of&lt;br/&gt;
org.apache.mahout.cf.taste.hadoop.item by adding&lt;br/&gt;
org.apache.mahout.cf.taste.hadoop.cooccurrence, which is a similar&lt;br/&gt;
recommender job (item cooccurrence-based) but with a different&lt;br/&gt;
implementation. &quot;.item&quot; ultimately does not distribute the matrix-user&lt;br/&gt;
vector multiply, and &quot;.coocurrence&quot; highly distributes it.&lt;/p&gt;

&lt;p&gt;.item accomplished this by side-loading the co-occurrence matrix into&lt;br/&gt;
a reducer, by accessing it from disk as MapFiles. This way of&lt;br/&gt;
accessing columns proved to be very slow.&lt;/p&gt;

&lt;p&gt;After much experimentation, I&apos;ve completely overhauled .item by&lt;br/&gt;
grafting in ideas from .cooccurrence. It is a sort of&lt;br/&gt;
best-of-both-worlds hybrid of the two. It borrows a clever way to join&lt;br/&gt;
two kinds of input into one MapReduce, in order to join the&lt;br/&gt;
co-occurrence matrix columns and individual elements of each user&lt;br/&gt;
vector. The product is output and recombined later. This hybrid&lt;br/&gt;
retains features of .item like accommodating user ratings.&lt;/p&gt;

&lt;p&gt;Letting Hadoop manage the data flow, even though it takes a bit more&lt;br/&gt;
copying, avoiding reading from MapFile in a random-access manner,&lt;br/&gt;
using features like the Combiner, and being smarter about Writables&lt;br/&gt;
has sped this up for me by at least a factor of 10 &amp;#8211; mostly that&lt;br/&gt;
avoiding MapFiles.&lt;/p&gt;</comment>
                            <comment id="12860223" author="srowen" created="Fri, 23 Apr 2010 12:37:56 +0100"  >&lt;p&gt;And now more thoughts:&lt;/p&gt;

&lt;p&gt;Yes all the code is checked in. &lt;/p&gt;

&lt;p&gt;This is still running perhaps slower than I&apos;d like. The step to distributing the computation more slowed things down considerably in the I/O phases &amp;#8211; but avoided use of MapFile which was in the end just being used very wrongly. So a net win.&lt;/p&gt;

&lt;p&gt;The slowest step by far is outputting the partial vector products. Each is as big as a column of the co-occurrence matrix (which is sparse, yes), and one is output for each preference value. That&apos;s huge. This would be an ideal place for a combiner but it&apos;s a reducer, so it&apos;s not available &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/help_16.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Co-occurrence is also slowish. It does use a combiner but to get a good hit rate, it needs to have a very large buffer.&lt;/p&gt;

&lt;p&gt;Everything works quite well if you&apos;re willing to prune data. For example, very roughly, on a 10M rating data set &amp;#8211; &lt;b&gt;but keeping only 20 prefs per user for each of 70,000 users&lt;/b&gt; &amp;#8211; the total time per users is in seconds of machine time. Not too bad.&lt;/p&gt;

&lt;p&gt;But take that off and this still balloons quite a bit. Naturally, pruning is a good thing but it seems like we should be able to speed up more.&lt;/p&gt;</comment>
                            <comment id="12860265" author="ankur" created="Fri, 23 Apr 2010 16:11:12 +0100"  >&lt;p&gt;&amp;gt; Co-occurrence is also slowish ..&lt;br/&gt;
I am thinking this can be speeded up using the secondary sort trick so that values need not be cached. Also items that fall below a threshold can be pruned. This will help in reducing the size of co-occurrence vector reducing the I/O load. This will speed up recommendations computation also.&lt;/p&gt;
</comment>
                            <comment id="12860284" author="srowen" created="Fri, 23 Apr 2010 16:37:10 +0100"  >&lt;p&gt;What do you mean about the secondary sort and is it something I can set about porting / writing?&lt;/p&gt;

&lt;p&gt;Yes it may be inevitable that one has to prune the data to run this reasonably. It doesn&apos;t necessarily hurt. We probably need better facilities than hard-coding taking the top 20.&lt;/p&gt;

&lt;p&gt;I suggest we merge into the .item implementation, since it resembles the rest of the code base (both other MR jobs and recommender bits) somewhat more. The framework of has been kind of debugged by a few users, and now the machinery has the best of both worlds inside, including a few more new tricks. You&apos;re welcome to continue messing with it.&lt;/p&gt;</comment>
                            <comment id="12860882" author="ankur" created="Mon, 26 Apr 2010 11:19:21 +0100"  >&lt;p&gt;CooccurrenceCombiner caches items internally and increments counts whenever it sees a new value. This might lead to memory issues with some real big datasets. Moreover, for every (item-id, count)  cached, a new object is created to apply a simple procedure. Looks an overkill to me.&lt;/p&gt;

&lt;p&gt;With the secondary sort (item1, item2)  pairs are already sorted so that for each key (item1) all the (item1, item2) pairs appear before (item1, item3) assuming item2 &amp;lt; item3. With this we simple increment the count each time we see item2 and put the (item2, count) entry into a priority queue as soon as we see item3 or something else. The size of the priority queue can be limited to N.  Check out ItemSimilarityEstimator.java.&lt;/p&gt;

&lt;p&gt;Agreed we need better facilities for pruning, something like support-count (any other?).&lt;/p&gt;

&lt;p&gt;About merging, I feel CooccurrenceCombiner would be better with secondary sort. Also it will be good if we can retain TupleWritable for future use. Other than these I have no issues with throwing away code under o.a.m.cf.taste.hadoop.cooccurrence&lt;/p&gt;</comment>
                            <comment id="12860895" author="srowen" created="Mon, 26 Apr 2010 12:17:35 +0100"  >&lt;p&gt;Most broadly, the input is item1-&amp;gt;item2 pairs and the final output of the co-occurrence step is item1-&amp;gt;((item2,count2),(item3,count3),...) &amp;#8211; rows of the co-occurrence matrix. I&apos;m trying to do it in one pass, so need the reducer keyed by item1 instead of a pair. So I generate item1-&amp;gt;(item2,count) in the mapper, combine, and then reduce to the vector in one go.&lt;/p&gt;


&lt;p&gt;But OK you&apos;re suggesting to unpack that, and try to first output (item1,item2)-&amp;gt;count, and from there a second stage can generate the rows.&lt;/p&gt;

&lt;p&gt;In ItemSimilarityEstimator, the map seems to emit (item1,item2)&lt;del&gt;&amp;gt;(item2,count). OK, so I understood the &apos;redundant&apos; item2 in the key to be the secondary sort trick, so you can efficiently sum up counts and emit the final (item1,item2)&lt;/del&gt;&amp;gt;count. But then why is the value (item2,count) and not just count?&lt;/p&gt;

&lt;p&gt;And why is a priority queue needed? Is it just used to store and emit only the top item-item pairs by count? That makes sense, though you&apos;ve made the size 20, isn&apos;t that too small? or is it that the user must set this to a reasonable size. Or you could scrap the priority queue and filter based on a count cutoff. I&apos;m fond of culling co-occurrence of 1 and keeping everything else. No queue needed for that though it doesn&apos;t cap the size of the resulting matrix.&lt;/p&gt;




&lt;p&gt;What worries me is the size of the map output. Spilling an (item1,item2) pair for every co-occurrence is absolutely massive. With P preferences, U users, and I items, you&apos;re spilling about U*(P/U)^2 pairs = P^2/U. With a billion preferences that&apos;s getting easily into quadrillions.&lt;/p&gt;

&lt;p&gt;Now of course the combiner, in theory, prevents almost all of this from spilling to disk. It sums up counts, so the number of pairs output is more on the order of I^2. In practice, I think the combiner doesn&apos;t have enough memory. Before it has to spill its queue through the combiner, it rarely tallies up the same item-item cooccurrence more than once. On a smallish data set I find the &apos;hit rate&apos; about 10% in this regards, even with io.sort.mb increased from a healthy 200MB to 1000MB.&lt;/p&gt;

&lt;p&gt;And this is what&apos;s killing the job, I think, emitting so many low-count pairs. So that&apos;s why I was trying to be very aggressive in the combiner in throwing out data, and maybe need to do more. And being super-aggressive can mean capping the size of that intermediate map quite a bit more. And then that also kind of addresses the scalability bottleneck issue, and enables this to happen in one go anyway.&lt;/p&gt;

&lt;p&gt;Or perhaps I miss why emitting pairs with count is actually going to be very scalable. It&apos;s very attractive after the map phase, but it&apos;s the spill after the map that&apos;s the problem I think.&lt;/p&gt;


&lt;p&gt;TupleWritable is copied and paste from Hadoop right?&lt;/p&gt;</comment>
                            <comment id="12860914" author="srowen" created="Mon, 26 Apr 2010 13:54:45 +0100"  >&lt;p&gt;OK, I think I get the (item1,item2) -&amp;gt; (item2,count) part, at least, why it can be used in conjunction with a one-pass solution.&lt;/p&gt;

&lt;p&gt;I wasn&apos;t sure how you guarantee that all (item1,item2) for item1 arrive at the same reducer. But the answer is the partitioner?&lt;/p&gt;

&lt;p&gt;Then it works; I still think there is a need for lots of pruning and big combiner buffers after the map but that&apos;s different.&lt;/p&gt;

&lt;p&gt;Am I right that (item1,item2) -&amp;gt; count is all that&apos;s needed?&lt;/p&gt;</comment>
                            <comment id="12860931" author="tdunning" created="Mon, 26 Apr 2010 15:22:44 +0100"  >
&lt;p&gt;I think that the key to speed with a cooccurrence counter is not to prune small support items, but rather to prune the most common items.  The problem is that the overall speed is proportional to the average of the square of the number of items per user.   You can either prune each item to only be seen as related to a bounded number of users, or you can limit the number of items each user is considered to have seen.  Either has pretty much the desired impact and neither causes any significant loss of information (partly because users who touch toooo many items are often spammers and partly because we don&apos;t learn anything new about them after the first several hundred items).&lt;/p&gt;

&lt;p&gt;Another alternative is to use a decomposition-based approximation of the cooccurrence matrix.  I think both techniques should be available so the direct counting approach is still valuable.&lt;/p&gt;</comment>
                            <comment id="12860939" author="ankur" created="Mon, 26 Apr 2010 15:39:16 +0100"  >&lt;p&gt;&amp;gt; But the answer is the partitioner ?&lt;br/&gt;
Yes&lt;/p&gt;

&lt;p&gt;&amp;gt; Am I right that (item1, item2) -&amp;gt;count is all that&apos;s needed ?&lt;br/&gt;
Yes&lt;/p&gt;

&lt;p&gt;&amp;gt; And why is the priority queue needed ...&lt;/p&gt;

&lt;p&gt;You could use both a co-occurrence count (your favorite) and max number co-occurrent pair (say 1000). I have chosen a size 100. So for any given item the top-100 co-occurrent items (by count) would be output. Though the size is limited with this it still can cause explosion if there are very long histories. From netflix dataset recall the users who have rated more than 10K movies. So one way of taking care of them is to apply &apos;sessionization&apos; i.e. output a co-occurrence pair only if they are part of a session or satisfy some other constraint. But that is not implemented yet. &lt;/p&gt;

&lt;p&gt;&amp;gt; TupleWritable ...&lt;br/&gt;
Not really. I have a specialized implementation for my own purpose using GenericWritable that wraps each object of TupleWritable.&lt;/p&gt;</comment>
                            <comment id="12861095" author="srowen" created="Mon, 26 Apr 2010 21:46:18 +0100"  >&lt;p&gt;I&apos;m about to commit another pass at this since it&apos;s getting better and better, that would formally collapse the two together per this thread. Then some more ideas can be added in that I didn&apos;t already.&lt;/p&gt;

&lt;p&gt;Ted how do you like to pick which items to pay attention to for co-occurrence? I&apos;m looking for something simple to start.&lt;/p&gt;

&lt;p&gt;Though it&apos;s running pretty well (well a lot better than it was) at the moment, with the aggressive combiner chucking out low-frequency co-occurrence.&lt;/p&gt;</comment>
                            <comment id="12861144" author="srowen" created="Mon, 26 Apr 2010 23:22:25 +0100"  >&lt;p&gt;Ted says he likes LLR, and doesn&apos;t like throwing out the low-count co-occurrences.&lt;/p&gt;

&lt;p&gt;I agree, in the sense that low-count doesn&apos;t mean unimportant. It&apos;s something that LLR that figures out whether it&apos;s meaningless or contains a lot of info.&lt;/p&gt;

&lt;p&gt;I think the sentiment reduces to, this would be a better system if LLRs were used instead of simple co-occurrence counts as weights, which is right. It would involve the whole step of computing all item-item LLRs right? which can be done.&lt;/p&gt;

&lt;p&gt;My vision is to start with this simple system and work towards generalizing, so I can stick in a different means of generation the weights matrix, and different strategies for pruning..&lt;/p&gt;

&lt;p&gt;So if generalizing to create a second, LLR-based system comes next, does it make sense to leave in the dumb co-occurrence based system as well? meh, probably for now. So what&apos;s the appropriately dumb pruning method for co-occurrence counts?&lt;/p&gt;

&lt;p&gt;Since pruning a co-occurrence means setting its count to 0, it made sense to me that the error from pruning is minimized by pruning those with lowest counts (already closest to 0).&lt;/p&gt;

&lt;p&gt;(By the way I meant &apos;running well&apos; in the sense of quickly; I haven&apos;t run much evaluation of the output yet.)&lt;/p&gt;</comment>
                            <comment id="12861171" author="tdunning" created="Tue, 27 Apr 2010 00:45:26 +0100"  >&lt;blockquote&gt;
&lt;p&gt;Ted says he ... doesn&apos;t like throwing out the low-count co-occurrences.&lt;/p&gt;

&lt;p&gt;I agree, in the sense that low-count doesn&apos;t mean unimportant. It&apos;s something that LLR that figures out whether it&apos;s meaningless or contains a lot of info.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Close.  But I would go further and say that on average individual data records that are high count are generally less useful than those with low counts and they are quadratically more expensive to deal with.  That combination of much higher expense and considerably lower value makes it seem to be a good idea to nuke (aka downsample) those records rather than lose the low count stuff.  &lt;/p&gt;

&lt;p&gt;Dropping low count items in the combiner is even worse since there might have been quite a number scattered around that could have added up to interesting levels.&lt;/p&gt;
</comment>
                            <comment id="12861381" author="srowen" created="Tue, 27 Apr 2010 13:15:57 +0100"  >&lt;p&gt;I see, fair enough. Even for this simplistic initial system, something better is called for. Perhaps the mappers can keep a count of how many times each item has been seen and favor co-occurrences among items that have &lt;b&gt;not&lt;/b&gt; been seen. they wouldn&apos;t have a global count but such a simple heuristic may be efficient and effective. For now I might arbitrarily prune, say, for user vectors with more than 50 preferences.&lt;/p&gt;</comment>
                            <comment id="12861520" author="tdunning" created="Tue, 27 Apr 2010 21:08:22 +0100"  >&lt;p&gt;My own approach in the past was to group on user to get a count as well as a list of items for that user.  This can be done in one MR step with a bit of fancy footwork or in two if you want simple.  The fancy footwork involves reading the item list into memory as we sample to avoid keeping too many.  It is relatively easy to do the sampling in a completely fair way, allowing all samples equal chance of survival by using a swapping algorithm.  A completely sampling is also trivial.  With some thought, it is probably possible to do various recency weighted samples as well.  &lt;/p&gt;

&lt;p&gt;With the count for the items for each user, or a clever on-line sampling algorithm I can down-sample the user list before running the actual cooccurrence counting step.  This is a good point to drop users with &amp;lt; k_min items.  k_min should be at least 2 since users with one item cannot give rise to non-trivial cooccurrence.  A value of 3-5 isn&apos;t bad either.&lt;/p&gt;

&lt;p&gt;The total time involved is pretty dominated by the original data reading so the extra MR step doesn&apos;t hurt all that much.  The win obtained by avoiding quadratic explosion of the cooccurrence step is massive.&lt;/p&gt;
</comment>
                            <comment id="12861674" author="srowen" created="Wed, 28 Apr 2010 05:42:26 +0100"  >&lt;p&gt;I think with my latest commit this is substantially done. It weaves together the spirit of your implementation, plus Ted&apos;s ideas about pruning, into the existing shell I had under .item. I retained TupleWritable though it&apos;s not now used. The unified implementation remains open for business for anyone to keep working on.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Mon, 22 Feb 2010 08:13:50 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9760</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hxy5xr:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>23113</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                </customfields>
    </item>
</channel>
</rss>