<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 04:24:48 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/MAHOUT-350/MAHOUT-350.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[MAHOUT-350] add  one &quot;JobName&quot; and reduceNumber parameter to org.apache.mahout.cf.taste.hadoop.item.RecommenderJob</title>
                <link>https://issues.apache.org/jira/browse/MAHOUT-350</link>
                <project id="12310751" key="MAHOUT">Mahout</project>
                    <description>&lt;p&gt;Can add one  &quot;JobName&quot; parameter to org.apache.mahout.cf.taste.hadoop.item.RecommenderJob?&lt;br/&gt;
if there&apos;s a lot of RecommenderJob,it&apos;s hard to distinguish  those jobs.&lt;/p&gt;

&lt;p&gt;also RecommenderJob has four sub jobs (or phase ) ,can add sub-job name to those phase ?&lt;/p&gt;

&lt;p&gt;Because RecommenderJob has not setNumReduceTasks ,it seems that the performance is not good in reduce phase.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12460557">MAHOUT-350</key>
            <summary>add  one &quot;JobName&quot; and reduceNumber parameter to org.apache.mahout.cf.taste.hadoop.item.RecommenderJob</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="srowen">Sean Owen</assignee>
                                    <reporter username="huiwenhan">Han Hui Wen </reporter>
                        <labels>
                            <label>JobName</label>
                            <label>RecommenderJob</label>
                    </labels>
                <created>Mon, 29 Mar 2010 16:35:51 +0100</created>
                <updated>Sun, 31 Oct 2010 15:49:16 +0000</updated>
                            <resolved>Fri, 2 Apr 2010 10:12:39 +0100</resolved>
                                    <version>0.4</version>
                                    <fixVersion>0.4</fixVersion>
                                    <component>Collaborative Filtering</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="12850985" author="srowen" created="Mon, 29 Mar 2010 17:23:36 +0100"  >&lt;p&gt;I can add options for both of those, yes. I am not sure why Hadoop defaults to one reducer when even the docs suggest a better default size. The number of mappers is chosen intelligently.&lt;/p&gt;

&lt;p&gt;While I&apos;m at this, I&apos;d like to take some time to reorganize how AbstractJob works to refactor some of this code. I think we can reduce duplication even as I add more options here. For example I&apos;m not sure if we still need DefaultOptionBuilder? I&apos;ll see what needs to stay there.&lt;/p&gt;</comment>
                            <comment id="12851015" author="jake.mannix" created="Mon, 29 Mar 2010 18:34:52 +0100"  >&lt;p&gt;Don&apos;t the jobs which implement Tool allow for hadoop options to be passed in, so -Dmapred.reduce.tasks=10 should work?&lt;/p&gt;</comment>
                            <comment id="12851030" author="srowen" created="Mon, 29 Mar 2010 19:05:15 +0100"  >&lt;p&gt;Ah good call. You can set job name, it seems, with &apos;mapreduce.job.name&apos;? Hui can you try that?&lt;/p&gt;</comment>
                            <comment id="12851322" author="huiwenhan" created="Tue, 30 Mar 2010 10:09:13 +0100"  >&lt;p&gt;I&#12288;run job using following command:&lt;/p&gt;

&lt;p&gt;hadoop org.apache.mahout.cf.taste.hadoop.item.RecommenderJob -Dmapred.job.name=HADOOP_REC_tap_tag\ -Dmapred.reduce.tasks=20 --input /steer/item/in --tempDir /steer/item/temp --output /steer/item/out --jarFile mahout-0.4-SNAPSHOT.jar --numRecommendations 10 --usersFile /steer/item/usersFile&lt;/p&gt;

&lt;p&gt;-Dmapred.reduce.tasks=20 seems that does not work,&lt;br/&gt;
also there seems no options related to job name.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://hadoop.apache.org/common/docs/current/mapred-default.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hadoop.apache.org/common/docs/current/mapred-default.html&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;http://hadoop.apache.org/common/docs/current/core-default.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hadoop.apache.org/common/docs/current/core-default.html&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="12851827" author="srowen" created="Wed, 31 Mar 2010 12:48:36 +0100"  >&lt;p&gt;If you don&apos;t mind, try this again? I changed AbstractJob to properly use the Configured class, which may be the key to getting Hadoop to properly parse these environment args for you. I also removed the --jarFile argument, since you should no longer need it with the change is just made.&lt;/p&gt;

&lt;p&gt;I do not know if it works, but would be grateful if you can try it.&lt;/p&gt;</comment>
                            <comment id="12851835" author="huiwenhan" created="Wed, 31 Mar 2010 13:13:38 +0100"  >&lt;p&gt;Very thanks &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; ,  I&#12288;try it later.&lt;/p&gt;</comment>
                            <comment id="12851912" author="huiwenhan" created="Wed, 31 Mar 2010 16:37:49 +0100"  >&lt;p&gt;get following error:&lt;/p&gt;

&lt;p&gt;10/03/31 10:34:12 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.&lt;br/&gt;
10/03/31 10:34:12 WARN mapred.JobClient: No job jar file set.  User classes may not be found. See JobConf(Class) or JobConf#setJar(String).&lt;br/&gt;
10/03/31 10:34:12 INFO mapred.FileInputFormat: Total input paths to process : 1&lt;br/&gt;
10/03/31 10:34:13 INFO mapred.JobClient: Running job: job_201003221228_0349&lt;br/&gt;
10/03/31 10:34:14 INFO mapred.JobClient:  map 0% reduce 0%&lt;br/&gt;
10/03/31 10:34:25 INFO mapred.JobClient: Task Id : attempt_201003221228_0349_m_000000_0, Status : FAILED&lt;br/&gt;
java.lang.RuntimeException: Error in configuring object&lt;br/&gt;
        at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:93)&lt;br/&gt;
        at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:64)&lt;br/&gt;
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)&lt;br/&gt;
        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:354)&lt;br/&gt;
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)&lt;br/&gt;
        at org.apache.hadoop.mapred.Child.main(Child.java:170)&lt;br/&gt;
Caused by: java.lang.reflect.InvocationTargetException&lt;br/&gt;
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)&lt;br/&gt;
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)&lt;br/&gt;
        at java.lang.reflect.Method.invoke(Method.java:597)&lt;br/&gt;
        at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:88)&lt;br/&gt;
        ... 5 more&lt;br/&gt;
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: org.apache.mahout.cf.taste.hadoop.item.ItemIDIndexMapper&lt;br/&gt;
        at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:840)&lt;br/&gt;
        at org.apache.hadoop.mapred.JobConf.getMapperClass(JobConf.java:771)&lt;br/&gt;
        at org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:34)&lt;br/&gt;
        ... 10 more&lt;br/&gt;
Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: org.apache.mahout.cf.taste.hadoop.item.ItemIDIndexMapper&lt;br/&gt;
        at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:808)&lt;br/&gt;
        at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:832)&lt;br/&gt;
        ... 12 more&lt;br/&gt;
Caused by: java.lang.ClassNotFoundException: org.apache.mahout.cf.taste.hadoop.item.ItemIDIndexMapper&lt;br/&gt;
        at java.net.URLClassLoader$1.run(URLClassLoader.java:202)&lt;br/&gt;
        at java.security.AccessController.doPrivileged(Native Method)&lt;br/&gt;
        at java.net.URLClassLoader.findClass(URLClassLoader.java:190)&lt;br/&gt;
        at java.lang.ClassLoader.loadClass(ClassLoader.java:307)&lt;br/&gt;
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)&lt;br/&gt;
        at java.lang.ClassLoader.loadClass(ClassLoader.java:248)&lt;br/&gt;
        at java.lang.Class.forName0(Native Method)&lt;br/&gt;
        at java.lang.Class.forName(Class.java:247)&lt;br/&gt;
        at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:761)&lt;br/&gt;
        at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:806)&lt;br/&gt;
        ... 13 more&lt;/p&gt;


&lt;p&gt;may be need following in AbstractJob :&lt;/p&gt;

&lt;p&gt;jobConf.setJarByClass(AbstractJob.class);&lt;/p&gt;</comment>
                            <comment id="12851920" author="huiwenhan" created="Wed, 31 Mar 2010 17:06:06 +0100"  >&lt;p&gt;it work now.&lt;br/&gt;
need jobConf.setJarByClass(AbstractJob.class); in AbstractJob.java&lt;/p&gt;

&lt;p&gt;run command like:&lt;br/&gt;
 hadoop jar mahout-core-0.4-SNAPSHOT.jar org.apache.mahout.cf.taste.hadoop.item.RecommenderJob -Dmapred.job.name=HADOOP_REC_tap_tag -Dmapred.reduce.tasks=200 --input /steer/item/in  --tempDir /steer/item/temp --output /steer/item/out --numRecommendations 10&lt;/p&gt;

&lt;p&gt;but also has question,&lt;/p&gt;

&lt;p&gt;AbstractJob has four sub-job ,they displayed the same name,&lt;br/&gt;
It hard to know which phase does the job run in .&lt;/p&gt;</comment>
                            <comment id="12851927" author="srowen" created="Wed, 31 Mar 2010 17:15:24 +0100"  >&lt;p&gt;I&apos;ll have to look at the Hadoop source code. I thought it would set the job .jar from the classpath, given the new way I set up the JobConf like in their sample code.&lt;/p&gt;</comment>
                            <comment id="12851931" author="srowen" created="Wed, 31 Mar 2010 17:30:01 +0100"  >&lt;p&gt;Hmm, I don&apos;t understand that. AbstractJob.class and ItemIDIndexMapper.class are in the same .jar file. There is only one .jar, with both, so both ought to end up selecting the same .jar file. I&apos;d rather specify the Mapper class instead just because maybe someday someone subclasses AbstractJob, puts the implementation in a different jar, and then this line won&apos;t work.&lt;/p&gt;

&lt;p&gt;I can set it to AbstractJob.class for now... but still think there&apos;s something wrong here.&lt;/p&gt;

&lt;p&gt;About the other arguemnts, yes, I agree there should be an option.&lt;br/&gt;
I was looking to see how Hadoop accepts arguments like &quot;-Dmapred....&quot; on the command line. I can&apos;t find it parsing these anywhere. So I don&apos;t know this exists.&lt;/p&gt;

&lt;p&gt;So I can now start adding custom parameters for things like this.&lt;/p&gt;</comment>
                            <comment id="12852012" author="drew.farris" created="Wed, 31 Mar 2010 19:16:35 +0100"  >&lt;p&gt;Not sure if this is helpful Sean, GenericOptionsParser should be picking up the -Dproperty=value arugments &amp;#8211; iirc this handled somewhere in the ToolRunner. The run method&apos;s args would have any args left over after the GenericOptionsParser had done its thing. As long as the JobConf is created using the right configuration object, the job should be picking them up. It looks like AbstractJob should be doing this.&lt;/p&gt;

&lt;p&gt;GenericOptionsParser handles a bunch of other handy arguments, the javadoc covers it. It should work without having to do a bunch of argument parsing. If you&apos;d like I can take a closer look this evening.&lt;/p&gt;
</comment>
                            <comment id="12852224" author="drew.farris" created="Thu, 1 Apr 2010 03:34:56 +0100"  >&lt;blockquote&gt;
&lt;p&gt;Hmm, I don&apos;t understand that. AbstractJob.class and ItemIDIndexMapper.class are in the same .jar file. There is only one .jar, with both, so both ought to end up selecting the same .jar file. I&apos;d rather specify the Mapper class instead just because maybe someday someone subclasses AbstractJob, puts the implementation in a different jar, and then this line won&apos;t work. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;How about jobConf.setJarByClass(getClass()) in AbstractJob&apos;s prepareJobConf? This will always set the jar based on the AbstractJob implementation being executed.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I was looking to see how Hadoop accepts arguments like &quot;-Dmapred....&quot; on the command line. I can&apos;t find it parsing these anywhere. So I don&apos;t know this exists.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;ToolRunner.run(..) runs the args through GenericOptionsParser, which adds the results to the conf, which then gets set back on the object being run that implements the Configured interface. These are pulled in by AbstractJob when it creates the jobConf using the getConf() argument.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;AbstractJob has four sub-job ,they displayed the same name, It hard to know which phase does the job run in .&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;In this vein, it would be handy if AbstractJob&apos;s prepareJobConf method could take a string argument for the subjob name and allow the name to be specified by the class calling it &amp;#8211; requiring that the name only be specified on the command-line forces all jobs run under the umbrella of the command to have the same name. Maybe take any current value of mapred.job.name (if specified) and append the string to it?&lt;/p&gt;</comment>
                            <comment id="12852273" author="huiwenhan" created="Thu, 1 Apr 2010 08:43:09 +0100"  >&lt;p&gt;parameter  -Dmapred.job.name=HADOOP_REC_tap_tag -Dmapred.reduce.tasks=200  can work now.&lt;/p&gt;

&lt;p&gt;the problem is :&lt;br/&gt;
AbstractJob has four sub-job ,they displayed the same name&lt;/p&gt;

&lt;p&gt;---take the current value of mapred.job.name and append the sub job &apos;s mapper and reducer class name as  the sub job&apos;s name &lt;/p&gt;</comment>
                            <comment id="12852315" author="srowen" created="Thu, 1 Apr 2010 11:16:46 +0100"  >&lt;p&gt;Thanks Drew, I see the parsing now that you prompted me to look a second time. It sounds like the -D options work. So, we don&apos;t need to do things like parse and set the number of reducers with our own argument.&lt;/p&gt;

&lt;p&gt;I like your version of setJarByClass() and will commit that. That also gets rid of the --jarFile option I had put it in earlier.&lt;/p&gt;

&lt;p&gt;So, perhaps I should also get rid of custom --input and --output arguments? &lt;/p&gt;

&lt;p&gt;(Incidentally, now might be a good time to plug &apos;AbstractJob&apos;. I think Robin was also keen to rally around refactoring Hadoop jobs to use this class, so we can capture all this good knowledge and practice in one place, and approach job handling consistently. It will no doubt need modification to really serve all needs.)&lt;/p&gt;

&lt;p&gt;Hui I also like your proposal to take a name suffix argument to prepareJob(). I will work on that and commit soon.&lt;/p&gt;</comment>
                            <comment id="12852336" author="srowen" created="Thu, 1 Apr 2010 12:02:55 +0100"  >&lt;p&gt;OK, try my latest commit. --input and --output are now gone in favor of the standard -Dmapred.input.dir and -Dmapred.output.dir. I made changes mentioned in other issues too.&lt;/p&gt;

&lt;p&gt;It&apos;s a little aggressive to commit this but it would solve several issues, is a step forward, and Hui is in a position to really test this.&lt;/p&gt;</comment>
                            <comment id="12852360" author="drew.farris" created="Thu, 1 Apr 2010 13:40:04 +0100"  >&lt;blockquote&gt;&lt;p&gt;(Incidentally, now might be a good time to plug &apos;AbstractJob&apos;. I think Robin was also keen to rally around refactoring Hadoop jobs to use this class, so we can capture all this good knowledge and practice in one place, and approach job handling consistently. It will no doubt need modification to really serve all needs.)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I was thinking about this last night when looking at the code &amp;#8211; taking the CollocDriver for example and adapting it to use AbstractJob. I was wondering should I depend on it as-is, or should we consider moving it to org.apache.mahout.common?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;OK, try my latest commit. --input and --output are now gone in favor of the standard -Dmapred.input.dir and -Dmapred.output.dir&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ahh, that&apos;s a great idea Sean. The less command-line parsing code anyone has to write the better.&lt;/p&gt;

&lt;p&gt;What do you think can be done about job naming for sub-jobs?&lt;/p&gt;

</comment>
                            <comment id="12852471" author="srowen" created="Thu, 1 Apr 2010 18:36:48 +0100"  >&lt;p&gt;Yes it should move to common. I suppose I hadn&apos;t wanted to be presumptuous and do that without some support, but sounds like this has some consensus.&lt;/p&gt;

&lt;p&gt;I also committed a naming change. If a custom name is set (mapred.job.name), it will add to that &quot;-MapperClass-ReducerClass&quot;. The custom name uniquely identifies the job, really, and then the suffix identifies what part of that job is happening.&lt;/p&gt;</comment>
                            <comment id="12852477" author="jake.mannix" created="Thu, 1 Apr 2010 18:49:35 +0100"  >&lt;blockquote&gt;&lt;p&gt;I suppose I hadn&apos;t wanted to be presumptuous and do that without some support, but sounds like this has some consensus.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yep, I&apos;ve been subclassing AbstractJob all over in the decomposer / DistributedRowMatrix stuff already.  &lt;/p&gt;

&lt;p&gt;I&apos;ve had some odd issues with pseudo-distributed vs. really distributed usage and command line options parsing with &lt;b&gt;optional&lt;/b&gt; options with it though.  I&apos;ll try to post the exact problem, but I think I&apos;m not properly using the mixture of commons.cli2 together with the GenericOptionsParser when I subclass AbstractJob though...&lt;/p&gt;</comment>
                            <comment id="12852483" author="srowen" created="Thu, 1 Apr 2010 19:04:12 +0100"  >&lt;p&gt;I might well have broken your classes in the recent change. Or maybe the arg parsing was borked in some way from the start. Take a look and/or point me at issues you see next time you dig in.&lt;/p&gt;</comment>
                            <comment id="12852707" author="huiwenhan" created="Fri, 2 Apr 2010 05:57:36 +0100"  >&lt;p&gt;it works now,&lt;/p&gt;

&lt;p&gt;I run following command:&lt;/p&gt;

&lt;p&gt; hadoop  org.apache.mahout.cf.taste.hadoop.item.RecommenderJob -Dmapred.job.name=RECOMMENDATION_tap_tag -Dmapred.reduce.tasks=2 -Dmapred.input.dir=/steer/item/in -Dmapred.output.dir=/steer/item/out -Dmapred.output.compress=false --tempDir /steer/item/temp --numRecommendations 10&lt;/p&gt;


&lt;p&gt;it need place all -D options before the main arguments.&lt;/p&gt;

&lt;p&gt;this way will cause parsing error&lt;/p&gt;

&lt;p&gt; hadoop  org.apache.mahout.cf.taste.hadoop.item.RecommenderJob -Dmapred.job.name=RECOMMENDATION_tap_tag -Dmapred.reduce.tasks=2 -Dmapred.input.dir=/steer/item/in  --tempDir /steer/item/temp  -Dmapred.output.dir=/steer/item/out  --numRecommendations 10 -Dmapred.output.compress=false&lt;/p&gt;</comment>
                            <comment id="12852748" author="srowen" created="Fri, 2 Apr 2010 09:05:09 +0100"  >&lt;p&gt;Good one Hui, I&apos;ll make note of that in examples.&lt;/p&gt;</comment>
                            <comment id="12852757" author="srowen" created="Fri, 2 Apr 2010 10:12:38 +0100"  >&lt;p&gt;OK, I think all the stuff in this issue has been resolved.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12440072" name="screenshot-1.jpg" size="102885" author="huiwenhan" created="Mon, 29 Mar 2010 17:03:50 +0100"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Mon, 29 Mar 2010 16:23:36 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9715</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hxy5nr:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>23068</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                </customfields>
    </item>
</channel>
</rss>