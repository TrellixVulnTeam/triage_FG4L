<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 04:21:47 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/MAHOUT-1541/MAHOUT-1541.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[MAHOUT-1541] Create CLI Driver for Spark Cooccurrence Analysis</title>
                <link>https://issues.apache.org/jira/browse/MAHOUT-1541</link>
                <project id="12310751" key="MAHOUT">Mahout</project>
                    <description>&lt;p&gt;Create a CLI driver to import data in a flexible manner, create an IndexedDataset with BiMap ID translation dictionaries, call the Spark CooccurrenceAnalysis with the appropriate params, then write output with external IDs optionally reattached.&lt;/p&gt;

&lt;p&gt;Ultimately it should be able to read input as the legacy mr does but will support reading externally defined IDs and flexible formats. Output will be of the legacy format or text files of the user&apos;s specification with reattached Item IDs. &lt;/p&gt;

&lt;p&gt;Support for legacy formats is a question, users can always use the legacy code if they want this. Internal to the IndexedDataset is a Spark DRM so pipelining can be accomplished without any writing to an actual file so the legacy sequence file output may not be needed.&lt;/p&gt;

&lt;p&gt;Opinions?&lt;/p&gt;</description>
                <environment></environment>
        <key id="12712081">MAHOUT-1541</key>
            <summary>Create CLI Driver for Spark Cooccurrence Analysis</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="10">Implemented</resolution>
                                        <assignee username="pferrel">Pat Ferrel</assignee>
                                    <reporter username="pferrel">Pat Ferrel</reporter>
                        <labels>
                            <label>DSL</label>
                            <label>scala</label>
                            <label>spark</label>
                    </labels>
                <created>Sat, 3 May 2014 01:26:54 +0100</created>
                <updated>Mon, 13 Apr 2015 10:56:56 +0100</updated>
                            <resolved>Wed, 18 Mar 2015 13:56:40 +0000</resolved>
                                                    <fixVersion>0.10.0</fixVersion>
                                    <component>CLI</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>7</watches>
                                                                <comments>
                            <comment id="13988488" author="pferrel" created="Sat, 3 May 2014 01:35:25 +0100"  >&lt;p&gt;progress can be followed here: &lt;a href=&quot;https://github.com/pferrel/harness&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/pferrel/harness&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;nothing much checked in except an option parser in the POM (scopt MIT license) and skeleton for the driver.&lt;/p&gt;</comment>
                            <comment id="13988603" author="ssc" created="Sat, 3 May 2014 07:53:44 +0100"  >&lt;p&gt;I&apos;m not sure whether it is a good idea to again introduce custom preprocessing code for each algorithm. I think we should rather wait for the generic dataframe that we are aiming to build and integrate the cooccurrence code with that. I have suggested to merge the IndexedDataset functionality with a dataframe.&lt;/p&gt;</comment>
                            <comment id="13988735" author="pferrel" created="Sat, 3 May 2014 17:20:30 +0100"  >&lt;p&gt;Agreed (mostly), only the CLI is custom for each algo.&lt;/p&gt;

&lt;p&gt;The preprocessor was a remnant of your old example patch and isn&apos;t meant to be repeated. Not planning to have separate code for every algo at all, in fact it should be quite the opposite. There will be a custom CLI for each algo and one of a couple customizable but general purpose importer/exporters (text delimited, sequencefile?) with some method of specifying input and output schema. &lt;/p&gt;

&lt;p&gt;The IndexedDataset would be identical in structure in all cases. Should have some of the IndexedDataset improvements (mostly BiMaps) today and I&apos;m willing to merge them with some other dataframe in the future. &lt;/p&gt;

&lt;p&gt;What I am doing is exactly what we agreed to in &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1518&quot; title=&quot;Preprocessing for collaborative filtering with the Scala DSL&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1518&quot;&gt;&lt;del&gt;MAHOUT-1518&lt;/del&gt;&lt;/a&gt; There is another Jira about dataframes but I wasn&apos;t aware of any progress made on it. Don&apos;t want to &quot;wait&quot; I only have limited time in windows, if I wait I may get nothing done. And I could use this right now to rebuild the solr recommender and the other Mahout recommenders. This work seems at worse independant of some other r-like dataframe, or a best can be integrated as that solidifies.&lt;/p&gt;

&lt;p&gt;In the meantime any suggestions about using another effort like some usable dataframe-ish object is fine. I had though we&apos;d convinced ourselves that the needs of an r-like dataframe and an import/export IndexedDataset were too different. Dmitriy certainly made strong arguments to that effect.&lt;/p&gt;

&lt;p&gt;Just using the cooccurrence analysis to have an end to end example.&lt;/p&gt;

&lt;p&gt;BTW do we really need to support sequencefiles where the legacy code does?&lt;/p&gt;</comment>
                            <comment id="13988743" author="tdunning" created="Sat, 3 May 2014 17:53:37 +0100"  >&lt;blockquote&gt;
&lt;p&gt;BTW do we really need to support sequencefiles where the legacy code does?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I sincerely hope not.&lt;/p&gt;</comment>
                            <comment id="13988768" author="pferrel" created="Sat, 3 May 2014 18:54:12 +0100"  >&lt;p&gt;Good, the only reason to do this, that I can think of, is so it will fit in existing workflows and the legacy code fits there already.&lt;/p&gt;

&lt;p&gt;The first version of this will be for text delimited import/export, I assume that other formats may be nice, like JSON, or others. Any guidance here would be appreciated. &lt;/p&gt;</comment>
                            <comment id="13988916" author="kanjilal" created="Sun, 4 May 2014 06:21:42 +0100"  >&lt;p&gt;Pat,&lt;br/&gt;
Just one comment on the &quot;no-progress around the dataframes JIRA&quot;, I assume you are referring to 1490, there is indeed quite a bit of progress presenting APIs around a set of generic operations around a dataFrame, based on Dmitry&apos;s recommendation I took the path of creating a proposal rather than blasting off and writing code to do this and have that be heavily criticized and not meeting the committable expectations, this way the design will be in place and have general consensus before any coding efforts begin, I&apos;d love to get feedback from you and others to move 1490 along, please see blog and comment on JIRA if you&apos;d like.&lt;/p&gt;

&lt;p&gt;Regards&lt;/p&gt;</comment>
                            <comment id="13989040" author="pferrel" created="Sun, 4 May 2014 17:51:54 +0100"  >&lt;p&gt;For something as complicated as an r-like dataframe that&apos;s a good approach and I did read it. &lt;/p&gt;

&lt;p&gt;The sole reason for IndexedDataset in my use is import/export. You&apos;ll see the code in my github in a few days. If the needs match I&apos;ll be happy to merge IndexedDataset and/or this driver and import code with whatever comes out of 1490.&lt;/p&gt;

&lt;p&gt;For now I have an actual need for this code in the solr-recommender running on the demo site and the import/export code will have minimal impact on the internals of IndexedDataset so I&apos;m going with it for now only for expediency. There is no need for slices by label or the like so there should be little duplicated work.&lt;/p&gt;

&lt;p&gt;The IndexedDataset is defined as:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
/**
 * Comments: Wraps a Mahout DrmLike object and includes two BiMaps to store translation
 *   dictionaries. This may be replaced with a Mahout DSL dataframe-like object in the &lt;span class=&quot;code-keyword&quot;&gt;future&lt;/span&gt;.
 *   The primary use of &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; is &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; and export, keeping track of external IDs and
 *   preserving them all the way to output.
 *
 * Example: For a transpose job the &apos;matrix: DrmLike[Int]&apos; is passed into the DSL code
 *   that transposes the values, then the dictionaries are swapped and a &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt;
 *   IndexedDataset is returned from the job, which will be exported to files using
 *   the labels.reverse(ID: Int) thereby preserving the external ID.
 *
 * @param matrix  DrmLike[Int], representing the distributed matrix storing the actual data.
 * @param rowLabels BiMap[&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;, Int] storing a bidirectional mapping of external &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; ID to
 *                  and from the ordinal Mahout Int ID. This one holds row labels
 * @param columnLabels BiMap[&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;, Int] storing a bidirectional mapping of external &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;
 *                  ID to and from the ordinal Mahout Int ID. This one holds column labels
 *
 * @&lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt;
 */

&lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; class IndexedDataset(matrix: DrmLike[Int], rowLabels: BiMap[&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;,Int], columnLabels: BiMap[&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;,Int])
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note the BiMaps are actually java BiHashMaps from Guava. That will be sufficient for my current needs.&lt;/p&gt;

&lt;p&gt;Note that the Cooccurrence driver is a proposed template of CLI drivers in general. The code is being designed to work for any CLI access to Mahout-Spark. I&apos;ll have it running on the demo site solr recommender as soon as it&apos;s tested out and before any official Mahout commit so there is plenty of time to give opinions.&lt;/p&gt;</comment>
                            <comment id="13991013" author="pferrel" created="Tue, 6 May 2014 20:13:05 +0100"  >&lt;p&gt;I have a partial implementation of the CLI driver, Importer, IndexedDataset and some tests.&lt;/p&gt;

&lt;p&gt;The design object is to support all Mahout V2 CLIs for blackbox type jobs similar to the legacy CLI but allow more flexible text file import/export maintaining user specified IDs. &lt;/p&gt;

&lt;p&gt;Anyone interested please take a look at the github repo and its wiki here: &lt;a href=&quot;https://github.com/pferrel/harness/wiki&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/pferrel/harness/wiki&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I would greatly appreciate comments. This is a very early version and was shamelessly stolen from some examples Sebastian provided. It does actually run the cross-cooccurrence Spark code and display example output. It reads from a text-delimited file but there is only console output at present. Most options are not implemented yet because I&apos;d like to get feedback now.&lt;/p&gt;</comment>
                            <comment id="13994045" author="pferrel" created="Sat, 10 May 2014 00:26:04 +0100"  >&lt;p&gt;The basic import, do the cooccurrence, export is working on a local Spark version. Works for self and cross cooccurrence. user specified IDs are preserved. There is a proposed format for drm-like things encoded as text delimited files and a way to change the schema for it.&lt;/p&gt;

&lt;p&gt;By no means have all the option combos been tested yet&lt;/p&gt;

&lt;p&gt;It would be great if someone could take a look at it.&lt;/p&gt;</comment>
                            <comment id="14000861" author="pferrel" created="Sat, 17 May 2014 21:16:11 +0100"  >&lt;p&gt;More progress: it reads and writes. A CLI option parser is taking about all params needed and some minimal validation and consistency checking is started.&lt;/p&gt;

&lt;p&gt;The structure of IndexedDataset and the &apos;Store&apos; types are ready for some early review. The idea is that Stores create IndexedDataset(s) by reading text files and write them out too. The design allows for non-text read/write but nothing is implemented for this yet. There is also an example driver that may be split into generic and job specific parts but I haven&apos;t tackled this yet.&lt;/p&gt;

&lt;p&gt;The read/write have not been tested on a cluster yet.&lt;/p&gt;

&lt;p&gt;Feedback is appreciated, especially where noted on the github description. &lt;a href=&quot;https://github.com/pferrel/harness/wiki&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/pferrel/harness/wiki&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14006042" author="pferrel" created="Thu, 22 May 2014 16:43:49 +0100"  >&lt;p&gt;Current design ends up with some sorta ugly code but I think the file schema can be factored out to simplify things. Hold off on comments until I have a chance to do this.&lt;/p&gt;</comment>
                            <comment id="14018986" author="githubbot" created="Thu, 5 Jun 2014 18:19:18 +0100"  >&lt;p&gt;GitHub user pferrel opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/11&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/11&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Mahout 1541&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1541&quot; title=&quot;Create CLI Driver for Spark Cooccurrence Analysis&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1541&quot;&gt;&lt;del&gt;MAHOUT-1541&lt;/del&gt;&lt;/a&gt; WIP, need access to an RDD, which used to be in a DrmLike&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;. Still need the DrmLike to pass in to cooccurrence but need the RDD to write a text file. CheckpointedDrmSpark doesn&apos;t have the DrmLike? &lt;/p&gt;

&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/pferrel/mahout&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/pferrel/mahout&lt;/a&gt; mahout-1541&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/11.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/11.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #11&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 107a0ba9605241653a85b113661a8fa5c055529f&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-04T19:54:22Z&lt;/p&gt;

&lt;p&gt;    added Sebastian&apos;s CooccurrenceAnalysis patch updated it to use current Mahout-DSL&lt;/p&gt;

&lt;p&gt;commit 74b9921c4c9bd8903585bbd74d9e66298ea8b7a0&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-04T20:09:07Z&lt;/p&gt;

&lt;p&gt;    Adding stuff for itemsimilarity driver for Spark&lt;/p&gt;

&lt;p&gt;commit a59265931ed3a51ba81e1a0a7171ebb102be4fa4&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-04T20:13:13Z&lt;/p&gt;

&lt;p&gt;    added scopt to pom deps&lt;/p&gt;

&lt;p&gt;commit 16c03f7fa73c156859d1dba3a333ef9e8bf922b0&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-04T21:32:18Z&lt;/p&gt;

&lt;p&gt;    added Sebastian&apos;s MurmurHash changes&lt;/p&gt;

&lt;p&gt;    Signed-off-by: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;/p&gt;

&lt;p&gt;commit 8a4b4347ddb7b9ac97590aa20189d89d8a07a80a&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-04T21:33:11Z&lt;/p&gt;

&lt;p&gt;    Merge branch &apos;mahout-1464&apos; into mahout-1541&lt;/p&gt;

&lt;p&gt;commit 2f87f5433f90fa2c49ef386ca245943e1fc73beb&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-05T01:44:16Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1541&quot; title=&quot;Create CLI Driver for Spark Cooccurrence Analysis&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1541&quot;&gt;&lt;del&gt;MAHOUT-1541&lt;/del&gt;&lt;/a&gt; still working on this, some refactoring in the DSL for abstracting away Spark has moved access to rddsno Jira is closed yet&lt;/p&gt;

&lt;p&gt;commit c6adaa44c80bba99d41600e260bbb1ad5c972e69&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-05T16:52:23Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1464&quot; title=&quot;Cooccurrence Analysis on Spark&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1464&quot;&gt;&lt;del&gt;MAHOUT-1464&lt;/del&gt;&lt;/a&gt; import cleanup, minor changes to examples for running on Spark Cluster&lt;/p&gt;

&lt;p&gt;commit 2caceab31703ed214c1e66d5fc63b8bdb05d37a3&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-05T16:55:09Z&lt;/p&gt;

&lt;p&gt;    Merge branch &apos;mahout-1464&apos; into mahout-1541&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="14020169" author="pferrel" created="Fri, 6 Jun 2014 19:25:53 +0100"  >&lt;p&gt;The mahout-1541 branch in the above PR currently does not build because of core DSL changes since it was written.&lt;/p&gt;

&lt;p&gt;The root issue is that I need DrmLike&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;s to pass to CooccurrenceAnalysis.cooccurrence. The CLI reads these from text files. To read text files I need a SparkContext, which is not a problem. But to write the output I need an RDD. The DrmLike&apos;s that I was keeping used to contain the RDD but no longer do. A question that is blocking at present is:&lt;/p&gt;

&lt;p&gt;1) What is the preferred way to pass around an object with both a DrmLike interface and an Rdd reference? Should I create this or does it already exist? I&apos;ve looked in the PDF doc and the scaladocs but couldn&apos;t find the answer. Therefore I&apos;ll create something unless someone has better advice.&lt;/p&gt;</comment>
                            <comment id="14020232" author="pferrel" created="Fri, 6 Jun 2014 19:59:52 +0100"  >&lt;p&gt;The existing itemsimilarity outputs indicators, one per row. This is more compact but seems like not the ideal way to get them. The indicators will likely be associated in rows to a specific item id. Therefore I&apos;ve implemented the Spark version to output what amounts the a sparse matrix with external IDs intact. A row will (if no formatting params are specified) look like this:&lt;/p&gt;

&lt;p&gt;user1&amp;lt;tab&amp;gt;user2:strength1,user3:strength2&lt;br/&gt;
user2&amp;lt;tab&amp;gt;user5:strength3,user6:strength4,...&lt;/p&gt;

&lt;p&gt;To make this directly ingestible by Solr or ElasticSearch we probably need to drop the strength values via a CLI option and maybe support named output formats like CSV, JSON. CSV can be done with the current options if the strengths are dropped.&lt;/p&gt;</comment>
                            <comment id="14032628" author="githubbot" created="Mon, 16 Jun 2014 17:49:36 +0100"  >&lt;p&gt;Github user pferrel closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/11&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/11&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14037628" author="githubbot" created="Thu, 19 Jun 2014 19:24:49 +0100"  >&lt;p&gt;GitHub user pferrel opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/22&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/22&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1541&quot; title=&quot;Create CLI Driver for Spark Cooccurrence Analysis&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1541&quot;&gt;&lt;del&gt;MAHOUT-1541&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Not ready for merge, just looking for a checkpoint review. The code works and I&apos;m reasonable happy with it. It needs review for:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Design: will it be flexible, are there better ways to organize the code?&lt;/li&gt;
	&lt;li&gt;Scala usage: I&apos;m pretty new to Scala.&lt;/li&gt;
	&lt;li&gt;Package structure: no clue here so its all in &quot;drivers&quot;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Please don&apos;t take the tests too seriously and some things are not implemented. I&apos;d like a once over before I do a bunch of test cases and finish polishing up.&lt;/p&gt;

&lt;p&gt;    This will close three tickets when complete. &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1541&quot; title=&quot;Create CLI Driver for Spark Cooccurrence Analysis&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1541&quot;&gt;&lt;del&gt;MAHOUT-1541&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1568&quot; title=&quot;Build an I/O model that can replace sequence files for import/export&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1568&quot;&gt;&lt;del&gt;MAHOUT-1568&lt;/del&gt;&lt;/a&gt;, and &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1569&quot; title=&quot;Create CLI driver that supports Spark jobs&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1569&quot;&gt;&lt;del&gt;MAHOUT-1569&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/pferrel/mahout&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/pferrel/mahout&lt;/a&gt; mahout-1541&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/22.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/22.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #22&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 107a0ba9605241653a85b113661a8fa5c055529f&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-04T19:54:22Z&lt;/p&gt;

&lt;p&gt;    added Sebastian&apos;s CooccurrenceAnalysis patch updated it to use current Mahout-DSL&lt;/p&gt;

&lt;p&gt;commit 74b9921c4c9bd8903585bbd74d9e66298ea8b7a0&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-04T20:09:07Z&lt;/p&gt;

&lt;p&gt;    Adding stuff for itemsimilarity driver for Spark&lt;/p&gt;

&lt;p&gt;commit a59265931ed3a51ba81e1a0a7171ebb102be4fa4&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-04T20:13:13Z&lt;/p&gt;

&lt;p&gt;    added scopt to pom deps&lt;/p&gt;

&lt;p&gt;commit 16c03f7fa73c156859d1dba3a333ef9e8bf922b0&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-04T21:32:18Z&lt;/p&gt;

&lt;p&gt;    added Sebastian&apos;s MurmurHash changes&lt;/p&gt;

&lt;p&gt;    Signed-off-by: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;/p&gt;

&lt;p&gt;commit 8a4b4347ddb7b9ac97590aa20189d89d8a07a80a&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-04T21:33:11Z&lt;/p&gt;

&lt;p&gt;    Merge branch &apos;mahout-1464&apos; into mahout-1541&lt;/p&gt;

&lt;p&gt;commit 2f87f5433f90fa2c49ef386ca245943e1fc73beb&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-05T01:44:16Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1541&quot; title=&quot;Create CLI Driver for Spark Cooccurrence Analysis&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1541&quot;&gt;&lt;del&gt;MAHOUT-1541&lt;/del&gt;&lt;/a&gt; still working on this, some refactoring in the DSL for abstracting away Spark has moved access to rddsno Jira is closed yet&lt;/p&gt;

&lt;p&gt;commit c6adaa44c80bba99d41600e260bbb1ad5c972e69&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-05T16:52:23Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1464&quot; title=&quot;Cooccurrence Analysis on Spark&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1464&quot;&gt;&lt;del&gt;MAHOUT-1464&lt;/del&gt;&lt;/a&gt; import cleanup, minor changes to examples for running on Spark Cluster&lt;/p&gt;

&lt;p&gt;commit 2caceab31703ed214c1e66d5fc63b8bdb05d37a3&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-05T16:55:09Z&lt;/p&gt;

&lt;p&gt;    Merge branch &apos;mahout-1464&apos; into mahout-1541&lt;/p&gt;

&lt;p&gt;commit 6df6a54e3ff174d39bd817caf7d16c2d362be3f8&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-07T20:39:25Z&lt;/p&gt;

&lt;p&gt;    Merge branch &apos;master&apos; into mahout-1541&lt;/p&gt;

&lt;p&gt;commit a2f84dea3f32d3df3e98c61f085bc1fabd453551&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-07T21:27:06Z&lt;/p&gt;

&lt;p&gt;    drmWrap seems to be the answer to the changed DrmLike interface. Code works again but more to do.&lt;/p&gt;

&lt;p&gt;commit d3a2ba5027436d0abef67a1a5e82557064f4ba49&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-17T16:00:38Z&lt;/p&gt;

&lt;p&gt;    merged master, got new cooccurrence code&lt;/p&gt;

&lt;p&gt;commit 4b2fb07b21a8ac2d532ee51b65b27d1482293cb0&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-19T17:08:02Z&lt;/p&gt;

&lt;p&gt;    for high level review, not ready for merge&lt;/p&gt;

&lt;p&gt;commit 996ccfb82a8ed3ff90f51968e661b2449f3c4759&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-19T17:46:23Z&lt;/p&gt;

&lt;p&gt;    for high level review, not ready for merge. changed to dot notation&lt;/p&gt;

&lt;p&gt;commit f62ab071869ee205ad398a3e094d871138e11a9e&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-19T18:13:44Z&lt;/p&gt;

&lt;p&gt;    for high level review, not ready for merge. fixed a couple scaladoc refs&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="14037635" author="githubbot" created="Thu, 19 Jun 2014 19:30:33 +0100"  >&lt;p&gt;Github user pferrel commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/22#issuecomment-46598676&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/22#issuecomment-46598676&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Docs written describing this here: &lt;a href=&quot;https://github.com/pferrel/harness/wiki&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/pferrel/harness/wiki&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14037639" author="githubbot" created="Thu, 19 Jun 2014 19:32:27 +0100"  >&lt;p&gt;Github user pferrel commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/22#issuecomment-46598913&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/22#issuecomment-46598913&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    BTW untested on a cluster. Still trying to get mine back working.&lt;/p&gt;</comment>
                            <comment id="14037702" author="githubbot" created="Thu, 19 Jun 2014 20:11:54 +0100"  >&lt;p&gt;Github user dlyubimov commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/22#discussion_r13987402&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/22#discussion_r13987402&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: spark/src/main/scala/org/apache/mahout/cf/examples/Recommendations.scala &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,172 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
    + * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
    + * this work for additional information regarding copyright ownership.&lt;br/&gt;
    + * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
    + * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
    + * the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.mahout.cf.examples&lt;br/&gt;
    +&lt;br/&gt;
    +import scala.io.Source&lt;br/&gt;
    +import org.apache.mahout.math._&lt;br/&gt;
    +import scalabindings._&lt;br/&gt;
    +import RLikeOps._&lt;br/&gt;
    +import drm._&lt;br/&gt;
    +import RLikeDrmOps._&lt;br/&gt;
    +import org.apache.mahout.sparkbindings._&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.mahout.cf.CooccurrenceAnalysis._&lt;br/&gt;
    +import scala.collection.JavaConversions._&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * The Epinions dataset contains ratings from users to items and a trust-network between the users.&lt;br/&gt;
    + * We use co-occurrence analysis to compute &quot;users who like these items, also like that items&quot; and&lt;br/&gt;
    + * &quot;users who trust these users, like that items&quot;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Download and unpack the dataset files from:&lt;br/&gt;
    + *&lt;br/&gt;
    + * &lt;a href=&quot;http://www.trustlet.org/datasets/downloaded_epinions/ratings_data.txt.bz2&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.trustlet.org/datasets/downloaded_epinions/ratings_data.txt.bz2&lt;/a&gt;&lt;br/&gt;
    + * &lt;a href=&quot;http://www.trustlet.org/datasets/downloaded_epinions/trust_data.txt.bz2&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.trustlet.org/datasets/downloaded_epinions/trust_data.txt.bz2&lt;/a&gt;&lt;br/&gt;
    + **/&lt;br/&gt;
    +object RunCrossCooccurrenceAnalysisOnEpinions {&lt;br/&gt;
    +&lt;br/&gt;
    +  def main(args: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;): Unit = {&lt;br/&gt;
    +&lt;br/&gt;
    +    if (args.length == 0) &lt;/p&gt;
{
    +      println(&quot;Usage: RunCooccurrenceAnalysisOnMovielens1M &amp;lt;path-to-dataset-folder&amp;gt;&quot;)
    +      println(&quot;Download the dataset from http://www.trustlet.org/datasets/downloaded_epinions/ratings_data.txt.bz2 and&quot;)
    +      println(&quot;http://www.trustlet.org/datasets/downloaded_epinions/trust_data.txt.bz2&quot;)
    +      sys.exit(-1)
    +    }
&lt;p&gt;    +&lt;br/&gt;
    +    val datasetDir = args(0)&lt;br/&gt;
    +&lt;br/&gt;
    +    val epinionsRatings = new SparseMatrix(49290, 139738)&lt;br/&gt;
    +&lt;br/&gt;
    +    var firstLineSkipped = false&lt;br/&gt;
    +    for (line &amp;lt;- Source.fromFile(datasetDir + &quot;/ratings_data.txt&quot;).getLines()) {&lt;br/&gt;
    +      if (line.contains(&apos; &apos;) &amp;amp;&amp;amp; firstLineSkipped) &lt;/p&gt;
{
    +        val tokens = line.split(&apos; &apos;)
    +        val userID = tokens(0).toInt - 1
    +        val itemID = tokens(1).toInt - 1
    +        val rating = tokens(2).toDouble
    +        epinionsRatings(userID, itemID) = rating
    +      }
&lt;p&gt;    +      firstLineSkipped = true&lt;br/&gt;
    +    }&lt;br/&gt;
    +&lt;br/&gt;
    +    val epinionsTrustNetwork = new SparseMatrix(49290, 49290)&lt;br/&gt;
    +    firstLineSkipped = false&lt;br/&gt;
    +    for (line &amp;lt;- Source.fromFile(datasetDir + &quot;/trust_data.txt&quot;).getLines()) {&lt;br/&gt;
    +      if (line.contains(&apos; &apos;) &amp;amp;&amp;amp; firstLineSkipped) &lt;/p&gt;
{
    +        val tokens = line.trim.split(&apos; &apos;)
    +        val userID = tokens(0).toInt - 1
    +        val trustedUserId = tokens(1).toInt - 1
    +        epinionsTrustNetwork(userID, trustedUserId) = 1
    +      }
&lt;p&gt;    +      firstLineSkipped = true&lt;br/&gt;
    +    }&lt;br/&gt;
    +&lt;br/&gt;
    +    System.setProperty(&quot;spark.kryo.referenceTracking&quot;, &quot;false&quot;)&lt;br/&gt;
    +    System.setProperty(&quot;spark.kryoserializer.buffer.mb&quot;, &quot;100&quot;)&lt;br/&gt;
    +/* to run on local, can provide number of core by changing to local&lt;span class=&quot;error&quot;&gt;&amp;#91;4&amp;#93;&lt;/span&gt; */&lt;br/&gt;
    +    implicit val distributedContext = mahoutSparkContext(masterUrl = &quot;local&quot;, appName = &quot;MahoutLocalContext&quot;,&lt;br/&gt;
    +      customJars = Traversable.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;)&lt;br/&gt;
    +&lt;br/&gt;
    +    /* to run on a Spark cluster provide the Spark Master URL&lt;br/&gt;
    +    implicit val distributedContext = mahoutSparkContext(masterUrl = &quot;spark://occam4:7077&quot;, appName = &quot;MahoutClusteredContext&quot;,&lt;br/&gt;
    +      customJars = Traversable.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;)&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    if no custom jars are used, parameter does not have to be there. Scala default parameters are pretty useful alternative to tons of overloaded functions.&lt;/p&gt;</comment>
                            <comment id="14037704" author="githubbot" created="Thu, 19 Jun 2014 20:12:34 +0100"  >&lt;p&gt;Github user dlyubimov commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/22#discussion_r13987451&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/22#discussion_r13987451&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: spark/src/main/scala/org/apache/mahout/cf/examples/Recommendations.scala &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,172 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
    + * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
    + * this work for additional information regarding copyright ownership.&lt;br/&gt;
    + * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
    + * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
    + * the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.mahout.cf.examples&lt;br/&gt;
    +&lt;br/&gt;
    +import scala.io.Source&lt;br/&gt;
    +import org.apache.mahout.math._&lt;br/&gt;
    +import scalabindings._&lt;br/&gt;
    +import RLikeOps._&lt;br/&gt;
    +import drm._&lt;br/&gt;
    +import RLikeDrmOps._&lt;br/&gt;
    +import org.apache.mahout.sparkbindings._&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.mahout.cf.CooccurrenceAnalysis._&lt;br/&gt;
    +import scala.collection.JavaConversions._&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * The Epinions dataset contains ratings from users to items and a trust-network between the users.&lt;br/&gt;
    + * We use co-occurrence analysis to compute &quot;users who like these items, also like that items&quot; and&lt;br/&gt;
    + * &quot;users who trust these users, like that items&quot;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Download and unpack the dataset files from:&lt;br/&gt;
    + *&lt;br/&gt;
    + * &lt;a href=&quot;http://www.trustlet.org/datasets/downloaded_epinions/ratings_data.txt.bz2&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.trustlet.org/datasets/downloaded_epinions/ratings_data.txt.bz2&lt;/a&gt;&lt;br/&gt;
    + * &lt;a href=&quot;http://www.trustlet.org/datasets/downloaded_epinions/trust_data.txt.bz2&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.trustlet.org/datasets/downloaded_epinions/trust_data.txt.bz2&lt;/a&gt;&lt;br/&gt;
    + **/&lt;br/&gt;
    +object RunCrossCooccurrenceAnalysisOnEpinions {&lt;br/&gt;
    +&lt;br/&gt;
    +  def main(args: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;): Unit = {&lt;br/&gt;
    +&lt;br/&gt;
    +    if (args.length == 0) &lt;/p&gt;
{
    +      println(&quot;Usage: RunCooccurrenceAnalysisOnMovielens1M &amp;lt;path-to-dataset-folder&amp;gt;&quot;)
    +      println(&quot;Download the dataset from http://www.trustlet.org/datasets/downloaded_epinions/ratings_data.txt.bz2 and&quot;)
    +      println(&quot;http://www.trustlet.org/datasets/downloaded_epinions/trust_data.txt.bz2&quot;)
    +      sys.exit(-1)
    +    }
&lt;p&gt;    +&lt;br/&gt;
    +    val datasetDir = args(0)&lt;br/&gt;
    +&lt;br/&gt;
    +    val epinionsRatings = new SparseMatrix(49290, 139738)&lt;br/&gt;
    +&lt;br/&gt;
    +    var firstLineSkipped = false&lt;br/&gt;
    +    for (line &amp;lt;- Source.fromFile(datasetDir + &quot;/ratings_data.txt&quot;).getLines()) {&lt;br/&gt;
    +      if (line.contains(&apos; &apos;) &amp;amp;&amp;amp; firstLineSkipped) &lt;/p&gt;
{
    +        val tokens = line.split(&apos; &apos;)
    +        val userID = tokens(0).toInt - 1
    +        val itemID = tokens(1).toInt - 1
    +        val rating = tokens(2).toDouble
    +        epinionsRatings(userID, itemID) = rating
    +      }
&lt;p&gt;    +      firstLineSkipped = true&lt;br/&gt;
    +    }&lt;br/&gt;
    +&lt;br/&gt;
    +    val epinionsTrustNetwork = new SparseMatrix(49290, 49290)&lt;br/&gt;
    +    firstLineSkipped = false&lt;br/&gt;
    +    for (line &amp;lt;- Source.fromFile(datasetDir + &quot;/trust_data.txt&quot;).getLines()) {&lt;br/&gt;
    +      if (line.contains(&apos; &apos;) &amp;amp;&amp;amp; firstLineSkipped) &lt;/p&gt;
{
    +        val tokens = line.trim.split(&apos; &apos;)
    +        val userID = tokens(0).toInt - 1
    +        val trustedUserId = tokens(1).toInt - 1
    +        epinionsTrustNetwork(userID, trustedUserId) = 1
    +      }
&lt;p&gt;    +      firstLineSkipped = true&lt;br/&gt;
    +    }&lt;br/&gt;
    +&lt;br/&gt;
    +    System.setProperty(&quot;spark.kryo.referenceTracking&quot;, &quot;false&quot;)&lt;br/&gt;
    +    System.setProperty(&quot;spark.kryoserializer.buffer.mb&quot;, &quot;100&quot;)&lt;br/&gt;
    +/* to run on local, can provide number of core by changing to local&lt;span class=&quot;error&quot;&gt;&amp;#91;4&amp;#93;&lt;/span&gt; */&lt;br/&gt;
    +    implicit val distributedContext = mahoutSparkContext(masterUrl = &quot;local&quot;, appName = &quot;MahoutLocalContext&quot;,&lt;br/&gt;
    +      customJars = Traversable.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;)&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    again, no need to specify customJars if no jars are added.&lt;/p&gt;</comment>
                            <comment id="14037708" author="githubbot" created="Thu, 19 Jun 2014 20:14:23 +0100"  >&lt;p&gt;Github user dlyubimov commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/22#discussion_r13987539&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/22#discussion_r13987539&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: spark/src/main/scala/org/apache/mahout/cf/examples/Recommendations.scala &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,172 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
    + * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
    + * this work for additional information regarding copyright ownership.&lt;br/&gt;
    + * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
    + * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
    + * the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.mahout.cf.examples&lt;br/&gt;
    +&lt;br/&gt;
    +import scala.io.Source&lt;br/&gt;
    +import org.apache.mahout.math._&lt;br/&gt;
    +import scalabindings._&lt;br/&gt;
    +import RLikeOps._&lt;br/&gt;
    +import drm._&lt;br/&gt;
    +import RLikeDrmOps._&lt;br/&gt;
    +import org.apache.mahout.sparkbindings._&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.mahout.cf.CooccurrenceAnalysis._&lt;br/&gt;
    +import scala.collection.JavaConversions._&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * The Epinions dataset contains ratings from users to items and a trust-network between the users.&lt;br/&gt;
    + * We use co-occurrence analysis to compute &quot;users who like these items, also like that items&quot; and&lt;br/&gt;
    + * &quot;users who trust these users, like that items&quot;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Download and unpack the dataset files from:&lt;br/&gt;
    + *&lt;br/&gt;
    + * &lt;a href=&quot;http://www.trustlet.org/datasets/downloaded_epinions/ratings_data.txt.bz2&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.trustlet.org/datasets/downloaded_epinions/ratings_data.txt.bz2&lt;/a&gt;&lt;br/&gt;
    + * &lt;a href=&quot;http://www.trustlet.org/datasets/downloaded_epinions/trust_data.txt.bz2&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.trustlet.org/datasets/downloaded_epinions/trust_data.txt.bz2&lt;/a&gt;&lt;br/&gt;
    + **/&lt;br/&gt;
    +object RunCrossCooccurrenceAnalysisOnEpinions {&lt;br/&gt;
    +&lt;br/&gt;
    +  def main(args: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;): Unit = {&lt;br/&gt;
    +&lt;br/&gt;
    +    if (args.length == 0) &lt;/p&gt;
{
    +      println(&quot;Usage: RunCooccurrenceAnalysisOnMovielens1M &amp;lt;path-to-dataset-folder&amp;gt;&quot;)
    +      println(&quot;Download the dataset from http://www.trustlet.org/datasets/downloaded_epinions/ratings_data.txt.bz2 and&quot;)
    +      println(&quot;http://www.trustlet.org/datasets/downloaded_epinions/trust_data.txt.bz2&quot;)
    +      sys.exit(-1)
    +    }
&lt;p&gt;    +&lt;br/&gt;
    +    val datasetDir = args(0)&lt;br/&gt;
    +&lt;br/&gt;
    +    val epinionsRatings = new SparseMatrix(49290, 139738)&lt;br/&gt;
    +&lt;br/&gt;
    +    var firstLineSkipped = false&lt;br/&gt;
    +    for (line &amp;lt;- Source.fromFile(datasetDir + &quot;/ratings_data.txt&quot;).getLines()) {&lt;br/&gt;
    +      if (line.contains(&apos; &apos;) &amp;amp;&amp;amp; firstLineSkipped) &lt;/p&gt;
{
    +        val tokens = line.split(&apos; &apos;)
    +        val userID = tokens(0).toInt - 1
    +        val itemID = tokens(1).toInt - 1
    +        val rating = tokens(2).toDouble
    +        epinionsRatings(userID, itemID) = rating
    +      }
&lt;p&gt;    +      firstLineSkipped = true&lt;br/&gt;
    +    }&lt;br/&gt;
    +&lt;br/&gt;
    +    val epinionsTrustNetwork = new SparseMatrix(49290, 49290)&lt;br/&gt;
    +    firstLineSkipped = false&lt;br/&gt;
    +    for (line &amp;lt;- Source.fromFile(datasetDir + &quot;/trust_data.txt&quot;).getLines()) {&lt;br/&gt;
    +      if (line.contains(&apos; &apos;) &amp;amp;&amp;amp; firstLineSkipped) &lt;/p&gt;
{
    +        val tokens = line.trim.split(&apos; &apos;)
    +        val userID = tokens(0).toInt - 1
    +        val trustedUserId = tokens(1).toInt - 1
    +        epinionsTrustNetwork(userID, trustedUserId) = 1
    +      }
&lt;p&gt;    +      firstLineSkipped = true&lt;br/&gt;
    +    }&lt;br/&gt;
    +&lt;br/&gt;
    +    System.setProperty(&quot;spark.kryo.referenceTracking&quot;, &quot;false&quot;)&lt;br/&gt;
    +    System.setProperty(&quot;spark.kryoserializer.buffer.mb&quot;, &quot;100&quot;)&lt;br/&gt;
    +/* to run on local, can provide number of core by changing to local&lt;span class=&quot;error&quot;&gt;&amp;#91;4&amp;#93;&lt;/span&gt; */&lt;br/&gt;
    +    implicit val distributedContext = mahoutSparkContext(masterUrl = &quot;local&quot;, appName = &quot;MahoutLocalContext&quot;,&lt;br/&gt;
    +      customJars = Traversable.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;)&lt;br/&gt;
    +&lt;br/&gt;
    +    /* to run on a Spark cluster provide the Spark Master URL&lt;br/&gt;
    +    implicit val distributedContext = mahoutSparkContext(masterUrl = &quot;spark://occam4:7077&quot;, appName = &quot;MahoutClusteredContext&quot;,&lt;br/&gt;
    +      customJars = Traversable.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;)&lt;br/&gt;
    +*/&lt;br/&gt;
    +    val drmEpinionsRatings = drmParallelize(epinionsRatings, numPartitions = 2)&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    num partitions here &amp;#8211; is it a &quot;magic: number or you know it is enough for this particular dataset and dataset cannot change?&lt;/p&gt;</comment>
                            <comment id="14037711" author="githubbot" created="Thu, 19 Jun 2014 20:17:37 +0100"  >&lt;p&gt;Github user dlyubimov commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/22#discussion_r13987730&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/22#discussion_r13987730&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: spark/src/main/scala/org/apache/mahout/drivers/ReaderWriter.scala &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,222 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
    + * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
    + * this work for additional information regarding copyright ownership.&lt;br/&gt;
    + * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
    + * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
    + * the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.mahout.drivers&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.spark.SparkContext._&lt;br/&gt;
    +import org.apache.mahout.math.RandomAccessSparseVector&lt;br/&gt;
    +import org.apache.spark.SparkContext&lt;br/&gt;
    +import com.google.common.collect.&lt;/p&gt;
{BiMap, HashBiMap}
&lt;p&gt;    +import scala.collection.JavaConversions._&lt;br/&gt;
    +import org.apache.mahout.math.drm.&lt;/p&gt;
{CheckpointedDrm, DrmLike}
&lt;p&gt;    +import org.apache.mahout.sparkbindings._&lt;br/&gt;
    +&lt;br/&gt;
    +&lt;br/&gt;
    +/** Reader trait is abstract in the sense that the reader function must be defined by an extending trait, which also defines the type to be read.&lt;br/&gt;
    +  * @tparam T type of object read, usually supplied by an extending trait.&lt;br/&gt;
    +  * @todo the reader need not create both dictionaries but does at present. There are cases where one or the other dictionary is never used so saving the memory for a very large dictionary may be worth the optimization to specify which dictionaries are created.&lt;br/&gt;
    +  */&lt;br/&gt;
    +trait Reader&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;&lt;/p&gt;
{
    +  val mc: SparkContext
    +  val readSchema: Schema
    +  protected def reader(mc: SparkContext, readSchema: Schema, source: String): T
    +  def readFrom(source: String): T = reader(mc, readSchema, source)
    +}
&lt;p&gt;    +&lt;br/&gt;
    +/** Writer trait is abstract in the sense that the writer method must be supplied by an extending trait, which also defines the type to be written.&lt;br/&gt;
    +  * @tparam T&lt;br/&gt;
    +  */&lt;br/&gt;
    +trait Writer&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;&lt;/p&gt;
{
    +  val mc: SparkContext
    +  val writeSchema: Schema
    +  protected def writer(mc: SparkContext, writeSchema: Schema, dest: String, collection: T): Unit
    +  def writeTo(collection: T, dest: String) = writer(mc, writeSchema, dest, collection)
    +}
&lt;p&gt;    +&lt;br/&gt;
    +/** Extends Reader trait to supply the [&lt;span class=&quot;error&quot;&gt;&amp;#91;org.apache.mahout.drivers.IndexedDataset&amp;#93;&lt;/span&gt;] as the type read and a reader function for reading text delimited files as described in the [&lt;span class=&quot;error&quot;&gt;&amp;#91;org.apache.mahout.drivers.Schema&amp;#93;&lt;/span&gt;]&lt;br/&gt;
    +  */&lt;br/&gt;
    +trait TDIndexedDatasetReader extends Reader&lt;span class=&quot;error&quot;&gt;&amp;#91;IndexedDataset&amp;#93;&lt;/span&gt;{&lt;br/&gt;
    +  /** Read in text delimited tuples from all URIs in this comma delimited source String. &lt;br/&gt;
    +    * &lt;br/&gt;
    +    * @param mc context for the Spark job&lt;br/&gt;
    +    * @param readSchema describes the delimiters and positions of values in the text delimited file.&lt;br/&gt;
    +    * @param source comma delimited URIs of text files to be read into the [&lt;span class=&quot;error&quot;&gt;&amp;#91;org.apache.mahout.drivers.IndexedDataset&amp;#93;&lt;/span&gt;]&lt;br/&gt;
    +    * @return&lt;br/&gt;
    +    */&lt;br/&gt;
    +  protected def reader(mc: SparkContext, readSchema: Schema, source: String): IndexedDataset = {&lt;br/&gt;
    +    try {&lt;br/&gt;
    +      val delimiter = readSchema(&quot;delim&quot;).asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;&lt;br/&gt;
    +      val rowIDPosition = readSchema(&quot;rowIDPosition&quot;).asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;&lt;br/&gt;
    +      val columnIDPosition = readSchema(&quot;columnIDPosition&quot;).asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;&lt;br/&gt;
    +      val filterPosition = readSchema(&quot;filterPosition&quot;).asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;&lt;br/&gt;
    +      val filterBy = readSchema(&quot;filter&quot;).asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;&lt;br/&gt;
    +      //instance vars must be put into locally scoped vals when used in closures that are&lt;br/&gt;
    +      //executed but Spark&lt;br/&gt;
    +&lt;br/&gt;
    +      assert(!source.isEmpty, &lt;/p&gt;
{
    +        println(this.getClass.toString + &quot;: has no files to read&quot;)
    +        throw new IllegalArgumentException
    +      }
&lt;p&gt;)&lt;br/&gt;
    +&lt;br/&gt;
    +      var columns = mc.textFile(source).map(&lt;/p&gt;
{ line =&amp;gt; line.split(delimiter)}
&lt;p&gt;)&lt;br/&gt;
    +&lt;br/&gt;
    +      columns = columns.filter(&lt;/p&gt;
{ tokens =&amp;gt; tokens(filterPosition) == filterBy}
&lt;p&gt;)&lt;br/&gt;
    +&lt;br/&gt;
    +      val interactions = columns.map(&lt;/p&gt;
{ tokens =&amp;gt; tokens(rowIDPosition) -&amp;gt; tokens(columnIDPosition)}
&lt;p&gt;)&lt;br/&gt;
    +&lt;br/&gt;
    +      interactions.cache()&lt;br/&gt;
    +&lt;br/&gt;
    +      val rowIDs = interactions.map(&lt;/p&gt;
{ case (rowID, _) =&amp;gt; rowID}
&lt;p&gt;).distinct().collect()&lt;br/&gt;
    +      val columnIDs = interactions.map(&lt;/p&gt;
{ case (_, columnID) =&amp;gt; columnID}
&lt;p&gt;).distinct().collect()&lt;br/&gt;
    +&lt;br/&gt;
    +      val numRows = rowIDs.size&lt;br/&gt;
    +      val numColumns = columnIDs.size&lt;br/&gt;
    +&lt;br/&gt;
    +      val rowIDDictionary = asOrderedDictionary(rowIDs)&lt;br/&gt;
    +      val rowIDDictionary_bcast = mc.broadcast(rowIDDictionary)&lt;br/&gt;
    +&lt;br/&gt;
    +      val columnIDDictionary = asOrderedDictionary(columnIDs)&lt;br/&gt;
    +      val columnIDDictionary_bcast = mc.broadcast(columnIDDictionary)&lt;br/&gt;
    +&lt;br/&gt;
    +      val indexedInteractions =&lt;br/&gt;
    +        interactions.map(&lt;/p&gt;
{ case (rowID, columnID) =&amp;gt;
    +          val rowIndex = rowIDDictionary_bcast.value.get(rowID).get
    +          val columnIndex = columnIDDictionary_bcast.value.get(columnID).get
    +
    +          rowIndex -&amp;gt; columnIndex
    +        }
&lt;p&gt;).groupByKey().map({ case (rowIndex, columnIndexes) =&amp;gt;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Here and elsewhere. If compound closure, parenthesis could be ommitted &lt;/p&gt;


&lt;p&gt;         interactions.map { &lt;br/&gt;
              case (rowID, columnID) =&amp;gt;&lt;br/&gt;
                 val rowIndex = rowIDDictionary_bcast.value.get(rowID).get&lt;br/&gt;
    ...&lt;/p&gt;
</comment>
                            <comment id="14037713" author="githubbot" created="Thu, 19 Jun 2014 20:18:23 +0100"  >&lt;p&gt;Github user dlyubimov commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/22#discussion_r13987777&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/22#discussion_r13987777&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: spark/src/main/scala/org/apache/mahout/drivers/ReaderWriter.scala &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,222 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
    + * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
    + * this work for additional information regarding copyright ownership.&lt;br/&gt;
    + * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
    + * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
    + * the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.mahout.drivers&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.spark.SparkContext._&lt;br/&gt;
    +import org.apache.mahout.math.RandomAccessSparseVector&lt;br/&gt;
    +import org.apache.spark.SparkContext&lt;br/&gt;
    +import com.google.common.collect.&lt;/p&gt;
{BiMap, HashBiMap}
&lt;p&gt;    +import scala.collection.JavaConversions._&lt;br/&gt;
    +import org.apache.mahout.math.drm.&lt;/p&gt;
{CheckpointedDrm, DrmLike}
&lt;p&gt;    +import org.apache.mahout.sparkbindings._&lt;br/&gt;
    +&lt;br/&gt;
    +&lt;br/&gt;
    +/** Reader trait is abstract in the sense that the reader function must be defined by an extending trait, which also defines the type to be read.&lt;br/&gt;
    +  * @tparam T type of object read, usually supplied by an extending trait.&lt;br/&gt;
    +  * @todo the reader need not create both dictionaries but does at present. There are cases where one or the other dictionary is never used so saving the memory for a very large dictionary may be worth the optimization to specify which dictionaries are created.&lt;br/&gt;
    +  */&lt;br/&gt;
    +trait Reader&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;{&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    here and elsewhere: style spacing&lt;/p&gt;</comment>
                            <comment id="14037717" author="githubbot" created="Thu, 19 Jun 2014 20:19:37 +0100"  >&lt;p&gt;Github user dlyubimov commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/22#discussion_r13987854&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/22#discussion_r13987854&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: spark/src/main/scala/org/apache/mahout/drivers/ReaderWriter.scala &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,222 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
    + * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
    + * this work for additional information regarding copyright ownership.&lt;br/&gt;
    + * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
    + * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
    + * the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.mahout.drivers&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.spark.SparkContext._&lt;br/&gt;
    +import org.apache.mahout.math.RandomAccessSparseVector&lt;br/&gt;
    +import org.apache.spark.SparkContext&lt;br/&gt;
    +import com.google.common.collect.&lt;/p&gt;
{BiMap, HashBiMap}
&lt;p&gt;    +import scala.collection.JavaConversions._&lt;br/&gt;
    +import org.apache.mahout.math.drm.&lt;/p&gt;
{CheckpointedDrm, DrmLike}
&lt;p&gt;    +import org.apache.mahout.sparkbindings._&lt;br/&gt;
    +&lt;br/&gt;
    +&lt;br/&gt;
    +/** Reader trait is abstract in the sense that the reader function must be defined by an extending trait, which also defines the type to be read.&lt;br/&gt;
    +  * @tparam T type of object read, usually supplied by an extending trait.&lt;br/&gt;
    +  * @todo the reader need not create both dictionaries but does at present. There are cases where one or the other dictionary is never used so saving the memory for a very large dictionary may be worth the optimization to specify which dictionaries are created.&lt;br/&gt;
    +  */&lt;br/&gt;
    +trait Reader&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;{&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Also, it is a matter of convention, but I usually prefer to put public traits and classes in their own file. One may argue &quot;old habits die hard&quot;, but i still think it is the right thing to do for publicly visible things.&lt;/p&gt;</comment>
                            <comment id="14037720" author="githubbot" created="Thu, 19 Jun 2014 20:23:08 +0100"  >&lt;p&gt;Github user dlyubimov commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/22#discussion_r13988052&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/22#discussion_r13988052&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: spark/src/main/scala/org/apache/mahout/drivers/Schema.scala &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,29 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
    + * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
    + * this work for additional information regarding copyright ownership.&lt;br/&gt;
    + * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
    + * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
    + * the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.mahout.drivers&lt;br/&gt;
    +&lt;br/&gt;
    +import scala.collection.mutable.HashMap&lt;br/&gt;
    +&lt;br/&gt;
    +/** Syntactic sugar for HashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;String, Any&amp;#93;&lt;/span&gt;&lt;br/&gt;
    +  *&lt;br/&gt;
    +  * @param params list of mappings for instantiation {{&lt;/p&gt;
{val mySchema = new Schema(&quot;one&quot; -&amp;gt; 1, &quot;two&quot; -&amp;gt; &quot;2&quot;, ...)}
&lt;p&gt;}}&lt;br/&gt;
    +  */&lt;br/&gt;
    +class Schema(params: Tuple2&lt;span class=&quot;error&quot;&gt;&amp;#91;String, Any&amp;#93;&lt;/span&gt;*) extends HashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;String, Any&amp;#93;&lt;/span&gt; {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Hm. i&apos;d still rather delegate than extend, &lt;/p&gt;

&lt;p&gt;    Then you can either expose parameters as a public value; or, assuming you just want to be able to write schema.any-MapLike-method, you can provide an explicit conversion from Schema to MapLike.&lt;/p&gt;</comment>
                            <comment id="14037730" author="githubbot" created="Thu, 19 Jun 2014 20:32:28 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/22#issuecomment-46606053&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/22#issuecomment-46606053&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    General note: a lot of style problems. &lt;br/&gt;
    Code lines not to exceed 120 charactes (i think i saw some suspiciously long). &lt;/p&gt;

&lt;p&gt;    Definitely lack of comments. &lt;/p&gt;

&lt;p&gt;    FYI Spark comment style is the following: every comment starts with a captial letter and is formatted to cut off at 100th character. And they are very draconian about it. And that&apos;s what i followed here as well. &lt;/p&gt;

&lt;p&gt;    Well the 100th character is questionable since it cannot be auto formatted in IDEA, but I do believe comments do need some justification applied on the right. &lt;/p&gt;

&lt;p&gt;    for closure stacks, i&apos;d suggest the following comment /etc style&lt;/p&gt;

&lt;p&gt;        val b = A&lt;/p&gt;

&lt;p&gt;           // I want to map&lt;br/&gt;
          .map &lt;/p&gt;
{ tuple =&amp;gt; 
            ....
          }

&lt;p&gt;          // I want to filter &lt;br/&gt;
          .filter &lt;/p&gt;
{ tuple =&amp;gt; 
           .... 
          }

&lt;p&gt;    So it would be useful to state in plain words  what closure is to accomplish, since they are functional units, i.e. function-grade citizens, and as such, imo deserve some explanation.&lt;/p&gt;

</comment>
                            <comment id="14038129" author="githubbot" created="Fri, 20 Jun 2014 00:55:40 +0100"  >&lt;p&gt;Github user pferrel commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/22#discussion_r14001263&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/22#discussion_r14001263&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: spark/src/main/scala/org/apache/mahout/cf/examples/Recommendations.scala &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,172 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
    + * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
    + * this work for additional information regarding copyright ownership.&lt;br/&gt;
    + * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
    + * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
    + * the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.mahout.cf.examples&lt;br/&gt;
    +&lt;br/&gt;
    +import scala.io.Source&lt;br/&gt;
    +import org.apache.mahout.math._&lt;br/&gt;
    +import scalabindings._&lt;br/&gt;
    +import RLikeOps._&lt;br/&gt;
    +import drm._&lt;br/&gt;
    +import RLikeDrmOps._&lt;br/&gt;
    +import org.apache.mahout.sparkbindings._&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.mahout.cf.CooccurrenceAnalysis._&lt;br/&gt;
    +import scala.collection.JavaConversions._&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * The Epinions dataset contains ratings from users to items and a trust-network between the users.&lt;br/&gt;
    + * We use co-occurrence analysis to compute &quot;users who like these items, also like that items&quot; and&lt;br/&gt;
    + * &quot;users who trust these users, like that items&quot;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Download and unpack the dataset files from:&lt;br/&gt;
    + *&lt;br/&gt;
    + * &lt;a href=&quot;http://www.trustlet.org/datasets/downloaded_epinions/ratings_data.txt.bz2&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.trustlet.org/datasets/downloaded_epinions/ratings_data.txt.bz2&lt;/a&gt;&lt;br/&gt;
    + * &lt;a href=&quot;http://www.trustlet.org/datasets/downloaded_epinions/trust_data.txt.bz2&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.trustlet.org/datasets/downloaded_epinions/trust_data.txt.bz2&lt;/a&gt;&lt;br/&gt;
    + **/&lt;br/&gt;
    +object RunCrossCooccurrenceAnalysisOnEpinions {&lt;br/&gt;
    +&lt;br/&gt;
    +  def main(args: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;): Unit = {&lt;br/&gt;
    +&lt;br/&gt;
    +    if (args.length == 0) &lt;/p&gt;
{
    +      println(&quot;Usage: RunCooccurrenceAnalysisOnMovielens1M &amp;lt;path-to-dataset-folder&amp;gt;&quot;)
    +      println(&quot;Download the dataset from http://www.trustlet.org/datasets/downloaded_epinions/ratings_data.txt.bz2 and&quot;)
    +      println(&quot;http://www.trustlet.org/datasets/downloaded_epinions/trust_data.txt.bz2&quot;)
    +      sys.exit(-1)
    +    }
&lt;p&gt;    +&lt;br/&gt;
    +    val datasetDir = args(0)&lt;br/&gt;
    +&lt;br/&gt;
    +    val epinionsRatings = new SparseMatrix(49290, 139738)&lt;br/&gt;
    +&lt;br/&gt;
    +    var firstLineSkipped = false&lt;br/&gt;
    +    for (line &amp;lt;- Source.fromFile(datasetDir + &quot;/ratings_data.txt&quot;).getLines()) {&lt;br/&gt;
    +      if (line.contains(&apos; &apos;) &amp;amp;&amp;amp; firstLineSkipped) &lt;/p&gt;
{
    +        val tokens = line.split(&apos; &apos;)
    +        val userID = tokens(0).toInt - 1
    +        val itemID = tokens(1).toInt - 1
    +        val rating = tokens(2).toDouble
    +        epinionsRatings(userID, itemID) = rating
    +      }
&lt;p&gt;    +      firstLineSkipped = true&lt;br/&gt;
    +    }&lt;br/&gt;
    +&lt;br/&gt;
    +    val epinionsTrustNetwork = new SparseMatrix(49290, 49290)&lt;br/&gt;
    +    firstLineSkipped = false&lt;br/&gt;
    +    for (line &amp;lt;- Source.fromFile(datasetDir + &quot;/trust_data.txt&quot;).getLines()) {&lt;br/&gt;
    +      if (line.contains(&apos; &apos;) &amp;amp;&amp;amp; firstLineSkipped) &lt;/p&gt;
{
    +        val tokens = line.trim.split(&apos; &apos;)
    +        val userID = tokens(0).toInt - 1
    +        val trustedUserId = tokens(1).toInt - 1
    +        epinionsTrustNetwork(userID, trustedUserId) = 1
    +      }
&lt;p&gt;    +      firstLineSkipped = true&lt;br/&gt;
    +    }&lt;br/&gt;
    +&lt;br/&gt;
    +    System.setProperty(&quot;spark.kryo.referenceTracking&quot;, &quot;false&quot;)&lt;br/&gt;
    +    System.setProperty(&quot;spark.kryoserializer.buffer.mb&quot;, &quot;100&quot;)&lt;br/&gt;
    +/* to run on local, can provide number of core by changing to local&lt;span class=&quot;error&quot;&gt;&amp;#91;4&amp;#93;&lt;/span&gt; */&lt;br/&gt;
    +    implicit val distributedContext = mahoutSparkContext(masterUrl = &quot;local&quot;, appName = &quot;MahoutLocalContext&quot;,&lt;br/&gt;
    +      customJars = Traversable.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;)&lt;br/&gt;
    +&lt;br/&gt;
    +    /* to run on a Spark cluster provide the Spark Master URL&lt;br/&gt;
    +    implicit val distributedContext = mahoutSparkContext(masterUrl = &quot;spark://occam4:7077&quot;, appName = &quot;MahoutClusteredContext&quot;,&lt;br/&gt;
    +      customJars = Traversable.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;)&lt;br/&gt;
    +*/&lt;br/&gt;
    +    val drmEpinionsRatings = drmParallelize(epinionsRatings, numPartitions = 2)&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Needs to be taken out before merge. Was forced for testing, not production.&lt;/p&gt;</comment>
                            <comment id="14038131" author="githubbot" created="Fri, 20 Jun 2014 00:57:26 +0100"  >&lt;p&gt;Github user pferrel commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/22#discussion_r14001316&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/22#discussion_r14001316&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: spark/src/main/scala/org/apache/mahout/drivers/ReaderWriter.scala &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,222 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
    + * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
    + * this work for additional information regarding copyright ownership.&lt;br/&gt;
    + * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
    + * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
    + * the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.mahout.drivers&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.spark.SparkContext._&lt;br/&gt;
    +import org.apache.mahout.math.RandomAccessSparseVector&lt;br/&gt;
    +import org.apache.spark.SparkContext&lt;br/&gt;
    +import com.google.common.collect.&lt;/p&gt;
{BiMap, HashBiMap}
&lt;p&gt;    +import scala.collection.JavaConversions._&lt;br/&gt;
    +import org.apache.mahout.math.drm.&lt;/p&gt;
{CheckpointedDrm, DrmLike}
&lt;p&gt;    +import org.apache.mahout.sparkbindings._&lt;br/&gt;
    +&lt;br/&gt;
    +&lt;br/&gt;
    +/** Reader trait is abstract in the sense that the reader function must be defined by an extending trait, which also defines the type to be read.&lt;br/&gt;
    +  * @tparam T type of object read, usually supplied by an extending trait.&lt;br/&gt;
    +  * @todo the reader need not create both dictionaries but does at present. There are cases where one or the other dictionary is never used so saving the memory for a very large dictionary may be worth the optimization to specify which dictionaries are created.&lt;br/&gt;
    +  */&lt;br/&gt;
    +trait Reader&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;{&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Yeah, I agree, the files need to be rearranged but I wanted to get an eye on the design in case there was some deeper need to refactor.&lt;/p&gt;</comment>
                            <comment id="14038139" author="githubbot" created="Fri, 20 Jun 2014 01:05:05 +0100"  >&lt;p&gt;Github user pferrel commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/22#discussion_r14001554&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/22#discussion_r14001554&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: spark/src/main/scala/org/apache/mahout/cf/examples/Recommendations.scala &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,172 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
    + * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
    + * this work for additional information regarding copyright ownership.&lt;br/&gt;
    + * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
    + * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
    + * the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.mahout.cf.examples&lt;br/&gt;
    +&lt;br/&gt;
    +import scala.io.Source&lt;br/&gt;
    +import org.apache.mahout.math._&lt;br/&gt;
    +import scalabindings._&lt;br/&gt;
    +import RLikeOps._&lt;br/&gt;
    +import drm._&lt;br/&gt;
    +import RLikeDrmOps._&lt;br/&gt;
    +import org.apache.mahout.sparkbindings._&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.mahout.cf.CooccurrenceAnalysis._&lt;br/&gt;
    +import scala.collection.JavaConversions._&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * The Epinions dataset contains ratings from users to items and a trust-network between the users.&lt;br/&gt;
    + * We use co-occurrence analysis to compute &quot;users who like these items, also like that items&quot; and&lt;br/&gt;
    + * &quot;users who trust these users, like that items&quot;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Download and unpack the dataset files from:&lt;br/&gt;
    + *&lt;br/&gt;
    + * &lt;a href=&quot;http://www.trustlet.org/datasets/downloaded_epinions/ratings_data.txt.bz2&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.trustlet.org/datasets/downloaded_epinions/ratings_data.txt.bz2&lt;/a&gt;&lt;br/&gt;
    + * &lt;a href=&quot;http://www.trustlet.org/datasets/downloaded_epinions/trust_data.txt.bz2&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.trustlet.org/datasets/downloaded_epinions/trust_data.txt.bz2&lt;/a&gt;&lt;br/&gt;
    + **/&lt;br/&gt;
    +object RunCrossCooccurrenceAnalysisOnEpinions {&lt;br/&gt;
    +&lt;br/&gt;
    +  def main(args: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;): Unit = {&lt;br/&gt;
    +&lt;br/&gt;
    +    if (args.length == 0) &lt;/p&gt;
{
    +      println(&quot;Usage: RunCooccurrenceAnalysisOnMovielens1M &amp;lt;path-to-dataset-folder&amp;gt;&quot;)
    +      println(&quot;Download the dataset from http://www.trustlet.org/datasets/downloaded_epinions/ratings_data.txt.bz2 and&quot;)
    +      println(&quot;http://www.trustlet.org/datasets/downloaded_epinions/trust_data.txt.bz2&quot;)
    +      sys.exit(-1)
    +    }
&lt;p&gt;    +&lt;br/&gt;
    +    val datasetDir = args(0)&lt;br/&gt;
    +&lt;br/&gt;
    +    val epinionsRatings = new SparseMatrix(49290, 139738)&lt;br/&gt;
    +&lt;br/&gt;
    +    var firstLineSkipped = false&lt;br/&gt;
    +    for (line &amp;lt;- Source.fromFile(datasetDir + &quot;/ratings_data.txt&quot;).getLines()) {&lt;br/&gt;
    +      if (line.contains(&apos; &apos;) &amp;amp;&amp;amp; firstLineSkipped) &lt;/p&gt;
{
    +        val tokens = line.split(&apos; &apos;)
    +        val userID = tokens(0).toInt - 1
    +        val itemID = tokens(1).toInt - 1
    +        val rating = tokens(2).toDouble
    +        epinionsRatings(userID, itemID) = rating
    +      }
&lt;p&gt;    +      firstLineSkipped = true&lt;br/&gt;
    +    }&lt;br/&gt;
    +&lt;br/&gt;
    +    val epinionsTrustNetwork = new SparseMatrix(49290, 49290)&lt;br/&gt;
    +    firstLineSkipped = false&lt;br/&gt;
    +    for (line &amp;lt;- Source.fromFile(datasetDir + &quot;/trust_data.txt&quot;).getLines()) {&lt;br/&gt;
    +      if (line.contains(&apos; &apos;) &amp;amp;&amp;amp; firstLineSkipped) &lt;/p&gt;
{
    +        val tokens = line.trim.split(&apos; &apos;)
    +        val userID = tokens(0).toInt - 1
    +        val trustedUserId = tokens(1).toInt - 1
    +        epinionsTrustNetwork(userID, trustedUserId) = 1
    +      }
&lt;p&gt;    +      firstLineSkipped = true&lt;br/&gt;
    +    }&lt;br/&gt;
    +&lt;br/&gt;
    +    System.setProperty(&quot;spark.kryo.referenceTracking&quot;, &quot;false&quot;)&lt;br/&gt;
    +    System.setProperty(&quot;spark.kryoserializer.buffer.mb&quot;, &quot;100&quot;)&lt;br/&gt;
    +/* to run on local, can provide number of core by changing to local&lt;span class=&quot;error&quot;&gt;&amp;#91;4&amp;#93;&lt;/span&gt; */&lt;br/&gt;
    +    implicit val distributedContext = mahoutSparkContext(masterUrl = &quot;local&quot;, appName = &quot;MahoutLocalContext&quot;,&lt;br/&gt;
    +      customJars = Traversable.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;)&lt;br/&gt;
    +&lt;br/&gt;
    +    /* to run on a Spark cluster provide the Spark Master URL&lt;br/&gt;
    +    implicit val distributedContext = mahoutSparkContext(masterUrl = &quot;spark://occam4:7077&quot;, appName = &quot;MahoutClusteredContext&quot;,&lt;br/&gt;
    +      customJars = Traversable.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;)&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Ok, seems like I remember these had to be passed in when I wrote that, my mistake,&lt;/p&gt;</comment>
                            <comment id="14038144" author="githubbot" created="Fri, 20 Jun 2014 01:08:37 +0100"  >&lt;p&gt;Github user pferrel commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/22#discussion_r14001642&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/22#discussion_r14001642&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: spark/src/main/scala/org/apache/mahout/cf/examples/Recommendations.scala &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,172 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
    + * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
    + * this work for additional information regarding copyright ownership.&lt;br/&gt;
    + * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
    + * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
    + * the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.mahout.cf.examples&lt;br/&gt;
    +&lt;br/&gt;
    +import scala.io.Source&lt;br/&gt;
    +import org.apache.mahout.math._&lt;br/&gt;
    +import scalabindings._&lt;br/&gt;
    +import RLikeOps._&lt;br/&gt;
    +import drm._&lt;br/&gt;
    +import RLikeDrmOps._&lt;br/&gt;
    +import org.apache.mahout.sparkbindings._&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.mahout.cf.CooccurrenceAnalysis._&lt;br/&gt;
    +import scala.collection.JavaConversions._&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * The Epinions dataset contains ratings from users to items and a trust-network between the users.&lt;br/&gt;
    + * We use co-occurrence analysis to compute &quot;users who like these items, also like that items&quot; and&lt;br/&gt;
    + * &quot;users who trust these users, like that items&quot;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Download and unpack the dataset files from:&lt;br/&gt;
    + *&lt;br/&gt;
    + * &lt;a href=&quot;http://www.trustlet.org/datasets/downloaded_epinions/ratings_data.txt.bz2&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.trustlet.org/datasets/downloaded_epinions/ratings_data.txt.bz2&lt;/a&gt;&lt;br/&gt;
    + * &lt;a href=&quot;http://www.trustlet.org/datasets/downloaded_epinions/trust_data.txt.bz2&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.trustlet.org/datasets/downloaded_epinions/trust_data.txt.bz2&lt;/a&gt;&lt;br/&gt;
    + **/&lt;br/&gt;
    +object RunCrossCooccurrenceAnalysisOnEpinions {&lt;br/&gt;
    +&lt;br/&gt;
    +  def main(args: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;): Unit = {&lt;br/&gt;
    +&lt;br/&gt;
    +    if (args.length == 0) &lt;/p&gt;
{
    +      println(&quot;Usage: RunCooccurrenceAnalysisOnMovielens1M &amp;lt;path-to-dataset-folder&amp;gt;&quot;)
    +      println(&quot;Download the dataset from http://www.trustlet.org/datasets/downloaded_epinions/ratings_data.txt.bz2 and&quot;)
    +      println(&quot;http://www.trustlet.org/datasets/downloaded_epinions/trust_data.txt.bz2&quot;)
    +      sys.exit(-1)
    +    }
&lt;p&gt;    +&lt;br/&gt;
    +    val datasetDir = args(0)&lt;br/&gt;
    +&lt;br/&gt;
    +    val epinionsRatings = new SparseMatrix(49290, 139738)&lt;br/&gt;
    +&lt;br/&gt;
    +    var firstLineSkipped = false&lt;br/&gt;
    +    for (line &amp;lt;- Source.fromFile(datasetDir + &quot;/ratings_data.txt&quot;).getLines()) {&lt;br/&gt;
    +      if (line.contains(&apos; &apos;) &amp;amp;&amp;amp; firstLineSkipped) &lt;/p&gt;
{
    +        val tokens = line.split(&apos; &apos;)
    +        val userID = tokens(0).toInt - 1
    +        val itemID = tokens(1).toInt - 1
    +        val rating = tokens(2).toDouble
    +        epinionsRatings(userID, itemID) = rating
    +      }
&lt;p&gt;    +      firstLineSkipped = true&lt;br/&gt;
    +    }&lt;br/&gt;
    +&lt;br/&gt;
    +    val epinionsTrustNetwork = new SparseMatrix(49290, 49290)&lt;br/&gt;
    +    firstLineSkipped = false&lt;br/&gt;
    +    for (line &amp;lt;- Source.fromFile(datasetDir + &quot;/trust_data.txt&quot;).getLines()) {&lt;br/&gt;
    +      if (line.contains(&apos; &apos;) &amp;amp;&amp;amp; firstLineSkipped) &lt;/p&gt;
{
    +        val tokens = line.trim.split(&apos; &apos;)
    +        val userID = tokens(0).toInt - 1
    +        val trustedUserId = tokens(1).toInt - 1
    +        epinionsTrustNetwork(userID, trustedUserId) = 1
    +      }
&lt;p&gt;    +      firstLineSkipped = true&lt;br/&gt;
    +    }&lt;br/&gt;
    +&lt;br/&gt;
    +    System.setProperty(&quot;spark.kryo.referenceTracking&quot;, &quot;false&quot;)&lt;br/&gt;
    +    System.setProperty(&quot;spark.kryoserializer.buffer.mb&quot;, &quot;100&quot;)&lt;br/&gt;
    +/* to run on local, can provide number of core by changing to local&lt;span class=&quot;error&quot;&gt;&amp;#91;4&amp;#93;&lt;/span&gt; */&lt;br/&gt;
    +    implicit val distributedContext = mahoutSparkContext(masterUrl = &quot;local&quot;, appName = &quot;MahoutLocalContext&quot;,&lt;br/&gt;
    +      customJars = Traversable.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;)&lt;br/&gt;
    +&lt;br/&gt;
    +    /* to run on a Spark cluster provide the Spark Master URL&lt;br/&gt;
    +    implicit val distributedContext = mahoutSparkContext(masterUrl = &quot;spark://occam4:7077&quot;, appName = &quot;MahoutClusteredContext&quot;,&lt;br/&gt;
    +      customJars = Traversable.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;)&lt;br/&gt;
    +*/&lt;br/&gt;
    +    val drmEpinionsRatings = drmParallelize(epinionsRatings, numPartitions = 2)&lt;br/&gt;
    +    val drmEpinionsTrustNetwork = drmParallelize(epinionsTrustNetwork, numPartitions = 2)&lt;br/&gt;
    +&lt;br/&gt;
    +    val indicatorMatrices = cooccurrences(drmEpinionsRatings, randomSeed = 0xdeadbeef,&lt;br/&gt;
    +        maxInterestingItemsPerThing = 100, maxNumInteractions = 500, Array(drmEpinionsTrustNetwork))&lt;br/&gt;
    +&lt;br/&gt;
    +/* local storage */&lt;br/&gt;
    +    RecommendationExamplesHelper.saveIndicatorMatrix(indicatorMatrices(0),&lt;br/&gt;
    +        &quot;/tmp/co-occurrence-on-epinions/indicators-item-item/&quot;)&lt;br/&gt;
    +    RecommendationExamplesHelper.saveIndicatorMatrix(indicatorMatrices(1),&lt;br/&gt;
    +        &quot;/tmp/co-occurrence-on-epinions/indicators-trust-item/&quot;)&lt;br/&gt;
    +&lt;br/&gt;
    +/*  To run on HDFS put your path to the data here, example of fully qualified path on my cluster provided&lt;br/&gt;
    +    RecommendationExamplesHelper.saveIndicatorMatrix(indicatorMatrices(0),&lt;br/&gt;
    +      &quot;hdfs://occam4:54310/user/pat/xrsj/indicators-item-item/&quot;)&lt;br/&gt;
    +    RecommendationExamplesHelper.saveIndicatorMatrix(indicatorMatrices(1),&lt;br/&gt;
    +      &quot;hdfs://occam4:54310/user/pat/xrsj/indicators-trust-item/&quot;)&lt;br/&gt;
    +*/&lt;br/&gt;
    +    distributedContext.close()&lt;br/&gt;
    +&lt;br/&gt;
    +    println(&quot;Saved indicators to /tmp/co-occurrence-on-epinions/&quot;)&lt;br/&gt;
    +  }&lt;br/&gt;
    +}&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * The movielens1M dataset contains movie ratings, we use co-occurrence analysis to compute&lt;br/&gt;
    + * &quot;users who like these movies, also like that movies&quot;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Download and unpack the dataset files from:&lt;br/&gt;
    + * &lt;a href=&quot;http://files.grouplens.org/datasets/movielens/ml-1m.zip&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://files.grouplens.org/datasets/movielens/ml-1m.zip&lt;/a&gt;&lt;br/&gt;
    + */&lt;br/&gt;
    +object RunCooccurrenceAnalysisOnMovielens1M {&lt;br/&gt;
    +&lt;br/&gt;
    +  def main(args: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;): Unit = {&lt;br/&gt;
    +&lt;br/&gt;
    +    if (args.length == 0) &lt;/p&gt;
{
    +      println(&quot;Usage: RunCooccurrenceAnalysisOnMovielens1M &amp;lt;path-to-dataset-folder&amp;gt;&quot;)
    +      println(&quot;Download the dataset from http://files.grouplens.org/datasets/movielens/ml-1m.zip&quot;)
    +      sys.exit(-1)
    +    }
&lt;p&gt;    +&lt;br/&gt;
    +    val datasetDir = args(0)&lt;br/&gt;
    +&lt;br/&gt;
    +    System.setProperty(&quot;spark.kryo.referenceTracking&quot;, &quot;false&quot;)&lt;br/&gt;
    +    System.setProperty(&quot;spark.kryoserializer.buffer.mb&quot;, &quot;100&quot;)&lt;br/&gt;
    +&lt;br/&gt;
    +    implicit val sc = mahoutSparkContext(masterUrl = &quot;local&quot;, appName = &quot;MahoutLocalContext&quot;,&lt;br/&gt;
    +      customJars = Traversable.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;)&lt;br/&gt;
    +&lt;br/&gt;
    +    System.setProperty(&quot;mahout.math.AtA.maxInMemNCol&quot;, 4000.toString)&lt;br/&gt;
    +&lt;br/&gt;
    +    val movielens = new SparseMatrix(6040, 3952)&lt;br/&gt;
    +&lt;br/&gt;
    +    for (line &amp;lt;- Source.fromFile(datasetDir + &quot;/ratings.dat&quot;).getLines()) &lt;/p&gt;
{
    +      val tokens = line.split(&quot;::&quot;)
    +      val userID = tokens(0).toInt - 1
    +      val itemID = tokens(1).toInt - 1
    +      val rating = tokens(2).toDouble
    +      movielens(userID, itemID) = rating
    +    }
&lt;p&gt;    +&lt;br/&gt;
    +    val drmMovielens = drmParallelize(movielens, numPartitions = 2)&lt;br/&gt;
    +&lt;br/&gt;
    +    val indicatorMatrix = cooccurrences(drmMovielens).head&lt;br/&gt;
    +&lt;br/&gt;
    +    RecommendationExamplesHelper.saveIndicatorMatrix(indicatorMatrix,&lt;br/&gt;
    +        &quot;/tmp/co-occurrence-on-movielens/indicators-item-item/&quot;)&lt;br/&gt;
    +&lt;br/&gt;
    +    sc.stop()&lt;br/&gt;
    +&lt;br/&gt;
    +    println(&quot;Saved indicators to /tmp/co-occurrence-on-movielens/&quot;)&lt;br/&gt;
    +  }&lt;br/&gt;
    +}&lt;br/&gt;
    +&lt;br/&gt;
    +object RecommendationExamplesHelper {&lt;br/&gt;
    +&lt;br/&gt;
    +  def saveIndicatorMatrix(indicatorMatrix: DrmLike&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;, path: String) = {&lt;br/&gt;
    +    indicatorMatrix.rdd.flatMap({ case (thingID, itemVector) =&amp;gt;&lt;br/&gt;
    +        for (elem &amp;lt;- itemVector.nonZeroes()) yield &lt;/p&gt;
{ thingID + &apos;\t&apos; + elem.index }
&lt;p&gt;    +      })&lt;br/&gt;
    +      .saveAsTextFile(path)&lt;br/&gt;
    +  }&lt;br/&gt;
    +}&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    This file is from Sebastian, it will be updated to use the driver and kept in examples. Was originally from a patch for &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1464&quot; title=&quot;Cooccurrence Analysis on Spark&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1464&quot;&gt;&lt;del&gt;MAHOUT-1464&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14039872" author="githubbot" created="Sat, 21 Jun 2014 16:40:07 +0100"  >&lt;p&gt;Github user pferrel commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/22#issuecomment-46756833&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/22#issuecomment-46756833&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    ok. well I&apos;ll assume the naming conventions are ok the Scala is reasonable and start the polish and tests. There are a lot of combinations of options. &lt;/p&gt;</comment>
                            <comment id="14043735" author="githubbot" created="Wed, 25 Jun 2014 17:56:32 +0100"  >&lt;p&gt;Github user pferrel commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/22#issuecomment-47129279&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/22#issuecomment-47129279&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    This is getting close to merge time. There are several test cases, which exercise most of the code in the PR. It is building with tests on the current master.&lt;/p&gt;

&lt;p&gt;    Missing but needs to be done before merge:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;a modified mahout script that runs this.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Significantly missing, but planned for another PR:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;two, or more input streams for cross-cooccurrence. This version will do a cross-cooccurrence calc by filtering one input tuple stream into two matrices. This allows testing and may be a common use case but should not be the only option for this use.&lt;/li&gt;
	&lt;li&gt;uses HashBiMaps from Guava for &lt;em&gt;all&lt;/em&gt; ID management, even when the IDs are Mahout ordinals. Also all four ID indexes are created even though in this case the External Row/User IDs are never used. An optimization would calculate only the dictionaries needed.&lt;/li&gt;
	&lt;li&gt;HashBiMaps are created once and broadcast to the rest of the jobs. These are not based on rdds and so we may want to do something about these in the future. Haven&apos;t thought much about this so suggestions are welcome.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14046162" author="githubbot" created="Fri, 27 Jun 2014 18:13:57 +0100"  >&lt;p&gt;Github user pferrel commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/22#issuecomment-47375744&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/22#issuecomment-47375744&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;pom edited&lt;/li&gt;
	&lt;li&gt;assembly/job.xml added&lt;/li&gt;
	&lt;li&gt;mahout script edited, no mahout.cmd but will ask someone with a windows machine to update it.&lt;/li&gt;
	&lt;li&gt;updated MahoutKryoRegistrator to add HashBiMap&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Runs on clustered HDFS + Spark, still using Hadoop 1.2.1 but after some more testing and hopefully some review of the stuff in this comment, it&apos;s about ready to merge.&lt;/p&gt;</comment>
                            <comment id="14046911" author="githubbot" created="Sat, 28 Jun 2014 17:33:53 +0100"  >&lt;p&gt;Github user pferrel commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/22#discussion_r14325027&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/22#discussion_r14325027&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: spark/src/main/scala/org/apache/mahout/drivers/ReaderWriter.scala &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,222 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
    + * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
    + * this work for additional information regarding copyright ownership.&lt;br/&gt;
    + * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
    + * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
    + * the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.mahout.drivers&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.spark.SparkContext._&lt;br/&gt;
    +import org.apache.mahout.math.RandomAccessSparseVector&lt;br/&gt;
    +import org.apache.spark.SparkContext&lt;br/&gt;
    +import com.google.common.collect.&lt;/p&gt;
{BiMap, HashBiMap}
&lt;p&gt;    +import scala.collection.JavaConversions._&lt;br/&gt;
    +import org.apache.mahout.math.drm.&lt;/p&gt;
{CheckpointedDrm, DrmLike}
&lt;p&gt;    +import org.apache.mahout.sparkbindings._&lt;br/&gt;
    +&lt;br/&gt;
    +&lt;br/&gt;
    +/** Reader trait is abstract in the sense that the reader function must be defined by an extending trait, which also defines the type to be read.&lt;br/&gt;
    +  * @tparam T type of object read, usually supplied by an extending trait.&lt;br/&gt;
    +  * @todo the reader need not create both dictionaries but does at present. There are cases where one or the other dictionary is never used so saving the memory for a very large dictionary may be worth the optimization to specify which dictionaries are created.&lt;br/&gt;
    +  */&lt;br/&gt;
    +trait Reader&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;{&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    I hear you but maybe a compromise is best in this case. The Reader and Writer  traits are core but trivial. I put them in their own file. The TextDelimited stuff is much more complext but have some one liner syntactic sugar to make them easier to use so I put these in another file--all related to read/write of text delimited files. Gives some better logical grouping, which can be missing from one class one file in Java. &lt;/p&gt;</comment>
                            <comment id="14046923" author="githubbot" created="Sat, 28 Jun 2014 17:54:37 +0100"  >&lt;p&gt;Github user pferrel commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/22#discussion_r14325090&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/22#discussion_r14325090&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: spark/src/main/scala/org/apache/mahout/drivers/Schema.scala &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,29 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
    + * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
    + * this work for additional information regarding copyright ownership.&lt;br/&gt;
    + * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
    + * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
    + * the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.mahout.drivers&lt;br/&gt;
    +&lt;br/&gt;
    +import scala.collection.mutable.HashMap&lt;br/&gt;
    +&lt;br/&gt;
    +/** Syntactic sugar for HashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;String, Any&amp;#93;&lt;/span&gt;&lt;br/&gt;
    +  *&lt;br/&gt;
    +  * @param params list of mappings for instantiation {{&lt;/p&gt;
{val mySchema = new Schema(&quot;one&quot; -&amp;gt; 1, &quot;two&quot; -&amp;gt; &quot;2&quot;, ...)}
&lt;p&gt;}}&lt;br/&gt;
    +  */&lt;br/&gt;
    +class Schema(params: Tuple2&lt;span class=&quot;error&quot;&gt;&amp;#91;String, Any&amp;#93;&lt;/span&gt;*) extends HashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;String, Any&amp;#93;&lt;/span&gt; {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    OK, so create a class that contains an instance of MapLike&lt;span class=&quot;error&quot;&gt;&amp;#91;String, Any&amp;#93;&lt;/span&gt; then use the above constructors and supply a new implicit conversion in the companion Object? I&apos;ve seen this pattern but wasn&apos;t sure why to use it. I&apos;ll give it a try here but if you could say why this is good practice i&apos;d appreciate it.&lt;/p&gt;</comment>
                            <comment id="14049205" author="githubbot" created="Tue, 1 Jul 2014 20:11:27 +0100"  >&lt;p&gt;Github user pferrel commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/22#issuecomment-47697509&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/22#issuecomment-47697509&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Doing final tests before push, will close &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1541&quot; title=&quot;Create CLI Driver for Spark Cooccurrence Analysis&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1541&quot;&gt;&lt;del&gt;MAHOUT-1541&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1568&quot; title=&quot;Build an I/O model that can replace sequence files for import/export&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1568&quot;&gt;&lt;del&gt;MAHOUT-1568&lt;/del&gt;&lt;/a&gt;, and &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1569&quot; title=&quot;Create CLI driver that supports Spark jobs&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1569&quot;&gt;&lt;del&gt;MAHOUT-1569&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14049339" author="githubbot" created="Tue, 1 Jul 2014 22:18:32 +0100"  >&lt;p&gt;Github user pferrel commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/22#issuecomment-47711165&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/22#issuecomment-47711165&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    And the tests fail. Legacy itemsimilarity is not producing the same results on the same files. Looks like a problem in CooccurrenceAnalysis.cooccurrence. The tests for that class have the wrong values and so pass incorrectly.&lt;/p&gt;</comment>
                            <comment id="14051741" author="githubbot" created="Thu, 3 Jul 2014 18:35:09 +0100"  >&lt;p&gt;Github user pferrel commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/22#issuecomment-47960471&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/22#issuecomment-47960471&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    The discrepancy between Spark and mrlegacy is in the final output value from the legacy code&lt;/p&gt;

&lt;p&gt;    ```&lt;br/&gt;
    return 1.0 - 1.0 / (1.0 + logLikelihood);&lt;br/&gt;
    ```&lt;/p&gt;

&lt;p&gt;    This produces the same results for spark and mrlegacy so i&apos;ll go with it but would love an explanation.&lt;/p&gt;</comment>
                            <comment id="14051984" author="githubbot" created="Thu, 3 Jul 2014 23:59:30 +0100"  >&lt;p&gt;Github user asfgit closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/22&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/22&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14052663" author="pferrel" created="Fri, 4 Jul 2014 22:05:23 +0100"  >&lt;p&gt;There is still work to do on this so I&apos;ll leave the Jira open for a bit.&lt;/p&gt;

&lt;p&gt;Two issues:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Do we care about output as tuple, like the hadoop version? The Spark version outputs a DRM with app specific item and user IDs.&lt;/li&gt;
	&lt;li&gt;We need an option to  sort the indicators by strength and omit strength values from the output. This will allow the output to be indexed directly by a search engine. One step from logfiles to indexable indicators, can&apos;t wait.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Something on the horizon. The algo for epinions data seems to need 5g of Spark executor memory. This seems like a lot and may have to do with the use of HashBiMaps of IDs that are broadcast to every node. This can be optimized first by not calculating ID dictionaries for users, since they are not in the output. When using legacy Mahout int IDs we don&apos;t need any Dictionaries. Also looking at the lifetime of the broadcast vals. The A&apos;A dictionaries may be kept alive while doing the B&apos;A calc--still investigating this. &lt;/p&gt;

&lt;p&gt;Quite a bit more difficult will be a truly scalable treatment of the dictionaries.&lt;/p&gt;</comment>
                            <comment id="14052701" author="hudson" created="Fri, 4 Jul 2014 23:43:47 +0100"  >&lt;p&gt;SUCCESS: Integrated in Mahout-Quality #2684 (See &lt;a href=&quot;https://builds.apache.org/job/Mahout-Quality/2684/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/Mahout-Quality/2684/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1541&quot; title=&quot;Create CLI Driver for Spark Cooccurrence Analysis&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1541&quot;&gt;&lt;del&gt;MAHOUT-1541&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1568&quot; title=&quot;Build an I/O model that can replace sequence files for import/export&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1568&quot;&gt;&lt;del&gt;MAHOUT-1568&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1569&quot; title=&quot;Create CLI driver that supports Spark jobs&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1569&quot;&gt;&lt;del&gt;MAHOUT-1569&lt;/del&gt;&lt;/a&gt; fixed a build test problem, drivers have an option new to not search for MAHOUT_HOME and SPARK_HOME (pat: rev 32badb1d360ddf514e6b253f2dea9ae7e5df078a)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;spark/src/main/scala/org/apache/mahout/drivers/MahoutDriver.scala&lt;/li&gt;
	&lt;li&gt;spark/src/main/scala/org/apache/mahout/drivers/ItemSimilarityDriver.scala&lt;/li&gt;
	&lt;li&gt;spark/src/test/scala/org/apache/mahout/drivers/ItemSimilarityDriverSuite.scala&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14052749" author="tdunning" created="Sat, 5 Jul 2014 02:30:59 +0100"  >&lt;p&gt;The use of the 1/(1+LLR) as a relatedness score by the map-reduce version is an attempt to shoehorn the LLR based cooccurrence into the framework of the other recommenders.  That score should normally be ignored in search based recommendation systems.&lt;/p&gt;</comment>
                            <comment id="14052935" author="pferrel" created="Sat, 5 Jul 2014 19:24:08 +0100"  >&lt;p&gt;Ah, makes sense. I&apos;ll take it out since the other measures won&apos;t be supported (afaik). This formulation does seem to invert the meaning of a high score since LLR is in the denominator.&lt;/p&gt;

&lt;p&gt;BTW I was thinking of implementing something like the old hadoop all-recs-for-all-users since a lot of people seem to use it and it&apos;s now almost trivial to support. You can do it in the shell with a few lines of code. and it runs in a few minutes instead of hours.&lt;/p&gt;</comment>
                            <comment id="14052968" author="hudson" created="Sat, 5 Jul 2014 22:31:23 +0100"  >&lt;p&gt;SUCCESS: Integrated in Mahout-Quality #2686 (See &lt;a href=&quot;https://builds.apache.org/job/Mahout-Quality/2686/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/Mahout-Quality/2686/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1541&quot; title=&quot;Create CLI Driver for Spark Cooccurrence Analysis&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1541&quot;&gt;&lt;del&gt;MAHOUT-1541&lt;/del&gt;&lt;/a&gt; backed out compatability with legacy Item Similarity, now outputs raw LLR scores (pat: rev 24cb5576f720737b73906ebb15be486d540ac629)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;spark/src/test/scala/org/apache/mahout/cf/CooccurrenceAnalysisSuite.scala&lt;/li&gt;
	&lt;li&gt;spark/src/main/scala/org/apache/mahout/cf/CooccurrenceAnalysis.scala&lt;/li&gt;
	&lt;li&gt;spark/src/test/scala/org/apache/mahout/drivers/ItemSimilarityDriverSuite.scala&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14053800" author="hudson" created="Mon, 7 Jul 2014 17:09:15 +0100"  >&lt;p&gt;SUCCESS: Integrated in Mahout-Quality #2688 (See &lt;a href=&quot;https://builds.apache.org/job/Mahout-Quality/2688/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/Mahout-Quality/2688/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1541&quot; title=&quot;Create CLI Driver for Spark Cooccurrence Analysis&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1541&quot;&gt;&lt;del&gt;MAHOUT-1541&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1568&quot; title=&quot;Build an I/O model that can replace sequence files for import/export&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1568&quot;&gt;&lt;del&gt;MAHOUT-1568&lt;/del&gt;&lt;/a&gt;, added option to ItemSimilarityDriver to allow output that is directly search engine indexable, also some default schema&apos;s for input and output of TDF tuples and DRMs (pat: rev 9bfb767323833586873272af4db446f68f357f1f)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;spark/src/test/scala/org/apache/mahout/drivers/ItemSimilarityDriverSuite.scala&lt;/li&gt;
	&lt;li&gt;spark/src/main/scala/org/apache/mahout/drivers/TextDelimitedReaderWriter.scala&lt;/li&gt;
	&lt;li&gt;spark/src/main/scala/org/apache/mahout/drivers/Schema.scala&lt;/li&gt;
	&lt;li&gt;spark/src/main/scala/org/apache/mahout/drivers/ItemSimilarityDriver.scala&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14065509" author="githubbot" created="Thu, 17 Jul 2014 21:34:39 +0100"  >&lt;p&gt;GitHub user pferrel opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/31&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/31&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1541&quot; title=&quot;Create CLI Driver for Spark Cooccurrence Analysis&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1541&quot;&gt;&lt;del&gt;MAHOUT-1541&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Covers &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1464&quot; title=&quot;Cooccurrence Analysis on Spark&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1464&quot;&gt;&lt;del&gt;MAHOUT-1464&lt;/del&gt;&lt;/a&gt;, and &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1541&quot; title=&quot;Create CLI Driver for Spark Cooccurrence Analysis&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1541&quot;&gt;&lt;del&gt;MAHOUT-1541&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Fixes cross-cooccurrence, which originally was of the wrong order. If A is the primary interaction matrix, them cross should be A&apos;B but was B&apos;A. Also added tests for A and B of different column cardinality, which should is allowed.&lt;/p&gt;

&lt;p&gt;    Looked a bit deeper regarding @dev thread on changing cardinality of a sparse matrix. This is needed in ItemSimilarity because only once all cross-interaction matrices have been read in can the true cardinality of all be known. The much more typical case is that they can be computed from the input. So there needs to be a method to update the cardinality after the matrices have been read in.&lt;/p&gt;

&lt;p&gt;    This implementation creates a new abstract method in CheckpointedDrm and implementation in CheckpointedDrmSpark. Here is the reasoning.&lt;/p&gt;

&lt;p&gt;    1) for sparse DRMs there is no need for any representation of an empty row (or column) not even the keys need to be known only the cardinality. You only have to think about a transpose of sparse vectors to see that this must be so. Further it works and I&#8217;ve relied on it since the Hadoop mr version. Baring any revelation from the math gods--it is so.&lt;/p&gt;

&lt;p&gt;    2) rbind semantics apply to dense matrices. This IMO should be avoided in this case because even if we rejigger rbind to only change the cardinality without inserting real rows it would seem to violate its semantics. Sparse matrices don&#8217;t fit the default R semantics in a few areas (in my non-expert opinion) and this is one. Unless someone feels strongly it will be in CheckpointedDrm as abstract and implemented in CheckpoimtedDrmSpark#addToRowCardinality(n: Int): Unit. Creating an op that returns a new CheckpointedDrm is also possible if there is something unsafe about my implementation, but rbind? &lt;/p&gt;

&lt;p&gt;    3) I have implemented this so that there is no call to drm.nrow neither to read nor modify it. So it will remain lazy evaluated until needed by other math.&lt;/p&gt;

&lt;p&gt;    4) ItemSimilarity for the A&#8217;B case now passes several asymmetric input cases and outputs the correct external IDs.&lt;/p&gt;


&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/pferrel/mahout&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/pferrel/mahout&lt;/a&gt; mahout-1541&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/31.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/31.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #31&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 107a0ba9605241653a85b113661a8fa5c055529f&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-04T19:54:22Z&lt;/p&gt;

&lt;p&gt;    added Sebastian&apos;s CooccurrenceAnalysis patch updated it to use current Mahout-DSL&lt;/p&gt;

&lt;p&gt;commit 74b9921c4c9bd8903585bbd74d9e66298ea8b7a0&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-04T20:09:07Z&lt;/p&gt;

&lt;p&gt;    Adding stuff for itemsimilarity driver for Spark&lt;/p&gt;

&lt;p&gt;commit a59265931ed3a51ba81e1a0a7171ebb102be4fa4&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-04T20:13:13Z&lt;/p&gt;

&lt;p&gt;    added scopt to pom deps&lt;/p&gt;

&lt;p&gt;commit 16c03f7fa73c156859d1dba3a333ef9e8bf922b0&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-04T21:32:18Z&lt;/p&gt;

&lt;p&gt;    added Sebastian&apos;s MurmurHash changes&lt;/p&gt;

&lt;p&gt;    Signed-off-by: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;/p&gt;

&lt;p&gt;commit 8a4b4347ddb7b9ac97590aa20189d89d8a07a80a&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-04T21:33:11Z&lt;/p&gt;

&lt;p&gt;    Merge branch &apos;mahout-1464&apos; into mahout-1541&lt;/p&gt;

&lt;p&gt;commit 2f87f5433f90fa2c49ef386ca245943e1fc73beb&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-05T01:44:16Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1541&quot; title=&quot;Create CLI Driver for Spark Cooccurrence Analysis&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1541&quot;&gt;&lt;del&gt;MAHOUT-1541&lt;/del&gt;&lt;/a&gt; still working on this, some refactoring in the DSL for abstracting away Spark has moved access to rddsno Jira is closed yet&lt;/p&gt;

&lt;p&gt;commit c6adaa44c80bba99d41600e260bbb1ad5c972e69&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-05T16:52:23Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1464&quot; title=&quot;Cooccurrence Analysis on Spark&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1464&quot;&gt;&lt;del&gt;MAHOUT-1464&lt;/del&gt;&lt;/a&gt; import cleanup, minor changes to examples for running on Spark Cluster&lt;/p&gt;

&lt;p&gt;commit 2caceab31703ed214c1e66d5fc63b8bdb05d37a3&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-05T16:55:09Z&lt;/p&gt;

&lt;p&gt;    Merge branch &apos;mahout-1464&apos; into mahout-1541&lt;/p&gt;

&lt;p&gt;commit 6df6a54e3ff174d39bd817caf7d16c2d362be3f8&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-07T20:39:25Z&lt;/p&gt;

&lt;p&gt;    Merge branch &apos;master&apos; into mahout-1541&lt;/p&gt;

&lt;p&gt;commit a2f84dea3f32d3df3e98c61f085bc1fabd453551&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-07T21:27:06Z&lt;/p&gt;

&lt;p&gt;    drmWrap seems to be the answer to the changed DrmLike interface. Code works again but more to do.&lt;/p&gt;

&lt;p&gt;commit d3a2ba5027436d0abef67a1a5e82557064f4ba49&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-17T16:00:38Z&lt;/p&gt;

&lt;p&gt;    merged master, got new cooccurrence code&lt;/p&gt;

&lt;p&gt;commit 4b2fb07b21a8ac2d532ee51b65b27d1482293cb0&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-19T17:08:02Z&lt;/p&gt;

&lt;p&gt;    for high level review, not ready for merge&lt;/p&gt;

&lt;p&gt;commit 996ccfb82a8ed3ff90f51968e661b2449f3c4759&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-19T17:46:23Z&lt;/p&gt;

&lt;p&gt;    for high level review, not ready for merge. changed to dot notation&lt;/p&gt;

&lt;p&gt;commit f62ab071869ee205ad398a3e094d871138e11a9e&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-19T18:13:44Z&lt;/p&gt;

&lt;p&gt;    for high level review, not ready for merge. fixed a couple scaladoc refs&lt;/p&gt;

&lt;p&gt;commit cbef0ee6264c28d0597cb2507427a647771c9bcd&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-23T21:49:20Z&lt;/p&gt;

&lt;p&gt;    adding tests, had to modify some test framework Scala to make the masterUrl visible to tests&lt;/p&gt;

&lt;p&gt;commit ab8009f6176f0c21a07e15cc5cc8a9717dd7cc4c&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-25T15:41:54Z&lt;/p&gt;

&lt;p&gt;    adding more tests for ItemSimilarityDriver&lt;/p&gt;

&lt;p&gt;commit 47258f59df7f215b1bb25830d13d9b85fa8d19e9&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-25T15:44:47Z&lt;/p&gt;

&lt;p&gt;    merged master changes and fixed a pom conflict&lt;/p&gt;

&lt;p&gt;commit 9a02e2a5ea8540723c1bfc6ea01b045bb4175922&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-25T16:57:55Z&lt;/p&gt;

&lt;p&gt;    remove tmp after all tests, fixed dangling comma in input file list&lt;/p&gt;

&lt;p&gt;commit 3c343ff18600f0a0e59f5bfd63bd86db0db0e8c5&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-26T22:19:48Z&lt;/p&gt;

&lt;p&gt;    changes to pom, mahout driver script, and cleaned up help text&lt;/p&gt;

&lt;p&gt;commit 213b18dee259925de82c703451bdea640e1f068e&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-26T22:26:17Z&lt;/p&gt;

&lt;p&gt;    added a job.xml assembly for creation of an all-dependencies jar&lt;/p&gt;

&lt;p&gt;commit 627d39f30860e4ab43783c72cc2cf8926060b73c&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-27T16:44:37Z&lt;/p&gt;

&lt;p&gt;    registered HashBiMap with JavaSerializer in Kryo&lt;/p&gt;

&lt;p&gt;commit c273dc7de3c740189ce8157b334c2eef3a4c23ea&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-27T21:30:13Z&lt;/p&gt;

&lt;p&gt;    increased the default max heep for mahout/JVM to 4g, using max of 4g for Spark executor&lt;/p&gt;

&lt;p&gt;commit 9dd2f2eabf1bf64660de6b5b5e49aafe18229a7a&lt;br/&gt;
Author: Pat Ferrel &amp;lt;pat@farfetchers.com&amp;gt;&lt;br/&gt;
Date:   2014-06-30T17:06:49Z&lt;/p&gt;

&lt;p&gt;    tweaking memory requirements to process epinions with the ItemSimilarityDriver&lt;/p&gt;

&lt;p&gt;commit 6ec98f32775c791ee001fc996f475215e427f368&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-30T17:08:49Z&lt;/p&gt;

&lt;p&gt;    refactored to use a DistributedContext instead of raw SparkContext&lt;/p&gt;

&lt;p&gt;commit 48774e154a6e55e04037c787f8d64bc9e545f1bd&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-30T17:08:59Z&lt;/p&gt;

&lt;p&gt;    merging changes made on to run a large dataset through itemsimilarity on the cluster&lt;/p&gt;

&lt;p&gt;commit 8e70091a564c8464ea70bf90006d8124c3a7f208&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-30T20:11:42Z&lt;/p&gt;

&lt;p&gt;    fixed a bug, SparkConf in driver was ignored and blank one passed in to create a DistributedContext&lt;/p&gt;

&lt;p&gt;commit 01a0341f56071d2244aabd6de8c6f528ad35b164&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-30T20:33:39Z&lt;/p&gt;

&lt;p&gt;    added option for configuring Spark executor memory&lt;/p&gt;

&lt;p&gt;commit 2d9efd73def8207dded5cd1dd8699035a8cc1b34&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-30T22:37:19Z&lt;/p&gt;

&lt;p&gt;    removed some outdated examples&lt;/p&gt;

&lt;p&gt;commit 9fb281022cba7666dd26701b3d97d200b13c35f8&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-07-01T18:17:42Z&lt;/p&gt;

&lt;p&gt;    test naming and pom changed to up the jvm heap max to 512m for scalatests&lt;/p&gt;

&lt;p&gt;commit 674c9b7862f0bd0723de026eb4527546b52e8a0b&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-07-01T18:18:59Z&lt;/p&gt;

&lt;p&gt;    Merge branch &apos;mahout-1541&apos; of &lt;a href=&quot;https://github.com/pferrel/mahout&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/pferrel/mahout&lt;/a&gt; into mahout-1541&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="14065515" author="githubbot" created="Thu, 17 Jul 2014 21:43:09 +0100"  >&lt;p&gt;Github user pferrel commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/31#issuecomment-49362805&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/31#issuecomment-49362805&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    BTW I also moved CF into math-scala, leaving tests in spark. There may be a way to move those into math-scala with the new engine neutral test stuff, I&apos;ll look into that before I merge. &lt;/p&gt;</comment>
                            <comment id="14067985" author="githubbot" created="Sun, 20 Jul 2014 18:18:35 +0100"  >&lt;p&gt;Github user pferrel commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/31#discussion_r15150741&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/31#discussion_r15150741&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: spark/src/main/scala/org/apache/mahout/drivers/IndexedDataset.scala &amp;#8212;&lt;br/&gt;
    @@ -39,14 +40,29 @@ import org.apache.mahout.math.drm.CheckpointedDrm&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;to be not created when not needed.&lt;br/&gt;
       */&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    -case class IndexedDataset(matrix: CheckpointedDrm&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;, rowIDs: BiMap&lt;span class=&quot;error&quot;&gt;&amp;#91;String,Int&amp;#93;&lt;/span&gt;, columnIDs: BiMap&lt;span class=&quot;error&quot;&gt;&amp;#91;String,Int&amp;#93;&lt;/span&gt;) {&lt;br/&gt;
    +case class IndexedDataset(var matrix: CheckpointedDrm&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;, var rowIDs: BiMap&lt;span class=&quot;error&quot;&gt;&amp;#91;String,Int&amp;#93;&lt;/span&gt;, var columnIDs: BiMap&lt;span class=&quot;error&quot;&gt;&amp;#91;String,Int&amp;#93;&lt;/span&gt;) {&lt;br/&gt;
    +&lt;br/&gt;
    +  // we must allow the row dimension to be adjusted in the case where the data read in is incomplete and we&lt;br/&gt;
    +  // learn this afterwards&lt;br/&gt;
    +&lt;br/&gt;
    +  /**&lt;br/&gt;
    +   * Adds the equivalent of blank rows to the sparse CheckpointedDrm, which only changes the row cardinality value.&lt;br/&gt;
    +   * No physical changes are made to the underlying drm.&lt;br/&gt;
    +   * @param n number to increase row carnindality by&lt;br/&gt;
    +   * @note should be done before any BLAS optimizer actions are performed on the matrix or you&apos;ll get unpredictable&lt;br/&gt;
    +   *       results.&lt;br/&gt;
    +   */&lt;br/&gt;
    +  def addToRowCardinality(n: Int): Unit = &lt;/p&gt;
{
    +    assert(n &amp;gt; -1)
    +    matrix.asInstanceOf[CheckpointedDrmSpark[Int]].addToRowCardinality(n)
    +  }
&lt;p&gt;     }&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    This supports an immutable CheckpointedDrm. It will create a new CheckpointedDrm with row cardinality that leaves some rows with no representation in the underlying rdd. The tests that use this seem to work but it may be an accident of the test data and this may not be a good implementation.&lt;/p&gt;

&lt;p&gt;    The question is if a CheckpointedDrm, needs to be backed by an rdd with n =&amp;gt; {} for all zero valued rows or if the absence of anything in the rdd, in other words a gap in the row key sequence, is equivalent to inserting an n = {}.&lt;/p&gt;</comment>
                            <comment id="14067987" author="githubbot" created="Sun, 20 Jul 2014 18:20:03 +0100"  >&lt;p&gt;Github user pferrel commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/31#discussion_r15150759&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/31#discussion_r15150759&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: spark/src/main/scala/org/apache/mahout/sparkbindings/drm/CheckpointedDrmSpark.scala &amp;#8212;&lt;br/&gt;
    @@ -46,6 +46,19 @@ class CheckpointedDrmSpark&lt;span class=&quot;error&quot;&gt;&amp;#91;K: ClassTag&amp;#93;&lt;/span&gt;(&lt;br/&gt;
       private var cached: Boolean = false&lt;br/&gt;
       override val context: DistributedContext = rdd.context&lt;/p&gt;

&lt;p&gt;    +  /**&lt;br/&gt;
    +   * Adds the equivalent of blank rows to the sparse CheckpointedDrm, which only changes the&lt;br/&gt;
    +   * [[org.apache.mahout.sparkbindings.drm&lt;br/&gt;
    +.CheckpointedDrmSpark#nrow]] value.&lt;br/&gt;
    +   * No physical changes are made to the underlying rdd, now blank rows are added as would be done with rbind(blankRows)&lt;br/&gt;
    +   * @param n number to increase row cardinality by&lt;br/&gt;
    +   * @note should be done before any BLAS optimizer actions are performed on the matrix or you&apos;ll get unpredictable&lt;br/&gt;
    +   *       results.&lt;br/&gt;
    +   */&lt;br/&gt;
    +  override def addToRowCardinality(n: Int): CheckpointedDrm&lt;span class=&quot;error&quot;&gt;&amp;#91;K&amp;#93;&lt;/span&gt; = &lt;/p&gt;
{
    +    assert(n &amp;gt; -1)
    +    new CheckpointedDrmSpark[K](rdd, nrow + n, ncol, _cacheStorageLevel )
    +  }
&lt;p&gt;    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    This supports an immutable CheckpointedDrm. It will create a new CheckpointedDrm with row cardinality that leaves some rows with no representation in the underlying rdd. The tests that use this seem to work but it may be an accident of the test data and this may not be a good implementation.&lt;/p&gt;

&lt;p&gt;    The question is if a CheckpointedDrm, needs to be backed by an rdd with n =&amp;gt; {} for all zero valued rows or if the absence of anything in the rdd, in other words a gap in the row key sequence, is equivalent to inserting an n = {}.&lt;/p&gt;</comment>
                            <comment id="14067990" author="githubbot" created="Sun, 20 Jul 2014 18:32:32 +0100"  >&lt;p&gt;Github user avati commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/31#discussion_r15150836&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/31#discussion_r15150836&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: spark/src/main/scala/org/apache/mahout/sparkbindings/drm/CheckpointedDrmSpark.scala &amp;#8212;&lt;br/&gt;
    @@ -46,6 +46,19 @@ class CheckpointedDrmSpark&lt;span class=&quot;error&quot;&gt;&amp;#91;K: ClassTag&amp;#93;&lt;/span&gt;(&lt;br/&gt;
       private var cached: Boolean = false&lt;br/&gt;
       override val context: DistributedContext = rdd.context&lt;/p&gt;

&lt;p&gt;    +  /**&lt;br/&gt;
    +   * Adds the equivalent of blank rows to the sparse CheckpointedDrm, which only changes the&lt;br/&gt;
    +   * [[org.apache.mahout.sparkbindings.drm&lt;br/&gt;
    +.CheckpointedDrmSpark#nrow]] value.&lt;br/&gt;
    +   * No physical changes are made to the underlying rdd, now blank rows are added as would be done with rbind(blankRows)&lt;br/&gt;
    +   * @param n number to increase row cardinality by&lt;br/&gt;
    +   * @note should be done before any BLAS optimizer actions are performed on the matrix or you&apos;ll get unpredictable&lt;br/&gt;
    +   *       results.&lt;br/&gt;
    +   */&lt;br/&gt;
    +  override def addToRowCardinality(n: Int): CheckpointedDrm&lt;span class=&quot;error&quot;&gt;&amp;#91;K&amp;#93;&lt;/span&gt; = &lt;/p&gt;
{
    +    assert(n &amp;gt; -1)
    +    new CheckpointedDrmSpark[K](rdd, nrow + n, ncol, _cacheStorageLevel )
    +  }
&lt;p&gt;    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    This fixes the immutability problem, but the missing rows still create the following issues:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;AewScalar: math errors&lt;/li&gt;
	&lt;li&gt;AewB: java exception&lt;/li&gt;
	&lt;li&gt;CbindAB: java exception&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    All three are non-trivial to fix (i.e no one liner fixes).&lt;/p&gt;
</comment>
                            <comment id="14068936" author="githubbot" created="Mon, 21 Jul 2014 19:05:44 +0100"  >&lt;p&gt;Github user pferrel commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/31#discussion_r15184775&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/31#discussion_r15184775&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: spark/src/main/scala/org/apache/mahout/sparkbindings/drm/CheckpointedDrmSpark.scala &amp;#8212;&lt;br/&gt;
    @@ -46,6 +46,19 @@ class CheckpointedDrmSpark&lt;span class=&quot;error&quot;&gt;&amp;#91;K: ClassTag&amp;#93;&lt;/span&gt;(&lt;br/&gt;
       private var cached: Boolean = false&lt;br/&gt;
       override val context: DistributedContext = rdd.context&lt;/p&gt;

&lt;p&gt;    +  /**&lt;br/&gt;
    +   * Adds the equivalent of blank rows to the sparse CheckpointedDrm, which only changes the&lt;br/&gt;
    +   * [[org.apache.mahout.sparkbindings.drm&lt;br/&gt;
    +.CheckpointedDrmSpark#nrow]] value.&lt;br/&gt;
    +   * No physical changes are made to the underlying rdd, now blank rows are added as would be done with rbind(blankRows)&lt;br/&gt;
    +   * @param n number to increase row cardinality by&lt;br/&gt;
    +   * @note should be done before any BLAS optimizer actions are performed on the matrix or you&apos;ll get unpredictable&lt;br/&gt;
    +   *       results.&lt;br/&gt;
    +   */&lt;br/&gt;
    +  override def addToRowCardinality(n: Int): CheckpointedDrm&lt;span class=&quot;error&quot;&gt;&amp;#91;K&amp;#93;&lt;/span&gt; = &lt;/p&gt;
{
    +    assert(n &amp;gt; -1)
    +    new CheckpointedDrmSpark[K](rdd, nrow + n, ncol, _cacheStorageLevel )
    +  }
&lt;p&gt;    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    I see no fundamental reason for these not to work but it may not be part of the DRM contract. So maybe I&apos;ll make a feature request Jira to support this. &lt;/p&gt;

&lt;p&gt;    In the meantime rbind will not solve this because A will have missing rows at the end but B may have them throughout--let alone some future C. So I think reading in all data into one drm with one row and column id space then chopping into two or more drms based on column ranges should give us empty rows where they are needed (I certainly hope so or I&apos;m in trouble). Will have to keep track of which column ids go in which slice but that&apos;s doable. &lt;/p&gt;</comment>
                            <comment id="14069342" author="githubbot" created="Mon, 21 Jul 2014 22:41:13 +0100"  >&lt;p&gt;Github user dlyubimov commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/31#discussion_r15198464&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/31#discussion_r15198464&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: spark/src/main/scala/org/apache/mahout/sparkbindings/drm/CheckpointedDrmSpark.scala &amp;#8212;&lt;br/&gt;
    @@ -46,6 +46,19 @@ class CheckpointedDrmSpark&lt;span class=&quot;error&quot;&gt;&amp;#91;K: ClassTag&amp;#93;&lt;/span&gt;(&lt;br/&gt;
       private var cached: Boolean = false&lt;br/&gt;
       override val context: DistributedContext = rdd.context&lt;/p&gt;

&lt;p&gt;    +  /**&lt;br/&gt;
    +   * Adds the equivalent of blank rows to the sparse CheckpointedDrm, which only changes the&lt;br/&gt;
    +   * [[org.apache.mahout.sparkbindings.drm&lt;br/&gt;
    +.CheckpointedDrmSpark#nrow]] value.&lt;br/&gt;
    +   * No physical changes are made to the underlying rdd, now blank rows are added as would be done with rbind(blankRows)&lt;br/&gt;
    +   * @param n number to increase row cardinality by&lt;br/&gt;
    +   * @note should be done before any BLAS optimizer actions are performed on the matrix or you&apos;ll get unpredictable&lt;br/&gt;
    +   *       results.&lt;br/&gt;
    +   */&lt;br/&gt;
    +  override def addToRowCardinality(n: Int): CheckpointedDrm&lt;span class=&quot;error&quot;&gt;&amp;#91;K&amp;#93;&lt;/span&gt; = &lt;/p&gt;
{
    +    assert(n &amp;gt; -1)
    +    new CheckpointedDrmSpark[K](rdd, nrow + n, ncol, _cacheStorageLevel )
    +  }
&lt;p&gt;    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    -1 on this PR. &lt;/p&gt;

&lt;p&gt;    i am not sure what problem it is solving but there has got to be a different way to solve it. Matlab/R semantics is deemed sufficient to solve algebraic problems historically and they did not have a need for this. So shouldn&apos;t we. &lt;/p&gt;

&lt;p&gt;    if nothing else, ultimately one can always exit to RDD level and re-format RDD content to whatever liking.&lt;/p&gt;



</comment>
                            <comment id="14069362" author="githubbot" created="Mon, 21 Jul 2014 22:48:47 +0100"  >&lt;p&gt;Github user pferrel commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/31#discussion_r15198911&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/31#discussion_r15198911&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: spark/src/main/scala/org/apache/mahout/sparkbindings/drm/CheckpointedDrmSpark.scala &amp;#8212;&lt;br/&gt;
    @@ -46,6 +46,19 @@ class CheckpointedDrmSpark&lt;span class=&quot;error&quot;&gt;&amp;#91;K: ClassTag&amp;#93;&lt;/span&gt;(&lt;br/&gt;
       private var cached: Boolean = false&lt;br/&gt;
       override val context: DistributedContext = rdd.context&lt;/p&gt;

&lt;p&gt;    +  /**&lt;br/&gt;
    +   * Adds the equivalent of blank rows to the sparse CheckpointedDrm, which only changes the&lt;br/&gt;
    +   * [[org.apache.mahout.sparkbindings.drm&lt;br/&gt;
    +.CheckpointedDrmSpark#nrow]] value.&lt;br/&gt;
    +   * No physical changes are made to the underlying rdd, now blank rows are added as would be done with rbind(blankRows)&lt;br/&gt;
    +   * @param n number to increase row cardinality by&lt;br/&gt;
    +   * @note should be done before any BLAS optimizer actions are performed on the matrix or you&apos;ll get unpredictable&lt;br/&gt;
    +   *       results.&lt;br/&gt;
    +   */&lt;br/&gt;
    +  override def addToRowCardinality(n: Int): CheckpointedDrm&lt;span class=&quot;error&quot;&gt;&amp;#91;K&amp;#93;&lt;/span&gt; = &lt;/p&gt;
{
    +    assert(n &amp;gt; -1)
    +    new CheckpointedDrmSpark[K](rdd, nrow + n, ncol, _cacheStorageLevel )
    +  }
&lt;p&gt;    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    you are looking at old code and this is not meant to be merged. &lt;/p&gt;</comment>
                            <comment id="14069380" author="githubbot" created="Mon, 21 Jul 2014 22:55:24 +0100"  >&lt;p&gt;Github user pferrel closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/31&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/31&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14079886" author="githubbot" created="Wed, 30 Jul 2014 21:13:15 +0100"  >&lt;p&gt;GitHub user pferrel opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/36&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/36&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1541&quot; title=&quot;Create CLI Driver for Spark Cooccurrence Analysis&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1541&quot;&gt;&lt;del&gt;MAHOUT-1541&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Parts of this address &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1541&quot; title=&quot;Create CLI Driver for Spark Cooccurrence Analysis&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1541&quot;&gt;&lt;del&gt;MAHOUT-1541&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1568&quot; title=&quot;Build an I/O model that can replace sequence files for import/export&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1568&quot;&gt;&lt;del&gt;MAHOUT-1568&lt;/del&gt;&lt;/a&gt;, and &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1569&quot; title=&quot;Create CLI driver that supports Spark jobs&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1569&quot;&gt;&lt;del&gt;MAHOUT-1569&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    The previous merge of &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1541&quot; title=&quot;Create CLI Driver for Spark Cooccurrence Analysis&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1541&quot;&gt;&lt;del&gt;MAHOUT-1541&lt;/del&gt;&lt;/a&gt; was supporting A&apos;A primarily, this merge support A&apos;B as well with all features. Lots of refactoring and new tests for A and B of different cardinality and using different item ID spaces. Took the forced cardinality matching from Math and put in the data prep part. This means passing in an nrow to drmWrap, which may be larger than the actual number of rows embodied in the drm/rdd. I&apos;ve added tests for B.t %*% A as well as the actual driver for these cases (missing row cases).&lt;/p&gt;

&lt;p&gt;    Can&apos;t complete the full epinions cross-cooccurrence on a single machine with an out of Java heap exception. So I&apos;m now testing it on a cluster. The cooccurrence of A&apos;A does complete on a single machine.&lt;/p&gt;

&lt;p&gt;    One known improvement is to limit the use of dictionaries if they are not need and to look at replacing the Guava HashBiMap with a minimal Scala verison. This version uses dictionaries for IDs even if the input is using Mahout sequential int IDs.&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1568&quot; title=&quot;Build an I/O model that can replace sequence files for import/export&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1568&quot;&gt;&lt;del&gt;MAHOUT-1568&lt;/del&gt;&lt;/a&gt;: Proposed standards for text versions of DRM-ish output. These preserve the IDs of the application while using Mahout IDs internally. In other words output has application IDs. There are several configurable readers and writers of TD files. Reading Tuples into a DRM is implemented, Writing a DRM-ish TD file is also implemented.&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1569&quot; title=&quot;Create CLI driver that supports Spark jobs&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1569&quot;&gt;&lt;del&gt;MAHOUT-1569&lt;/del&gt;&lt;/a&gt;: There is a refactored MahoutOptionParser and MahoutDriver with some default behavior that should make creating drivers a bit easier and DRYer than the last merge. The options are being proposed as standards across all drivers so we have one way to specify the formats of input/output files and other common options.&lt;/p&gt;

&lt;p&gt;    This is for comment, I won&apos;t merge until several larger dataset are working on a cluster.&lt;/p&gt;


&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/pferrel/mahout&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/pferrel/mahout&lt;/a&gt; mahout-1541&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/36.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/36.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #36&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 107a0ba9605241653a85b113661a8fa5c055529f&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-04T19:54:22Z&lt;/p&gt;

&lt;p&gt;    added Sebastian&apos;s CooccurrenceAnalysis patch updated it to use current Mahout-DSL&lt;/p&gt;

&lt;p&gt;commit 74b9921c4c9bd8903585bbd74d9e66298ea8b7a0&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-04T20:09:07Z&lt;/p&gt;

&lt;p&gt;    Adding stuff for itemsimilarity driver for Spark&lt;/p&gt;

&lt;p&gt;commit a59265931ed3a51ba81e1a0a7171ebb102be4fa4&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-04T20:13:13Z&lt;/p&gt;

&lt;p&gt;    added scopt to pom deps&lt;/p&gt;

&lt;p&gt;commit 16c03f7fa73c156859d1dba3a333ef9e8bf922b0&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-04T21:32:18Z&lt;/p&gt;

&lt;p&gt;    added Sebastian&apos;s MurmurHash changes&lt;/p&gt;

&lt;p&gt;    Signed-off-by: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;/p&gt;

&lt;p&gt;commit 8a4b4347ddb7b9ac97590aa20189d89d8a07a80a&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-04T21:33:11Z&lt;/p&gt;

&lt;p&gt;    Merge branch &apos;mahout-1464&apos; into mahout-1541&lt;/p&gt;

&lt;p&gt;commit 2f87f5433f90fa2c49ef386ca245943e1fc73beb&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-05T01:44:16Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1541&quot; title=&quot;Create CLI Driver for Spark Cooccurrence Analysis&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1541&quot;&gt;&lt;del&gt;MAHOUT-1541&lt;/del&gt;&lt;/a&gt; still working on this, some refactoring in the DSL for abstracting away Spark has moved access to rddsno Jira is closed yet&lt;/p&gt;

&lt;p&gt;commit c6adaa44c80bba99d41600e260bbb1ad5c972e69&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-05T16:52:23Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1464&quot; title=&quot;Cooccurrence Analysis on Spark&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1464&quot;&gt;&lt;del&gt;MAHOUT-1464&lt;/del&gt;&lt;/a&gt; import cleanup, minor changes to examples for running on Spark Cluster&lt;/p&gt;

&lt;p&gt;commit 2caceab31703ed214c1e66d5fc63b8bdb05d37a3&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-05T16:55:09Z&lt;/p&gt;

&lt;p&gt;    Merge branch &apos;mahout-1464&apos; into mahout-1541&lt;/p&gt;

&lt;p&gt;commit 6df6a54e3ff174d39bd817caf7d16c2d362be3f8&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-07T20:39:25Z&lt;/p&gt;

&lt;p&gt;    Merge branch &apos;master&apos; into mahout-1541&lt;/p&gt;

&lt;p&gt;commit a2f84dea3f32d3df3e98c61f085bc1fabd453551&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-07T21:27:06Z&lt;/p&gt;

&lt;p&gt;    drmWrap seems to be the answer to the changed DrmLike interface. Code works again but more to do.&lt;/p&gt;

&lt;p&gt;commit d3a2ba5027436d0abef67a1a5e82557064f4ba49&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-17T16:00:38Z&lt;/p&gt;

&lt;p&gt;    merged master, got new cooccurrence code&lt;/p&gt;

&lt;p&gt;commit 4b2fb07b21a8ac2d532ee51b65b27d1482293cb0&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-19T17:08:02Z&lt;/p&gt;

&lt;p&gt;    for high level review, not ready for merge&lt;/p&gt;

&lt;p&gt;commit 996ccfb82a8ed3ff90f51968e661b2449f3c4759&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-19T17:46:23Z&lt;/p&gt;

&lt;p&gt;    for high level review, not ready for merge. changed to dot notation&lt;/p&gt;

&lt;p&gt;commit f62ab071869ee205ad398a3e094d871138e11a9e&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-19T18:13:44Z&lt;/p&gt;

&lt;p&gt;    for high level review, not ready for merge. fixed a couple scaladoc refs&lt;/p&gt;

&lt;p&gt;commit cbef0ee6264c28d0597cb2507427a647771c9bcd&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-23T21:49:20Z&lt;/p&gt;

&lt;p&gt;    adding tests, had to modify some test framework Scala to make the masterUrl visible to tests&lt;/p&gt;

&lt;p&gt;commit ab8009f6176f0c21a07e15cc5cc8a9717dd7cc4c&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-25T15:41:54Z&lt;/p&gt;

&lt;p&gt;    adding more tests for ItemSimilarityDriver&lt;/p&gt;

&lt;p&gt;commit 47258f59df7f215b1bb25830d13d9b85fa8d19e9&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-25T15:44:47Z&lt;/p&gt;

&lt;p&gt;    merged master changes and fixed a pom conflict&lt;/p&gt;

&lt;p&gt;commit 9a02e2a5ea8540723c1bfc6ea01b045bb4175922&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-25T16:57:55Z&lt;/p&gt;

&lt;p&gt;    remove tmp after all tests, fixed dangling comma in input file list&lt;/p&gt;

&lt;p&gt;commit 3c343ff18600f0a0e59f5bfd63bd86db0db0e8c5&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-26T22:19:48Z&lt;/p&gt;

&lt;p&gt;    changes to pom, mahout driver script, and cleaned up help text&lt;/p&gt;

&lt;p&gt;commit 213b18dee259925de82c703451bdea640e1f068e&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-26T22:26:17Z&lt;/p&gt;

&lt;p&gt;    added a job.xml assembly for creation of an all-dependencies jar&lt;/p&gt;

&lt;p&gt;commit 627d39f30860e4ab43783c72cc2cf8926060b73c&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-27T16:44:37Z&lt;/p&gt;

&lt;p&gt;    registered HashBiMap with JavaSerializer in Kryo&lt;/p&gt;

&lt;p&gt;commit c273dc7de3c740189ce8157b334c2eef3a4c23ea&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-27T21:30:13Z&lt;/p&gt;

&lt;p&gt;    increased the default max heep for mahout/JVM to 4g, using max of 4g for Spark executor&lt;/p&gt;

&lt;p&gt;commit 9dd2f2eabf1bf64660de6b5b5e49aafe18229a7a&lt;br/&gt;
Author: Pat Ferrel &amp;lt;pat@farfetchers.com&amp;gt;&lt;br/&gt;
Date:   2014-06-30T17:06:49Z&lt;/p&gt;

&lt;p&gt;    tweaking memory requirements to process epinions with the ItemSimilarityDriver&lt;/p&gt;

&lt;p&gt;commit 6ec98f32775c791ee001fc996f475215e427f368&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-30T17:08:49Z&lt;/p&gt;

&lt;p&gt;    refactored to use a DistributedContext instead of raw SparkContext&lt;/p&gt;

&lt;p&gt;commit 48774e154a6e55e04037c787f8d64bc9e545f1bd&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-30T17:08:59Z&lt;/p&gt;

&lt;p&gt;    merging changes made on to run a large dataset through itemsimilarity on the cluster&lt;/p&gt;

&lt;p&gt;commit 8e70091a564c8464ea70bf90006d8124c3a7f208&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-30T20:11:42Z&lt;/p&gt;

&lt;p&gt;    fixed a bug, SparkConf in driver was ignored and blank one passed in to create a DistributedContext&lt;/p&gt;

&lt;p&gt;commit 01a0341f56071d2244aabd6de8c6f528ad35b164&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-30T20:33:39Z&lt;/p&gt;

&lt;p&gt;    added option for configuring Spark executor memory&lt;/p&gt;

&lt;p&gt;commit 2d9efd73def8207dded5cd1dd8699035a8cc1b34&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-30T22:37:19Z&lt;/p&gt;

&lt;p&gt;    removed some outdated examples&lt;/p&gt;

&lt;p&gt;commit 9fb281022cba7666dd26701b3d97d200b13c35f8&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-07-01T18:17:42Z&lt;/p&gt;

&lt;p&gt;    test naming and pom changed to up the jvm heap max to 512m for scalatests&lt;/p&gt;

&lt;p&gt;commit 674c9b7862f0bd0723de026eb4527546b52e8a0b&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-07-01T18:18:59Z&lt;/p&gt;

&lt;p&gt;    Merge branch &apos;mahout-1541&apos; of &lt;a href=&quot;https://github.com/pferrel/mahout&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/pferrel/mahout&lt;/a&gt; into mahout-1541&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="14085262" author="githubbot" created="Mon, 4 Aug 2014 21:44:09 +0100"  >&lt;p&gt;Github user pferrel commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/36#issuecomment-51114808&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/36#issuecomment-51114808&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Success on a cluster with cross-cooccurrence using epinions ratings and trust data.&lt;/p&gt;
</comment>
                            <comment id="14088487" author="githubbot" created="Thu, 7 Aug 2014 00:57:56 +0100"  >&lt;p&gt;Github user pferrel closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/36&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/36&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14088622" author="hudson" created="Thu, 7 Aug 2014 02:22:52 +0100"  >&lt;p&gt;SUCCESS: Integrated in Mahout-Quality #2733 (See &lt;a href=&quot;https://builds.apache.org/job/Mahout-Quality/2733/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/Mahout-Quality/2733/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1541&quot; title=&quot;Create CLI Driver for Spark Cooccurrence Analysis&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1541&quot;&gt;&lt;del&gt;MAHOUT-1541&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1568&quot; title=&quot;Build an I/O model that can replace sequence files for import/export&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1568&quot;&gt;&lt;del&gt;MAHOUT-1568&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1569&quot; title=&quot;Create CLI driver that supports Spark jobs&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1569&quot;&gt;&lt;del&gt;MAHOUT-1569&lt;/del&gt;&lt;/a&gt; refactoring the options parser and option defaults to DRY up individual driver code putting more in base classes, tightened up the test suite with a better way of comparing actual with correct (pat: rev a80974037853c5227f9e5ef1c384a1fca134746e)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;math-scala/src/main/scala/org/apache/mahout/math/cf/CooccurrenceAnalysis.scala&lt;/li&gt;
	&lt;li&gt;spark/src/main/scala/org/apache/mahout/drivers/ReaderWriter.scala&lt;/li&gt;
	&lt;li&gt;spark/src/main/scala/org/apache/mahout/sparkbindings/io/MahoutKryoRegistrator.scala&lt;/li&gt;
	&lt;li&gt;spark/src/main/scala/org/apache/mahout/drivers/MahoutOptionParser.scala&lt;/li&gt;
	&lt;li&gt;spark/src/main/scala/org/apache/mahout/drivers/IndexedDataset.scala&lt;/li&gt;
	&lt;li&gt;spark/src/main/scala/org/apache/mahout/drivers/MahoutDriver.scala&lt;/li&gt;
	&lt;li&gt;spark/src/main/scala/org/apache/mahout/cf/CooccurrenceAnalysis.scala&lt;/li&gt;
	&lt;li&gt;spark/src/main/scala/org/apache/mahout/sparkbindings/drm/CheckpointedDrmSpark.scala&lt;/li&gt;
	&lt;li&gt;spark/src/main/scala/org/apache/mahout/drivers/TextDelimitedReaderWriter.scala&lt;/li&gt;
	&lt;li&gt;spark/src/test/scala/org/apache/mahout/drivers/ItemSimilarityDriverSuite.scala&lt;/li&gt;
	&lt;li&gt;spark/src/main/scala/org/apache/mahout/drivers/ItemSimilarityDriver.scala&lt;/li&gt;
	&lt;li&gt;spark/src/main/scala/org/apache/mahout/drivers/Schema.scala&lt;/li&gt;
	&lt;li&gt;spark/src/test/scala/org/apache/mahout/cf/CooccurrenceAnalysisSuite.scala&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14121681" author="hudson" created="Thu, 4 Sep 2014 19:08:03 +0100"  >&lt;p&gt;SUCCESS: Integrated in Mahout-Quality #2779 (See &lt;a href=&quot;https://builds.apache.org/job/Mahout-Quality/2779/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/Mahout-Quality/2779/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1604&quot; title=&quot;Create a RowSimilarity for Spark&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1604&quot;&gt;&lt;del&gt;MAHOUT-1604&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1541&quot; title=&quot;Create CLI Driver for Spark Cooccurrence Analysis&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1541&quot;&gt;&lt;del&gt;MAHOUT-1541&lt;/del&gt;&lt;/a&gt; changes all reference to positon in the CLI to columns (pat: rev e24c4afb699c2930d372c701fe2de874a2a2f6c0)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;spark/src/main/scala/org/apache/mahout/drivers/Schema.scala&lt;/li&gt;
	&lt;li&gt;spark/src/test/scala/org/apache/mahout/drivers/ItemSimilarityDriverSuite.scala&lt;/li&gt;
	&lt;li&gt;spark/src/main/scala/org/apache/mahout/drivers/ItemSimilarityDriver.scala&lt;/li&gt;
	&lt;li&gt;spark/src/main/scala/org/apache/mahout/drivers/MahoutOptionParser.scala&lt;/li&gt;
	&lt;li&gt;spark/src/main/scala/org/apache/mahout/drivers/TextDelimitedReaderWriter.scala&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14367150" author="pferrel" created="Wed, 18 Mar 2015 13:56:40 +0000"  >&lt;p&gt;Seems to work.&lt;/p&gt;</comment>
                            <comment id="14492146" author="sslavic" created="Mon, 13 Apr 2015 10:56:56 +0100"  >&lt;p&gt;Bulk closing all 0.10.0 resolved issues&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Sat, 3 May 2014 06:53:44 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>390400</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hzp3nr:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>390653</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                </customfields>
    </item>
</channel>
</rss>