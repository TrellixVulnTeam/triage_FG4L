<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 04:25:52 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/MAHOUT-672/MAHOUT-672.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[MAHOUT-672] Implementation of Conjugate Gradient for solving large linear systems</title>
                <link>https://issues.apache.org/jira/browse/MAHOUT-672</link>
                <project id="12310751" key="MAHOUT">Mahout</project>
                    <description>&lt;p&gt;This patch contains an implementation of conjugate gradient, an iterative algorithm for solving large linear systems. In particular, it is well suited for large sparse systems where a traditional QR or Cholesky decomposition is infeasible. Conjugate gradient only works for matrices that are square, symmetric, and positive definite (basically the same types where Cholesky decomposition is applicable). Systems like these commonly occur in statistics and machine learning problems (e.g. regression). &lt;/p&gt;

&lt;p&gt;Both a standard (in memory) solver and a distributed hadoop-based solver (basically the standard solver run using a DistributedRowMatrix a la DistributedLanczosSolver) are included.&lt;/p&gt;

&lt;p&gt;There is already a version of this algorithm in taste package, but it doesn&apos;t operate on standard mahout matrix/vector objects, nor does it implement a distributed version. I believe this implementation will be more generically useful to the community than the specialized one in taste.&lt;/p&gt;

&lt;p&gt;This implementation solves the following types of systems:&lt;/p&gt;

&lt;p&gt;Ax = b, where A is square, symmetric, and positive definite&lt;br/&gt;
A&apos;Ax = b where A is arbitrary but A&apos;A is positive definite. Directly solving this system is more efficient than computing A&apos;A explicitly then solving.&lt;br/&gt;
(A + lambda * I)x = b and (A&apos;A + lambda * I)x = b, for systems where A or A&apos;A is singular and/or not full rank. This occurs commonly if A is large and sparse. Solving a system of this form is used, for example, in ridge regression.&lt;/p&gt;

&lt;p&gt;In addition to the normal conjugate gradient solver, this implementation also handles preconditioning, and has a sample Jacobi preconditioner included as an example. More work will be needed to build more advanced preconditioners if desired.&lt;/p&gt;
</description>
                <environment></environment>
        <key id="12504431">MAHOUT-672</key>
            <summary>Implementation of Conjugate Gradient for solving large linear systems</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="jtraupman">Jonathan Traupman</reporter>
                        <labels>
                    </labels>
                <created>Sat, 16 Apr 2011 03:31:08 +0100</created>
                <updated>Thu, 9 Feb 2012 14:01:09 +0000</updated>
                            <resolved>Tue, 25 Oct 2011 03:00:55 +0100</resolved>
                                    <version>0.5</version>
                                    <fixVersion>0.6</fixVersion>
                                    <component>Math</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                    <timeoriginalestimate seconds="172800">48h</timeoriginalestimate>
                            <timeestimate seconds="172800">48h</timeestimate>
                                        <comments>
                            <comment id="13020522" author="jtraupman" created="Sat, 16 Apr 2011 03:34:11 +0100"  >&lt;p&gt;Based on r1092853.&lt;/p&gt;

&lt;p&gt;Patch consists of 11 new files in 4 new directories under both core/ and math/. No changes to existing code.&lt;/p&gt;

&lt;p&gt;Includes 3 new unit tests. All unit tests pass.&lt;/p&gt;</comment>
                            <comment id="13020547" author="tdunning" created="Sat, 16 Apr 2011 04:32:54 +0100"  >&lt;p&gt;Jonathan,&lt;/p&gt;

&lt;p&gt;This is a cool thing.  What do you think of it in relationship to the LSMR code I have up on github as part of &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-525&quot; title=&quot;Implement LatentFactorLogLinear models&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-525&quot;&gt;&lt;del&gt;MAHOUT-525&lt;/del&gt;&lt;/a&gt; ?&lt;/p&gt;</comment>
                            <comment id="13021138" author="jtraupman" created="Mon, 18 Apr 2011 18:38:38 +0100"  >&lt;p&gt;Ted-&lt;/p&gt;

&lt;p&gt;I&apos;d have to dig a little deeper to get a full understanding of all that your doing with the SGD regression stuff. (BTW, I think you mean &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-529&quot; title=&quot;Implement LinearRegression&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-529&quot;&gt;&lt;del&gt;MAHOUT-529&lt;/del&gt;&lt;/a&gt;?) Broadly speaking, though, I&apos;d say the two patches are complementary. Conjugate gradient is just a iterative method for solving linear systems. Regression is one obvious application, but linear systems come up a lot in a whole range of algorithms, making CG a fairly general building block. &lt;/p&gt;

&lt;p&gt;As a linear system solver, the big advantage of CG over e.g. the Cholesky&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/star_yellow.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; decomposition is a) being iterative, it&apos;s very easy to adapt it to map/reduce for very large datasets and b) for matrices of the form (cI + A), where A is of rank k, CG will typically run in O(kn^2) time instead of O(n^3). CG also has a few disadvantages, namely that for full rank matrices, it requires n^3 multiplies compared to IIRC n^3/3 for Cholesky &amp;#8211; the same asymptotic performance, but that constant factor difference can add up in the real world. Another large disadvantage is that if you are solving a collection of linear systems, i.e. AX = B, where X and B are both matrices instead of vectors, you have to run a separate CG solver for each of the k columns of X for a total O(kn^3) runtime. Traditional matrix decomposition methods are usually O(n^3) to do the decomposition, but only O(n^2) to solve the system using the decomposed matrix, so you can solve a collection of k systems in O(n^3 + kn^2).&lt;/p&gt;

&lt;p&gt;As for a linear regression implementation using CG compared to one using SGD, it would be hard for me to reach any conclusions without comparing the two approaches head to head on the same data. CG would probably gain some benefit from being easily parallelizable, but the individual updates in SGD seem very fast and lightweight, so any speed advantage to CG would probably only come up for truly massive datasets. The SGD implementation in your patch also has a lot of regularization support that a simple CG implementation of LMS would lack (ridge regression i.e. L2 regularization comes for free, but L1 is considerably harder). I&apos;m also unaware of how one would do the automatic validation/hyperparameter tuning using CG that your SGD implementation does.&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/star_yellow.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; FWIW, I also have an implementation of the Cholesky decomposition, which I&apos;ve been meaning to Mahout-ize and submit when I can find the time to do it.&lt;/p&gt;</comment>
                            <comment id="13021144" author="tdunning" created="Mon, 18 Apr 2011 18:46:23 +0100"  >&lt;p&gt;Jonathan, &lt;/p&gt;

&lt;p&gt;This all sounds good.  There is a point of confusion, I think.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I&apos;d have to dig a little deeper to get a full understanding of all that your doing with the SGD regression stuff. (BTW, I think you mean &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-529&quot; title=&quot;Implement LinearRegression&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-529&quot;&gt;&lt;del&gt;MAHOUT-529&lt;/del&gt;&lt;/a&gt;?) Broadly speaking, though, I&apos;d say the two patches are complementary. Conjugate gradient is just a iterative method for solving linear systems. Regression is one obvious application, but linear systems come up a lot in a whole range of algorithms, making CG a fairly general building block.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I was talking about the LSMR implementation.  It is an iterative sparse solver similar to LSQR, but with better convergence properties.  Like your code, it requires only a forward product.  I should pull out a separate patch and attach it here.&lt;/p&gt;</comment>
                            <comment id="13021145" author="tdunning" created="Mon, 18 Apr 2011 18:50:32 +0100"  >&lt;blockquote&gt;
&lt;p&gt;As for a linear regression implementation using CG compared to one using SGD, it would be hard for me to reach any conclusions without comparing the two approaches head to head on the same data. CG would probably gain some benefit from being easily parallelizable, but the individual updates in SGD seem very fast and lightweight, so any speed advantage to CG would probably only come up for truly massive datasets. The SGD implementation in your patch also has a lot of regularization support that a simple CG implementation of LMS would lack (ridge regression i.e. L2 regularization comes for free, but L1 is considerably harder). I&apos;m also unaware of how one would do the automatic validation/hyperparameter tuning using CG that your SGD implementation does.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The other big difference, btw, is that all of our parallel approaches require at least one pass through the data.  The SGD stuff can stop early and often only needs a small fraction of the input to converge.  That gives sub-linear convergence time in terms of input size (which sounds whacky, but is real).  Any approach that needs to read the entire data set obviously can&apos;t touch that scaling factor.&lt;/p&gt;

&lt;p&gt;Offsetting this is the idea that if we don&apos;t need all the data for a given complexity of model, then we probably don&apos;t want to stop but would rather just have a more complex model.  This is where the non-parametric approaches come in.  The would give simple answers with small inputs and more nuanced answers with large data.&lt;/p&gt;</comment>
                            <comment id="13021158" author="jtraupman" created="Mon, 18 Apr 2011 19:18:17 +0100"  >&lt;p&gt;OK, yeah, I think I misunderstood which code you were talking about.&lt;/p&gt;

&lt;p&gt;I assume this is the reference for the LSMR stuff: &lt;a href=&quot;http://www.stanford.edu/group/SOL/reports/SOL-2010-2R1.pdf&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.stanford.edu/group/SOL/reports/SOL-2010-2R1.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I&apos;ll have to take some time to digest it, but based on a quick skim it looks like both LSMR and LSQR are more or less mathematically equivalent to CG applied to least squares regression, but with better convergence and numeric properties with inexact arithmetic. &lt;/p&gt;

&lt;p&gt;BTW, do you have any links to a SGD bibliography or other list of resources on it? From what I&apos;ve seen in the code and some of your comments, it looks like a cool technology that I&apos;d like to know more about.&lt;/p&gt;</comment>
                            <comment id="13021159" author="jtraupman" created="Mon, 18 Apr 2011 19:19:04 +0100"  >&lt;p&gt;Also, can you point me to the specific branch and path in your repo to the LSMR implementation? I poked around but couldn&apos;t readily find it.&lt;/p&gt;</comment>
                            <comment id="13021168" author="tdunning" created="Mon, 18 Apr 2011 19:35:45 +0100"  >&lt;p&gt;See &lt;a href=&quot;https://github.com/tdunning/LatentFactorLogLinear/tree/lsmr&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/tdunning/LatentFactorLogLinear/tree/lsmr&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As I mentioned, I will try rebasing back to trunk to get a clean patch set you can apply there.&lt;/p&gt;</comment>
                            <comment id="13022278" author="tdunning" created="Wed, 20 Apr 2011 19:30:47 +0100"  >&lt;p&gt;Jonathan, &lt;/p&gt;

&lt;p&gt;Here is the LSMR implementation in a form that can be applied to trunk.&lt;/p&gt;

&lt;p&gt;Does this help you out?&lt;/p&gt;</comment>
                            <comment id="13022355" author="tdunning" created="Wed, 20 Apr 2011 21:32:49 +0100"  >&lt;p&gt;Jonathan,&lt;/p&gt;

&lt;p&gt;What would you think about adapting this code so that we actually define some special matrix types that do&lt;br/&gt;
the A&apos;A x, (A + lambda I) x and (A&apos;A + lambda I) x products efficiently.&lt;/p&gt;

&lt;p&gt;That would allow all product only methods like LSMR, CG and in-memory random projection for SVD to work on&lt;br/&gt;
all of these cases directly without having the extra convenience methods.&lt;/p&gt;

&lt;p&gt;What do you think?&lt;/p&gt;</comment>
                            <comment id="13022361" author="jake.mannix" created="Wed, 20 Apr 2011 21:45:49 +0100"  >&lt;p&gt;DistributedRowMatrix already does A&apos;A x (it&apos;s the timesSquared() method), and I&apos;ve long thought about doing the other two as well (for easy PageRank computation).&lt;/p&gt;</comment>
                            <comment id="13022369" author="jtraupman" created="Wed, 20 Apr 2011 21:59:11 +0100"  >&lt;p&gt;I think it&apos;s a good idea to do this and would be happy to make it happen. Probably won&apos;t be before the weekend with my schedule unfortunately.&lt;/p&gt;

&lt;p&gt;The only problem the current times()/timesSquared() implementation in DistributedRowMatrix is that each algorithm that uses it needs a flag to determine whether to use times() or timesSquared(). If we created a few subclasses of DistributedRowMatrix for the A&apos;A, (A + lambda * I), etc. cases, we could have them just expose the times() method, implementing it as appropriate through calls to the superclass methods.&lt;/p&gt;</comment>
                            <comment id="13022377" author="jake.mannix" created="Wed, 20 Apr 2011 22:31:44 +0100"  >&lt;p&gt;What flag?  Doing (A&apos;A)x vs. (A)x is pretty fundamental to the algorithm.  Why would you prefer to switch choices of which to do based on class type?  I can see how it would make the logic &lt;b&gt;inside&lt;/b&gt; of something like LanczosSolver simpler (you just always do &quot;times(vector)&quot;, instead of &quot;if symmetric, do times(), else do timesSquared()&quot;), but if you &lt;b&gt;really&lt;/b&gt; want to do it generally, what you want is a mini MapReduce paradigm:  define a method which takes a pair of functions:&lt;/p&gt;

&lt;p&gt;  Vector DistributedRowMatrix#mapRowsWithCombiner(Vector input, Function&amp;lt;Vector, Vector&amp;gt; rowMapper, Function&amp;lt;Vector, Vector&amp;gt; resultReducer) &lt;/p&gt;
{

    // for each row, emit Vectors: rowMapper.apply(input, row)

    // combine/reduce: for each intermediateOutputRow, output &amp;lt;- resultReducer.apply(intermediateOutputRow, output)
    
    return output;
}

&lt;p&gt;But I&apos;m not sure this has the right level of generality (too general?  Not general enough?).&lt;/p&gt;

&lt;p&gt;I&apos;m definitely not convinced that overloading times() to mean many different things is really wise.  Having it mean (A)x vs. (A + lambda I)x could totally be fine, however.  It&apos;s defining the matrix to have a extremely compressed way of showing it&apos;s diagonal.&lt;/p&gt;</comment>
                            <comment id="13022381" author="tdunning" created="Wed, 20 Apr 2011 22:38:09 +0100"  >&lt;p&gt;I prefer a class that implements a virtual matrix. &lt;/p&gt;

&lt;p&gt;I have a patch that I will attach shortly that illustrates this.&lt;/p&gt;</comment>
                            <comment id="13022382" author="tdunning" created="Wed, 20 Apr 2011 22:40:48 +0100"  >&lt;p&gt;Here is an alternative to a switch in the solver.  We would use&lt;/p&gt;

&lt;p&gt;   s.solve(new SquaredMatrix(A, lambda), b)&lt;/p&gt;

&lt;p&gt;to solve A&apos;A + lambda I&lt;/p&gt;</comment>
                            <comment id="13022396" author="jtraupman" created="Wed, 20 Apr 2011 23:08:09 +0100"  >&lt;p&gt;Yes, I&apos;m talking about the logic inside of things like LanczosSolver. Ted pointed out that we have a number of algorithms that use matrix/vector multiply as a fundamental operation but that have special case code to handle certain common matrix forms. It&apos;s a bit redundant to have these cases in each algorithm. It also means that every time you create new special form matrix, you have to modify each of these algorithms to handle that form.&lt;/p&gt;

&lt;p&gt;I don&apos;t think we&apos;re suggesting overloading what times() means. Rather, we&apos;re suggesting having subclasses of DistributedRowMatrix (or possibly separate implementations of VectorIterable) for special form matrices whose internal representations may be done in a more efficient manner. E.g. define a &quot;SquaredDistributedMatrix&quot; class that represents a matrix of the form B = A&apos;A. All the operations, including times() mathematically mean exactly what they should: B.times&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/error.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; means Bx. Under the hood, it will be implemented as A&apos;(Ax) because it&apos;s more efficient, but that implementation detail shouldn&apos;t matter or be exposed to an algorithm that&apos;s just interested in doing a matrix/vector multiply. Likewise for (A + lambda * I) or (A&apos;A + lamdba * I) or (A&apos;A + B&apos;B) or band-diagonal matrices. The specific implementation of the times() method takes care of the representational details so that any algorithm that accepts one of these matrix types can operate on any of them. &lt;/p&gt;</comment>
                            <comment id="13022410" author="tdunning" created="Thu, 21 Apr 2011 00:07:02 +0100"  >&lt;p&gt;All,&lt;/p&gt;

&lt;p&gt;Sorry for posting a patch that is different in function and which shadows the older patch.  The latest one I posted&lt;br/&gt;
has just the virtual matrix stuff.  The earlier one that Jonathan posted has his solver code as well.&lt;/p&gt;

&lt;p&gt;So are we ready to integrate the three strands of development here (LSMR, CG and VirtualMatrix?)&lt;/p&gt;

&lt;p&gt;Or do we need to think and talk a bit more?&lt;/p&gt;</comment>
                            <comment id="13022415" author="jake.mannix" created="Thu, 21 Apr 2011 00:09:19 +0100"  >&lt;p&gt;Ok, you guys have convinced me (esp Ted&apos;s version, with the virtual matrix idea, but it&apos;s basically the same thing, because DRM just wraps an HDFS file).  We&apos;ll either need to throw UnsupportedOperationException for a lot of methods, or else define a nice simple super-interface to VectorIterable which only defines Vector times(Vector input), and maybe VectorIterable times(VectorIterable m).&lt;/p&gt;</comment>
                            <comment id="13022598" author="jake.mannix" created="Thu, 21 Apr 2011 03:31:07 +0100"  >&lt;p&gt;In fact, I&apos;ll say that I&apos;d love to have a chance to kill the oh-so-poorly-named timesSquared() method.  If we just had a virtual SquaredMatrix whose times() method was the implementation we currently have, that would be awesome.&lt;/p&gt;</comment>
                            <comment id="13023139" author="jtraupman" created="Fri, 22 Apr 2011 07:51:02 +0100"  >&lt;p&gt;I should have some time this weekend to pull all this stuff together into an integrated patch with the virtual matrix code, conj. gradient, and LSMR.&lt;/p&gt;

&lt;p&gt;A few questions, though: &lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;with the virtual matrices, do we want two classes for A + lambda * I and A&apos;A + lambda * I, or 4 classes for each of A, A&apos;A, A + lambda * I, and A&apos;A + lambda * I?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;should we make the virtual matrices subclasses of AbstractMatrix as in Ted&apos;s patch or implementations of VectorIterable like the current DistributedRowMatrix?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;what should we do with timesSquared() and the isSymmetric flag in DistributedLanczos solver? Remove it? Mark it deprecated? Leave it unchanged?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;is several separate patches for the different pieces preferred or is one big patch easier?&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13023158" author="hudson" created="Fri, 22 Apr 2011 09:47:26 +0100"  >&lt;p&gt;Integrated in Mahout-Quality #769 (See &lt;a href=&quot;https://builds.apache.org/hudson/job/Mahout-Quality/769/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/hudson/job/Mahout-Quality/769/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-672&quot; title=&quot;Implementation of Conjugate Gradient for solving large linear systems&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-672&quot;&gt;&lt;del&gt;MAHOUT-672&lt;/del&gt;&lt;/a&gt; - the forgotten files&lt;/p&gt;</comment>
                            <comment id="13023258" author="tdunning" created="Fri, 22 Apr 2011 17:06:53 +0100"  >&lt;blockquote&gt;
&lt;p&gt;with the virtual matrices, do we want two classes for A + lambda * I and A&apos;A + lambda * I, or 4 classes for each of A, A&apos;A, A + lambda * I, and A&apos;A + lambda * I?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I think 2.  It is easy enough to handle the lambda = 0 case with another constructor.  I think that A and A&apos; A are fundamentally different, however.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;should we make the virtual matrices subclasses of AbstractMatrix as in Ted&apos;s patch or implementations of VectorIterable like the current DistributedRowMatrix?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Or make two variants?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;what should we do with timesSquared() and the isSymmetric flag in DistributedLanczos solver? Remove it? Mark it deprecated? Leave it unchanged?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Deprecating it would be nice.  Jake should have a major vote.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;is several separate patches for the different pieces preferred or is one big patch easier?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;One big one is much easier to deal with.&lt;/p&gt;</comment>
                            <comment id="13023332" author="jake.mannix" created="Fri, 22 Apr 2011 19:44:52 +0100"  >&lt;p&gt;Yes, a SquaredMatrix and a PlusIdentityMultipleMatrix (? ugly name) would be enough, if composable properly.&lt;/p&gt;

&lt;p&gt;We might need two variants, sadly.  Maybe we should migrate VectorIterable to some other abstract base class (get rid of interface, for previously discussed interface/abstract class reasons), and give it a better name.  Maybe that would make it easier to &lt;b&gt;not&lt;/b&gt; have two variants?  It would be a class which just has the times(Vector) and times(Matrix) methods, and that&apos;s almost it?  (numRows/numCols too, I guess).&lt;/p&gt;

&lt;p&gt;As for LanczosSolver, please check out the patch for &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-319&quot; title=&quot;SVD solvers should be gracefully stoppable/restartable&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-319&quot;&gt;&lt;del&gt;MAHOUT-319&lt;/del&gt;&lt;/a&gt;.  The api for solve is most likely changing anyways.  And I&apos;m in favor of just &lt;b&gt;removing&lt;/b&gt; timesSquared() and isSymmetric, not marking deprecated.  Still pre-1.0-days, folks! &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13023359" author="tdunning" created="Fri, 22 Apr 2011 20:54:42 +0100"  >&lt;blockquote&gt;
&lt;p&gt;PlusIdentityMultipleMatrix&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;DiagonalOffsetMatrix?&lt;/p&gt;

&lt;p&gt;Sounds like removing the timesSquared method is moving ahead of deprecating.  I would prefer to remove it, but would defer to anybody who had an objection.&lt;/p&gt;</comment>
                            <comment id="13023364" author="jake.mannix" created="Fri, 22 Apr 2011 20:58:05 +0100"  >&lt;p&gt;I thought about calling it DiagonalOffsetMatrix, but these are a proper subset of multiples of the identity. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13023366" author="jake.mannix" created="Fri, 22 Apr 2011 20:59:31 +0100"  >&lt;p&gt;Strike that, reverse it.&lt;/p&gt;</comment>
                            <comment id="13023401" author="tdunning" created="Fri, 22 Apr 2011 21:55:02 +0100"  >&lt;p&gt;OK.&lt;/p&gt;

&lt;p&gt;IdentityOffsetMatrix?&lt;/p&gt;

&lt;p&gt;For that matter, why not call it DiagonalOffsetMatrix and just have the identity case be a special case?&lt;/p&gt;</comment>
                            <comment id="13024737" author="jtraupman" created="Mon, 25 Apr 2011 08:58:51 +0100"  >&lt;p&gt;Got a bit of this done this evening, but ran into a roadblock: I had to separate out the new VirtualMatrix interface from VectorIterable. VirtualMatrix contains the times() method while VectorIterable keeps the iterator() and iterateAll() methods. I had to do this because there&apos;s no efficient way to iterate the rows of a squared A&apos;A matrix without actually constructing the full product matrix.&lt;/p&gt;

&lt;p&gt;However, when I started looking at porting the Lanczos solver to use the new VirtualMatrix type, the core algorithm translates fine but the getInitialVector() routine, which relies on an iteration through the data matrix, presents difficulties.&lt;/p&gt;

&lt;p&gt;The easiest path through this impasse that I can see would be defining the new DistributedSquaredMatrix (which implements A&apos;A) to also implement VectorIterable, but have it iterate over its underlying source matrix A, rather than the product matrix. This would preserve the current behavior of Lanczos solver albeit at the expense of an iterator on DistributedSquaredMatrix that doesn&apos;t make a great deal of sense in a more general context. This solution would probably not work for the diagonal offset case, because it&apos;s unclear how to transform the iterated rows using the offset. We could always define a &quot;IterableVirtualMatrix&quot; interface that extends both VectorIterable and VirtualMatrix for algorithms, like Lanczos, that require it, though I&apos;m still bothered by the weird iterator semantics.&lt;/p&gt;

&lt;p&gt;The other possible solution I considered would be to add the times(VirtualMatrix m) method to the VirtualMatrix interface, then rewriting the getInitialVector() routine in terms of fundamental matrix operations: for the symmetric case, scaleFactor looks to be trace(A*A) and the initial vector looks to be A times a vector of all ones. The asymmetric case is mathematically different, but I don&apos;t know enough about Lanczos to fully understand why. Unfortunately, implementing matrix multiplication with virtual matrices may be hard, or at the very least computationally expensive. &lt;/p&gt;

&lt;p&gt;Thoughts?&lt;/p&gt;</comment>
                            <comment id="13024846" author="jake.mannix" created="Mon, 25 Apr 2011 18:06:07 +0100"  >&lt;p&gt;I think we need to kill VectorIterable, and replace it with something like &quot;LinearOperator&quot;, which just has:&lt;/p&gt;

&lt;p&gt;Vector times(Vector)&lt;br/&gt;
LinearOperator times(LinearOperator)&lt;br/&gt;
LinearOperator transpose()&lt;br/&gt;
int domainDimension() // ie numCols&lt;br/&gt;
int rangeDimension() // ie numRows&lt;/p&gt;

&lt;p&gt;and no iterator methods.&lt;/p&gt;

&lt;p&gt;getInitialVector() doesn&apos;t need to be implemented the way it is.  LanczosSolver uses the iterator to calculate a good starting vector, but it doesn&apos;t need to: DistributedLanczosSolver just takes the vector of all 1&apos;s (normalized), and that works great in practice.  Let&apos;s just change the behavior of LanczosSolver to do this as well, skipping on the iteration.&lt;/p&gt;

&lt;p&gt;Before you get too involved with this refactoring on trunk, Jonathan, you should be careful: as I mentioned above, you&apos;re likely going to conflict with my changes for &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-319&quot; title=&quot;SVD solvers should be gracefully stoppable/restartable&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-319&quot;&gt;&lt;del&gt;MAHOUT-319&lt;/del&gt;&lt;/a&gt;.  They&apos;re API changes to LanczosSolver&apos;s core solve() method.&lt;/p&gt;</comment>
                            <comment id="13024889" author="jake.mannix" created="Mon, 25 Apr 2011 19:01:46 +0100"  >&lt;p&gt;For on-the-fly collaboration, I&apos;ve cloned apache/mahout.git on GitHub, and applied my &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-319&quot; title=&quot;SVD solvers should be gracefully stoppable/restartable&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-319&quot;&gt;&lt;del&gt;MAHOUT-319&lt;/del&gt;&lt;/a&gt; patch to it, and will be continuing on there throughout the week.  Clone me there and we can avoid collisions.&lt;/p&gt;</comment>
                            <comment id="13024892" author="jtraupman" created="Mon, 25 Apr 2011 19:08:51 +0100"  >&lt;p&gt;OK, sounds good. The VirtualMatrix stuff I&apos;ve written so far looks a lot like the signature for LinearOperator you described. I can rename it easily enough, and &quot;LinearOperator&quot; has a good ring to it.&lt;/p&gt;

&lt;p&gt;I looked over the mahout-319 patch and I don&apos;t think the conflicts will be too bad. Mostly it will just be replacing VectorIterable -&amp;gt; LinearOperator in a bunch of places. I&apos;ll clone your github repo to do the work off that. If mahout-319 is going out soon, we should probably just back this work up behind it, since I don&apos;t think there&apos;s any urgency to it.&lt;/p&gt;

&lt;p&gt;I&apos;ll make the changes to getInitialVector() as you suggest, that should be easy. &lt;/p&gt;</comment>
                            <comment id="13030461" author="jtraupman" created="Sun, 8 May 2011 08:28:54 +0100"  >&lt;p&gt;OK. Here is a patch combining all the stuff that we&apos;ve been talking about with this issue:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;LinearOperators and refactor of various algorithms to use LinearOperators instead of VectorIterables&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Conjugate gradient solver from the original patch&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Ted&apos;s LSMR implementation, refactored to use LinearOperators&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The patch is from git diff, so you&apos;ll need to use &quot;patch -p1&quot; to apply it to trunk in svn.&lt;/p&gt;

&lt;p&gt;I&apos;ve tested that it applies successfully to a copy of trunk checked out from SVN. Everything compiles and tests all pass.&lt;/p&gt;</comment>
                            <comment id="13030466" author="tdunning" created="Sun, 8 May 2011 09:23:21 +0100"  >&lt;p&gt;Jonathan,&lt;/p&gt;

&lt;p&gt;On Mahout, we don&apos;t have the magic application of patches that Hadoop and Zookeeper have.  If we did, then using git diff --no-index would prevent the need for -p1 and would make the scripts work.  As I mentioned, this doesn&apos;t matter here, but is very nice on those other projects.&lt;/p&gt;</comment>
                            <comment id="13030619" author="jtraupman" created="Mon, 9 May 2011 06:57:44 +0100"  >&lt;p&gt;Canceling the old stuff uploaded to this issue.&lt;/p&gt;</comment>
                            <comment id="13030620" author="jtraupman" created="Mon, 9 May 2011 06:59:06 +0100"  >&lt;p&gt;OK, here&apos;s a combined patch in SVN format. Applied it to a clean trunk w/o problems. All tests pass.&lt;/p&gt;</comment>
                            <comment id="13030621" author="jtraupman" created="Mon, 9 May 2011 06:59:34 +0100"  >&lt;p&gt;Use the &quot;mahout-672-combined.patch&quot; attachment and ignore the rest.&lt;/p&gt;</comment>
                            <comment id="13069297" author="jtraupman" created="Fri, 22 Jul 2011 00:53:18 +0100"  >&lt;p&gt;Sorry I haven&apos;t updated this issue in a while &amp;#8211; got very busy with work, etc.&lt;/p&gt;

&lt;p&gt;Anyway, this is a new patch based against the latest trunk. All the unit tests pass and it should be ready to go with the LSMR, conjugate gradient, and linear operator stuff.&lt;/p&gt;</comment>
                            <comment id="13069984" author="srowen" created="Sat, 23 Jul 2011 16:36:57 +0100"  >&lt;p&gt;This is a big patch and I&apos;m not qualified to review it. But I noticed a few small issues. Look at DistributedRowMatrix for instance &amp;#8211; these changes should not be applied. There are also some spurious whitespace and import changes.&lt;/p&gt;

&lt;p&gt;It&apos;s also a pretty big patch. Are there more opportunities for reuse? thinking of AbstractLinearOperator for instance.&lt;/p&gt;</comment>
                            <comment id="13069994" author="jtraupman" created="Sat, 23 Jul 2011 17:17:07 +0100"  >&lt;p&gt;Yes, it is a big a patch...wasn&apos;t always this way. Here&apos;s a brief summary of this came to be the monster it is:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;originally, this was just an implementation of a conjugate gradient solver for linear systems that worked with either normal or distributed matrices.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Ted mentioned that he had some mostly completed LSMR code that did very similar stuff and asked if I could integrate it with this patch, which I did.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;a long discussion between Ted, Jake and me ensued about how a lot of algorithms (e.g. CG, LSMR, Lanczos SVD) all used the same concept of a matrix that could be multiplied by a vector but that didn&apos;t need row-wise or element-wise access to the data in the matrix. After much back and forth, we settled on the name &quot;LinearOperator.&quot;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The LinearOperator stuff is disruptive to the code base, but it does provide some nice functionality, too. For example, you can implement things like preconditioners, diagonal offsets (e.g. ridge regression) and other transformations to the data efficiently using linear operators without needing to either actually modify your underlying data set or add the functionality to the specific algorithm you&apos;re using. This was the original motivation behind it, since I had included a diagonal offset option for low-rank matrices in my CG code but that wasn&apos;t in Ted&apos;s LSMR implementation. We decided that it might be better to put this in some common place that all similar algorithms could use for free. Since all matrices are linear operators, but the converse isn&apos;t true, it was decided that Matrix should be a subclass of LinearOperator, not the other way around.&lt;/p&gt;

&lt;p&gt;One thing I&apos;m not 100% comfortable with is the parallel interface and class hierarchies (i.e. LinearOperator, Matrix vs. AbstractLinearOperator, AbstractMatrix). I&apos;d like to see the interfaces go away in favor of abstract classes, but I don&apos;t recall us reaching any consensus on this.&lt;/p&gt;

&lt;p&gt;I have some time to work on this stuff now (after a long busy spell), so if you can send me specific issues (e.g. the DistributedRowMatrix stuff you mentioned) I&apos;ll try to take a look at it.&lt;/p&gt;</comment>
                            <comment id="13070080" author="lancenorskog" created="Sun, 24 Jul 2011 01:31:06 +0100"  >&lt;p&gt;May I suggest that a redo of Matrices include a solution to the double-dispatch problem?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Double_dispatch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;Double Dispatch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In this case, there are many variations of the exact code to apply for Matrix :: Vector operations, and way too many uses of &lt;b&gt;instanceof&lt;/b&gt;.&lt;/p&gt;

&lt;p&gt;Also, the LinearOperator suite is big enough to be its own patch.&lt;/p&gt;

</comment>
                            <comment id="13070324" author="jtraupman" created="Mon, 25 Jul 2011 07:04:22 +0100"  >&lt;p&gt;I removed all the matrix/vector changes and linear operator stuff from this patch, so this code just implements the conjugate gradient and LSMR solvers using the 0.5 standard linear algebra stuff.&lt;/p&gt;

&lt;p&gt;I&apos;ll create a new issue for the linear operators and other linear algebra refactoring. I&apos;m not sure when I&apos;ll have the time to work on it, but I&apos;ll try to implement the suggestions made here.&lt;/p&gt;

&lt;p&gt;Since I&apos;d like to get the linear operator stuff out sooner rather than later, I did not add the code for the A&apos;A and (A + kI) cases back to the CG implementation. So for now, the CG solver will only work for symmetric pos. def. matrices.&lt;/p&gt;</comment>
                            <comment id="13131505" author="srowen" created="Thu, 20 Oct 2011 11:18:58 +0100"  >&lt;p&gt;Folks &amp;#8211; what&apos;s the status on this? It&apos;s been sitting about for a few months. Is this going to go in for 0.6 or should we retire it?&lt;/p&gt;</comment>
                            <comment id="13131731" author="jtraupman" created="Thu, 20 Oct 2011 17:24:17 +0100"  >&lt;p&gt;The currently attached patch was good to go when I uploaded it a few months back. I can verify this weekend that it still applies cleanly. I took out all the linear operator stuff per request, but the current patch has working implementations of CG and LSMR for symmetric positive definite matrices.&lt;/p&gt;

&lt;p&gt;The ability to operate on other matrices (e.g. of the form A&apos;A) is no longer in this patch/issue because it relies on the linear operator stuff, but there is no reason to wait for that to get this core functionality. &lt;/p&gt;</comment>
                            <comment id="13132817" author="dlyubimov" created="Fri, 21 Oct 2011 17:48:43 +0100"  >&lt;p&gt;Jonathan,&lt;/p&gt;

&lt;p&gt;Thank you for this work. It would be very interesting to have solver like this.&lt;br/&gt;
One question I have is, for a mapreduce version, how many Map reduce steps this would require? And secondly, does it use any interestingly sized side information in mapppers or reducers?&lt;/p&gt;

&lt;p&gt;I skimmed the patch and it mentions it may take up to numcols iterations assuming fixed lambda &amp;#8211; how do these iterations map into mapreduce steps?&lt;/p&gt;

&lt;p&gt;Thank you.&lt;/p&gt;</comment>
                            <comment id="13133841" author="jtraupman" created="Mon, 24 Oct 2011 07:04:25 +0100"  >&lt;p&gt;Here is an updated patch that applies to the current trunk. It compiles and all tests pass. &lt;/p&gt;

&lt;p&gt;The only change from the July 25 patch is to remove a call to the now removed Vector.addTo() method.&lt;/p&gt;</comment>
                            <comment id="13133849" author="jtraupman" created="Mon, 24 Oct 2011 07:14:31 +0100"  >&lt;p&gt;Dmitriy- &lt;/p&gt;

&lt;p&gt;Basically, at the core of the CG solver is a matrix/vector multiply, so we get the map/reduce implementation by using a DistributedRowMatrix instead of an in-memory matrix. Since we do one matrix/vector multiply per iteration, it will require one map/reduce job per iteration, which somewhat limits its performance &amp;#8211; there&apos;s a large range of data sizes that could benefit from distributed computation but that get bogged down by Hadoop&apos;s slow job setup/teardown. Essentially, we&apos;re looking at many of the same tradeoffs we have with the distributed Lanczos decomposition stuff.&lt;/p&gt;

&lt;p&gt;The mappers and reducers do not use much memory for side storage. I&apos;m not super familiar with the guts of DistributedRowMatrix, but I believe the only side information it needs is the vector it is multiplying the matrix by, so a couple of MB max.&lt;/p&gt;

&lt;p&gt;If Mahout ever adopts something like Giraph, we can probably make a lot of these distributed iterative linear algebra algorithms a lot more efficient. &lt;/p&gt;</comment>
                            <comment id="13133862" author="tdunning" created="Mon, 24 Oct 2011 08:02:47 +0100"  >&lt;p&gt;Jonathan,&lt;/p&gt;

&lt;p&gt;Is it possible to multiply by many vectors at once to accelerate the convergence here by essentially exploring in multiple directions at once?&lt;/p&gt;
</comment>
                            <comment id="13134246" author="jake.mannix" created="Mon, 24 Oct 2011 18:05:51 +0100"  >&lt;p&gt;Hey Jonathan,&lt;/p&gt;

&lt;p&gt;  I gather you replaced a.addTo(b) with b.assign(a, Functions.PLUS)?  If so, then all will be well.&lt;/p&gt;</comment>
                            <comment id="13134267" author="jtraupman" created="Mon, 24 Oct 2011 18:27:53 +0100"  >&lt;p&gt;Reply to Ted&apos;s and Jake&apos;s comments:&lt;/p&gt;

&lt;p&gt;&amp;gt; Is it possible to multiply by many vectors at once to accelerate the convergence here by essentially exploring in multiple directions at once?&lt;/p&gt;

&lt;p&gt;This might be possible, but I don&apos;t think it&apos;s just an easy tweak. At iteration i, we compute the conjugate gradient, then move up it to a local min and repeat. We don&apos;t know the direction we&apos;re going to go at i+1 until after we&apos;ve finished iteration i. To do what you suggest would mean moving along a vector that&apos;s different than the gradient, trying to collapse multiple CG steps into one. I&apos;d imagine there&apos;s some literature on this (if not, might be a fruitful avenue of research). &lt;/p&gt;

&lt;p&gt;However, my gut reaction is 1) this won&apos;t just be a simple modification to this algorithm and 2) the technique used to approximate the gradient at i+1 while still on iteration i might involve operations that will make a distributed version more difficult/impossible. E.g. I&apos;ve seen that the LSMR solver often converges in fewer steps than the CG one, but it&apos;s doing enough additional stuff with the data matrix that creating a map/reduce version would be a lot more work than just using a DistributedRowMatrix.&lt;/p&gt;

&lt;p&gt;&amp;gt; I gather you replaced a.addTo(b) with b.assign(a, Functions.PLUS)? If so, then all will be well.&lt;/p&gt;

&lt;p&gt;Yes, that&apos;s all I did.&lt;/p&gt;

&lt;p&gt;Anyway, I&apos;d really like to reach some closure on this issue. These two algorithms aren&apos;t the end-all be-all of linear system solvers, but I think they&apos;re useful in their current form and can be a foundation for further work.&lt;/p&gt;</comment>
                            <comment id="13134273" author="jake.mannix" created="Mon, 24 Oct 2011 18:34:30 +0100"  >&lt;p&gt;&amp;gt; Anyway, I&apos;d really like to reach some closure on this issue. These two algorithms aren&apos;t the end-all be-all of linear system solvers, but I think they&apos;re useful in their current form and can be a foundation for further work.&lt;/p&gt;

&lt;p&gt;I&apos;ll try out the patch now!&lt;/p&gt;</comment>
                            <comment id="13134311" author="tdunning" created="Mon, 24 Oct 2011 19:09:15 +0100"  >&lt;p&gt;Regarding doing many multiplications at once, I did some of the math just now and it looks like you can build a solver that does this sort of thing, but the resulting algorithm really begins to look more like the stochastic projection for SVD than like CG.&lt;/p&gt;

&lt;p&gt;Let&apos;s get this in place.&lt;/p&gt;</comment>
                            <comment id="13134327" author="dlyubimov" created="Mon, 24 Oct 2011 19:26:26 +0100"  >&lt;blockquote&gt;&lt;p&gt;Basically, at the core of the CG solver is a matrix/vector multiply, so we get the map/reduce implementation by using a DistributedRowMatrix instead of an in-memory matrix. Since we do one matrix/vector multiply per iteration, it will require one map/reduce job per iteration, which somewhat limits its performance &#8211; there&apos;s a large range of data sizes that could benefit from distributed computation but that get bogged down by Hadoop&apos;s slow job setup/teardown. Essentially, we&apos;re looking at many of the same tradeoffs we have with the distributed Lanczos decomposition stuff.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Thank you, Jonathan.&lt;/p&gt;

&lt;p&gt;Yeah, so i figured. that&apos;s my concern. That&apos;s the Achilles&apos; heel of much of distributed stuff in Mahout. I.e. space of iterations (I feel) must be very close to O(1), otherwise it severely affects stuff. Even using side information is not that painful it seems compared to iteration growth. That severely decreases pragmatical use. &lt;/p&gt;

&lt;p&gt;My thinking is that we need to keep algorithms we recommend accountable to some standard. My understanding is that there&apos;s similar problem with ALS WR implementation right now. I.e. we can have it in the codebase but Sebastien stops short of recommending it to folks on the list.&lt;/p&gt;

&lt;p&gt;That&apos;s kind of the problem i touched recently : Mahout stuff is different in a sense that it requires deeper investigation of best parallelization strategy than just let&apos;s throw-it-at-it approach. Even with matrix multiplications ther&apos;re notions that decrease computation time tenfold compared to DRM approach for some cases which are less than general but in practice are suprisingly common (example of such are multiplication steps in SSVD). Hadoop sorting is not as inexpensive as its pitch may suggest. And tenfold is sort of interesting...it&apos;s a difference between 10 hours and 1 hour. &lt;/p&gt;

&lt;p&gt;Anyway. I am ok with committing this with @Maturity(Experimental) until we confirm running time on some input. I will probably even have to check this out soon, i may have a use case for it soon.&lt;/p&gt;</comment>
                            <comment id="13134347" author="jake.mannix" created="Mon, 24 Oct 2011 19:45:19 +0100"  >&lt;p&gt;&amp;gt; That&apos;s the Achilles&apos; heel of much of distributed stuff in Mahout. I.e. space of iterations (I feel) must be very close to O(1), otherwise it severely affects stuff. Even using side information is not that painful it seems compared to iteration growth. That severely decreases pragmatical use.&lt;/p&gt;

&lt;p&gt;I think it&apos;s a bit extreme to say we need to have nearly O(1) Map-reduce passes to be useful.  Lots of iterative stuff requires quite a few passes before convergence (as you say: Lanczos and LDA both fall into this realm), yet it&apos;s just the price you have to pay sometimes.&lt;/p&gt;

&lt;p&gt;This may be similar.&lt;/p&gt;

&lt;p&gt;Jonathan, what size inputs have you run this on, with what running time in comparison to the other algorithms we have?  From what I can see, this looks good to commit as well.&lt;/p&gt;</comment>
                            <comment id="13134388" author="jtraupman" created="Mon, 24 Oct 2011 20:28:12 +0100"  >&lt;p&gt;&amp;gt; Jonathan, what size inputs have you run this on, with what running time in comparison to the other algorithms we have? From what I can see, this looks good to commit as well.&lt;/p&gt;

&lt;p&gt;The largest dataset I&apos;ve run it on was a synthetic set of about the same size as the old distributed Lanczos test matrix (before it was made smaller to speed up the tests). I ended up using a smaller matrix in the test because it was taking too long to run. I haven&apos;t tested it on truly huge matrices, but I can do that at some point if people are interested.&lt;/p&gt;

&lt;p&gt;As for comparisons to other algorithms, both CG and LSMR ran faster than the QR decomp/solve that&apos;s already in Mahout on most of the test inputs I was playing with. They also ran faster than a Cholesky decomp/solve that I had implemented but haven&apos;t submitted. I don&apos;t have numbers on me right now, though. &lt;/p&gt;

&lt;p&gt;Between the two, LSMR often converges faster than CG, and when it does, it&apos;s faster. For problems where they both take the same number of steps, performance is about the same, with CG sometimes being a bit quicker since it does less computation per iteration. The big advantage of the CG implementation is that it can use m/r, so should be scalable to larger matrices.&lt;/p&gt;</comment>
                            <comment id="13134419" author="jake.mannix" created="Mon, 24 Oct 2011 20:54:54 +0100"  >&lt;p&gt;I&apos;ve run the test suite and all tests are passing on my box too, so I&apos;m going to commit this later today if nobody objects.&lt;/p&gt;</comment>
                            <comment id="13134503" author="dlyubimov" created="Mon, 24 Oct 2011 22:20:14 +0100"  >&lt;blockquote&gt;&lt;p&gt;I think it&apos;s a bit extreme to say we need to have nearly O(1)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;These may mean quite different things in practice. the devil is in the details. by ~O(1) i meant ok, if in practice it grows so slow that it&apos;s enought to process several billion rows (m) of input by having 40 iterations, that&apos;s perhaps still &apos;nearly&apos; O(1) in my definiton.&lt;/p&gt;

&lt;p&gt;I just said that i seem to have gleaned in the javadoc explanation that with this patch &lt;br/&gt;
num of iterations ~ n (num of columns) so if for million columns it means a million iterations, that&apos;s probably not cool. On the other side, conversion will unlikely require a million. Then what? I kind of still did not get a clear clarifications on the estimate of num iterations (except that it is not exactly O(1)).&lt;/p&gt;</comment>
                            <comment id="13134698" author="jake.mannix" created="Tue, 25 Oct 2011 03:00:26 +0100"  >&lt;p&gt;I agree that having things that scale &lt;b&gt;not&lt;/b&gt; as numDocs or numFeatures is pretty critical, if we&apos;re talking about map-reduce passes.  This code doesn&apos;t fall into that trap, from what I can see.&lt;/p&gt;

&lt;p&gt;As I&apos;ve heard no objections, I committed this (revision 1188491).  Thanks Jonathan!&lt;/p&gt;</comment>
                            <comment id="13134699" author="jake.mannix" created="Tue, 25 Oct 2011 03:00:55 +0100"  >&lt;p&gt;committed at svn revision 1188491&lt;/p&gt;</comment>
                            <comment id="13135034" author="hudson" created="Tue, 25 Oct 2011 14:18:19 +0100"  >&lt;p&gt;Integrated in Mahout-Quality #1118 (See &lt;a href=&quot;https://builds.apache.org/job/Mahout-Quality/1118/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/Mahout-Quality/1118/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-672&quot; title=&quot;Implementation of Conjugate Gradient for solving large linear systems&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-672&quot;&gt;&lt;del&gt;MAHOUT-672&lt;/del&gt;&lt;/a&gt; on behalf of jtraupman&lt;/p&gt;

&lt;p&gt;jmannix : &lt;a href=&quot;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1188491&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1188491&lt;/a&gt;&lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/solver&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/solver/DistributedConjugateGradientSolver.java&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/solver&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/solver/TestDistributedConjugateGradientSolver.java&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/solver/TestDistributedConjugateGradientSolverCLI.java&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/math/src/main/java/org/apache/mahout/math/solver&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/math/src/main/java/org/apache/mahout/math/solver/ConjugateGradientSolver.java&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/math/src/main/java/org/apache/mahout/math/solver/JacobiConditioner.java&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/math/src/main/java/org/apache/mahout/math/solver/LSMR.java&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/math/src/main/java/org/apache/mahout/math/solver/Preconditioner.java&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/math/src/test/java/org/apache/mahout/math/solver&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/math/src/test/java/org/apache/mahout/math/solver/LSMRTest.java&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/math/src/test/java/org/apache/mahout/math/solver/TestConjugateGradientSolver.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                            <outwardlinks description="blocks">
                                        <issuelink>
            <issuekey id="12473641">MAHOUT-499</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12515218">MAHOUT-772</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12477079" name="0001-MAHOUT-672-LSMR-iterative-linear-solver.patch" size="24485" author="tdunning" created="Fri, 22 Apr 2011 06:03:39 +0100"/>
                            <attachment id="12476928" name="0001-MAHOUT-672-LSMR-iterative-linear-solver.patch" size="24485" author="tdunning" created="Wed, 20 Apr 2011 19:30:47 +0100"/>
                            <attachment id="12476940" name="MAHOUT-672.patch" size="3082" author="tdunning" created="Wed, 20 Apr 2011 22:40:48 +0100"/>
                            <attachment id="12500404" name="mahout-672-111023.patch" size="59011" author="jtraupman" created="Mon, 24 Oct 2011 07:04:25 +0100"/>
                            <attachment id="12487670" name="mahout-672.patch" size="59001" author="jtraupman" created="Mon, 25 Jul 2011 07:04:22 +0100"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>5.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Sat, 16 Apr 2011 03:32:54 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9390</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hxy3ov:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>22749</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                </customfields>
    </item>
</channel>
</rss>