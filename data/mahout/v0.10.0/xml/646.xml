<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 04:24:35 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/MAHOUT-646/MAHOUT-646.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[MAHOUT-646] Cannot run Wikipedia example on Amazon Elastic MapReduce (EMR)</title>
                <link>https://issues.apache.org/jira/browse/MAHOUT-646</link>
                <project id="12310751" key="MAHOUT">Mahout</project>
                    <description>&lt;p&gt;When I tried to run the Wikipedia example on EMR with all the categories existing in the Wikipedia dump, I got this error :&lt;/p&gt;

&lt;p&gt;org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException: failed to create file /yatter.tagger/wikipedia/input/&lt;em&gt;temporary/_attempt&lt;/em&gt;_0000_r_000000_0/part-r-00000 for DFSClient_attempt_201103292134_0010_r_000000_0 on client 10.240.10.157 because current leaseholder is trying to recreate file.&lt;br/&gt;
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1045)&lt;br/&gt;
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:981)&lt;br/&gt;
    at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:377)&lt;br/&gt;
    at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)&lt;/p&gt;

&lt;p&gt;    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)&lt;br/&gt;
    at java.lang.reflect.Method.invoke(Method.java:597)&lt;br/&gt;
    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)&lt;br/&gt;
    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:961)&lt;br/&gt;
    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:957)&lt;br/&gt;
    at java.security.AccessController.doPrivileged(Native Method)&lt;br/&gt;
    at javax.security.auth.Subject.doAs(Subject.java:396)&lt;br/&gt;
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:955)&lt;/p&gt;

&lt;p&gt;    at org.apache.hadoop.ipc.Client.call(Client.java:740)&lt;br/&gt;
    at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)&lt;br/&gt;
    at $Proxy1.create(Unknown Source)&lt;/p&gt;

&lt;p&gt;    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)&lt;br/&gt;
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)&lt;br/&gt;
    at java.lang.reflect.Method.invoke(Method.java:597)&lt;br/&gt;
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)&lt;br/&gt;
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)&lt;br/&gt;
    at $Proxy1.create(Unknown Source)&lt;br/&gt;
    at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.&amp;lt;init&amp;gt;(DFSClient.java:2709)&lt;br/&gt;
    at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:491)&lt;br/&gt;
    at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:195)&lt;br/&gt;
    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:524)&lt;br/&gt;
    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:505)&lt;br/&gt;
    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:412)&lt;br/&gt;
    at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:128)&lt;br/&gt;
    at org.apache.mahout.classifier.bayes.MultipleTextOutputFormat.getBaseRecordWriter(MultipleTextOutputFormat.java:41)&lt;br/&gt;
    at org.apache.mahout.classifier.bayes.MultipleOutputFormat$1.write(MultipleOutputFormat.java:81)&lt;br/&gt;
    at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:517)&lt;br/&gt;
    at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)&lt;br/&gt;
    at org.apache.mahout.classifier.bayes.WikipediaDatasetCreatorReducer.reduce(WikipediaDatasetCreatorReducer.java:35)&lt;br/&gt;
    at org.apache.mahout.classifier.bayes.WikipediaDatasetCreatorReducer.reduce(WikipediaDatasetCreatorReducer.java:28)&lt;br/&gt;
    at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)&lt;br/&gt;
    at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:575)&lt;br/&gt;
    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:412)&lt;br/&gt;
    at org.apache.hadoop.mapred.Child.main(Child.java:170)&lt;/p&gt;

&lt;p&gt;org.apache.hadoop.ipc.RemoteException: java.io.IOException: failed to create file /yatter.tagger/wikipedia/input/&lt;em&gt;temporary/_attempt&lt;/em&gt;_0000_r_000000_0/part-r-00000 on client 10.240.10.157 either because the filename is invalid or the file exists&lt;br/&gt;
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1092)&lt;br/&gt;
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:981)&lt;br/&gt;
    at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:377)&lt;br/&gt;
    at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)&lt;/p&gt;

&lt;p&gt;    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)&lt;br/&gt;
    at java.lang.reflect.Method.invoke(Method.java:597)&lt;br/&gt;
    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)&lt;br/&gt;
    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:961)&lt;br/&gt;
    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:957)&lt;br/&gt;
    at java.security.AccessController.doPrivileged(Native Method)&lt;br/&gt;
    at javax.security.auth.Subject.doAs(Subject.java:396)&lt;br/&gt;
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:955)&lt;/p&gt;

&lt;p&gt;    at org.apache.hadoop.ipc.Client.call(Client.java:740)&lt;br/&gt;
    at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)&lt;br/&gt;
    at $Proxy1.create(Unknown Source)&lt;/p&gt;

&lt;p&gt;    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)&lt;br/&gt;
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)&lt;br/&gt;
    at java.lang.reflect.Method.invoke(Method.java:597)&lt;br/&gt;
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)&lt;br/&gt;
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)&lt;br/&gt;
    at $Proxy1.create(Unknown Source)&lt;br/&gt;
    at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.&amp;lt;init&amp;gt;(DFSClient.java:2709)&lt;br/&gt;
    at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:491)&lt;br/&gt;
    at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:195)&lt;br/&gt;
    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:524)&lt;br/&gt;
    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:505)&lt;br/&gt;
    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:412)&lt;br/&gt;
    at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:128)&lt;br/&gt;
    at org.apache.mahout.classifier.bayes.MultipleTextOutputFormat.getBaseRecordWriter(MultipleTextOutputFormat.java:41)&lt;br/&gt;
    at org.apache.mahout.classifier.bayes.MultipleOutputFormat$1.write(MultipleOutputFormat.java:81)&lt;br/&gt;
    at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:517)&lt;br/&gt;
    at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)&lt;br/&gt;
    at org.apache.mahout.classifier.bayes.WikipediaDatasetCreatorReducer.reduce(WikipediaDatasetCreatorReducer.java:35)&lt;br/&gt;
    at org.apache.mahout.classifier.bayes.WikipediaDatasetCreatorReducer.reduce(WikipediaDatasetCreatorReducer.java:28)&lt;br/&gt;
    at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)&lt;br/&gt;
    at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:575)&lt;br/&gt;
    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:412)&lt;br/&gt;
    at org.apache.hadoop.mapred.Child.main(Child.java:170)&lt;/p&gt;

&lt;p&gt;    4 more :&lt;br/&gt;
   org.apache.hadoop.ipc.RemoteException: java.io.IOException: failed to create file /yatter.tagger/wikipedia/input/&lt;em&gt;temporary/_attempt&lt;/em&gt;_0000_r_000000_0/part-r-00000 on client 10.240.10.157 either because the filename is invalid or the file exists&lt;/p&gt;</description>
                <environment></environment>
        <key id="12503056">MAHOUT-646</key>
            <summary>Cannot run Wikipedia example on Amazon Elastic MapReduce (EMR)</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="srowen">Sean Owen</assignee>
                                    <reporter username="vivrass">Martin Provencher</reporter>
                        <labels>
                    </labels>
                <created>Thu, 31 Mar 2011 19:18:37 +0100</created>
                <updated>Sat, 21 May 2011 04:18:46 +0100</updated>
                            <resolved>Sun, 3 Apr 2011 21:40:39 +0100</resolved>
                                    <version>0.5</version>
                                    <fixVersion>0.5</fixVersion>
                                    <component>Classification</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="13014115" author="vivrass" created="Thu, 31 Mar 2011 19:20:12 +0100"  >&lt;p&gt;To fix it, I&apos;ve applied this patch :&lt;/p&gt;

&lt;p&gt;===================================================================&lt;br/&gt;
&amp;#8212; examples/src/main/java/org/apache/mahout/classifier/bayes/WikipediaDatasetCreatorDriver.java	(revision 1087334)&lt;br/&gt;
+++ examples/src/main/java/org/apache/mahout/classifier/bayes/WikipediaDatasetCreatorDriver.java	(working copy)&lt;br/&gt;
@@ -185,7 +185,7 @@&lt;br/&gt;
     //TODO: job.setNumMapTasks(100);&lt;br/&gt;
     job.setInputFormatClass(XmlInputFormat.class);&lt;br/&gt;
     job.setReducerClass(WikipediaDatasetCreatorReducer.class);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;job.setOutputFormatClass(WikipediaDatasetCreatorOutputFormat.class);&lt;br/&gt;
+    //job.setOutputFormatClass(WikipediaDatasetCreatorOutputFormat.class);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     FileInputFormat.setInputPaths(job, new Path(input));&lt;br/&gt;
     Path outPath = new Path(output);&lt;/p&gt;</comment>
                            <comment id="13014144" author="srowen" created="Thu, 31 Mar 2011 20:03:35 +0100"  >&lt;p&gt;OK. Commenting out the line means the output format is not being used at all. Is that valid? (I don&apos;t know.)&lt;br/&gt;
But in any event there seems to be an actual problem, here:&lt;/p&gt;

&lt;p&gt;{{...&lt;br/&gt;
at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:128)&lt;br/&gt;
at org.apache.mahout.classifier.bayes.MultipleTextOutputFormat.getBaseRecordWriter(MultipleTextOutputFormat.java:41)&lt;br/&gt;
at org.apache.mahout.classifier.bayes.MultipleOutputFormat$1.write(MultipleOutputFormat.java:81)&lt;br/&gt;
at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:517)&lt;br/&gt;
at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)&lt;br/&gt;
at org.apache.mahout.classifier.bayes.WikipediaDatasetCreatorReducer.reduce(WikipediaDatasetCreatorReducer.java:35)&lt;br/&gt;
...}}&lt;/p&gt;

&lt;p&gt;It&apos;s this attempt to create a file here that&apos;s resulting in the &quot;already being created&quot; exception. Anyone have any ideas here?&lt;/p&gt;</comment>
                            <comment id="13014179" author="mat_kelcey" created="Thu, 31 Mar 2011 21:09:24 +0100"  >&lt;p&gt;Could this be anything to do with running on EMR?&lt;/p&gt;</comment>
                            <comment id="13014526" author="srowen" created="Fri, 1 Apr 2011 12:55:28 +0100"  >&lt;p&gt;Could be... but somehow I doubt it. I note that in &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-614&quot; title=&quot;org.apache.mahout.classifier.baytes.MultipleOutputFormat not working as intended with Hadoop 0.20?&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-614&quot;&gt;&lt;del&gt;MAHOUT-614&lt;/del&gt;&lt;/a&gt; we fixed up an apparent problem with this class. The change might have uncovered a different issue (or, er, actually messed it up in a different way).&lt;/p&gt;

&lt;p&gt;One way forward is to fix it. Another way is to delete it. I say that&apos;s an option since WikipediaDatasetCreatorOutputFormat is the one and only class which depends on two classes copied and modified from Hadoop. Going forward we&apos;d want to go back to the main-line version somehow.&lt;/p&gt;

&lt;p&gt;And if removing use of this custom output format doesn&apos;t &quot;hurt&quot;, as the issue implies, well, why not just remove it?&lt;/p&gt;

&lt;p&gt;But the question is... is it really just as well to let this dump to a sequence file or does that defeat the purpose? Robin?&lt;/p&gt;</comment>
                            <comment id="13015071" author="robinanil" created="Sat, 2 Apr 2011 20:43:24 +0100"  >&lt;p&gt;Unfortunately Bayes classifier reads from a text input format. And I wanted to split the output of different categories into multiple files for the wikipedia example, which btw is not necessary for Bayes, it reads of all the files anyways. Dropping it wouldnt create any problems. Just have to update all tutorials and references which mention that fact.&lt;/p&gt;</comment>
                            <comment id="13015208" author="srowen" created="Sun, 3 Apr 2011 21:40:39 +0100"  >&lt;p&gt;OK done.&lt;/p&gt;</comment>
                            <comment id="13015233" author="hudson" created="Sun, 3 Apr 2011 23:16:33 +0100"  >&lt;p&gt;Integrated in Mahout-Quality #715 (See &lt;a href=&quot;https://hudson.apache.org/hudson/job/Mahout-Quality/715/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://hudson.apache.org/hudson/job/Mahout-Quality/715/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-646&quot; title=&quot;Cannot run Wikipedia example on Amazon Elastic MapReduce (EMR)&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-646&quot;&gt;&lt;del&gt;MAHOUT-646&lt;/del&gt;&lt;/a&gt; Just output one text file for Wikipedia example to avoid some bug in MultipleOutputFormat subclass&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 31 Mar 2011 19:03:35 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9415</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hxy3u7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>22773</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                </customfields>
    </item>
</channel>
</rss>