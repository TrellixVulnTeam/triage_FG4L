<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 04:19:42 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/MAHOUT-588/MAHOUT-588.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[MAHOUT-588] Benchmark Mahout&apos;s clustering performance on EC2 and publish the results</title>
                <link>https://issues.apache.org/jira/browse/MAHOUT-588</link>
                <project id="12310751" key="MAHOUT">Mahout</project>
                    <description>&lt;p&gt;For Taming Text, I&apos;ve commissioned some benchmarking work on Mahout&apos;s clustering algorithms.  I&apos;ve asked the two doing the project to do all the work in the open here.  The goal is to use a publicly reusable dataset (for now, the ASF mail archives, assuming it is big enough) and run on EC2 and make all resources available so others can reproduce/improve.&lt;/p&gt;

&lt;p&gt;I&apos;d like to add the setup code to utils (although it could possibly be done as a Vectorizer) and the publication of the results will be put up on the Wiki as well as in the book.  This issue is to track the patches, etc.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12496369">MAHOUT-588</key>
            <summary>Benchmark Mahout&apos;s clustering performance on EC2 and publish the results</summary>
                <type id="3" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/task.png">Task</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="gsingers">Grant Ingersoll</assignee>
                                    <reporter username="gsingers">Grant Ingersoll</reporter>
                        <labels>
                    </labels>
                <created>Sat, 22 Jan 2011 12:32:59 +0000</created>
                <updated>Thu, 5 Jan 2012 23:45:22 +0000</updated>
                            <resolved>Sun, 22 May 2011 17:02:41 +0100</resolved>
                                    <version>0.5</version>
                                    <fixVersion>0.5</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                <comments>
                            <comment id="12987627" author="thelabdude" created="Thu, 27 Jan 2011 16:21:46 +0000"  >&lt;p&gt;While working on this issue with Grant, I found an issue with using seq2sparse in EMR, see &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-598&quot; title=&quot;Downstream steps in the seq2sparse job flow looking in wrong location for output from previous steps when running in Elastic MapReduce (EMR) cluster&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-598&quot;&gt;&lt;del&gt;MAHOUT-598&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="12987907" author="thelabdude" created="Fri, 28 Jan 2011 03:15:38 +0000"  >&lt;p&gt;The SequenceFilesFromMailArchives is based on SequenceFilesFromDirectory, but adds block compression to the SequenceFile.Writer and parses individual mail messages using simple regex patterns. I used ^From \S+@\S.*\d&lt;/p&gt;
{4}
&lt;p&gt;$ for my message boundary pattern.&lt;/p&gt;

&lt;p&gt;Running on the asf-mail-archives, I get 6,107,076 messages (which is slightly more than Szymon&apos;s?). &lt;/p&gt;

&lt;p&gt;To run this, you would need to save to utils/src/main/java/org/apache/mahout/text and then do something like:&lt;/p&gt;

&lt;p&gt;$MAHOUT_HOME/bin/mahout org.apache.mahout.text.SequenceFilesFromMailArchives \&lt;br/&gt;
--input /mnt/asf-mail-archives/extracted \&lt;br/&gt;
--output /mnt/asf-mail-archives/sequence-files \&lt;br/&gt;
-c UTF-8 -chunk 1024 -prefix TamingText&lt;/p&gt;

&lt;p&gt;The chunk size is rather large because it is the raw size before compression.&lt;/p&gt;</comment>
                            <comment id="12988075" author="szymek" created="Fri, 28 Jan 2011 13:23:14 +0000"  >&lt;p&gt;Tim,&lt;br/&gt;
Uncompress.java is a short script I run at the very beginning to place all dumps in one directory. Next step is a SequenceFilesFromMailArchives.java (or for comparison version (2) I soon upload, which basically uses all mails content and MIME headings)&lt;/p&gt;

&lt;p&gt;Szymon&lt;/p&gt;</comment>
                            <comment id="12988097" author="szymek" created="Fri, 28 Jan 2011 15:04:08 +0000"  >&lt;p&gt;Tim,&lt;br/&gt;
I modified slightly your Parser, and started it on my machine for comparison. I understand that you really parse the messages. I did simple splitting. Hence, I preserve all MIME headings (I know that most of it will become redundant after tfidf calculation, but at least we will not decrease the size of the dataset). I also think that the Taming is addressed to a general audience and finally we should drastically simplify the code, currently it is nicely generic but maybe to large to interpret within a single page (so we can later consider using pure SequenceFile.createWriter  without Mahout&apos;s utils and wrappers, but it is just a detail). &lt;/p&gt;

&lt;p&gt;Regards&lt;/p&gt;
</comment>
                            <comment id="12988603" author="thelabdude" created="Sun, 30 Jan 2011 16:12:48 +0000"  >&lt;p&gt;Vectorization process using seq2sparse is complete and are available in my S3 bucket :&lt;/p&gt;

&lt;p&gt;s3://thelabdude/asf-mail-archives/vectors/&lt;/p&gt;

&lt;p&gt;(Note: I&apos;ll move to Grant&apos;s asf-mail-archives bucket once we have some of the clustering algorithms working as I didn&apos;t want to move all this data around if it&apos;s not correct)&lt;/p&gt;

&lt;p&gt;Here are the parameters I used to create the vectors:&lt;/p&gt;

&lt;p&gt;org.apache.mahout.driver.MahoutDriver	seq2sparse \&lt;br/&gt;
  -i s3n://thelabdude/asf-mail-archives/mahout-0.4/sequence-files/ \&lt;br/&gt;
  -o /asf-mail-archives/mahout-0.4/vectors/ \&lt;br/&gt;
  --weight tfidf --chunkSize 100 --minSupport 2 \&lt;br/&gt;
  --minDF 1 --maxDFPercent 90 --norm 2 \&lt;br/&gt;
  --numReducers 31 --sequentialAccessVector&lt;/p&gt;

&lt;p&gt;Vectorizing the sequence files took some serious horse-power; took 52 minutes on a 19 Node Cluster of Extra Large instances in EMR with 31 reducers. The log from the successful run is attached &amp;#8211; seq2sparse_xlarge_ok.log. Notice that I built sequential access vectors (which I&apos;ve heard may help with kmeans performance).&lt;/p&gt;

&lt;p&gt;A few lessons learned:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;The resulting tf-vectors or tfidf-vectors files are large (~11.5GB) so you need to have at least 3 reducers if you intend to load the vectors into S3 as the max file size is 5GB. I&apos;m storing the vectors in S3 so that we can re-use them for multiple clustering job runs.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;The MR job has 20 steps and benefits greatly from distributing the processing; don&apos;t try to vectorize this much data on a single node and multiple reducers is a must!&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;After failing to get this working on a development machine, I started with a cluster of 9 m1.small instances (in Amazon EMR) and the job crashed (see attached log - seq2sparse_small_failed.log). Then I used a cluster of 13 large instances and the process completed successfully after a couple of hours, but I wasn&apos;t able to &quot;distcp&quot; the results to S3 &amp;#8211; real bummer! (see attached log - distcp_large_to_s3_failed.log). This may be a configuration issue with Amazon&apos;s EMR large instance as xlarge works as expected???&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Here is the ls output for the aforementioned bucket:&lt;/p&gt;

&lt;p&gt;$ s3cmd ls s3://thelabdude/asf-mail-archives/vectors/&lt;/p&gt;

&lt;p&gt;                       DIR   s3://thelabdude/asf-mail-archives/vectors/df-count/&lt;br/&gt;
                       DIR   s3://thelabdude/asf-mail-archives/vectors/tf-vectors/&lt;br/&gt;
                       DIR   s3://thelabdude/asf-mail-archives/vectors/tfidf-vectors/&lt;br/&gt;
                       DIR   s3://thelabdude/asf-mail-archives/vectors/tokenized-documents/&lt;br/&gt;
                       DIR   s3://thelabdude/asf-mail-archives/vectors/wordcount/&lt;br/&gt;
2011-01-30 15:29         0   s3://thelabdude/asf-mail-archives/vectors/df-count_$folder$&lt;br/&gt;
2011-01-30 15:30  70926210   s3://thelabdude/asf-mail-archives/vectors/dictionary.file-0&lt;br/&gt;
2011-01-30 15:32  70863447   s3://thelabdude/asf-mail-archives/vectors/dictionary.file-1&lt;br/&gt;
2011-01-30 15:33  70892506   s3://thelabdude/asf-mail-archives/vectors/dictionary.file-2&lt;br/&gt;
2011-01-30 15:31  70877571   s3://thelabdude/asf-mail-archives/vectors/dictionary.file-3&lt;br/&gt;
2011-01-30 15:32  70824816   s3://thelabdude/asf-mail-archives/vectors/dictionary.file-4&lt;br/&gt;
2011-01-30 15:34  70895476   s3://thelabdude/asf-mail-archives/vectors/dictionary.file-5&lt;br/&gt;
2011-01-30 15:35  40982506   s3://thelabdude/asf-mail-archives/vectors/dictionary.file-6&lt;br/&gt;
2011-01-30 15:36  37160153   s3://thelabdude/asf-mail-archives/vectors/frequency.file-0&lt;br/&gt;
2011-01-30 15:36  37160173   s3://thelabdude/asf-mail-archives/vectors/frequency.file-1&lt;br/&gt;
2011-01-30 15:30  37160173   s3://thelabdude/asf-mail-archives/vectors/frequency.file-2&lt;br/&gt;
2011-01-30 15:31  37160173   s3://thelabdude/asf-mail-archives/vectors/frequency.file-3&lt;br/&gt;
2011-01-30 15:32  37160173   s3://thelabdude/asf-mail-archives/vectors/frequency.file-4&lt;br/&gt;
2011-01-30 15:33  37160173   s3://thelabdude/asf-mail-archives/vectors/frequency.file-5&lt;br/&gt;
2011-01-30 15:34  37160173   s3://thelabdude/asf-mail-archives/vectors/frequency.file-6&lt;br/&gt;
2011-01-30 15:34   1727033   s3://thelabdude/asf-mail-archives/vectors/frequency.file-7&lt;/p&gt;

&lt;p&gt;Now on to running the clustering algorithms! &lt;/p&gt;

&lt;p&gt;Szymon plans to start on the algorithms in the following order:&lt;/p&gt;

&lt;p&gt;canopy -&amp;gt; k-means -&amp;gt; fuzzy -&amp;gt; mean-shift -&amp;gt; dirichlet&lt;/p&gt;

&lt;p&gt;I&apos;ll start on the other end and begin with dirichlet and work backwards.&lt;/p&gt;</comment>
                            <comment id="12988608" author="srowen" created="Sun, 30 Jan 2011 16:57:02 +0000"  >&lt;p&gt;(By the way the S3 size limit is now 5TB)&lt;/p&gt;

&lt;p&gt;I think this would be great to put on the wiki at some point if you&apos;re so inclined.&lt;br/&gt;
&lt;a href=&quot;https://cwiki.apache.org/MAHOUT/mahout-wiki.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://cwiki.apache.org/MAHOUT/mahout-wiki.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I think you will also discover some bottlenecks that aren&apos;t yet obvious, and would be great to share what you find about where the time is spent.&lt;/p&gt;</comment>
                            <comment id="12988611" author="thelabdude" created="Sun, 30 Jan 2011 17:17:16 +0000"  >&lt;p&gt;Hi Sean,&lt;/p&gt;

&lt;p&gt;Will definitely look into updating the Wiki as I work through the process.&lt;/p&gt;

&lt;p&gt;Thanks for the heads-up on the 5TB limit &amp;#8211; it might be how distcp was accessing S3 as I definitely got back a &quot;max file size exceeded&quot; error from S3 when trying to upload files larger than 5GB. Will do some more research to find out the exact cause ...&lt;/p&gt;</comment>
                            <comment id="12988731" author="gsingers" created="Mon, 31 Jan 2011 11:33:26 +0000"  >&lt;p&gt;For the code, I think it makes sense that this goes into $MAHOUT_HOME/utils/src/main/.  Scripts can go in src/main/scripts/ec2 (or whatever).  Java code can go under the src/main/java path (see the benchmark package, for instance) but it can also go in other appropriate places.&lt;/p&gt;</comment>
                            <comment id="12988739" author="szymek" created="Mon, 31 Jan 2011 12:06:51 +0000"  >&lt;p&gt;Finally I managed to go through all steps (until ClusterDisplay) with the local development environment. In order to avoid OutOfMemory I changed /conf/mapred-site.xml and added:&lt;/p&gt;

&lt;p&gt;&amp;lt;parameter&amp;gt;&lt;br/&gt;
&amp;lt;name&amp;gt;mapred.child.java.opts&amp;lt;/name&amp;gt;&lt;br/&gt;
&amp;lt;value&amp;gt;-Xmx5000M &amp;lt;/value&amp;gt;&lt;br/&gt;
&amp;lt;/parameter&amp;gt;&lt;/p&gt;

&lt;p&gt;It enabled to go reach the end of the processing, but there were a few 600s timeout problems and a few jobs were killed. In order to avoid this, I also had to add&lt;/p&gt;

&lt;p&gt;&amp;lt;parameter&amp;gt;&lt;br/&gt;
&amp;lt;name&amp;gt;mapred.task.timeout&amp;lt;/name&amp;gt;&lt;br/&gt;
&amp;lt;value&amp;gt;0&amp;lt;/value&amp;gt;&lt;br/&gt;
&amp;lt;/parameter&amp;gt;&lt;/p&gt;

&lt;p&gt;I&apos;ll describe in detail all the steps (might be useful for EC2) and switch to process in the AWS. I&apos;ll also try to figure out the minimum requirements for -Xmx.&lt;/p&gt;</comment>
                            <comment id="12989192" author="szymek" created="Tue, 1 Feb 2011 13:06:15 +0000"  >&lt;p&gt;seq2sparse_small_fail does not have detailed info why a job did not succeed, e.g.&lt;/p&gt;

&lt;p&gt;2011-01-30 01:25:55,726 INFO org.apache.hadoop.mapred.JobClient (main): Task Id : attempt_201101300001_0003_m_000159_0, Status : FAILED&lt;/p&gt;

&lt;p&gt;I get such info locally, maybe changing mapred.child.java.opts parameter in conf/mapred-site.xml can help, I have: &lt;/p&gt;

&lt;p&gt;     &amp;lt;property&amp;gt;&lt;br/&gt;
          &amp;lt;name&amp;gt;mapred.child.java.opts&amp;lt;/name&amp;gt;&lt;br/&gt;
          &amp;lt;value&amp;gt;-Xmx2000M -XX:-HeapDumpOnOutOfMemoryError&amp;lt;/value&amp;gt;&lt;br/&gt;
     &amp;lt;/property&amp;gt;&lt;/p&gt;</comment>
                            <comment id="12989193" author="szymek" created="Tue, 1 Feb 2011 13:08:53 +0000"  >&lt;p&gt;The most common tokens are in Top1000Token_maybeStopWords, I will extend stopWord list with some of them (e.g. http, from, have, mail), default Lucene stop words are only:&lt;/p&gt;

&lt;p&gt; &quot;a&quot;, &quot;an&quot;, &quot;and&quot;, &quot;are&quot;, &quot;as&quot;, &quot;at&quot;, &quot;be&quot;, &quot;but&quot;, &quot;by&quot;,&quot;for&quot;, &quot;if&quot;, &quot;in&quot;, &quot;into&quot;, &quot;is&quot;, &quot;it&quot;,&quot;no&quot;, &quot;not&quot;, &quot;of&quot;, &quot;on&quot;, &quot;or&quot;, &quot;such&quot;,&quot;that&quot;, &quot;the&quot;, &quot;their&quot;, &quot;then&quot;, &quot;there&quot;, &quot;these&quot;, &quot;they&quot;, &quot;this&quot;, &quot;to&quot;, &quot;was&quot;, &quot;will&quot;, &quot;with&quot;&lt;/p&gt;</comment>
                            <comment id="12989195" author="szymek" created="Tue, 1 Feb 2011 13:15:02 +0000"  >&lt;p&gt;clusters_kMeans.txt is an overview of 20 clusters. Interpretation:&lt;/p&gt;

&lt;p&gt;1) n=23019&lt;br/&gt;
2) c=[aaaaaaaaaaa:0.002, ...&lt;br/&gt;
3) Top Terms: software                                =&amp;gt;  16.595137777966098&lt;/p&gt;

&lt;p&gt;ad 1) number of emails in a cluster&lt;br/&gt;
ad 2) the values of tfidf for nonempty coordinates of the centroid, sorted alphabetically&lt;br/&gt;
ad 3) Coordinates with the highest tfidf - maybe perceived as cluster&apos;s labels&lt;/p&gt;</comment>
                            <comment id="12989223" author="szymek" created="Tue, 1 Feb 2011 14:47:26 +0000"  >&lt;p&gt;Collocations bottle-neck.&lt;/p&gt;

&lt;p&gt;As I see, so far both of us did vectorization with 1-grams. I must admit that I am affraid to calculate 2-grams or 3-grams. My understanding of:&lt;br/&gt;
DictionaryVectorizer&lt;br/&gt;
CollocDriver&lt;br/&gt;
CollocMapper&lt;br/&gt;
CollocReducer&lt;/p&gt;

&lt;p&gt;is that, firstly all collocations are built and counted and only those with freq&amp;gt;minCount are restored. If it is so, I think that one more preprocessing step would make it faster in our setting (huge amount of typos and a long,long-tail) - i.e. building collocations only for tokens (1-grams) with freq&amp;gt;minCount. I&apos;ll experiment with this issue few hours today. &lt;/p&gt;</comment>
                            <comment id="12989285" author="thelabdude" created="Tue, 1 Feb 2011 17:16:49 +0000"  >&lt;p&gt;Hi Szymon,&lt;/p&gt;

&lt;p&gt;I&apos;d like to use the vectors you created with the more sophisticated analzyer. Can you post the tfidf-vectors directory from your HDFS to Grant&apos;s S3 bucket, such as: s3n://asf-mail-archives/mahout-0.4/tfidf-vectors&lt;/p&gt;

&lt;p&gt;Best way would be to use distcp out of Hadoop using:&lt;/p&gt;

&lt;p&gt;hadoop distcp HDFS_PATH/tfidf-vectors s3n://asf-mail-archives/mahout-0.4/&lt;/p&gt;

&lt;p&gt;You&apos;ll also need to setup your s3n access key and secret key in core-site.xml, such as:&lt;/p&gt;

&lt;p&gt;&amp;lt;property&amp;gt;&lt;br/&gt;
  &amp;lt;name&amp;gt;fs.s3n.awsAccessKeyId&amp;lt;/name&amp;gt;&lt;br/&gt;
  &amp;lt;value&amp;gt;ACCESSKEY&amp;lt;/value&amp;gt;&lt;br/&gt;
&amp;lt;/property&amp;gt;&lt;br/&gt;
&amp;lt;property&amp;gt;&lt;br/&gt;
  &amp;lt;name&amp;gt;fs.s3n.awsSecretAccessKey&amp;lt;/name&amp;gt;&lt;br/&gt;
  &amp;lt;value&amp;gt;SECRETKEY&amp;lt;/value&amp;gt;&lt;br/&gt;
&amp;lt;/property&amp;gt;&lt;/p&gt;
</comment>
                            <comment id="12989323" author="szymek" created="Tue, 1 Feb 2011 18:45:28 +0000"  >&lt;p&gt;I&apos;ll place both: tf-vectors and tfidf-vectors generated for 1-grams with TamingAnalyzer.java (uploaded to Mahout-588), parameters used for DictionaryVectorizer.createTermFrequencyVectors():&lt;/p&gt;

&lt;p&gt; int minSupport = 100;&lt;br/&gt;
 int maxNGramSize = 1;&lt;br/&gt;
 float minLLRValue = LLRReducer.DEFAULT_MIN_LLR;&lt;br/&gt;
 int reduceTasks = 10;&lt;br/&gt;
 int chunkSize = 128;&lt;br/&gt;
 boolean sequentialAccessOutput = false;&lt;br/&gt;
 // new parameters in new API&lt;br/&gt;
 float normPower=PartialVectorMerger.NO_NORMALIZING;&lt;br/&gt;
 boolean logNormalize=false;&lt;br/&gt;
 boolean namedVectors=false;&lt;/p&gt;

&lt;p&gt;parameters used in TFIDFConverter.processTfIdf():&lt;/p&gt;

&lt;p&gt; int reduceTasks = 10;&lt;br/&gt;
 int chunkSize = 128;&lt;br/&gt;
 boolean sequentialAccessOutput = false;&lt;br/&gt;
 int minDf = 1;&lt;br/&gt;
 int maxDFPercent = 80;&lt;br/&gt;
 float norm = PartialVectorMerger.NO_NORMALIZING;&lt;br/&gt;
 boolean logNormalize=false;&lt;br/&gt;
 boolean namedVectors=false;&lt;/p&gt;

&lt;p&gt;It is recommended in &quot;Mahout in Action&quot; that LDA should get tf-vectors as an input. Thanks for explaining distcp. Btw. so far I gave up with 3-grams (I was getting OutOfMem even for Xmx2000M), after setting Xmx4000M hadoop threw IO.Exception).&lt;/p&gt;</comment>
                            <comment id="12989326" author="szymek" created="Tue, 1 Feb 2011 18:50:00 +0000"  >&lt;p&gt;custom extension of org.apache.lucene.analysis.Analyzer;&lt;/p&gt;

&lt;p&gt;mail specific StopWords added, &lt;br/&gt;
alphanumeric tokens preserved,&lt;br/&gt;
token length limit is in range &lt;span class=&quot;error&quot;&gt;&amp;#91;2,40&amp;#93;&lt;/span&gt;&lt;/p&gt;</comment>
                            <comment id="12989506" author="thelabdude" created="Wed, 2 Feb 2011 03:58:22 +0000"  >&lt;p&gt;Thanks for posting the vectors Szymon. Unfortunately, I&apos;m not able to read them due to permissions:&lt;/p&gt;

&lt;p&gt;$ s3cmd get s3://asf-mail-archives/tfidf-vectors/part-r-00000&lt;br/&gt;
s3://asf-mail-archives/tfidf-vectors/part-r-00000 -&amp;gt; ./part-r-00000  &lt;span class=&quot;error&quot;&gt;&amp;#91;1 of 1&amp;#93;&lt;/span&gt;&lt;br/&gt;
ERROR: S3 error: 403 (Forbidden)&lt;/p&gt;

&lt;p&gt;Grant - Can you make s3://asf-mail-archives/tfidf-vectors/ readable to the public? I tried doing it from my end with no success:&lt;/p&gt;

&lt;p&gt;$ s3cmd setacl --acl-public --recursive s3://asf-mail-archives/tfidf-vectors/&lt;br/&gt;
ERROR: S3 error: 403 (AccessDenied): Access Denied&lt;/p&gt;</comment>
                            <comment id="12989674" author="gsingers" created="Wed, 2 Feb 2011 15:39:24 +0000"  >&lt;p&gt;I&apos;ve made them public.&lt;/p&gt;</comment>
                            <comment id="12990097" author="szymek" created="Thu, 3 Feb 2011 13:40:03 +0000"  >&lt;p&gt;TamingTokenizer.java&lt;br/&gt;
TamingAnalyzer.java&lt;/p&gt;

&lt;p&gt;were used to tokenize sequence files stored in HDFS. Parameters are hard coded (it is assumed that seqFiles are in TamingSEQ) &lt;/p&gt;</comment>
                            <comment id="12990099" author="szymek" created="Thu, 3 Feb 2011 13:47:48 +0000"  >&lt;p&gt;TamingDictVect invokes TamingDictionaryVectorizer. Parameters are hard-coded. It is assumed that the emails were tokenized with TamingToken into TamingVEC.&lt;/p&gt;

&lt;p&gt;TamingDictionaryVectorizer is a modified DictionaryVectorizer used in seq2sparse. Proposed implementation builds collocations only for words with a parametrized frequency, which requires one more preprocessing MR-job. This optimization is based on an observation that if we wish to restore n-grams with minCount&amp;gt;100, than each token in the n-gram must have count&amp;gt;100. In practice we can assume that the count of single tokens is few times higher than desired n-gram count.&lt;/p&gt;

&lt;p&gt;The implementation differences are as follows:&lt;/p&gt;

&lt;p&gt;/* Modified DictionaryVectorizer, now collocations are built only for tokens with defined minimum support. Most changes are enclosed by a commented informative statement. Following classes where also modified:&lt;br/&gt;
1. TamingCollocDriver&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;modified CollocDriver&lt;/li&gt;
	&lt;li&gt;changed CollocMapper to TamingCollocMapper&lt;/li&gt;
	&lt;li&gt;added frequent tokens to DistributedCache&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;2. TamingCollocMapper&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;modified CollocMapper&lt;/li&gt;
	&lt;li&gt;in setup() lilmited dictionary is loaded&lt;/li&gt;
	&lt;li&gt;only terms with all tokens in the dictionary are sent forward to reducers&lt;/li&gt;
	&lt;li&gt;used terminology: term is e.g. &quot;Coca cola&quot; tokens are &quot;coca&quot; &quot;cola&quot;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;3. TamingGramKeyGroupComparator&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;unmodified GramKeyGroupComparator&lt;/li&gt;
	&lt;li&gt;just moved to current package to obtain visibility for TamingCollocDriver&lt;br/&gt;
*/&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12990101" author="szymek" created="Thu, 3 Feb 2011 13:59:12 +0000"  >&lt;p&gt;TamingTFIDR.java&lt;br/&gt;
used after TamingDictVect.java&lt;br/&gt;
it calculates tfidf statistics for vectors that already contain only df statistics&lt;/p&gt;

&lt;p&gt;after this step we can run various clustering algorithms on vectors in&lt;br/&gt;
TamingVEC/tfidf-vectors&lt;br/&gt;
and&lt;br/&gt;
TamingVEC/tf-vectors&lt;/p&gt;</comment>
                            <comment id="12990115" author="szymek" created="Thu, 3 Feb 2011 14:49:13 +0000"  >&lt;p&gt;clusters1.txt&lt;/p&gt;

&lt;p&gt;First insight into clustering data with collocations. Clusters are of poor quality, but some 2-,3-grams appear among top coordinates. E.g.&lt;/p&gt;

&lt;p&gt;content type =&amp;gt; 2.0620927878006285&lt;br/&gt;
jira browse =&amp;gt;0.6988600139794434&lt;br/&gt;
hi all =&amp;gt;1.1158225446599421&lt;br/&gt;
apache software =&amp;gt;2.1034713022485554&lt;br/&gt;
pgp signature version =&amp;gt;3.9832009961801447&lt;/p&gt;

&lt;p&gt;Overview of 60 clusters obtained after 1 iteration of k-Means with RandomSeedGeneration. &lt;/p&gt;</comment>
                            <comment id="12990196" author="tdunning" created="Thu, 3 Feb 2011 17:46:48 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Overview of 60 clusters obtained after 1 iteration of k-Means with RandomSeedGeneration.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;One iteration?&lt;/p&gt;</comment>
                            <comment id="12990202" author="gsingers" created="Thu, 3 Feb 2011 17:59:27 +0000"  >&lt;p&gt;I&apos;m guessing he means one run of the clustering algorithm, not one iteration of the k-means algorithm, but I&apos;ll let him say for sure&lt;/p&gt;</comment>
                            <comment id="12990207" author="szymek" created="Thu, 3 Feb 2011 18:08:46 +0000"  >&lt;p&gt;One iteration was the most I could get for a few hours of struggling with both Mahout and Hadoop. The problem is a tricky one. Now I run 10-iterations task. A short description of the problem:&lt;/p&gt;

&lt;p&gt;0. I started a typical 10-iterations job&lt;br/&gt;
1. 60 &apos;canopies&apos; were initialized with RandomSeedGeneration successfully&lt;br/&gt;
2. First iteration run successfully over the canopies and output /clusters-1&lt;br/&gt;
3. The second iteration threw Error: Heap stack overflow&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;I suspected memory leak in KMeansDriver,&lt;/li&gt;
	&lt;li&gt;I set up 1 iteration of KMeansDriver with canopies in /clusters-1&lt;/li&gt;
	&lt;li&gt;the memory problem appeared again&lt;/li&gt;
	&lt;li&gt;it was surprising to me because there is virtually no difference between Iteration-1 and Iteration-2 (at least I thought so)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;4. The problem turned out to be the fact that random seed centroids are very sparse, and centroids we get after the first iteration are very dense. The size of 60-random seeds is 114KB, the size of 60-centroids after the first iteration is &amp;gt;400MB! I had mapred.tasktracker.map.tasks.maximum=40. So I run out of memory quickly during the setup of KMeansMapper.&lt;/p&gt;

&lt;p&gt;5. I played with variuos XMx vs maxMappers configurations and I dont get an error with:&lt;br/&gt;
 -Xmx3500 and mapred.tasktracker.map.tasks.maximum=1&lt;br/&gt;
I get an error with&lt;br/&gt;
-Xmx2000 and  mapred.tasktracker.map.tasks.maximum=2&lt;/p&gt;

&lt;p&gt;I think I can not put more than 2 mappers with more than Xmx2000, as I have 6GB nodes &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;
</comment>
                            <comment id="12990234" author="tdunning" created="Thu, 3 Feb 2011 18:54:14 +0000"  >&lt;p&gt;A hashed representation would help here.  With collocation features, your vector size can easily reach into the millions or more for a large corpus even with a frequency cutoff.&lt;/p&gt;

&lt;p&gt;With a hashed representation and multiple probing you could probably get decent results with size &amp;lt; 10^6, possible with size == 10^5&lt;/p&gt;</comment>
                            <comment id="12990302" author="szymek" created="Thu, 3 Feb 2011 21:07:26 +0000"  >&lt;p&gt;Thank you for the advice,&lt;br/&gt;
as I see currently CosineDistance is partially optimized in the context of kMeans and centroidLengthSquare is computed only once:&lt;/p&gt;

&lt;p&gt;public double distance(double centroidLengthSquare, Vector centroid, Vector v) &lt;/p&gt;

&lt;p&gt;which is significantly faster that standard&lt;/p&gt;

&lt;p&gt;public double distance(Vector v1, Vector v2) &lt;/p&gt;

&lt;p&gt;however it is assumed that v1 and v2 are sparse and time of dotProduct is proportional to the number of non-empty coordinates in both vectors:&lt;/p&gt;

&lt;p&gt;double dotProduct = v2.dot(v1); &lt;/p&gt;

&lt;p&gt;Your suggestion to implement v2 (centroid vector) by means of a hashmap would definitelly improve the speed of calculating the distance between points and centroids and as a result the kMeans itself.&lt;/p&gt;

&lt;p&gt;Regards&lt;/p&gt;</comment>
                            <comment id="12990317" author="tdunning" created="Thu, 3 Feb 2011 21:27:28 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Your suggestion to implement v2 (centroid vector) by means of a hashmap would definitelly improve the speed of calculating the distance between points and centroids and as a result the kMeans itself.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Uh.... my suggestion was to use a hashed feature encoding of the feature vector, not to use a hashmap.  Hashed feature encoding assigns multiple features to each position in the vector and multiple locations to each feature.  It says nothing about the matrix representation.&lt;/p&gt;

&lt;p&gt;For speed considerations alone, if the centroid vector has more than about 10-20% of the elements non-zero, then you should avoid sparse representations and just use the dense form.  If space is critical then you may want to use a sparse representation up to about 30-40% fill.&lt;/p&gt;

&lt;p&gt;So how sparse is your centroid vector?&lt;/p&gt;</comment>
                            <comment id="12991203" author="thelabdude" created="Sun, 6 Feb 2011 19:29:57 +0000"  >&lt;p&gt;Improved TamingAnalyzer to use existing Lucene Analyzers and document steps to run Mahout clustering job in an EC2 cluster using the hadoop/src/contrib/ec2 scripts.&lt;/p&gt;</comment>
                            <comment id="12991398" author="szymek" created="Mon, 7 Feb 2011 14:47:04 +0000"  >&lt;p&gt;Thank you Ted for your support,&lt;br/&gt;
the centroid vectors in our 3-gram set have 935 173 coordinates, on average 372 452 are non-empty (39.8%). Currently we limited the dimensionality to around 500K by preserving only 1- and 2-grams. When we are successful with the smaller dimensionality we come back to the issue of hashed feature encoding with 3-grams.&lt;br/&gt;
Regards&lt;/p&gt;</comment>
                            <comment id="12992082" author="szymek" created="Tue, 8 Feb 2011 18:16:48 +0000"  >&lt;p&gt;60_clusters_kmeans_10_iterations_100K_coordinates.txt&lt;/p&gt;

&lt;p&gt;contains clusters with 15 top terms. Obtained after 10 iterations of kMeans. The size of the vectors ~100K.&lt;/p&gt;

&lt;p&gt;A few interesting clusters with some of the top terms:&lt;/p&gt;

&lt;p&gt;1. fraud, pills, watches, money, prices, you, buy, euro, price, &lt;/p&gt;

&lt;p&gt;3. die, der, und, nicht, ich, ist, das, mit, sie, den, auf, oder, anfragen, zu,&lt;/p&gt;

&lt;p&gt;6. color, 1px, border, background, padding, font, 4px, margin, 10px, &lt;/p&gt;

&lt;p&gt;10. von, betreff, nachricht, gesendet, mittwoch, &lt;/p&gt;

&lt;p&gt;52. maven, repository, jar, build, pom, project, artifact, mvn, dependencies,&lt;/p&gt;

&lt;p&gt;54. tomcat, servlet, web, apache, jsp, file, server, mod, webapps,&lt;/p&gt;</comment>
                            <comment id="12992857" author="thelabdude" created="Thu, 10 Feb 2011 04:21:06 +0000"  >&lt;p&gt;Here are the steps I take to vectorize using Amazon&apos;s Elastic MapReduce.&lt;/p&gt;

&lt;p&gt;1. Install elastic-mapreduce-ruby tool:&lt;/p&gt;

&lt;p&gt;On Debian-based Linux:&lt;/p&gt;

&lt;p&gt;sudo apt-get install ruby1.8&lt;br/&gt;
sudo apt-get install libopenssl-ruby1.8&lt;br/&gt;
sudo apt-get install libruby1.8-extras&lt;/p&gt;

&lt;p&gt;Once these dependencies are installed, download and extract the elastic-mapreduce-ruby app:&lt;/p&gt;

&lt;p&gt;mkdir -p /mnt/dev/elastic-mapreduce /mnt/dev/downloads&lt;br/&gt;
cd /mnt/dev/downloads&lt;br/&gt;
wget &lt;a href=&quot;http://elasticmapreduce.s3.amazonaws.com/elastic-mapreduce-ruby.zip&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://elasticmapreduce.s3.amazonaws.com/elastic-mapreduce-ruby.zip&lt;/a&gt;&lt;br/&gt;
cd /mnt/dev/elastic-mapreduce&lt;br/&gt;
unzip /mnt/dev/downloads/elastic-mapreduce-ruby.zip&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;create a file named credentials.json in /mnt/dev/elastic-mapreduce&lt;/li&gt;
	&lt;li&gt;see: &lt;a href=&quot;http://aws.amazon.com/developertools/2264?_encoding=UTF8&amp;amp;jiveRedirect=1&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://aws.amazon.com/developertools/2264?_encoding=UTF8&amp;amp;jiveRedirect=1&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;credentials.json should contain the following, note the region is significant&lt;/li&gt;
&lt;/ol&gt;


{
  &quot;access-id&quot;:     &quot;ACCESS_KEY&quot;,
  &quot;private-key&quot;:   &quot;SECRET_KEY&quot;,
  &quot;key-pair&quot;:      &quot;gsg-keypair&quot;,
  &quot;key-pair-file&quot;: &quot;/mnt/dev/aws/gsg-keypair.pem&quot;,
  &quot;region&quot;:        &quot;us-east-1&quot;,
  &quot;log-uri&quot;:       &quot;s3n://BUCKET/asf-mail-archives/logs/&quot;
}

&lt;p&gt;Also, it&apos;s a good idea to add /mnt/dev/elastic-mapreduce to your PATH&lt;/p&gt;

&lt;p&gt;2. Once elastic-mapreduce is installed, start a cluster with no jobflow steps yet:&lt;/p&gt;

&lt;p&gt;elastic-mapreduce --create --alive \&lt;br/&gt;
  --log-uri s3n://BUCKET/asf-mail-archives/logs/ \&lt;br/&gt;
  --key-pair gsg-keypair \&lt;br/&gt;
  --slave-instance-type m1.xlarge \&lt;br/&gt;
  --master-instance-type m1.xlarge \&lt;br/&gt;
  --num-instances # \&lt;br/&gt;
  --name mahout-0.4-vectorize \&lt;br/&gt;
  --bootstrap-action s3://elasticmapreduce/bootstrap-actions/configurations/latest/memory-intensive&lt;/p&gt;

&lt;p&gt;This will create an EMR Job Flow named &quot;mahout-0.4-vectorize&quot; in the US-East region. Take note of&lt;br/&gt;
the Job ID returned as you will need it to add the &quot;seq2sparse&quot; step to the Job Flow.&lt;/p&gt;

&lt;p&gt;I&apos;ll leave it to you to decide how many instances to allocate, but keep in mind that one will be&lt;br/&gt;
dedicated as the master. Also, it took about 75 minutes to run the seq2sparse job on 19 xlarge &lt;br/&gt;
instances (~190 normalized instance hours &amp;#8211; not cheap). I think you&apos;ll be safe to use about 10-13&lt;br/&gt;
instances and still finish in under 2 hours.&lt;/p&gt;

&lt;p&gt;Also, notice I&apos;m using Amazon&apos;s bootstrap-action for configuring the cluster to run memory intensive&lt;br/&gt;
jobs. For more information about this, see:&lt;br/&gt;
&lt;a href=&quot;http://buyitnw.appspot.com/forums.aws.amazon.com/ann.jspa?annID=834&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://buyitnw.appspot.com/forums.aws.amazon.com/ann.jspa?annID=834&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;3. Mahout JAR&lt;/p&gt;

&lt;p&gt;The Mahout 0.4 Jobs JAR with our TamingAnalyzer is available at:&lt;br/&gt;
s3://thelabdude/mahout-examples-0.4-job-tt.jar&lt;/p&gt;

&lt;p&gt;If you need to change other Mahout code, then you&apos;ll need to post your own JAR to S3.&lt;br/&gt;
Remember to reference the JAR using the s3n Hadoop protocol.&lt;/p&gt;

&lt;p&gt;4. Schedule a jobflow step to vectorize using Mahout&apos;s seq2sparse:&lt;/p&gt;

&lt;p&gt;elastic-mapreduce --jar s3n://thelabdude/mahout-examples-0.4-job-tt.jar \&lt;br/&gt;
--main-class org.apache.mahout.driver.MahoutDriver \&lt;br/&gt;
--arg seq2sparse \&lt;br/&gt;
--arg -i --arg s3n://thelabdude/asf-mail-archives/mahout-0.4/sequence-files/ \&lt;br/&gt;
--arg -o --arg /asf-mail-archives/mahout-0.4/vectors/ \&lt;br/&gt;
--arg --weight --arg tfidf \&lt;br/&gt;
--arg --chunkSize --arg 100 \&lt;br/&gt;
--arg --minSupport --arg 400 \&lt;br/&gt;
--arg --minDF --arg 20 \&lt;br/&gt;
--arg --maxDFPercent --arg 80 \&lt;br/&gt;
--arg --norm --arg 2 \&lt;br/&gt;
--arg --numReducers --arg ## \&lt;br/&gt;
--arg --analyzerName --arg org.apache.mahout.text.TamingAnalyzer \&lt;br/&gt;
--arg --maxNGramSize --arg 2 \&lt;br/&gt;
--arg --minLLR --arg 50 \&lt;br/&gt;
--enable-debugging \&lt;br/&gt;
-j JOB_ID&lt;/p&gt;

&lt;p&gt;These settings are pretty aggressive in order to reduce the vectors&lt;br/&gt;
to around 100,000 dimensions.&lt;/p&gt;

&lt;p&gt;IMPORTANT: Set the number of reducers to 2 x (N-1) (where N is the size of your cluster)&lt;/p&gt;

&lt;p&gt;The job will send output to HDFS instead of S3 (see Mahout-598). Once the job&lt;br/&gt;
completes, we&apos;ll copy the results to S3 from our cluster&apos;s HDFS using distcp.&lt;/p&gt;

&lt;p&gt;NOTE: To monitor the status of the job, use:&lt;br/&gt;
elastic-mapreduce --logs -j JOB_ID&lt;/p&gt;

&lt;p&gt;5. Save log after completion&lt;/p&gt;

&lt;p&gt;Once the job completes, save the log output for further analysis:&lt;/p&gt;

&lt;p&gt;elastic-mapreduce --logs -j JOB_ID &amp;gt; seq2sparse.log&lt;/p&gt;

&lt;p&gt;6. SSH into the master node to run distcp:&lt;/p&gt;

&lt;p&gt;elastic-mapreduce --ssh -j JOB_ID&lt;/p&gt;

&lt;p&gt;hadoop fs -lsr /asf-mail-archives/mahout-0.4/vectors/&lt;br/&gt;
hadoop distcp /asf-mail-archives/mahout-0.4/vectors/ s3n://ACCESS_KEY:SECRET_KEY@BUCKET/asf-mail-archives/mahout-0.4/sparse-2-gram-stem/ &amp;amp;&lt;/p&gt;

&lt;p&gt;Note: You will need all the output from the vectorize step in order to run Mahout&apos;s clusterdump.&lt;/p&gt;

&lt;p&gt;7. Shut down your cluster&lt;/p&gt;

&lt;p&gt;Once you&apos;ve copied the seq2sparse output to S3, you can shutdown your cluster.&lt;/p&gt;

&lt;p&gt;elastic-mapreduce --terminate -j JOB_ID&lt;/p&gt;

&lt;p&gt;Verify the cluster is terminated in your Amazon console.&lt;/p&gt;

&lt;p&gt;8. Make the vectors public in S3 using the Amazon console or s3cmd:&lt;/p&gt;

&lt;p&gt;s3cmd setacl --acl-public --recursive s3://BUCKET/asf-mail-archives/mahout-0.4/sparse-2-gram-stem/&lt;/p&gt;

&lt;p&gt;9. Dump out the size of the vectors&lt;/p&gt;

&lt;p&gt;bin/mahout vectordump --seqFile s3n://ACCESS_KEY:SECRET_KEY@BUCKET/asf-mail-archives/mahout-0.4/sparse-2-gram-stem/tfidf-vectors/part-r-00000 --sizeOnly | more&lt;/p&gt;</comment>
                            <comment id="12992948" author="srowen" created="Thu, 10 Feb 2011 09:15:38 +0000"  >&lt;p&gt;All things that would be most excellent to put into the wiki!&lt;br/&gt;
&lt;a href=&quot;https://cwiki.apache.org/MAHOUT/mahout-wiki.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://cwiki.apache.org/MAHOUT/mahout-wiki.html&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="12997749" author="szymek" created="Tue, 22 Feb 2011 11:13:49 +0000"  >&lt;p&gt;Two classes:&lt;/p&gt;

&lt;p&gt;TamingSubset.java&lt;br/&gt;
TamingSubsetMapper.java&lt;/p&gt;

&lt;p&gt;can be used to extract a subset of vectors from e.g. a /tfidf-vectors/ directory. It is useful to get some results when clustering runs too long with available resources (or throws heap error).  &lt;/p&gt;

&lt;p&gt;Usage:&lt;br/&gt;
TamingSubset &lt;span class=&quot;error&quot;&gt;&amp;#91;inputPath&amp;#93;&lt;/span&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;outputPath&amp;#93;&lt;/span&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;percentage to retain (0,100)&amp;#93;&lt;/span&gt;&lt;/p&gt;
</comment>
                            <comment id="12997820" author="gsingers" created="Tue, 22 Feb 2011 14:36:14 +0000"  >&lt;p&gt;Hey Guys, &lt;/p&gt;

&lt;p&gt;Really coming along nicely.  I&apos;d suggest for any code you have, that we choose logical names for them based on what they do and not the fact that you are helping me out w/ the book (i.e. TamingAnalyzer, etc.) so that they make sense in a greater context, b/c I think when this is all said in done, it will make for a nice, packaged example that anyone can use, regardless of the purchase of the book (although, of course, they should buy it!)&lt;/p&gt;</comment>
                            <comment id="12998650" author="thelabdude" created="Thu, 24 Feb 2011 01:16:04 +0000"  >&lt;p&gt;Good point about the naming conventions ... will do.&lt;/p&gt;

&lt;p&gt;Also, as we&apos;re nearing completion on our side. I need to update the Mahout wiki with the EC2 / EMR information we&apos;ve gathered throughout this process. I see there are pages for EC2 and EMR already. Do you want me to add our EC2 / EMR stuff to these existing pages or should we have our own page, such as &quot;Benchmarking Mahout Clustering on EC2&quot;?&lt;/p&gt;</comment>
                            <comment id="12998780" author="isabel" created="Thu, 24 Feb 2011 09:27:05 +0000"  >&lt;p&gt;I think there are really three interesting views on your implementation that should be documented:&lt;/p&gt;

&lt;p&gt;Anything special that you found needed to be done to get Mahout up and running on EC2/EMR that is not yet included in the respective wiki pages would be great to have integrated and updated there.&lt;/p&gt;

&lt;p&gt;I&apos;d suggest adding your findings wrt. benchmarking (running times, experimental results, size of the corpus used for testing, any fancy performance comparison graphs you generated) to the Benchmark Wiki page:&lt;br/&gt;
&lt;a href=&quot;https://cwiki.apache.org/confluence/display/MAHOUT/Mahout+Benchmarks&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://cwiki.apache.org/confluence/display/MAHOUT/Mahout+Benchmarks&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As for the general benchmarking setup (design of your implementation, how to install and run it, limitations and constraints) - that I think would be nice to have on a separate wiki page linked to from the &quot;Implementations&quot; section:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/MAHOUT/Mahout+Wiki#MahoutWiki-ImplementationBackground&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://cwiki.apache.org/confluence/display/MAHOUT/Mahout+Wiki#MahoutWiki-ImplementationBackground&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Might make sense to provide links between those pages to make discovering information easier.&lt;/p&gt;</comment>
                            <comment id="12998856" author="szymek" created="Thu, 24 Feb 2011 14:11:38 +0000"  >&lt;p&gt;PDF file:&lt;/p&gt;

&lt;p&gt;mahout-588_distribution.pdf&lt;/p&gt;

&lt;p&gt;contains an analysis of the relationship between the minCount cutoff set in seq2sparse and maximum cardinality of tfidf-vectors or centroids.&lt;/p&gt;
</comment>
                            <comment id="12999621" author="thelabdude" created="Fri, 25 Feb 2011 22:53:59 +0000"  >&lt;p&gt;Thanks for the instructions Isabel. The only problem I see is that the current EC2 wiki is primarily based around creating your own Hadoop AMI, whereas my instructions are based on using an existing Hadoop 0.20.2 AMI from bixolabs (S3 bucket: 453820947548/bixolabs-public-amis). Moreover, I think our process is much easier, but the process that is currently on the wiki is still valid.&lt;/p&gt;

&lt;p&gt;My updated notes are attached along with the setup script I used to create the SequenceFiles.&lt;/p&gt;</comment>
                            <comment id="12999622" author="thelabdude" created="Fri, 25 Feb 2011 22:56:09 +0000"  >&lt;p&gt;Thanks for the feedback Isabel.&lt;/p&gt;

&lt;p&gt;The only caveat I see is that the current EC2 documentation is primarily based around creating your own Hadoop AMI, whereas my instructions are based on using an existing Hadoop 0.20.2 AMI from bixolabs (S3 bucket: 453820947548/bixolabs-public-amis). Moreover, I think our process is much easier, but the process that is currently on the wiki is still valid.&lt;/p&gt;

&lt;p&gt;My updated notes are attached along with the setup script I used to create the SequenceFiles. I&apos;m thinking I should just create a separate section on the EC2 page with our process...&lt;/p&gt;</comment>
                            <comment id="13000486" author="szymek" created="Mon, 28 Feb 2011 19:38:12 +0000"  >&lt;p&gt;The file:&lt;/p&gt;

&lt;p&gt;mahout-588_canopy.pdf contains a description of the steps I undertook in order to find T1 and T2, which would give small set of canopies within limited time constraint.&lt;/p&gt;</comment>
                            <comment id="13002945" author="mat_kelcey" created="Sat, 5 Mar 2011 07:33:43 +0000"  >&lt;p&gt;Hey Timothy!&lt;br/&gt;
I&apos;m a software engineer on the EMR project and I&apos;m interested in finding out more about your EMR/Mahout experiences.&lt;br/&gt;
Hoping that we can have a chat sometime?&lt;br/&gt;
I reckon I&apos;ll be able to get your some credits too to help your experimentation; you shouldn&apos;t be out of pocket for this work that&apos;s useful for everyone.&lt;br/&gt;
Cheers,&lt;br/&gt;
Mat&lt;/p&gt;</comment>
                            <comment id="13003187" author="thelabdude" created="Sun, 6 Mar 2011 17:00:52 +0000"  >&lt;p&gt;Updated the EMR page in the Mahout wiki with the steps we used to create vectors for benchmarking. Also, as requested by Grant, I&apos;ve renamed the text analyzer we&apos;re using to MailArchivesClusteringAnalyzer instead of TamingAnalyzer. Added test cases for the new code.&lt;/p&gt;</comment>
                            <comment id="13006246" author="thelabdude" created="Sun, 13 Mar 2011 18:34:53 +0000"  >&lt;p&gt;Added EC2 steps to wiki at:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/MAHOUT/Use+an+Existing+Hadoop+AMI&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://cwiki.apache.org/confluence/display/MAHOUT/Use+an+Existing+Hadoop+AMI&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;These instructions are complementary to the existing EC2 page in that I assume an AMI already exists, whereas the existing page is valid if there is no suitable AMI available.&lt;/p&gt;</comment>
                            <comment id="13010172" author="gsingers" created="Wed, 23 Mar 2011 15:23:01 +0000"  >&lt;p&gt;Tim or Syzmon,&lt;/p&gt;

&lt;p&gt;Can you put the code to be added to Mahout in patch form?  That will make it easy to see where everything lives, etc.  With that, I can review and look to commit.&lt;/p&gt;

&lt;p&gt;Thanks,&lt;br/&gt;
Grant&lt;/p&gt;</comment>
                            <comment id="13010173" author="gsingers" created="Wed, 23 Mar 2011 15:23:55 +0000"  >&lt;p&gt;See &lt;a href=&quot;https://cwiki.apache.org/confluence/display/MAHOUT/How+To+Contribute&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://cwiki.apache.org/confluence/display/MAHOUT/How+To+Contribute&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13010243" author="thelabdude" created="Wed, 23 Mar 2011 17:30:14 +0000"  >&lt;p&gt;Szymon,&lt;/p&gt;

&lt;p&gt;I&apos;ll create a patch for the following files. I&apos;m not sure what you&apos;re planning on the other Java classes (Taming*), so I&apos;ll leave those in your good hands ...&lt;/p&gt;

&lt;p&gt;MailArchivesClusteringAnalyzer.java&lt;br/&gt;
MailArchivesClusteringAnalyzerTest.java&lt;br/&gt;
SequenceFilesFromMailArchives.java&lt;br/&gt;
SequenceFilesFromMailArchivesTest.java&lt;br/&gt;
prep_asf_mail_archives.sh&lt;/p&gt;

&lt;p&gt;Cheers,&lt;br/&gt;
Tim&lt;/p&gt;</comment>
                            <comment id="13010551" author="thelabdude" created="Thu, 24 Mar 2011 04:33:26 +0000"  >&lt;p&gt;Patch file for trunk&lt;/p&gt;</comment>
                            <comment id="13011170" author="szymek" created="Fri, 25 Mar 2011 12:07:05 +0000"  >&lt;p&gt;Tim,&lt;/p&gt;

&lt;p&gt;I would have to rethink how the other utility classes could be used. If I&lt;br/&gt;
find a good place to inject into a broader analysis, I&apos;ll try to create a&lt;br/&gt;
patch.&lt;/p&gt;

&lt;p&gt;Cheers&lt;br/&gt;
ps. thank you for the linkedin invitation&lt;/p&gt;






&lt;p&gt;-----------------------------------------&lt;br/&gt;
This email was sent using SquirrelMail.&lt;br/&gt;
   &quot;Webmail for nuts!&quot;&lt;br/&gt;
&lt;a href=&quot;http://squirrelmail.org/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://squirrelmail.org/&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13011193" author="gsingers" created="Fri, 25 Mar 2011 13:49:41 +0000"  >&lt;p&gt;Hi Tim,&lt;/p&gt;

&lt;p&gt;For the shell script, can we parameterize that a bit more?  As in pass in the prep dir and output dir?&lt;/p&gt;

&lt;p&gt;Also, s3cmd is GPL, so we can&apos;t include it, but we should at least document that it is required for this script to work.  Perhaps we could use s3-curl which is BSD and does the same thing and could be bundled in?&lt;/p&gt;

&lt;p&gt;Thanks,&lt;br/&gt;
Grant&lt;/p&gt;</comment>
                            <comment id="13011207" author="gsingers" created="Fri, 25 Mar 2011 14:05:13 +0000"  >&lt;p&gt;I should say, otherwise the patch looks good!&lt;/p&gt;</comment>
                            <comment id="13011223" author="gsingers" created="Fri, 25 Mar 2011 14:39:48 +0000"  >&lt;p&gt;Tim, I committed the code piece, so that leaves just the shell script to update.  Thanks!&lt;/p&gt;</comment>
                            <comment id="13013694" author="thelabdude" created="Wed, 30 Mar 2011 22:53:36 +0100"  >&lt;p&gt;Script updated to allow the user to pass the prep working directory and output path as command-line args. Also added better documentation, basic sanity checking on the values, and simple error handling.&lt;/p&gt;</comment>
                            <comment id="13014288" author="hudson" created="Fri, 1 Apr 2011 00:30:46 +0100"  >&lt;p&gt;Integrated in Mahout-Quality #708 (See &lt;a href=&quot;https://hudson.apache.org/hudson/job/Mahout-Quality/708/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://hudson.apache.org/hudson/job/Mahout-Quality/708/&lt;/a&gt;)&lt;/p&gt;
</comment>
                            <comment id="13016546" author="gsingers" created="Wed, 6 Apr 2011 22:26:55 +0100"  >&lt;p&gt;I made some mods to check to see if it has already downloaded and extracted.  &lt;br/&gt;
Committed revision 1089630.&lt;/p&gt;

&lt;p&gt;Thanks, Tim! &lt;/p&gt;</comment>
                            <comment id="13016608" author="hudson" created="Thu, 7 Apr 2011 00:33:51 +0100"  >&lt;p&gt;Integrated in Mahout-Quality #725 (See &lt;a href=&quot;https://hudson.apache.org/hudson/job/Mahout-Quality/725/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://hudson.apache.org/hudson/job/Mahout-Quality/725/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-588&quot; title=&quot;Benchmark Mahout&amp;#39;s clustering performance on EC2 and publish the results&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-588&quot;&gt;&lt;del&gt;MAHOUT-588&lt;/del&gt;&lt;/a&gt;: Script for downloading and preparing the Apache Mail archives for clustering&lt;/p&gt;</comment>
                            <comment id="13037252" author="srowen" created="Sat, 21 May 2011 04:09:02 +0100"  >&lt;p&gt;Looks like a lot of great work. I&apos;m not clear on whether there are additional steps here &amp;#8211; is it &quot;done&quot;? In any event looks like something we should consider to be finished before 0.6 ships.&lt;/p&gt;</comment>
                            <comment id="13126299" author="nathanhalko" created="Thu, 13 Oct 2011 02:45:32 +0100"  >&lt;p&gt;Im looking to do some benchmarking with the asf-mail-archives data set.  Mainly compare lanczos svd with the new ssvd.  I can&apos;t get access to the bucket.  I see that Grant said they were public, but for the life of me I can&apos;t get at them.  I try as if its truly public like the /elasticmapreduce bucket and get an access denied.  Then with the /ACCESS_KEY:SECRET_KEY@.. and get&lt;/p&gt;

&lt;p&gt;The request signature we calculated does not match the signature you provided. Check your key and signing method.&lt;/p&gt;

&lt;p&gt;Any ideas??&lt;/p&gt;</comment>
                            <comment id="13126375" author="srowen" created="Thu, 13 Oct 2011 07:27:39 +0100"  >&lt;p&gt;That isn&apos;t a permission-denied error. It means the request wasn&apos;t quite signed correctly. I use their client libraries to get the signing right; it&apos;s tricky.&lt;/p&gt;</comment>
                            <comment id="13126482" author="gsingers" created="Thu, 13 Oct 2011 11:49:26 +0100"  >&lt;p&gt;I&apos;ve turned off access to mine.  You should now use the Amazon Public Dataset: &lt;a href=&quot;http://aws.amazon.com/datasets/7791434387204566&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://aws.amazon.com/datasets/7791434387204566&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13127830" author="nathanhalko" created="Fri, 14 Oct 2011 21:24:27 +0100"  >&lt;p&gt;So is there still access to the tfidf-vectors as detailed here &lt;a href=&quot;https://cwiki.apache.org/MAHOUT/dimensional-reduction.html?&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://cwiki.apache.org/MAHOUT/dimensional-reduction.html?&lt;/a&gt;  I mounted the Public Dataset in EBS but would really like to avoid parsing the raw files into vectors.  Sean do you have any references for getting the signing right?  I haven&apos;t found anything useful so far.  Thanks&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                                                <inwardlinks description="is blocked by">
                                        <issuelink>
            <issuekey id="12496956">MAHOUT-598</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12473933">MAHOUT-500</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12504266">MAHOUT-670</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12470610" name="60_clusters_kmeans_10_iterations_100K_coordinates.txt" size="7533" author="szymek" created="Tue, 8 Feb 2011 18:16:48 +0000"/>
                            <attachment id="12474465" name="MAHOUT-588.patch" size="35909" author="thelabdude" created="Thu, 24 Mar 2011 04:33:26 +0000"/>
                            <attachment id="12472786" name="MailArchivesClusteringAnalyzer.java" size="8403" author="thelabdude" created="Sun, 6 Mar 2011 17:00:52 +0000"/>
                            <attachment id="12472788" name="MailArchivesClusteringAnalyzerTest.java" size="2488" author="thelabdude" created="Sun, 6 Mar 2011 17:00:52 +0000"/>
                            <attachment id="12472787" name="SequenceFilesFromMailArchives.java" size="12084" author="thelabdude" created="Sun, 6 Mar 2011 17:00:52 +0000"/>
                            <attachment id="12469624" name="SequenceFilesFromMailArchives.java" size="12379" author="thelabdude" created="Fri, 28 Jan 2011 03:15:38 +0000"/>
                            <attachment id="12469676" name="SequenceFilesFromMailArchives2.java" size="10149" author="szymek" created="Fri, 28 Jan 2011 15:04:08 +0000"/>
                            <attachment id="12472789" name="SequenceFilesFromMailArchivesTest.java" size="7566" author="thelabdude" created="Sun, 6 Mar 2011 17:00:52 +0000"/>
                            <attachment id="12470408" name="TamingAnalyzer.java" size="2425" author="thelabdude" created="Sun, 6 Feb 2011 19:29:57 +0000"/>
                            <attachment id="12470135" name="TamingAnalyzer.java" size="2963" author="szymek" created="Thu, 3 Feb 2011 13:40:03 +0000"/>
                            <attachment id="12470407" name="TamingAnalyzerTest.java" size="1444" author="thelabdude" created="Sun, 6 Feb 2011 19:29:57 +0000"/>
                            <attachment id="12470137" name="TamingCollocDriver.java" size="10723" author="szymek" created="Thu, 3 Feb 2011 13:47:48 +0000"/>
                            <attachment id="12470141" name="TamingCollocMapper.java" size="7543" author="szymek" created="Thu, 3 Feb 2011 13:57:10 +0000"/>
                            <attachment id="12470136" name="TamingDictVect.java" size="1453" author="szymek" created="Thu, 3 Feb 2011 13:47:48 +0000"/>
                            <attachment id="12470140" name="TamingDictionaryVectorizer.java" size="14084" author="szymek" created="Thu, 3 Feb 2011 13:47:48 +0000"/>
                            <attachment id="12470139" name="TamingGramKeyGroupComparator.java" size="687" author="szymek" created="Thu, 3 Feb 2011 13:47:48 +0000"/>
                            <attachment id="12471605" name="TamingSubset.java" size="1922" author="szymek" created="Tue, 22 Feb 2011 11:13:49 +0000"/>
                            <attachment id="12471606" name="TamingSubsetMapper.java" size="945" author="szymek" created="Tue, 22 Feb 2011 11:13:49 +0000"/>
                            <attachment id="12470142" name="TamingTFIDF.java" size="912" author="szymek" created="Thu, 3 Feb 2011 13:59:12 +0000"/>
                            <attachment id="12470134" name="TamingTokenizer.java" size="844" author="szymek" created="Thu, 3 Feb 2011 13:40:03 +0000"/>
                            <attachment id="12469923" name="Top1000Tokens_maybe_stopWords" size="14426" author="szymek" created="Tue, 1 Feb 2011 13:08:53 +0000"/>
                            <attachment id="12469674" name="Uncompress.java" size="3687" author="szymek" created="Fri, 28 Jan 2011 13:23:14 +0000"/>
                            <attachment id="12470145" name="clusters1.txt" size="207900" author="szymek" created="Thu, 3 Feb 2011 14:49:13 +0000"/>
                            <attachment id="12469924" name="clusters_kMeans.txt" size="10800" author="szymek" created="Tue, 1 Feb 2011 13:15:02 +0000"/>
                            <attachment id="12469776" name="distcp_large_to_s3_failed.log" size="48399" author="thelabdude" created="Sun, 30 Jan 2011 16:12:48 +0000"/>
                            <attachment id="12470406" name="ec2_setup_notes.txt" size="6304" author="thelabdude" created="Sun, 6 Feb 2011 19:29:57 +0000"/>
                            <attachment id="12471989" name="ec2_setup_notes_v2.txt" size="6292" author="thelabdude" created="Fri, 25 Feb 2011 22:56:09 +0000"/>
                            <attachment id="12471987" name="ec2_setup_notes_v2.txt" size="6292" author="thelabdude" created="Fri, 25 Feb 2011 22:53:59 +0000"/>
                            <attachment id="12472217" name="mahout-588_canopy.pdf" size="165331" author="szymek" created="Mon, 28 Feb 2011 19:38:12 +0000"/>
                            <attachment id="12471836" name="mahout-588_distribution.pdf" size="318653" author="szymek" created="Thu, 24 Feb 2011 14:11:38 +0000"/>
                            <attachment id="12475020" name="prep_asf_mail_archives.sh" size="4525" author="thelabdude" created="Wed, 30 Mar 2011 22:53:36 +0100"/>
                            <attachment id="12471988" name="prep_asf_mail_archives.sh" size="2845" author="thelabdude" created="Fri, 25 Feb 2011 22:56:09 +0000"/>
                            <attachment id="12471986" name="prep_asf_mail_archives.sh" size="2845" author="thelabdude" created="Fri, 25 Feb 2011 22:53:59 +0000"/>
                            <attachment id="12469775" name="seq2sparse_small_failed.log" size="120434" author="thelabdude" created="Sun, 30 Jan 2011 16:12:48 +0000"/>
                            <attachment id="12469774" name="seq2sparse_xlarge_ok.log" size="235058" author="thelabdude" created="Sun, 30 Jan 2011 16:12:48 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>35.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 27 Jan 2011 16:21:46 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9473</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hxy46n:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>22829</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                </customfields>
    </item>
</channel>
</rss>