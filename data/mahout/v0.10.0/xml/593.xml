<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 04:16:01 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/MAHOUT-593/MAHOUT-593.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[MAHOUT-593] Backport of Stochastic SVD patch (Mahout-376) to hadoop 0.20 to ensure compatibility with current Mahout dependencies.</title>
                <link>https://issues.apache.org/jira/browse/MAHOUT-593</link>
                <project id="12310751" key="MAHOUT">Mahout</project>
                    <description>&lt;p&gt;Current Mahout-376 patch requries &apos;new&apos; hadoop API.  Certain elements of that API (namely, multiple outputs) are not available in standard hadoop 0.20.2 release. As such, that may work only with either CDH or 0.21 distributions. &lt;/p&gt;

&lt;p&gt; In order to bring it into sync with current Mahout dependencies, a backport of the patch to &apos;old&apos; API is needed. &lt;br/&gt;
Also, some work is needed to resolve math dependencies. Existing patch relies on apache commons-math 2.1 for eigen decomposition of small matrices. This dependency is not currently set up in the mahout core. So, certain snippets of code are either required to go to mahout-math or use Colt eigen decompositon (last time i tried, my results were mixed with that one. It seems to produce results inconsistent with those from mahout-math eigensolver, at the very least, it doesn&apos;t produce singular values in sorted order).&lt;/p&gt;

&lt;p&gt;So this patch is mainly moing some Mahout-376 code around.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12496624">MAHOUT-593</key>
            <summary>Backport of Stochastic SVD patch (Mahout-376) to hadoop 0.20 to ensure compatibility with current Mahout dependencies.</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="dlyubimov">Dmitriy Lyubimov</assignee>
                                    <reporter username="dlyubimov">Dmitriy Lyubimov</reporter>
                        <labels>
                    </labels>
                <created>Tue, 25 Jan 2011 01:02:45 +0000</created>
                <updated>Sat, 21 May 2011 04:18:48 +0100</updated>
                            <resolved>Mon, 28 Mar 2011 18:01:18 +0100</resolved>
                                    <version>0.4</version>
                                    <fixVersion>0.5</fixVersion>
                                    <component>Math</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="12988525" author="dlyubimov2" created="Sun, 30 Jan 2011 01:22:52 +0000"  >&lt;p&gt;Got a little stuck here. I was using task id (not attempt id) to figure out # of the block i am processing in the mapper. It is readily available thru Context in the new API. In old API it seems all i get is JobConf.&lt;/p&gt;

&lt;p&gt;If anybody knows how i could figure current mapper&apos;s task ID in old API, please feel free to suggest. &lt;/p&gt;</comment>
                            <comment id="12988551" author="dlyubimov2" created="Sun, 30 Jan 2011 07:00:08 +0000"  >&lt;p&gt;Backport of basic algorithm is done. &lt;/p&gt;

&lt;p&gt;Notes: &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;git patch, requires &apos;patch -p1&apos;&lt;/li&gt;
	&lt;li&gt;I only ran local unit test,which is MR run in a local mode, but not distributed one since i don&apos;t have 0.20.2 cluster anywhere (i work with CDH releases). So i verified 376 patches in distributed mode but not this one. This patch is really a stopgap measure until Mahout&apos;s hadoop dependencies upgraded to some version that has new API&apos;s MultipleOutputs and other little things.&lt;/li&gt;
	&lt;li&gt;apache commons-math dependencies is a mess. math module depends on 2.1 but core module depends on 1.2 so when run, there are all sorts of linkage errors because of classes being ocasionally picked up from either 1.2 or 2.1 . I switched both modules to 2.1. Actually 2.1 dependency in math was test scope only, but i removed that as i assumed math module is intended to have math dependencies and core module is intended to have hadoop (MR) stuff. But it turns out core module has math-commons dependency anyway... so please review commons-math dependency, i really need it working either in core or math and i had all SSVD tests ever with 2.1 (fyi).&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;on a side note, the issue with dependencies above is in part caused by inconsistent use of &amp;lt;dependencyManagement&amp;gt; tag: some dependencies use single versioning thru parent&apos;s &amp;lt;dependencyManagement&amp;gt; and some (most, actually) declare their own versioning in subprojects. I think Mahout really needs dependencies housecleaning work done and move all versions under dependency management in the parent pom.&lt;/p&gt;</comment>
                            <comment id="12994289" author="srowen" created="Mon, 14 Feb 2011 13:32:11 +0000"  >&lt;p&gt;This is looking good and so I want to get this on into the codebase. I have some broad suggestions from a first glance.&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;You may need to regenerate this patch as I think a few recent changes will conflict&lt;/li&gt;
	&lt;li&gt;I think it&apos;s a decent convention to not have Mappers and Reducers as inner classes. They can be top-level classes in the same package. It may be personal preference but I think much of our MapReduce code does this. In general I think lots of inner classes that don&apos;t need to be inner just makes things hard to find.&lt;/li&gt;
	&lt;li&gt;It&apos;s also preferable, I think to try to standardize how the Job is implemented. I believe we&apos;re trying to use this AbstractJob class as the template for Job implementations. Can this be put into that mold? I see at least one implementation does this.&lt;/li&gt;
	&lt;li&gt;Spacing looks uneven &amp;#8211; should be 2 spaces per indent&lt;/li&gt;
	&lt;li&gt;Might look at the coding conventions in surrounding code &amp;#8211; there are a fair number of differences. For example we don&apos;t use the &quot;m_&quot; convention for members. You might run &quot;mvn checkstyle:checkstyle&quot; for a complete list of what&apos;s at odds with the project style.&lt;/li&gt;
	&lt;li&gt;Apache code doesn&apos;t usually have @author tags&lt;/li&gt;
	&lt;li&gt;Don&apos;t throw RuntimeException &amp;#8211; prefer subclasses&lt;/li&gt;
	&lt;li&gt;Use RandomUtils.getRandom(), not new Random(), for test repeatability&lt;/li&gt;
	&lt;li&gt;We already have an IOUtils class that does (or can be augmented to do) what your IOUtil class does. Best to combine them.&lt;/li&gt;
	&lt;li&gt;Can you comment on how this replaces, extends, complements the 2+ existing SVD implementation we have already? (In the javadoc ideally)&lt;/li&gt;
	&lt;li&gt;extend MahoutTestCase, not TestCase&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12994464" author="dlyubimov2" created="Mon, 14 Feb 2011 20:09:52 +0000"  >&lt;p&gt;I&apos;ll attend to the code style changes. &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;You may need to regenerate this patch as I think a few recent changes will conflict&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I&apos;ll do that but i think it might be more convenient to address at the time of commit by doing git rebasing or something else having automatic 3-way merge capability. (that&apos;s how i am accustomed to manage this). Otherwise, chances are if you are delaying putting it in, those little changes in pom are touched pretty frequently by all the other committers.  I know Mahout uses svn though so it might be more of a problem.&lt;/p&gt;

&lt;p&gt;&lt;blockquote&gt;&lt;p&gt;It&apos;s also preferable, I think to try to standardize how the Job is implemented. I believe we&apos;re trying to use this AbstractJob class as the template for Job implementations. Can this be put into that mold? I see at least one implementation does this. &lt;/p&gt;&lt;/blockquote&gt;. Yes, the main driver does accept the configuration, but since it is multiple MR stuff, it just passes it on as a Configuration object. The reasons for that are: &lt;br/&gt;
1) There&apos;s only one command line, not an individual one per job. Therefore, there&apos;s only one Tool and only one TollRunner.run() call. &lt;br/&gt;
2) I need to control what is passed on to args. while general mapreduce parameters are mainly allowed to pass thru, in certain cases i need to control number of reducers and other specific hadoop stuff such as split size increase. This is much more conveniently done on Configuration object than on an unparsed args[]. I already thought of this and we already discussed it somewhere on the list. It seems we like the AbstractJob but in this case i think it would be more of a headache to interface thru run(args[]) instead of run(Configuration). &lt;br/&gt;
&lt;blockquote&gt;&lt;p&gt;I think it&apos;s a decent convention to not have Mappers and Reducers as inner classes. &lt;/p&gt;&lt;/blockquote&gt; Sorry, not to start a discussion on this or anything, but just for my information, is this is project-wide consensus or it is your opinion? I don&apos;t think i am with you on this, nor do i think i see consensus in the project. I did not scan all the Mahout tree for this but just scanning one package up, all the DRM jobs seem to be happily use inner classes. At least it seems there wasn&apos;t such consensus until pretty recently. Hadoop literature that i scanned seems to use inner classes too. &lt;/p&gt;

&lt;p&gt;If you really want to do this, do you also want partitioners and grouping comparators be separate classes? Are you against inner class paradigm in general? why just mappers? &lt;/p&gt;

&lt;p&gt;If we do that it would multiply # of classes at least 3-fold while breaking &quot;job-as-minicomponent&quot; paradigm. Inner classes are in a way natural concern separation tool in a similar way the namespaces are in C++, but since java doesn&apos;t really have grammar level concept of a namespace, hence inner static classes are kind of its way to build &apos;minicomponents&apos;. The only downside of inner classes is in case if there&apos;s too much code to have in one file, so there&apos;s nothing &apos;mini&apos; about the component anymore, in which case one might really consider packages to demarcate component boundaries. Which is not the case here. (we are still in &apos;mini&apos; realm I think).&lt;/p&gt;

&lt;p&gt;In short, like Straustroup put it, object oriented code doesn&apos;t make code shorter, nor more efficient. If anything, it makes it less efficient. Pretty much the only reason to have it is to have same problem domain addressed in one place because it presumably makes code more maintainable. If you really want to to dump 5-odd mappers and about same number of reducers and drivers plus all the helpers into a flat namespace, at least let&apos;s do it in packages then. But then using packages would create a tree deeper that it needs to be. (IMO).&lt;blockquote&gt;&lt;p&gt;In general I think lots of inner classes that don&apos;t need to be inner just makes things hard to find&lt;/p&gt;&lt;/blockquote&gt; Eclipse type lookup works regardless if class is inner or not. I don&apos;t use Idea a lot but I am pretty sure they have it the same way. &lt;/p&gt;

&lt;p&gt;If consensus is we&apos;d rather have a package per job than an inner class (which is what my preference in this case would be), i&apos;ll refactor each job into a package with a bunch of classes sprouting underneath. &lt;/p&gt;

&lt;p&gt;Makes sense? &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Can you comment on how this replaces, extends, complements the 2+ existing SVD implementation we have already? (In the javadoc ideally)&lt;/p&gt;&lt;/blockquote&gt; 
&lt;p&gt;The main issue for all this is 376. there&apos;s &lt;b&gt;a lot&lt;/b&gt; of discussion there &amp;#8211; probably more than it needs be.&lt;/p&gt;

&lt;p&gt;In short, &lt;em&gt;compared to Lanczos SVD, i think opinions are that stochastic approach can take on larger size problems and also compute them faster with less memory requirements that Lanczos would &amp;#8211; but Lanczos would of course be more precise. Some problems, in tens of billions of rows, may not require precise solution (such as LSI), in which case Stochastic method might be preferrable&lt;/em&gt;. I am not sure about Hebbian though. I can put that summary above in javadoc if that&apos;s the consensus, but as a matter of personal reputation, I actually purposely avoided stating/speculating any comparisons as I did not have a chance to run conclusive benchmarks on that kind of problem sizes over Lanczos. So i wouldn&apos;t put that on my own accord only at the moment. (I know that the speed of processing on this is comparable to or a little better than to seq2sparse with the same size of problem and it works reasonably well for us, but i don&apos;t have larger size experimental data).Like i suggested in 376, let&apos;s do benchmarks to confirm.&lt;/p&gt;

&lt;p&gt;The only thing that I am relatively sure about is that this SVD code adheres to row key contract as I commented on &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-322&quot; title=&quot;DistributedRowMatrix should live in SequenceFile&amp;lt;Writable,VectorWritable&amp;gt; instead of SequenceFile&amp;lt;IntWritable,VectorWritable&amp;gt;&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-322&quot;&gt;&lt;del&gt;MAHOUT-322&lt;/del&gt;&lt;/a&gt; and last time i checked, Lanczos did not, which is not a big deal, but renders Lanczos incompatible with direct output of seq2sparse. That&apos;s about it I am more or less sure about.&lt;/p&gt;</comment>
                            <comment id="12994470" author="tdunning" created="Mon, 14 Feb 2011 20:18:52 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Sorry, not to start a discussion on this or anything, but just for my information, is this is project-wide consensus or it is your opinion? I don&apos;t think i am with you on this, nor do i think i see consensus in the project. I did not scan all the Mahout tree for this but just scanning one package up, all the DRM jobs seem to be happily use inner classes. At least it seems there wasn&apos;t such consensus until pretty recently. Hadoop literature that i scanned seems to use inner classes too.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I typically prefer inner classes for mappers and reducers as long as they are static.  I have no problem finding them since IntelliJ hunts them down instantly.  It is a bit of a pain from vi.  From emails etags will find inner classes as easily as anything.&lt;/p&gt;</comment>
                            <comment id="12994584" author="dlyubimov2" created="Tue, 15 Feb 2011 00:15:35 +0000"  >&lt;p&gt;command line help document. Hope that helps with understanding the usage.&lt;/p&gt;</comment>
                            <comment id="13000132" author="dlyubimov2" created="Mon, 28 Feb 2011 06:21:36 +0000"  >&lt;p&gt;git patch.&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;attended to most style issues. Some checkstyle warnings remain but they are either legit or some really rare and obsessing identation discrepancies between eclipse formatting and this checker.&lt;/li&gt;
	&lt;li&gt;added javadoc reflecting differentiating points for SVD method&lt;/li&gt;
	&lt;li&gt;moved IOUtil stuff out to o.a.m.commons.IOUtils&lt;/li&gt;
	&lt;li&gt;verified all Mahout maven tests pass at this time&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13000133" author="dlyubimov2" created="Mon, 28 Feb 2011 06:24:51 +0000"  >&lt;p&gt;Ok i think i would like to commit it. It is a sizable patch and it is hard to maintain it on a side without getting it out of sync all the time. And then i will attend to remaining outstanding issues with separate commits/issues.&lt;/p&gt;

&lt;p&gt;What&apos;s next?&lt;/p&gt;</comment>
                            <comment id="13000137" author="dlyubimov2" created="Mon, 28 Feb 2011 06:39:42 +0000"  >&lt;ul&gt;
	&lt;li&gt;Minor javadoc edition&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13000190" author="srowen" created="Mon, 28 Feb 2011 09:47:55 +0000"  >&lt;p&gt;I have some first comments &amp;#8211;&lt;/p&gt;

&lt;p&gt;I am unable to really apply this patch. It&apos;s not quite against the root directory. Normally it&apos;s a matter of stripping some directories but the result I get from doing that even manually is not quite right I think. I end up with a patch against things like src/ in the root directory which doesn&apos;t exist now.&lt;/p&gt;

&lt;p&gt;From manually scanning the patch I have two minor questions:&lt;/p&gt;

&lt;p&gt;IOUtils already has methods to close Closeables, surely we want to reuse that?&lt;br/&gt;
And we are bringing in commons-math for its eigen decomposition implementation; is that duplicative of what&apos;s in Mahout?&lt;/p&gt;</comment>
                            <comment id="13000355" author="dlyubimov2" created="Mon, 28 Feb 2011 16:00:29 +0000"  >&lt;ul&gt;
	&lt;li&gt;re: applying the patch. As i mentioned above, it is a git patch (patch -p 1 should work last time i tested it, let me know if you still seem to have problems). But, once voted, i can commit it myself. I work in Git per suggested apache git workflow (there&apos;s a help page somewhere), maintaining separate branch for mahout-593 (or any other issue i worked on) in github. I can merge-squash it directly onto git-svn mahout branch and then push it as one incremental commit. Also, git patches are &lt;em&gt;always&lt;/em&gt; branch patches (i.e. you need to apply it on top of trunk). The git branch for this patch can be viewed and checked out here: &lt;a href=&quot;https://github.com/dlyubimov/ssvd-lsi/tree/MAHOUT-593&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/dlyubimov/ssvd-lsi/tree/MAHOUT-593&lt;/a&gt; (if it helps to get a better idea what it looks like in the Mahout trunk tree once applied).&lt;/li&gt;
	&lt;li&gt;I am not sure what IOUtils provide. It&apos;s quite likely I can reuse it, sure, the functionality is very basic here. (make sure all closeables are closed in a collection in the proper order). I&apos;ll take a look.&lt;/li&gt;
	&lt;li&gt;We are not bringing common-math to Mahout. It already uses it, as 1.2 in mahout-core and as 2.1 in mahout-math. (Surely, you can&apos;t have both versions in your final assembly even if you declare it &amp;#8211; so it&apos;s just fixing this inconsistency). common-math 1.2 is way outdated (in fact, so oudated that it is using a different artifact id but still uses the same package/class tree structure). The fact that it uses same package tree but has different artifact id causes actually both trees to be included in the final classpath causing linking errors (i actually ran into it cause i use eclipse; unlike maven, it can&apos;t tell test scope from runtime so it included both, because mahout-math is using apache common-math in test context only).&lt;/li&gt;
	&lt;li&gt;Colt eigensolver in Mahout is marked by &quot;@Deprecated do not use&quot;. Even so, i tried to use it as a first option before failing back on to commons-math. its results are inconsistent when checked against apache commons. Also, it doesn&apos;t seem to sort eigenvectors and eigenvalues in descendent order, which adds work. I understand Mahout is in the process of adopting Colt solvers. When it is done, we can easily migrate to using that (all eigensolver dependencies are strictly isolated in a single class called &quot;EigenSolverWrapper&quot; so whatever solver is used can be substituted their easily for a very small in-RAM matrix such as what B*B&apos; is.  Apache-commons results in SSVD solution that is then asserted against Colt SVD results with epsilon 1e-10 in the unit test. Which makes me think apache-commons results are consistent with Colt SVD and Colt eigensolver&apos;s  are not.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13000453" author="srowen" created="Mon, 28 Feb 2011 18:32:22 +0000"  >&lt;p&gt;OK, I don&apos;t use git though IntelliJ figures out these patches reasonably well automatically, so no problem. I thought it had made an error since I saw a src/ directory, but that&apos;s my error, since that path is still fine even after &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-608&quot; title=&quot;Collect various data directories in Mahout dir structure&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-608&quot;&gt;&lt;del&gt;MAHOUT-608&lt;/del&gt;&lt;/a&gt;. Nevermind there, I have the patch now.&lt;/p&gt;

&lt;p&gt;IOUtils has a method to quietly close and log Closeables already. It doesn&apos;t need two, but, feel free to merge them. It&apos;s picking nits but my theory is that there&apos;s not much to do to &quot;recover&quot; from failing to close a stream. You log it, and you move on. So I don&apos;t know if it needs to throw an exception in the end &amp;#8211; the caller doesn&apos;t even know which stream didn&apos;t close?&lt;/p&gt;

&lt;p&gt;DeleteOnClose is a cute way to stick in file deletion into a close method; if it really works nicely for you OK. There&apos;s File.deleteOnExit() if it&apos;s a question of temp file cleanup.&lt;/p&gt;

&lt;p&gt;Yes good move on standardizing commons-math dependency. That makes sense.&lt;/p&gt;

&lt;p&gt;If the existing eigen solver is perhaps wrong, and still deprecated, just delete it. And if that means you don&apos;t need the wrapper layer I think you&apos;re fine to clear that too.&lt;/p&gt;

&lt;p&gt;My only structural complaint is that this is not really using AbstractJob. I see the idea is to make several phases of the job runnable independently. Is that a realistic use case? Usually you run all the phases to get meaningful work done (perhaps with the option of restarting from phase n, to recover from failure &amp;#8211; but that&apos;s handled already in AbstractJob).&lt;/p&gt;

&lt;p&gt;It&apos;s a lot of duplication of Hadoop glue code at a time when we already have a problem with inconsistent and duplicative interaction with Hadoop. (Which is not this patch&apos;s &quot;fault&quot;.) And I know we say, well, we can fix that later, but I don&apos;t think it will ever come.&lt;/p&gt;

&lt;p&gt;We can just give up on standardization.&lt;/p&gt;

&lt;p&gt;... or I wonder if a simple halfway solution is expose the AbstractJob.prepareJob() as a static method so at least it can be reused by folks that still would prefer a different structure. At least the Hadoop Job configuration is centralized.&lt;/p&gt;

&lt;p&gt;I think there are still a number of tiny style issues, but, not worth holding things up over. It can be adjusted later.&lt;/p&gt;</comment>
                            <comment id="13000473" author="dlyubimov2" created="Mon, 28 Feb 2011 19:16:00 +0000"  >&lt;blockquote&gt;&lt;p&gt;DeleteOnClose is a cute way to stick in file deletion into a close method; if it really works nicely for you OK. There&apos;s File.deleteOnExit() if it&apos;s a question of temp file cleanup.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;As you might guess, it is Inadco&apos;s practice we use &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; The reason for that is difference in those approaches. I want the file deleted asap &amp;#8211; i.e. when the mapper or reducer is decommissioned, not when jvm exists. We actually use settings that allow mahout to reuse jvm for up to 10 tasks (to cope better with massive mapper numbers, spawning jvm for every 64M worth of input is way too expensive). Some jobs may create way too much files that we could afford to delay the cleanup.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Yes good move on standardizing commons-math dependency. That makes sense.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;in fact we discussed it somewhere already . This is partly due to the fact that Mahout doesn&apos;t follow maven (somewhat convoluted imo) doctrine where you&apos;d have to declare all you use in a parent under &amp;lt;dependencyManagement&amp;gt; &lt;em&gt;with&lt;/em&gt; versions and actual module would not have &lt;em&gt;versioned&lt;/em&gt; dependency. That achieves two main things: &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;all modules end up using the same version at runtime as they were compiled with;&lt;/li&gt;
	&lt;li&gt;the projects that wish to embed this code base, could import those dependencies into their maven build by using &amp;lt;scope&amp;gt;import&amp;lt;/scope&amp;gt; spec. (one can&apos;t import transitive dependencies, only those in &amp;lt;dependencyManagement&amp;gt; ones which is a problem with their approach imo). So... Ted was suggesting we create a separate issue devoted to bringing it all up in agreement with that maven ideology (clean out versions from dependencies in modules and move all of them under &amp;lt;dependencyManagement&amp;gt; in parent pom which is partly followed now, but not everywhere, as the case with math-commons demostrates). I was planning to look at it more closely and suggest a patch at a later time.&lt;/li&gt;
&lt;/ul&gt;


&lt;blockquote&gt;&lt;p&gt;My only structural complaint is that this is not really using AbstractJob. I see the idea is to make several phases of the job runnable independently. Is that a realistic use case? Usually you run all the phases to get meaningful work done (perhaps with the option of restarting from phase n, to recover from failure &#8211; but that&apos;s handled already in AbstractJob).&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I wouldn&apos;t describe it as &quot;giving up&quot;. &lt;br/&gt;
It&apos;s just sometimes we have MR pipelines that are closely coupled (i.e. expect predefined intermediate results, have common postprocessing to move/rearrange files, have some overarching logic which is certainly the case in this case). This is not exclusive to Mahout, such pipelines pop up everywhere MR is used. So, for tightly coupled MR steps, IMO it doesn&apos;t make sense to generate arg[] and then reparse them using a CLI parser. It&apos;s too much hassle for something that is never intended to be used as a standalone job. &lt;br/&gt;
On the other hand, for every job that we feel is uncoupled enough so that we are compelled to create its own CLI, sure, let&apos;s use AbstractJob and ToolRunner. I can only vote for it with both hands. In fact, i am all for having it as a standard. Got a CLI?-- use AbstractJob, that&apos;s the rule here. No CLI and never will be? &amp;#8211; single driver works better then.&lt;/p&gt;</comment>
                            <comment id="13000569" author="srowen" created="Mon, 28 Feb 2011 22:24:17 +0000"  >&lt;p&gt;All sounds good to me.&lt;/p&gt;

&lt;p&gt;On the last point, I agree, there is no sense in creating and re-parsing command line args in a program. That would indicate design failure.&lt;/p&gt;

&lt;p&gt;I think we agree. Mappers and Reducers rarely stand alone; they are almost always part of a pipeline of Mappers and Reducers that work together to accomplish something. That&apos;s a &quot;Job&quot; to me, one unit that invokes many Mappers/Reducers.&lt;/p&gt;

&lt;p&gt;My only question then is why those intermediate stages of Mappers/Reducers need to be exposed as stand-alone units (&quot;Jobs&quot; in your patch)? I agree they&apos;re not command-line &quot;Jobs&quot; that would be invoked independently, but they seem exposed that way.&lt;/p&gt;

&lt;p&gt;It&apos;s not any better a design really, but, I would have structured it as one executable that kicks off many MapReduces, and that&apos;s what AbstractJob is supporting.&lt;/p&gt;</comment>
                            <comment id="13000665" author="dlyubimov2" created="Tue, 1 Mar 2011 01:59:43 +0000"  >&lt;blockquote&gt;&lt;p&gt;My only question then is why those intermediate stages of Mappers/Reducers need to be exposed as stand-alone units (&quot;Jobs&quot; in your patch)? I agree they&apos;re not command-line &quot;Jobs&quot; that would be invoked independently, but they seem exposed that way.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I don&apos;t think they are exposed. here is the class diagram.&lt;/p&gt;

&lt;p&gt;There&apos;s only one CLI entity (SSVDCLI) which is a Tool as well as AbstractJob. The SSVDCLI is basically a CLI adapter to the SSVDSolver API. SSVDSolver api can be used inline in a program as much as a regular solver (distinction is that DRM input is specified by a Hadoop glob expression).&lt;/p&gt;

&lt;p&gt;SSVDSolver encapsulates overarching functionality of SSVD by driving map reduce jobs as well as small front-end computation (the latter by beans of instantiating an EigenSolverWrapper which solves BB&apos;=U&amp;Lambda;U&apos; ). All this is completely isolated from either CLI or Solver api. The function of SSVDCli is parse and establish job specific parameters as well as Hadoop&apos;s Configuration. (Solver may override some of them when passing them on to jobs).&lt;/p&gt;

&lt;p&gt;The idea here is that one might use it as embedded solver by using SSVDSolver, &lt;em&gt;or&lt;/em&gt; one might use command-line interface. But everything else is encapsulated and may change.&lt;/p&gt;

&lt;p&gt;The overarching sequence enforced by solver is QtJob -&amp;gt; BBtJob -&amp;gt; BtJob -&amp;gt; front end eigen solution -&amp;gt; (optional VJob and optional UJob in parallel).&lt;/p&gt;

&lt;p&gt;QtJob, VJob and UJob are map-only.&lt;/p&gt;



&lt;p&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/12472267/12472267_ssvdclassdiag.png&quot; align=&quot;absmiddle&quot; border=&quot;0&quot; height=&quot;700&quot; /&gt;&lt;/p&gt;</comment>
                            <comment id="13000818" author="srowen" created="Tue, 1 Mar 2011 10:27:40 +0000"  >&lt;p&gt;OK I am convinced by the whiteboarding. You have a good reason for the design and most importantly you are around to support it! I&apos;d commit (after having a quick think about other small items in this thread). If there are any another items to discuss we can handle it afterwards.&lt;/p&gt;</comment>
                            <comment id="13001127" author="dlyubimov2" created="Tue, 1 Mar 2011 21:35:45 +0000"  >&lt;p&gt;Ok... should it get more support before i commit or i may commit now?&lt;/p&gt;

&lt;p&gt;I looked at IOUtils, it seems it has quiet close methods which i can&apos;t use directly. I actually need a noisy close (as close is semantic equivalent of transaction confirmation in a way) so the idea here is to propagate an error after making sure all resources are released still. &lt;/p&gt;

&lt;p&gt;so I think i still need closeAll() method in the form it exists (albeit it perhaps may be changed to be more inline with existing names &amp;#8211; i don&apos;t have an opinion on that. ) Another piece of functionality is that it removes closeable from the collection after resource is closed in order to prevent repeating close attempts on something that has been closed (and also release the java ref). And another piece is that it observes the order of releases as per collection order. None of those features is supported by existing quiet close methods in IOUtils.&lt;/p&gt;

&lt;p&gt;So i think i am ready to commit then, no more patches for this issue.&lt;/p&gt;</comment>
                            <comment id="13001165" author="srowen" created="Tue, 1 Mar 2011 22:52:02 +0000"  >&lt;p&gt;You can&apos;t recover from failing to close a stream, but if you want to fail your processing if a stream can&apos;t be closed, OK. Is that wise?&lt;/p&gt;

&lt;p&gt;I don&apos;t understand the point about releasing a ref or ordering &amp;#8211; it is not different in this respect from the existing method. It iterates in order and keeps no references. There is also nothing that close()es more than once in either version &amp;#8211; and neither version prevents a caller from doing something like that.&lt;/p&gt;

&lt;p&gt;For those reasons I&apos;m still puzzled on this piece of code but think it&apos;s minor enough I wouldn&apos;t push more on it. If it makes sense for you and nobody else has thoughts, it&apos;s fine.&lt;/p&gt;</comment>
                            <comment id="13001215" author="dlyubimov2" created="Wed, 2 Mar 2011 00:47:15 +0000"  >&lt;p&gt;It&apos;s just housekeeping of the resources. It&apos;s not so much the case here, but in other situations you might have a resource tree open. e.g. you may have something representing a filesystem (on top), and bunch of files underneath it, plus might potentially have compressors attached to files etc. &lt;/p&gt;

&lt;p&gt;if you decide to collapse (commit) that system then you need to flush (close) compressors first, then close the files, and then potentially close a filesystem. Along those lines. Hence the order.&lt;/p&gt;

&lt;p&gt;Reference release has to do with the fact that you might have flushed some or all of the resources and closed them, so you want to release reference from that resource collection (list) as well in order for GC to collect it. This helper does it automatically. &lt;/p&gt;

&lt;p&gt;Finally, you do want to receive &quot;noise&quot; to promote it up when you commit the resource tree if one of the resources failed to commit (close). The only exception when you probably don&apos;t care about noise is if that&apos;s an error path already (aka &quot;second chance exception&quot;). &lt;/p&gt;

&lt;p&gt;Again, i see it is probably not so much the case in this case, but it is often the case. &lt;/p&gt;</comment>
                            <comment id="13001337" author="srowen" created="Wed, 2 Mar 2011 08:45:05 +0000"  >&lt;p&gt;I don&apos;t disagree with wanting to close things in order or not hold on to references unnecessarily - I am just saying that is not a difference between the two methods, and you were saying these were reasons you had to write a new method.&lt;/p&gt;

&lt;p&gt;close is not the same as flush/commit. (At least, that&apos;s how many of the key APIs work like JDBC and OutputStream.) close is a notification to the resource that it won&apos;t be used anymore, when the caller already has no more business with it, which is why recovering from a failed close doesn&apos;t seem to have useful semantics in a case I can imagine. I suppose that&apos;s why I slightly didn&apos;t like the idea of deleting a file via close.&lt;/p&gt;

&lt;p&gt;Well, it&apos;s a minor point. I wouldn&apos;t do it this way but it&apos;s not worth holding up over.&lt;/p&gt;</comment>
                            <comment id="13001464" author="dlyubimov@apache.org" created="Wed, 2 Mar 2011 16:02:37 +0000"  >

&lt;p&gt;Sean, sorry, can&apos;t agree with the above. Without going into specific&lt;br/&gt;
implementation details, in general close contract is twofold: it is a&lt;br/&gt;
handle release, but it is also an I/O operation. In particular,&lt;br/&gt;
failure to close output streams means that some of your previous write&lt;br/&gt;
operations and/or housekeeping I/O failed. As such, failure to close&lt;br/&gt;
output streams is equivalent to write operation error from client&apos;s&lt;br/&gt;
perspective.&lt;/p&gt;

&lt;p&gt;Truth to be spoken, close is not same as commit in sense  of&lt;br/&gt;
durability: in general case, you can&apos;t say that writes are durable if&lt;br/&gt;
close was successful &amp;#8211; but you know they weren&apos;t if you couldn&apos;t&lt;br/&gt;
close cleanly. But it so happens in case of hdfs they say closes (and&lt;br/&gt;
syncs in hadoop-append branch) are durable by replication.&lt;/p&gt;

&lt;p&gt;Hence if you can&apos;t close side files or MultipleOutputs cleanly, you&lt;br/&gt;
must not allow task to commit &amp;#8211; you must allow to schedule another&lt;br/&gt;
attempt or accept results of an opportunistic task attempt but not&lt;br/&gt;
this one. Ignoring close errors in this case may result in invalid&lt;br/&gt;
task output.&lt;/p&gt;


&lt;p&gt;-d&lt;/p&gt;

&lt;p&gt;On Wed, Mar 2, 2011 at 12:45 AM, Sean Owen (JIRA) &amp;lt;jira@apache.org&amp;gt; wrote:&lt;/p&gt;</comment>
                            <comment id="13001470" author="srowen" created="Wed, 2 Mar 2011 16:19:01 +0000"  >&lt;p&gt;I think we&apos;re just talking about semantics now. You are welcome to commit as you like.&lt;/p&gt;

&lt;p&gt;I do agree that a stream could buffer writes, and that it could commit as a way to clean up before close, and the commit could fail during close. If the caller is trying to commit by closing, that&apos;s really the problem. But, it&apos;s a real problem.Yes, I don&apos;t think that in any Java API I can think of or HDFS you would have a successful write and successful close but fail to write.&lt;/p&gt;</comment>
                            <comment id="13002411" author="dlyubimov2" created="Fri, 4 Mar 2011 02:42:07 +0000"  >&lt;p&gt;Ok if nobody has more thoughts here or in &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-376&quot; title=&quot;Implement Map-reduce version of stochastic SVD&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-376&quot;&gt;&lt;del&gt;MAHOUT-376&lt;/del&gt;&lt;/a&gt;, I will do minor renaming in IOUtils and commit this weekend.&lt;/p&gt;</comment>
                            <comment id="13003276" author="dlyubimov2" created="Mon, 7 Mar 2011 06:17:50 +0000"  >&lt;ul&gt;
	&lt;li&gt;minor renaming, final patch&lt;br/&gt;
 git diff trunk --stat&lt;br/&gt;
core/pom.xml                                       |    6 +-&lt;br/&gt;
.../java/org/apache/mahout/common/IOUtils.java     |   90 +++&lt;br/&gt;
.../mahout/math/hadoop/stochasticsvd/BBtJob.java   |  188 ++++++&lt;br/&gt;
.../mahout/math/hadoop/stochasticsvd/BtJob.java    |  303 ++++++++++&lt;br/&gt;
.../hadoop/stochasticsvd/DenseBlockWritable.java   |   85 +++&lt;br/&gt;
.../hadoop/stochasticsvd/GivensThinSolver.java     |  621 ++++++++++++++++++++&lt;br/&gt;
.../mahout/math/hadoop/stochasticsvd/Omega.java    |  124 ++++&lt;br/&gt;
.../hadoop/stochasticsvd/PartialRowEmitter.java    |   32 +&lt;br/&gt;
.../mahout/math/hadoop/stochasticsvd/QJob.java     |  351 +++++++++++&lt;br/&gt;
.../mahout/math/hadoop/stochasticsvd/SSVDCli.java  |  126 ++++&lt;br/&gt;
.../math/hadoop/stochasticsvd/SSVDPrototype.java   |  383 ++++++++++++&lt;br/&gt;
.../math/hadoop/stochasticsvd/SSVDSolver.java      |  489 +++++++++++++++&lt;br/&gt;
.../mahout/math/hadoop/stochasticsvd/UJob.java     |  156 +++++&lt;br/&gt;
.../math/hadoop/stochasticsvd/UpperTriangular.java |  151 +++++&lt;br/&gt;
.../mahout/math/hadoop/stochasticsvd/VJob.java     |  150 +++++&lt;br/&gt;
.../hadoop/stochasticsvd/LocalSSVDSolverTest.java  |  172 ++++++&lt;br/&gt;
.../hadoop/stochasticsvd/SSVDPrototypeTest.java    |  112 ++++&lt;br/&gt;
math/pom.xml                                       |    2 +-&lt;br/&gt;
.../mahout/math/ssvd/EigenSolverWrapper.java       |   61 ++&lt;br/&gt;
src/conf/driver.classes.props                      |    1 +&lt;br/&gt;
src/conf/ssvd.props                                |   13 +&lt;br/&gt;
21 files changed, 3613 insertions&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/add.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;, 3 deletions&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/forbidden.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13003284" author="hudson" created="Mon, 7 Mar 2011 07:50:58 +0000"  >&lt;p&gt;Integrated in Mahout-Quality #658 (See &lt;a href=&quot;https://hudson.apache.org/hudson/job/Mahout-Quality/658/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://hudson.apache.org/hudson/job/Mahout-Quality/658/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-593&quot; title=&quot;Backport of Stochastic SVD patch (Mahout-376) to hadoop 0.20 to ensure compatibility with current Mahout dependencies.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-593&quot;&gt;&lt;del&gt;MAHOUT-593&lt;/del&gt;&lt;/a&gt;: initial addition of Stochastic SVD method (related issue is &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-376&quot; title=&quot;Implement Map-reduce version of stochastic SVD&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-376&quot;&gt;&lt;del&gt;MAHOUT-376&lt;/del&gt;&lt;/a&gt;)&lt;/p&gt;</comment>
                            <comment id="13003303" author="hudson" created="Mon, 7 Mar 2011 09:41:23 +0000"  >&lt;p&gt;Integrated in Mahout-Quality #659 (See &lt;a href=&quot;https://hudson.apache.org/hudson/job/Mahout-Quality/659/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://hudson.apache.org/hudson/job/Mahout-Quality/659/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-593&quot; title=&quot;Backport of Stochastic SVD patch (Mahout-376) to hadoop 0.20 to ensure compatibility with current Mahout dependencies.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-593&quot;&gt;&lt;del&gt;MAHOUT-593&lt;/del&gt;&lt;/a&gt; fixing style problems in EigenSolverWrapper (somehow slipped thru in my previous patch).&lt;/p&gt;</comment>
                            <comment id="13003325" author="srowen" created="Mon, 7 Mar 2011 10:55:36 +0000"  >&lt;p&gt;(Dmitriiy I&apos;d assign to you but don&apos;t see you in JIRA at the moment?)&lt;/p&gt;

&lt;p&gt;Great to see, marking this resolved.&lt;/p&gt;</comment>
                            <comment id="13012122" author="dlyubimov" created="Mon, 28 Mar 2011 17:18:26 +0100"  >&lt;p&gt;Reopening. &lt;/p&gt;

&lt;p&gt;One of users reports problems with Mahout version. As i mentioned before, we are actually running CDH3 in the company and I don&apos;t have access to 0.20.2 to test it with (other than the local unit test that runs there). &lt;/p&gt;

&lt;p&gt;the CDH3 version that we use is available here &lt;a href=&quot;https://github.com/dlyubimov/ssvd-lsi/tree/ssvd-preprocessing&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/dlyubimov/ssvd-lsi/tree/ssvd-preprocessing&lt;/a&gt; and it acually tested on problems of around 100 M non-zero elements as well as Reuters dataset Mahout example.&lt;/p&gt;

&lt;p&gt;So Mahout patch needs to be tested as well more thoroughly. One of current problems is that I don&apos;t have easy access to a real 0.20.2 cluster to test it. But at least one user reported this patch got stuck for him. &lt;/p&gt;</comment>
                            <comment id="13012137" author="dlyubimov" created="Mon, 28 Mar 2011 18:01:18 +0100"  >&lt;p&gt;Ok, I think i see what that user&apos;s issue is. I will fail another issue since it is not really a bug but inefficiency in Mahout version.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12461748">MAHOUT-376</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12501085">MAHOUT-623</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12472804" name="MAHOUT-593.patch.gz" size="25425" author="dlyubimov2" created="Mon, 7 Mar 2011 06:17:50 +0000"/>
                            <attachment id="12472148" name="MAHOUT-593.patch.gz" size="25477" author="dlyubimov2" created="Mon, 28 Feb 2011 06:39:42 +0000"/>
                            <attachment id="12472147" name="MAHOUT-593.patch.gz" size="25473" author="dlyubimov2" created="Mon, 28 Feb 2011 06:21:36 +0000"/>
                            <attachment id="12469766" name="MAHOUT-593.patch.gz" size="27045" author="dlyubimov2" created="Sun, 30 Jan 2011 06:55:05 +0000"/>
                            <attachment id="12471041" name="SSVD-givens-CLI.pdf" size="263840" author="dlyubimov2" created="Tue, 15 Feb 2011 00:15:35 +0000"/>
                            <attachment id="12472267" name="ssvdclassdiag.png" size="119609" author="dlyubimov2" created="Tue, 1 Mar 2011 01:45:39 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>6.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Sun, 30 Jan 2011 01:22:52 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9468</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hxy45r:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>22825</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                </customfields>
    </item>
</channel>
</rss>