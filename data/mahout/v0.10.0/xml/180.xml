<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 04:20:29 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/MAHOUT-180/MAHOUT-180.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[MAHOUT-180] port Hadoop-ified Lanczos SVD implementation from decomposer</title>
                <link>https://issues.apache.org/jira/browse/MAHOUT-180</link>
                <project id="12310751" key="MAHOUT">Mahout</project>
                    <description>&lt;p&gt;I wrote up a hadoop version of the Lanczos algorithm for performing SVD on sparse matrices available at &lt;a href=&quot;http://decomposer.googlecode.com/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://decomposer.googlecode.com/&lt;/a&gt;, which is Apache-licensed, and I&apos;m willing to donate it.  I&apos;ll have to port over the implementation to use Mahout vectors, or else add in these vectors as well.&lt;/p&gt;

&lt;p&gt;Current issues with the decomposer implementation include: if your matrix is really big, you need to re-normalize before decomposition: find the largest eigenvalue first, and divide all your rows by that value, then decompose, or else you&apos;ll blow over Double.MAX_VALUE once you&apos;ve run too many iterations (the L^2 norm of intermediate vectors grows roughly as (largest-eigenvalue)^(num-eigenvalues-found-so-far), so losing precision on the lower end is better than blowing over MAX_VALUE).  When this is ported to Mahout, we should add in the capability to do this automatically (run a couple iterations to find the largest eigenvalue, save that, then iterate while scaling vectors by 1/max_eigenvalue).&lt;/p&gt;</description>
                <environment></environment>
        <key id="12436635">MAHOUT-180</key>
            <summary>port Hadoop-ified Lanczos SVD implementation from decomposer</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="jake.mannix">Jake Mannix</assignee>
                                    <reporter username="jake.mannix">Jake Mannix</reporter>
                        <labels>
                    </labels>
                <created>Fri, 25 Sep 2009 21:01:01 +0100</created>
                <updated>Sat, 21 May 2011 04:24:08 +0100</updated>
                            <resolved>Sat, 20 Feb 2010 15:47:11 +0000</resolved>
                                    <version>0.2</version>
                                    <fixVersion>0.3</fixVersion>
                                    <component>Math</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                <comments>
                            <comment id="12759864" author="isabel" created="Sat, 26 Sep 2009 08:31:54 +0100"  >&lt;p&gt;That sounds great! Thank you for offering to donate the code. If you need any help porting the code or any other support, we are happy to help.&lt;/p&gt;

&lt;p&gt;You may also want to have a look at &lt;a href=&quot;http://incubator.apache.org/ip-clearance/index.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://incubator.apache.org/ip-clearance/index.html&lt;/a&gt; that explains the legal steps for donating large code donations.&lt;/p&gt;</comment>
                            <comment id="12763911" author="prasen" created="Fri, 9 Oct 2009 09:34:45 +0100"  >&lt;p&gt;This will be awesome. BTW, any references/articles on your approach will be of great help. I too am interested in paralellizing SVD ( I am sure there are many many folks like me &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;  and will be willing to contribute in this. &lt;/p&gt;</comment>
                            <comment id="12764075" author="jake.mannix" created="Fri, 9 Oct 2009 17:19:50 +0100"  >&lt;p&gt;Hey Prasen,  the approach to doing parallelized Lanczos is parallelized multiplication of (the square of) your input matrix by vectors, so my decomposer project has a DistributedMatrix class which is backed by matrix with rows which live in HDFS, and then Lanczos iteration just leaves your matrix where it is (so no additional data transfer of order the size of your matrix) and sends one vector at a time out to it to do parallelized matrix-by-vector multiplication.&lt;/p&gt;

&lt;p&gt;Then, when you&apos;ve gone far enough, you do a non-parallel SVD on a very small matrix which lives in memory, and you&apos;ve got one half of your set of singular vector pairs (which is often all you need).&lt;/p&gt;

&lt;p&gt;I&apos;m not sure where the best write up on this approach is - once you decide on doing Lanczos, the parallelization procedure is straightforward (in the words of far-too-many CS professors: &quot;At this point, it&apos;s only a matter of &apos;engineering&apos;&quot; - heh).  The main not-completely-straightforward trick I used here is avoiding squaring the input matrix anywhere (which is needed in some form if you&apos;ve got a non-square or non-symmetric matrix), and staying in completely sparse row-matrix form.&lt;/p&gt;

&lt;p&gt;Currently I&apos;m a bit blocked on providing a patch for this, given that the underlying linear algebra primitives in Mahout are probably going to change (to commons-math vectors and matrices most likely), in which case my port of decomposer will need to get completely refactored, and just doing the refactor once makes a lot more sense.&lt;/p&gt;</comment>
                            <comment id="12799155" author="srowen" created="Tue, 12 Jan 2010 11:24:21 +0000"  >&lt;p&gt;Is this unblocked now that much of the Math stuff has been updated?&lt;/p&gt;</comment>
                            <comment id="12799295" author="jakemannix" created="Tue, 12 Jan 2010 18:01:04 +0000"  >&lt;p&gt;This is waiting on my &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-205&quot; title=&quot;Pull Writable (and anything else hadoop dependent) out of the matrix module&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-205&quot;&gt;&lt;del&gt;MAHOUT-205&lt;/del&gt;&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-206&quot; title=&quot;Separate and clearly label different SparseVector implementations&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-206&quot;&gt;&lt;del&gt;MAHOUT-206&lt;/del&gt;&lt;/a&gt; patches to go in, but is otherwise unblocked.&lt;/p&gt;</comment>
                            <comment id="12802641" author="jake.mannix" created="Wed, 20 Jan 2010 03:31:48 +0000"  >&lt;p&gt;contains much more (both the online/sequential Generalized Hebbian Algorithm SVD, as well as a simple Lanczos), but also much less (does not tie in to serialization/persistence of any sort in this patch, let alone an HDFS-backed form of this), than this JIRA ticket discusses.  &lt;/p&gt;

&lt;p&gt;It is completely self-contained, but the unit test (singular, ack!) needs too much memory to get committed currently (as it is in some ways a performance test, in other ways a functionality test, and in yet other ways a comparison test, between Lanczos and Hebbian).  Maybe if I comment out the unit test so it doesn&apos;t run until it&apos;s pared down a bit, this will be committable as a &quot;work in progress&quot;.&lt;/p&gt;

&lt;p&gt;This really is the &quot;meat&quot; of decomposer.  &lt;/p&gt;</comment>
                            <comment id="12802677" author="jake.mannix" created="Wed, 20 Jan 2010 05:24:29 +0000"  >&lt;p&gt;Hmm... further work on unit testing shows that Lanczos is doing great, but the stream-oriented (Hebbian) version has some issues with orthogonalization which need to be worked out.  Another patch will be forthcoming.&lt;/p&gt;</comment>
                            <comment id="12802710" author="jake.mannix" created="Wed, 20 Jan 2010 06:59:04 +0000"  >&lt;p&gt;Jeepers, for performance, I had switched from using SparseRowMatrix to DenseMatrix in a few places, and suddenly failed to keep orthogonality.  Why?  Because of this ugliness I thought was long since fixed, in DenseMatrix:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  @Override
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; Vector getRow(&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; row) {
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (row &amp;lt; 0 || row &amp;gt;= rowSize()) {
      &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; IndexException();
    }
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; DenseVector(values[row]);
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The lovely bug here?  This is a full deep copy of the row, not a shallow view which allows you to mutate the original matrix!  Arrrrrggggg!  I swear there was already a bug filed and fixed regarding this.  It&apos;s easy to do, for this method (make a &quot;shallow&quot; constructor for DenseVector, and use it here).  The right fix also takes care of getColumn (which requires a little more work, but not much).&lt;/p&gt;</comment>
                            <comment id="12802712" author="jake.mannix" created="Wed, 20 Jan 2010 07:01:38 +0000"  >&lt;p&gt;I have nobody to blame but myself.  &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-211&quot; title=&quot;DenseMatrix.getRow() and getColumn() return deep copies of the data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-211&quot;&gt;&lt;del&gt;MAHOUT-211&lt;/del&gt;&lt;/a&gt; was filed by me, assigned to me, but not fixed by me.&lt;/p&gt;</comment>
                            <comment id="12802754" author="jake.mannix" created="Wed, 20 Jan 2010 09:01:30 +0000"  >&lt;p&gt;This patch actually works, has disentangled unit tests (one for Lanczos, one for Hebbian, and a base junit test for them to share), and scaled down test parameters to run without a gazillion bytes of RAM available.&lt;/p&gt;

&lt;p&gt;Also includes a 90% solution for &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-211&quot; title=&quot;DenseMatrix.getRow() and getColumn() return deep copies of the data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-211&quot;&gt;&lt;del&gt;MAHOUT-211&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If I made the patch correctly, that is.&lt;/p&gt;</comment>
                            <comment id="12803297" author="jake.mannix" created="Thu, 21 Jan 2010 14:20:50 +0000"  >&lt;p&gt;Committed initial (non-hadoop dependent) pieces of this as of r901718.&lt;/p&gt;</comment>
                            <comment id="12803469" author="srowen" created="Thu, 21 Jan 2010 20:16:00 +0000"  >&lt;p&gt;Yeah this is done right?&lt;/p&gt;</comment>
                            <comment id="12803472" author="jakemannix" created="Thu, 21 Jan 2010 20:20:06 +0000"  >&lt;p&gt;We can call it done, and open new tickets related to hadoopification, bringing in decomposer-contrib-hadoop stuff, etc.  Sure.&lt;/p&gt;</comment>
                            <comment id="12830785" author="jake.mannix" created="Sun, 7 Feb 2010 22:53:22 +0000"  >&lt;p&gt;Reopening to finish what is described in the title: putting in the hadoop-version of the LanczosSolver.  Patch will be forthcoming.&lt;/p&gt;</comment>
                            <comment id="12831444" author="jake.mannix" created="Tue, 9 Feb 2010 13:04:49 +0000"  >&lt;p&gt;Ok, I&apos;ve got a basic unit test for the DistributedLanczosSolver, and I&apos;ve finally got it running on a real (1-node) cluster against DocumentVectorized a 20newsgroups corpus.  It&apos;s not crashing, but it&apos;s taking a while.   30 seconds per pass over the corpus, which means it&apos;s not going to finish anytime soon, so I&apos;m going to bed now (or do I get up for breakfast? hmm...), and I&apos;ll see how it&apos;s going and possibly post my current patch in &lt;span class=&quot;error&quot;&gt;&amp;#91;a few&amp;#93;&lt;/span&gt; hours.&lt;/p&gt;</comment>
                            <comment id="12831650" author="jake.mannix" created="Tue, 9 Feb 2010 20:29:35 +0000"  >&lt;p&gt;Ok, ugly, dirty patch which needs to be cleaned up, but it does work, in some circumstances, for some inputs (on my cluster).  &lt;b&gt;cough&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;This patch makes some extensions of the DocumentVectorizer as well.  Lets say you already have a SequenceFile&amp;lt;Text,Text&amp;gt; of your corpus (living at text_path, then you can produce some good output by doing: &lt;/p&gt;

&lt;p&gt;  $HADOOP_HOME/bin/hadoop jar examples/target/mahout-examples-0.3-SNAPSHOT.job org.apache.mahout.text.SparseVectorsFromSequenceFiles -i text_path -o corpus_as_vectors_path -seq true -w tfidf -chunk 1000 --minSupport 1 --minDF 5 --maxDFPercent 50 --norm 2&lt;/p&gt;

&lt;p&gt;now I&apos;ve got some SequentialAccessSparseVectors in corpus_as_vectors_path, tfidf weighted, stripping out terms which occur more than half of the time (L2 normalized), etc.  Now for the fun:  you need to know what the dimension of the vectors you spit out (you can do this by guessing and getting it wrong, and slightly more helpful CardinalityException will be spit out in the logs/console, or you can get it from the corpus_as_vectors entries themselves).  If the value you find is numFeatures, then try this hadoop job:&lt;/p&gt;

&lt;p&gt; $HADOOP_HOME/bin/hadoop jar examples/target/mahout-examples-0.3-SNAPSHOT.job org.apache.mahout.math.hadoop.decomposer.DistributedLanczosSolver -i corpus_as_vectors_path -o corpus_svd_path -nr 1 -nc &amp;lt;numFeatures&amp;gt; --rank 100 &lt;/p&gt;

&lt;p&gt;This will zip along making 100 passes over your data, then doing a decomposition of a nice and small (100x100) matrix in memory, and producing a SequenceFile&amp;lt;IntWritable,VectorWritable&amp;gt; (where the values are DenseVectors of dimension numFeatures - so should not be MAX_VALUE!), where the &quot;name&quot; of the vectors contains a string which is not actually the eigenvalue, but it&apos;s proportional to it - I&apos;m working on that part still.&lt;/p&gt;

&lt;p&gt;There&apos;s also a unit test (which currently takes about a minute on my laptop) - DistributedLanczosSolverTest, which validates accuracy.&lt;/p&gt;

&lt;p&gt;TODO: cleanup, stuff mentioned above,  a job which validates correctness explicitly after the fact, and some utilities for taking the eigenvectors and doing useful stuff with them.&lt;/p&gt;

&lt;p&gt;NOTE: Lanczos spits out desiredRank - 1 orthogonal vectors which are pretty close to being eigenvectors of the square of your matrix (ie they are right-singular vectors of the input corpus), but they span the spectrum: the first few are the ones with the highest singular values, the last few are the ones with the lowest singular values.  If you really want, e.g.  the highest 100 singular vectors, ask Lanczos for &lt;b&gt;300&lt;/b&gt; as the rank, and then only keep the top 100, and this will give you 100 &quot;of the largest&quot; singular vectors, but no guarantee that you don&apos;t miss part of that top of the spectrum.  For most cases, this isn&apos;t a worry, but you should keep it in mind.&lt;/p&gt;
</comment>
                            <comment id="12831922" author="jake.mannix" created="Wed, 10 Feb 2010 09:47:51 +0000"  >&lt;p&gt;Adds an EigenVerificationJob, which takes just as long as the DistributedLanczosJob, but at the end of the day prunes away superfluous eigenvectors and those with eigenvalue below a chosen threshold, and saves them back to hdfs, and you get nice output like this:&lt;/p&gt;

&lt;p&gt;10/02/10 01:24:06 INFO decomposer.EigenVerificationJob: appending e|0| = |2905105.325066675|, err = 8.231193504570911E-13 to verifiedOutput/largestCleanEigens&lt;br/&gt;
10/02/10 01:24:06 INFO decomposer.EigenVerificationJob: appending e|2| = |921522.2179480934|, err = 3.312905505481467E-13 to verifiedOutput/largestCleanEigens&lt;br/&gt;
10/02/10 01:24:06 INFO decomposer.EigenVerificationJob: appending e|5| = |422593.76267148677|, err = 9.690026558928366E-13 to verifiedOutput/largestCleanEigens&lt;br/&gt;
10/02/10 01:24:06 INFO decomposer.EigenVerificationJob: appending e|7| = |326205.19577841857|, err = 8.280043317654417E-13 to verifiedOutput/largestCleanEigens&lt;br/&gt;
10/02/10 01:24:06 INFO decomposer.EigenVerificationJob: appending e|8| = |284990.59331463446|, err = 2.398081733190338E-13 to verifiedOutput/largestCleanEigens&lt;br/&gt;
10/02/10 01:24:06 INFO decomposer.EigenVerificationJob: appending e|11| = |253500.28860628756|, err = 0.03797261160467913 to verifiedOutput/largestCleanEigens&lt;br/&gt;
10/02/10 01:24:06 INFO decomposer.EigenVerificationJob: appending e|12| = |253433.24512060767|, err = 1.4885870314174099E-12 to verifiedOutput/largestCleanEigens&lt;br/&gt;
10/02/10 01:24:06 INFO decomposer.EigenVerificationJob: appending e|15| = |222354.15336025952|, err = 7.081002451059248E-13 to verifiedOutput/largestCleanEigens&lt;br/&gt;
10/02/10 01:24:06 INFO decomposer.EigenVerificationJob: appending e|17| = |215156.97325760772|, err = 1.2456702336294256E-13 to verifiedOutput/largestCleanEigens&lt;br/&gt;
10/02/10 01:24:06 INFO decomposer.EigenVerificationJob: appending e|18| = |200592.7782982773|, err = 3.72257780156815E-13 to verifiedOutput/largestCleanEigens&lt;br/&gt;
10/02/10 01:24:06 INFO decomposer.EigenVerificationJob: appending e|19| = |191270.06867188454|, err = 3.610445276081009E-13 to verifiedOutput/largestCleanEigens&lt;br/&gt;
10/02/10 01:24:06 INFO decomposer.EigenVerificationJob: appending e|20| = |168446.0868356986|, err = 1.2598810883446276E-12 to verifiedOutput/largestCleanEigens&lt;br/&gt;
10/02/10 01:24:06 INFO decomposer.EigenVerificationJob: appending e|21| = |166320.23361954943|, err = 1.0635936575909E-13 to verifiedOutput/largestCleanEigens&lt;br/&gt;
10/02/10 01:24:06 INFO decomposer.EigenVerificationJob: appending e|22| = |158882.90261344844|, err = 2.708944180085382E-13 to verifiedOutput/largestCleanEigens&lt;br/&gt;
10/02/10 01:24:06 INFO decomposer.EigenVerificationJob: appending e|25| = |134577.41793521316|, err = 1.7830181775480014E-13 to verifiedOutput/largestCleanEigens&lt;br/&gt;
10/02/10 01:24:06 INFO decomposer.EigenVerificationJob: appending e|26| = |124021.7093344012|, err = 1.773026170326375E-13 to verifiedOutput/largestCleanEigens&lt;br/&gt;
10/02/10 01:24:06 INFO decomposer.EigenVerificationJob: appending e|27| = |121824.37156673158|, err = 1.3680168109431179E-12 to verifiedOutput/largestCleanEigens&lt;br/&gt;
10/02/10 01:24:06 INFO decomposer.EigenVerificationJob: appending e|28| = |119613.86741751489|, err = 1.1823875212257917E-12 to verifiedOutput/largestCleanEigens&lt;br/&gt;
10/02/10 01:24:06 INFO decomposer.EigenVerificationJob: appending e|29| = |119104.75278971005|, err = 1.2567724638756772E-13 to verifiedOutput/largestCleanEigens&lt;br/&gt;
10/02/10 01:24:06 INFO decomposer.EigenVerificationJob: appending e|30| = |100623.36519880772|, err = 1.155742168634788E-12 to verifiedOutput/largestCleanEigens&lt;br/&gt;
10/02/10 01:24:06 INFO decomposer.EigenVerificationJob: appending e|31| = |88661.27936320615|, err = 1.0856870957809406E-12 to verifiedOutput/largestCleanEigens&lt;/p&gt;


&lt;p&gt;the indexes skip (e|0|, e|2|, e|5|, e|7|, ... ) because superfluous ones were found (error too high, not orthogonal either), and these are in descending eigenvalue order (this is on the 20news dataset).&lt;/p&gt;

&lt;p&gt;Next to try it on wikipedia, and then some cleanup and this should be ready for public consumption.&lt;/p&gt;</comment>
                            <comment id="12832813" author="udanax" created="Fri, 12 Feb 2010 02:47:49 +0000"  >&lt;p&gt;Hi, Quick question.&lt;/p&gt;

&lt;p&gt;It works using M/R iterations?&lt;/p&gt;</comment>
                            <comment id="12832824" author="jake.mannix" created="Fri, 12 Feb 2010 03:25:17 +0000"  >&lt;p&gt;Yes.  Multiplication of a matrix (or the square of a matrix) by a vector is the primary operation of Lanczos, and that is done in a M/R iteration.  If you want the top-k singular vectors, you make k passes over the data.&lt;/p&gt;

&lt;p&gt;Once stochastic decomposition is added later, it will be O(1) passes over the data, but they will be slightly heavier duty passes.&lt;/p&gt;</comment>
                            <comment id="12834726" author="srowen" created="Wed, 17 Feb 2010 10:43:58 +0000"  >&lt;p&gt;This is our last open issue for 0.3. How&apos;s it looking?&lt;/p&gt;</comment>
                            <comment id="12834925" author="jake.mannix" created="Wed, 17 Feb 2010 18:40:00 +0000"  >&lt;p&gt;I need to regenerate the patch.  It currently works, but it&apos;s got some &quot;ugliness&quot; in the code I&apos;m trying to clean up  (some semi-hardcoded things, a kludgey api for eigenvectors, inconsistent method names).  I&apos;ll do some more tests (slightly bigger doc set) tonight and if it&apos;s still doing well and passing tests we can try to get it in there.  We can iterate on refactoring later.&lt;/p&gt;</comment>
                            <comment id="12836127" author="jake.mannix" created="Sat, 20 Feb 2010 07:27:21 +0000"  >&lt;p&gt;Ok, new patch.  Contains, in addition to the woefully unfinished DistributedRowMatrix (only timesSquared() is implemented really, for the purposes of Lanczos / SVD), DistributedLanczosSolver, for doing raw Lanczos, and EigenVerificationJob, which checks to see what the cosAngle errors on the purported eigenvectors are, what their eigenvalues are, and whether they&apos;re all orthonormal.  &lt;/p&gt;

&lt;p&gt;It then removes &quot;bad&quot; eigenvector/value pairs (based on a user-specified error margin), and also trims out any which have eigenvalue below a user-specified minimum eigenvalue threshold, then saves them back to HDFS, with a totally hacky &quot;decorated&quot; vector which has the eigenvalue and error baked into the name of the vector (that part should be redone less horribly).&lt;/p&gt;

&lt;p&gt;So usage is like so:&lt;/p&gt;

&lt;p&gt;$HADOOP_HOME/bin/hadoop -jar examples/target/mahout-examples-0.3-SNAPSHOT.job org.apache.mahout.math.hadoop.decomposer.DistributedLanczosSolver --input path/to/vector-sequence-file --output outpath --numRows 0 (currently ignored, not needed) --numCols 12345 --rank 100 &lt;/p&gt;
</comment>
                            <comment id="12836159" author="srowen" created="Sat, 20 Feb 2010 12:38:05 +0000"  >&lt;p&gt;It&apos;s looking good to me, from a cursory visual inspection. Since you&apos;ve been working on it a while, and have tests, I&apos;m sure it&apos;s substantially correct, and that seems about good enough to me for 0.x. I&apos;d say commit.&lt;/p&gt;</comment>
                            <comment id="12836189" author="jake.mannix" created="Sat, 20 Feb 2010 15:47:11 +0000"  >&lt;p&gt;Committed revision 912134.&lt;/p&gt;

&lt;p&gt;Wiki on usage forthcoming.&lt;/p&gt;</comment>
                            <comment id="12837310" author="dleshem" created="Tue, 23 Feb 2010 16:57:52 +0000"  >&lt;p&gt;While testing the new code, I encountered the following issue:&lt;/p&gt;

&lt;p&gt;...&lt;br/&gt;
10/02/23 18:11:17 INFO lanczos.LanczosSolver: LanczosSolver finished.&lt;br/&gt;
10/02/23 18:11:17 ERROR decomposer.EigenVerificationJob: Unexpected --input while processing Options&lt;br/&gt;
Usage:                                                                          &lt;br/&gt;
 [--eigenInput &amp;lt;eigenInput&amp;gt; --corpusInput &amp;lt;corpusInput&amp;gt; --help --output         &lt;br/&gt;
&amp;lt;output&amp;gt; --inMemory &amp;lt;inMemory&amp;gt; --maxError &amp;lt;maxError&amp;gt; --minEigenvalue            &lt;br/&gt;
&amp;lt;minEigenvalue&amp;gt;]                                                                &lt;br/&gt;
Options                  &lt;br/&gt;
...&lt;/p&gt;

&lt;p&gt;The problem seems to be in DistributedLanczosSolver.java &lt;span class=&quot;error&quot;&gt;&amp;#91;73&amp;#93;&lt;/span&gt;:&lt;br/&gt;
EigenVerificationJob expects the parameters&apos; names to be &quot;eigenInput&quot; and &quot;corpusInput&quot;, but you&apos;re mistakenly passing them as &quot;input&quot; and &quot;output&quot;.&lt;/p&gt;

&lt;p&gt;Other than this minor issue, the code seems to be working fine and indeed produces the right amount of dense (eigen?) vectors.&lt;/p&gt;</comment>
                            <comment id="12837324" author="jake.mannix" created="Tue, 23 Feb 2010 17:16:25 +0000"  >&lt;p&gt;Hi Danny, thanks for trying this out!  &lt;/p&gt;

&lt;p&gt;You have indeed found some testing code which snuck in - I was trying to add the EigenVerificationJob to the final step of Lanczos, to save people the trouble of having to &quot;clean&quot; their eigenvectors at the end of the day, but didn&apos;t finish and yet it got checked in.  &lt;/p&gt;

&lt;p&gt;The clue in the code is that I still have a line:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
 &lt;span class=&quot;code-comment&quot;&gt;// TODO ack!&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Which should be a hint that I should not have checked that file in just yet. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;I&apos;ve removed it now - svn up and try again!  &lt;/p&gt;

&lt;p&gt;If you want to see what your eigen-spectrum is like, after you&apos;ve run the DistributedLanczosSolver, the EigenVerificationJob can be run next (it cleans out eigenvectors with too high error or too low eigenvalue):&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
$HADOOP_HOME/bin/hadoop jar $MAHOUT_HOME/examples/target/mahout-examples-{version}.job org.apache.mahout.math.hadoop.decomposer.EigenVerificationJob \
--eigenInput path/&lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt;/svd-output --corpusInput path/to/corpus --output path/&lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt;/cleanOutput --maxError 0.1 --minEigenvalue 10.0 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Thanks for the bug report!&lt;/p&gt;</comment>
                            <comment id="12999274" author="danbri" created="Fri, 25 Feb 2011 09:13:00 +0000"  >&lt;p&gt;This looks great, but a little more documentation would really help those of us new to Mahout.&lt;/p&gt;

&lt;p&gt;(perhaps in &lt;a href=&quot;https://cwiki.apache.org/MAHOUT/svd-singular-value-decomposition.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://cwiki.apache.org/MAHOUT/svd-singular-value-decomposition.html&lt;/a&gt; ?)&lt;/p&gt;

&lt;p&gt; I would jump in and help document but I&apos;d like to be happy I&apos;m understanding things correctly first. Right now, I&apos;m not.&lt;/p&gt;

&lt;p&gt;Couple of trivial things that tripped me:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;the example above uses &apos;hadoop -jar&apos; but I found (using Hadoop 0.20.2+737) that I needed &apos;hadoop jar&apos; (no hyphen).&lt;/li&gt;
	&lt;li&gt;the example has &quot;--numRows 0 (currently ignored, not needed)&quot;; is this still not needed? text output suggests it is used now&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Conceptually (from a broad-brush understanding of SVD) I was initially expecting 3 matrices back, not a single eigenvectors matrix; am happy to RTFM there and brush up on the linear algebra but some pointers would really help. Is it possible to get the decomposition into U, s and V?&lt;/p&gt;
</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12402764">MAHOUT-76</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12436420" name="MAHOUT-180.patch" size="67880" author="jake.mannix" created="Sat, 20 Feb 2010 07:27:21 +0000"/>
                            <attachment id="12435429" name="MAHOUT-180.patch" size="64243" author="jake.mannix" created="Wed, 10 Feb 2010 09:47:51 +0000"/>
                            <attachment id="12435334" name="MAHOUT-180.patch" size="49207" author="jake.mannix" created="Tue, 9 Feb 2010 20:29:35 +0000"/>
                            <attachment id="12430860" name="MAHOUT-180.patch" size="67496" author="jake.mannix" created="Wed, 20 Jan 2010 09:01:30 +0000"/>
                            <attachment id="12430840" name="MAHOUT-180.patch" size="52996" author="jake.mannix" created="Wed, 20 Jan 2010 03:31:48 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>5.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Sat, 26 Sep 2009 07:31:54 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9885</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hxy6pr:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>23239</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                </customfields>
    </item>
</channel>
</rss>