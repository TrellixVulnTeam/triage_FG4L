<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 04:22:15 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/MAHOUT-123/MAHOUT-123.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[MAHOUT-123] Implement Latent Dirichlet Allocation</title>
                <link>https://issues.apache.org/jira/browse/MAHOUT-123</link>
                <project id="12310751" key="MAHOUT">Mahout</project>
                    <description>&lt;p&gt;(For GSoC)&lt;/p&gt;

&lt;p&gt;Abstract:&lt;/p&gt;

&lt;p&gt;Latent Dirichlet Allocation (Blei et al, 2003) is a powerful learning&lt;br/&gt;
algorithm for automatically and jointly clustering words into &quot;topics&quot;&lt;br/&gt;
and documents into mixtures of topics, and it has been successfully&lt;br/&gt;
applied to model change in scientific fields over time (Griffiths and&lt;br/&gt;
Steyver, 2004; Hall, et al. 2008). In this project, I propose to&lt;br/&gt;
implement a distributed variant of Latent Dirichlet Allocation using&lt;br/&gt;
MapReduce, and, time permitting, to investigate extensions of LDA and&lt;br/&gt;
possibly more efficient algorithms for distributed inference.&lt;/p&gt;

&lt;p&gt;Detailed Description:&lt;/p&gt;

&lt;p&gt;A topic model is, roughly, a hierarchical Bayesian model that&lt;br/&gt;
associates with each document a probability distribution over&lt;br/&gt;
&quot;topics&quot;, which are in turn distributions over words. For instance, a&lt;br/&gt;
topic in a collection of newswire might include words about &quot;sports&quot;,&lt;br/&gt;
such as &quot;baseball&quot;, &quot;home run&quot;, &quot;player&quot;, and a document about steroid&lt;br/&gt;
use in baseball might include &quot;sports&quot;, &quot;drugs&quot;, and &quot;politics&quot;. Note&lt;br/&gt;
that the labels &quot;sports&quot;, &quot;drugs&quot;, and &quot;politics&quot;, are post-hoc labels&lt;br/&gt;
assigned by a human, and that the algorithm itself only assigns&lt;br/&gt;
associate words with probabilities. The task of parameter estimation&lt;br/&gt;
in these models is to learn both what these topics are, and which&lt;br/&gt;
documents employ them in what proportions.&lt;/p&gt;

&lt;p&gt;One of the promises of unsupervised learning algorithms like Latent&lt;br/&gt;
Dirichlet Allocation (LDA; Blei et al, 2003) is the ability to take a&lt;br/&gt;
massive collections of documents and condense them down into a&lt;br/&gt;
collection of easily understandable topics. However, all available&lt;br/&gt;
open source implementations of LDA and related topics models are not&lt;br/&gt;
distributed, which hampers their utility. This project seeks to&lt;br/&gt;
correct this shortcoming.&lt;/p&gt;

&lt;p&gt;In the literature, there have been several proposals for paralellzing&lt;br/&gt;
LDA. Newman, et al (2007) proposed to create an &quot;approximate&quot; LDA in&lt;br/&gt;
which each processors gets its own subset of the documents to run&lt;br/&gt;
Gibbs sampling over. However, Gibbs sampling is slow and stochastic by&lt;br/&gt;
its very nature, which is not advantageous for repeated runs. Instead,&lt;br/&gt;
I propose to follow Nallapati, et al. (2007) and use a variational&lt;br/&gt;
approximation that is fast and non-random.&lt;/p&gt;


&lt;p&gt;References:&lt;/p&gt;

&lt;p&gt;David M. Blei, J McAuliffe. Supervised Topic Models. NIPS, 2007.&lt;/p&gt;

&lt;p&gt;David M. Blei , Andrew Y. Ng , Michael I. Jordan, Latent dirichlet&lt;br/&gt;
allocation, The Journal of Machine Learning Research, 3, p.993-1022,&lt;br/&gt;
3/1/2003&lt;/p&gt;

&lt;p&gt;T. L. Griffiths and M. Steyvers. Finding scienti&#64257;c topics. Proc Natl&lt;br/&gt;
Acad Sci U S A, 101 Suppl 1: 5228-5235, April 2004.&lt;/p&gt;

&lt;p&gt;David LW Hall, Daniel Jurafsky, and Christopher D. Manning. Studying&lt;br/&gt;
the History of Ideas Using Topic Models. EMNLP, Honolulu, 2008.&lt;/p&gt;

&lt;p&gt;Ramesh Nallapati, William Cohen, John Lafferty, Parallelized&lt;br/&gt;
variational EM for Latent Dirichlet Allocation: An experimental&lt;br/&gt;
evaluation of speed and scalability, ICDM workshop on high performance&lt;br/&gt;
data mining, 2007.&lt;/p&gt;

&lt;p&gt;Newman, D., Asuncion, A., Smyth, P., &amp;amp; Welling, M. Distributed&lt;br/&gt;
Inference for Latent Dirichlet Allocation. NIPS, 2007.&lt;/p&gt;


&lt;p&gt;Xuerui Wang , Andrew McCallum, Topics over time: a non-Markov&lt;br/&gt;
continuous-time model of topical trends. KDD, 2006&lt;/p&gt;


&lt;p&gt;Wolfe, J., Haghighi, A, and Klein, D. Fully distributed EM for very&lt;br/&gt;
large datasets. ICML, 2008.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12426199">MAHOUT-123</key>
            <summary>Implement Latent Dirichlet Allocation</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="gsingers">Grant Ingersoll</assignee>
                                    <reporter username="dlwh">David Hall</reporter>
                        <labels>
                    </labels>
                <created>Sat, 23 May 2009 20:08:16 +0100</created>
                <updated>Wed, 18 Nov 2009 14:05:54 +0000</updated>
                            <resolved>Sat, 12 Sep 2009 21:13:31 +0100</resolved>
                                    <version>0.2</version>
                                    <fixVersion>0.2</fixVersion>
                                    <component>Clustering</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                    <timeoriginalestimate seconds="1814400">504h</timeoriginalestimate>
                            <timeestimate seconds="1814400">504h</timeestimate>
                                        <comments>
                            <comment id="12720342" author="dlwh" created="Tue, 16 Jun 2009 21:49:38 +0100"  >&lt;p&gt;This is a roughcut implementation. Not ready to go yet. I&apos;ve been waiting on &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-126&quot; title=&quot;Prepare document vectors from the text&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-126&quot;&gt;&lt;del&gt;MAHOUT-126&lt;/del&gt;&lt;/a&gt; because it seems like the way to create the Vectors I need. Or perhaps there&apos;s a better way.&lt;/p&gt;

&lt;p&gt;Basic approach follows the Dirichlet implementation. There is a driver class (LDA Driver) which runs K mapreduces, and a Mapper and a Reducer. We also have an Inferencer, which is what the Mapper uses to compute expected sufficient statistics. A document is just a V-dimensional sparse vector of word counts.&lt;/p&gt;

&lt;p&gt;Map: Perform Inference on each document (~ E-step) and output log probabilities of p(word|topic)&lt;br/&gt;
Reduce: logSum the input log probabilities (~ M-Step), and output the result.&lt;/p&gt;

&lt;p&gt;Loop: use the results of the reduce as the log probabilities for the map.&lt;/p&gt;

&lt;p&gt;Remaining:&lt;br/&gt;
1) Actually run the thing&lt;br/&gt;
2) Number-of-non-zero elements in a sparse vector. Is that staying &quot;size&quot;?&lt;br/&gt;
3) Allow for computing of likelihood to determine when we&apos;re done.&lt;br/&gt;
4) What&apos;s the status of serializing as sparse vector and reading as a dense vector? Is that going to happen?&lt;br/&gt;
5) Find a fun data set to bundle...&lt;br/&gt;
6) Convenience method for running just inference on a set of documents and outputting MAP estimates of word probabilities.&lt;/p&gt;</comment>
                            <comment id="12721494" author="dlwh" created="Thu, 18 Jun 2009 22:13:38 +0100"  >&lt;p&gt;(Still in progress.)&lt;/p&gt;

&lt;p&gt;It seems to work, but it&apos;s much to slow because I underestimated the badness of using DenseVectors. Switching to an element wise system now.&lt;/p&gt;
</comment>
                            <comment id="12721685" author="dlwh" created="Fri, 19 Jun 2009 09:00:15 +0100"  >&lt;p&gt;Right issue this time.&lt;/p&gt;

&lt;p&gt;Mostly functional patch.&lt;/p&gt;</comment>
                            <comment id="12725158" author="gsingers" created="Mon, 29 Jun 2009 14:05:59 +0100"  >&lt;p&gt;Hey David,&lt;/p&gt;

&lt;p&gt;patch applies cleanly, but needs to be brought up to the date for the new Vector iterators.  Some tests for the various pieces are also needed.  A quick few lines on how to run it on the wiki would also be good, if you haven&apos;t done it already.&lt;/p&gt;

&lt;p&gt;Otherwise, starting to take shape, keep up the good work.&lt;/p&gt;</comment>
                            <comment id="12725290" author="dlwh" created="Mon, 29 Jun 2009 19:02:42 +0100"  >&lt;p&gt;Ok, here&apos;s the updates for the vectors. I&apos;ll add a page to the wiki shortly.&lt;/p&gt;

&lt;p&gt;As for testing, this is actually something I&apos;d like some direction on. It&apos;s never been clear to me how to test the actual implementation of clustering algorithms in any meaningful way. Looking at the Dirichlet clusterer, all that it tests are that serialization works, that things aren&apos;t null, and that it outputs the &quot;right&quot; number of things. Serialization in this case doesn&apos;t seem terribly necessary since my &quot;model&quot; are just serialized writables. So... I should just add some basic sanity checks?&lt;/p&gt;

&lt;p&gt;&amp;#8211; David&lt;/p&gt;</comment>
                            <comment id="12725300" author="dlwh" created="Mon, 29 Jun 2009 19:19:20 +0100"  >&lt;p&gt;Sigh, wrong command again. Reattached an actual patch.&lt;/p&gt;</comment>
                            <comment id="12725317" author="gsingers" created="Mon, 29 Jun 2009 20:12:57 +0100"  >&lt;p&gt;Tests should cover basic sanity of operation.  Serialization can sometimes bite things if you are implementing your own Writable stuff, but not a big deal.  Also, its reasonable to try and test boundary conditions, etc. and that bad input is properly handled (including simply throwing an exception).  For a first pass, sanity checks should suffice.&lt;/p&gt;</comment>
                            <comment id="12726289" author="dlwh" created="Thu, 2 Jul 2009 02:04:12 +0100"  >&lt;p&gt;Tests included, wiki page is created.&lt;/p&gt;</comment>
                            <comment id="12731702" author="gsingers" created="Wed, 15 Jul 2009 22:28:57 +0100"  >&lt;p&gt;Patch applies and tests pass.  I&apos;ll try to dig deeper soon.  Keep up the good work.&lt;/p&gt;</comment>
                            <comment id="12734217" author="gsingers" created="Wed, 22 Jul 2009 18:50:19 +0100"  >&lt;p&gt;Notes:&lt;/p&gt;

&lt;p&gt;1. LDADriver &amp;#8211; Switch to use Commons-CLI2 for arg processing.  See the other clustering algorithms.&lt;br/&gt;
2. Hadoop 0.20 introduces a lot of deprecations, we should clean those up here.  No need to put in new code based on deprecations&lt;br/&gt;
3. Some more comments inline in the Mapper/Reducer would be great, especially explaining what is being collected&lt;/p&gt;

&lt;p&gt;Would be good to see some small example.&lt;/p&gt;

&lt;p&gt;What you have now seems ready to commit given the minor changes above, what is next?&lt;/p&gt;

&lt;p&gt;General note:  Wiki like is &lt;a href=&quot;http://cwiki.apache.org/MAHOUT/latent-dirichlet-allocation.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://cwiki.apache.org/MAHOUT/latent-dirichlet-allocation.html&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="12734401" author="dlwh" created="Thu, 23 Jul 2009 01:21:18 +0100"  >&lt;p&gt;Everything fixed except adding an example.&lt;/p&gt;

&lt;p&gt;What&apos;s the best way to include data with Mahout? I&apos;ve never had luck autogenerating data for LDA.&lt;/p&gt;</comment>
                            <comment id="12735558" author="gsingers" created="Mon, 27 Jul 2009 12:02:01 +0100"  >&lt;p&gt;How big is the data?  Either we can put it in examples somewhere (resources, likely) or we can tell people to download it.  Do you have a pointer to it?&lt;/p&gt;</comment>
                            <comment id="12735607" author="gsingers" created="Mon, 27 Jul 2009 14:52:35 +0100"  >&lt;p&gt;Patch review:&lt;/p&gt;

&lt;p&gt;Still could use some more comments in the Mapper/Reducer about what is going on.  &lt;br/&gt;
Also still needs an example.  Note, &lt;a href=&quot;http://people.apache.org/~gsingers/wikipedia/chunks.tar.gz&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://people.apache.org/~gsingers/wikipedia/chunks.tar.gz&lt;/a&gt; contains a few hundred wikipedia articles, perhaps that would be a good reference?  It&apos;s pretty easy to automate the download of those.  Otherwise, we can just point people at them.&lt;/p&gt;</comment>
                            <comment id="12735705" author="dlwh" created="Mon, 27 Jul 2009 19:42:24 +0100"  >&lt;p&gt;Ok, I&apos;ll add more comments. &lt;/p&gt;

&lt;p&gt;I had been using Reuters 21578, but I&apos;m not convinced that it&apos;s ok to include it, and I was looking around for something better. I&apos;ll get the download automated for wikipedia chunks. Is a shell script ok to do most of it?&lt;/p&gt;

&lt;p&gt;&amp;#8211; David&lt;/p&gt;</comment>
                            <comment id="12735707" author="gsingers" created="Mon, 27 Jul 2009 19:46:49 +0100"  >&lt;p&gt;We&apos;ve automated download of Reuters in Lucene.  We are doing research in IR and NLP, IMO.&lt;/p&gt;

&lt;p&gt;I&apos;d like to use Reuters for some classification work, too, so +1 on it.  Otherwise, feel free to tell people they need to download it.  That&apos;s what we do for the Synthetic Control stuff. &lt;/p&gt;</comment>
                            <comment id="12736238" author="dlwh" created="Tue, 28 Jul 2009 20:35:01 +0100"  >&lt;p&gt;So it looks like the way Lucene does it is w/ an ant task. I can&apos;t figure out the maven way to do this, without my building some kind of jar from it. I&apos;m happy to do it, but I&apos;m not sure what the proper way to do this is.&lt;/p&gt;

&lt;p&gt;Thoughts?&lt;/p&gt;

&lt;p&gt;&amp;#8211; David&lt;/p&gt;</comment>
                            <comment id="12736273" author="gsingers" created="Tue, 28 Jul 2009 21:40:23 +0100"  >&lt;p&gt;The Mahout Examples pom.xml has a commented out version that does it for Wikipedia.  I, too, don&apos;t know how to get it too run standalone, as the Antrun task doesn&apos;t let you specify the id to run.  I guess I&apos;d just have the users download it.  Let&apos;s get an example working from the code out, then we&apos;ll figure out how to make it easy to run.&lt;/p&gt;</comment>
                            <comment id="12738115" author="dlwh" created="Sun, 2 Aug 2009 20:07:51 +0100"  >&lt;p&gt;Ok, core/bin/build-reuters will:&lt;/p&gt;

&lt;p&gt;download reuters to work/reuters(something or another), untar it, build an index with it using lucene, convert said index into vectors, run lda for 40 iterations (which is close enough to convergence) to work/lda, and then dump the top 100 words for each topic in into work/topics/topic-K, where K is the topic of interest.&lt;/p&gt;</comment>
                            <comment id="12738154" author="yanenli2" created="Mon, 3 Aug 2009 01:09:15 +0100"  >&lt;p&gt;David,&lt;/p&gt;

&lt;p&gt;cool!&lt;/p&gt;

&lt;p&gt;do you have instructions on how to run it on a Hadoop cluster? I don&apos;t think there is a maven tool installed in the Hadoop cluster, so we need to specify all libs and other java options. &lt;/p&gt;

&lt;p&gt;Since the Hadoop cluster I am working on is of version 0.19.0, is the LDA code&lt;br/&gt;
compatible with this?&lt;/p&gt;

&lt;p&gt;Yanen&lt;/p&gt;</comment>
                            <comment id="12738165" author="dlwh" created="Mon, 3 Aug 2009 02:46:41 +0100"  >&lt;p&gt;I unfortunately haven&apos;t run it on a Hadoop cluster yet. It should &quot;just work&quot; if you run it with the right Hadoop configuration. Shouldn&apos;t running it through the &quot;hadoop&quot; shell script add the configuration?&lt;/p&gt;

&lt;p&gt;I&apos;ll get it running on a hadoop cluster soon.&lt;/p&gt;

&lt;p&gt;The code actually requires Hadoop 0.20, because Mahout has decided to move in that direction.&lt;/p&gt;

&lt;p&gt;&amp;#8211; David&lt;/p&gt;</comment>
                            <comment id="12738399" author="yanenli2" created="Mon, 3 Aug 2009 16:53:18 +0100"  >&lt;p&gt;Now I can create the index using the Lucene program&lt;br/&gt;
An other error occurred when creating vector from index:&lt;/p&gt;

&lt;p&gt;(I am in the utils folder)&lt;/p&gt;


&lt;p&gt;========================================================================================&lt;br/&gt;
Creating vectors from index&lt;/p&gt;

&lt;p&gt;core-job:&lt;br/&gt;
      &lt;span class=&quot;error&quot;&gt;&amp;#91;jar&amp;#93;&lt;/span&gt; Building jar:&lt;br/&gt;
/workspace/Mahout_0.2/core/target/mahout-core-0.2-SNAPSHOT.job&lt;br/&gt;
+ Error stacktraces are turned on.&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Scanning for projects...&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Searching repository for plugin with prefix: &apos;exec&apos;.&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; ------------------------------------------------------------------------&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Building Mahout utilities&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt;    task-segment: &lt;span class=&quot;error&quot;&gt;&amp;#91;exec:java&amp;#93;&lt;/span&gt;&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; ------------------------------------------------------------------------&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Preparing exec:java&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; No goals needed for project - skipping&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;exec:java&amp;#93;&lt;/span&gt;&lt;br/&gt;
09/08/03 08:40:41 INFO vectors.Driver: Output File: ../core/work/vectors&lt;br/&gt;
09/08/03 08:40:41 WARN util.NativeCodeLoader: Unable to load&lt;br/&gt;
native-hadoop library for your platform... using builtin-java classes&lt;br/&gt;
where applicable&lt;br/&gt;
09/08/03 08:40:41 INFO compress.CodecPool: Got brand-new compressor&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; ------------------------------------------------------------------------&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; BUILD ERROR&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; ------------------------------------------------------------------------&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; An exception occured while executing the Java class. null&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; ------------------------------------------------------------------------&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Trace&lt;br/&gt;
org.apache.maven.lifecycle.LifecycleExecutionException: An exception&lt;br/&gt;
occured while executing the Java class. null&lt;br/&gt;
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoals(DefaultLifecycleExecutor.java:583)&lt;br/&gt;
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeStandaloneGoal(DefaultLifecycleExecutor.java:512)&lt;br/&gt;
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoal(DefaultLifecycleExecutor.java:482)&lt;br/&gt;
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoalAndHandleFailures(DefaultLifecycleExecutor.java:330)&lt;br/&gt;
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeTaskSegments(DefaultLifecycleExecutor.java:291)&lt;br/&gt;
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.execute(DefaultLifecycleExecutor.java:142)&lt;br/&gt;
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:336)&lt;br/&gt;
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:129)&lt;br/&gt;
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:287)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:616)&lt;br/&gt;
	at org.codehaus.classworlds.Launcher.launchEnhanced(Launcher.java:315)&lt;br/&gt;
	at org.codehaus.classworlds.Launcher.launch(Launcher.java:255)&lt;br/&gt;
	at org.codehaus.classworlds.Launcher.mainWithExitCode(Launcher.java:430)&lt;br/&gt;
	at org.codehaus.classworlds.Launcher.main(Launcher.java:375)&lt;br/&gt;
Caused by: org.apache.maven.plugin.MojoExecutionException: An&lt;br/&gt;
exception occured while executing the Java class. null&lt;br/&gt;
	at org.codehaus.mojo.exec.ExecJavaMojo.execute(ExecJavaMojo.java:338)&lt;br/&gt;
	at org.apache.maven.plugin.DefaultPluginManager.executeMojo(DefaultPluginManager.java:451)&lt;br/&gt;
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoals(DefaultLifecycleExecutor.java:558)&lt;br/&gt;
	... 16 more&lt;br/&gt;
Caused by: java.lang.reflect.InvocationTargetException&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:616)&lt;br/&gt;
	at org.codehaus.mojo.exec.ExecJavaMojo$1.run(ExecJavaMojo.java:283)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:636)&lt;br/&gt;
Caused by: java.lang.NullPointerException&lt;br/&gt;
	at org.apache.mahout.utils.vectors.lucene.LuceneIteratable$TDIterator.next(LuceneIteratable.java:111)&lt;br/&gt;
	at org.apache.mahout.utils.vectors.lucene.LuceneIteratable$TDIterator.next(LuceneIteratable.java:82)&lt;br/&gt;
	at org.apache.mahout.utils.vectors.io.SequenceFileVectorWriter.write(SequenceFileVectorWriter.java:42)&lt;br/&gt;
	at org.apache.mahout.utils.vectors.Driver.main(Driver.java:204)&lt;br/&gt;
	... 6 more&lt;/p&gt;

&lt;p&gt;=========================================================================================&lt;/p&gt;

&lt;p&gt;Any idea what is going wrong?&lt;/p&gt;

&lt;p&gt;Yanen&lt;/p&gt;</comment>
                            <comment id="12738516" author="dlwh" created="Mon, 3 Aug 2009 20:30:13 +0100"  >&lt;p&gt;Patch fixed for Yanen&apos;s problem. Apparently I messed up the dependencies somehow so that they&apos;d work on my machine, but not anywhere else. Now I think it&apos;s ok. (I nuked my Maven repo.)&lt;/p&gt;

&lt;p&gt;&amp;#8211; David&lt;/p&gt;</comment>
                            <comment id="12738646" author="yanenli2" created="Mon, 3 Aug 2009 23:33:17 +0100"  >&lt;p&gt;Now index is created, vectors are also created without a problem, but&lt;br/&gt;
there still be exceptions in the gson parser when running LDA in the&lt;br/&gt;
stand along mode:&lt;/p&gt;

&lt;p&gt;====================================================================================&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;WARNING&amp;#93;&lt;/span&gt; While downloading easymock:easymockclassextension:2.2&lt;br/&gt;
  This artifact has been relocated to org.easymock:easymockclassextension:2.2.&lt;/p&gt;


&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;exec:java&amp;#93;&lt;/span&gt;&lt;br/&gt;
09/08/03 15:18:35 INFO lda.LDADriver: Iteration 0&lt;br/&gt;
09/08/03 15:18:35 INFO jvm.JvmMetrics: Initializing JVM Metrics with&lt;br/&gt;
processName=JobTracker, sessionId=&lt;br/&gt;
09/08/03 15:18:35 WARN mapred.JobClient: Use GenericOptionsParser for&lt;br/&gt;
parsing the arguments. Applications should implement Tool for the&lt;br/&gt;
same.&lt;br/&gt;
09/08/03 15:18:35 WARN mapred.JobClient: No job jar file set.  User&lt;br/&gt;
classes may not be found. See JobConf(Class) or&lt;br/&gt;
JobConf#setJar(String).&lt;br/&gt;
09/08/03 15:18:35 INFO input.FileInputFormat: Total input paths to process : 1&lt;br/&gt;
09/08/03 15:18:36 INFO input.FileInputFormat: Total input paths to process : 1&lt;br/&gt;
09/08/03 15:18:36 INFO mapred.JobClient: Running job: job_local_0001&lt;br/&gt;
09/08/03 15:18:36 INFO mapred.MapTask: io.sort.mb = 100&lt;br/&gt;
09/08/03 15:18:36 INFO mapred.MapTask: data buffer = 79691776/99614720&lt;br/&gt;
09/08/03 15:18:36 INFO mapred.MapTask: record buffer = 262144/327680&lt;br/&gt;
09/08/03 15:18:37 INFO mapred.JobClient:  map 0% reduce 0%&lt;br/&gt;
09/08/03 15:18:37 WARN mapred.LocalJobRunner: job_local_0001&lt;br/&gt;
com.google.gson.JsonParseException: Failed parsing JSON source:&lt;br/&gt;
java.io.StringReader@4977fa9a to Json&lt;br/&gt;
	at com.google.gson.JsonParser.parse(JsonParser.java:57)&lt;br/&gt;
	at com.google.gson.Gson.fromJson(Gson.java:376)&lt;br/&gt;
	at com.google.gson.Gson.fromJson(Gson.java:329)&lt;br/&gt;
	at org.apache.mahout.matrix.AbstractVector.decodeVector(AbstractVector.java:358)&lt;br/&gt;
	at org.apache.mahout.matrix.AbstractVector.decodeVector(AbstractVector.java:342)&lt;br/&gt;
	at org.apache.mahout.clustering.lda.LDAMapper.map(LDAMapper.java:48)&lt;br/&gt;
	at org.apache.mahout.clustering.lda.LDAMapper.map(LDAMapper.java:39)&lt;br/&gt;
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)&lt;br/&gt;
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:518)&lt;br/&gt;
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:303)&lt;br/&gt;
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:176)&lt;br/&gt;
Caused by: com.google.gson.ParseException: Encountered &quot;SEQ&quot; at line&lt;br/&gt;
1, column 1.&lt;br/&gt;
Was expecting one of:&lt;br/&gt;
    &amp;lt;DIGITS&amp;gt; ...&lt;br/&gt;
    &quot;null&quot; ...&lt;br/&gt;
    &quot;NaN&quot; ...&lt;br/&gt;
    &quot;Infinity&quot; ...&lt;br/&gt;
    &amp;lt;BOOLEAN&amp;gt; ...&lt;br/&gt;
    &amp;lt;SINGLE_QUOTE_LITERAL&amp;gt; ...&lt;br/&gt;
    &amp;lt;DOUBLE_QUOTE_LITERAL&amp;gt; ...&lt;br/&gt;
    &quot;)]}\&apos;\n&quot; ...&lt;br/&gt;
    &quot;{&quot; ...&lt;br/&gt;
    &quot;[&quot; ...&lt;br/&gt;
    &quot;-&quot; ...&lt;/p&gt;

&lt;p&gt;====================================================================================&lt;/p&gt;



&lt;p&gt;Yanen&lt;/p&gt;</comment>
                            <comment id="12738647" author="yanenli2" created="Mon, 3 Aug 2009 23:33:18 +0100"  >&lt;p&gt;Another issue needed to be fixed, in the core DIR, when&lt;br/&gt;
mvn install&lt;/p&gt;

&lt;p&gt;build failed with message:&lt;/p&gt;

&lt;p&gt;===============================================================================&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; BUILD FAILURE&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; ------------------------------------------------------------------------&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Compilation failure&lt;br/&gt;
/home/yaneli/workspace/java/mahout_6/core/src/test/java/org/apache/mahout/clustering/lda/TestMapReduce.java:&lt;span class=&quot;error&quot;&gt;&amp;#91;123,12&amp;#93;&lt;/span&gt;&lt;br/&gt;
cannot find symbol&lt;br/&gt;
symbol  : method&lt;br/&gt;
map(&amp;lt;nulltype&amp;gt;,org.apache.hadoop.io.Text,org.apache.hadoop.mapreduce.Mapper&amp;lt;org.apache.hadoop.io.WritableComparable&amp;lt;?&amp;gt;,org.apache.mahout.matrix.Vector,org.apache.mahout.clustering.lda.IntPairWritable,org.apache.hadoop.io.DoubleWritable&amp;gt;.Context)&lt;br/&gt;
location: class org.apache.mahout.clustering.lda.LDAMapper&lt;/p&gt;



&lt;p&gt;/home/yaneli/workspace/java/mahout_6/core/src/test/java/org/apache/mahout/clustering/lda/TestMapReduce.java:&lt;span class=&quot;error&quot;&gt;&amp;#91;123,12&amp;#93;&lt;/span&gt;&lt;br/&gt;
cannot find symbol&lt;br/&gt;
symbol  : method&lt;br/&gt;
map(&amp;lt;nulltype&amp;gt;,org.apache.hadoop.io.Text,org.apache.hadoop.mapreduce.Mapper&amp;lt;org.apache.hadoop.io.WritableComparable&amp;lt;?&amp;gt;,org.apache.mahout.matrix.Vector,org.apache.mahout.clustering.lda.IntPairWritable,org.apache.hadoop.io.DoubleWritable&amp;gt;.Context)&lt;br/&gt;
location: class org.apache.mahout.clustering.lda.LDAMapper&lt;/p&gt;


&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; ------------------------------------------------------------------------&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; For more information, run Maven with the -e switch&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; ------------------------------------------------------------------------&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Total time: 21 seconds&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Finished at: Mon Aug 03 15:23:02 PDT 2009&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Final Memory: 34M/202M&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; ------------------------------------------------------------------------&lt;/p&gt;

&lt;p&gt;========================================================================================&lt;/p&gt;</comment>
                            <comment id="12738977" author="gsingers" created="Tue, 4 Aug 2009 14:53:09 +0100"  >&lt;p&gt;Moved bin/ to examples directory,  Added ASL to some headers.  Ran the test, which seems to go fine, but it didn&apos;t output any topics.&lt;/p&gt;

&lt;p&gt;To run, the instructions are now:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;cd &amp;lt;MAHOUT_HOME&amp;gt;/examples&lt;br/&gt;
bin/build-reuters.sh&lt;/p&gt;&lt;/blockquote&gt;</comment>
                            <comment id="12740828" author="dlwh" created="Sat, 8 Aug 2009 04:20:15 +0100"  >&lt;p&gt;The problem was that the edited patch wrote topics to .../examples/ and not ../examples , which took a frustratingly long time to figure out.&lt;/p&gt;

&lt;p&gt;I also played with the parameters a little longer. The topics aren&apos;t as great as I&apos;d like, but it&apos;s because I haven&apos;t figured out the right setting for getting rid of stop words. &quot;could&quot; and &quot;said&quot; are still in there. That said, they&apos;re mostly coherent topics, if kind of boring.&lt;/p&gt;

&lt;p&gt;&amp;#8211; David&lt;/p&gt;</comment>
                            <comment id="12741064" author="gsingers" created="Sun, 9 Aug 2009 12:03:02 +0100"  >&lt;blockquote&gt;&lt;p&gt;The problem was that the edited patch wrote topics to .../examples/ and not ../examples , which took a frustratingly long time to figure out.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ouch.  I will try it out.  &lt;/p&gt;</comment>
                            <comment id="12741084" author="gsingers" created="Sun, 9 Aug 2009 15:18:53 +0100"  >&lt;p&gt;Success!&lt;/p&gt;

&lt;p&gt;I will look to commit soon.  Good job, David!&lt;/p&gt;</comment>
                            <comment id="12741100" author="gsingers" created="Sun, 9 Aug 2009 17:02:49 +0100"  >&lt;p&gt;Agreed that better stopword handling should improve the results, but the brain quickly filters out those anyway.&lt;/p&gt;</comment>
                            <comment id="12744057" author="gsingers" created="Mon, 17 Aug 2009 14:33:27 +0100"  >&lt;p&gt;Committed revision 804979.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12415920" name="MAHOUT-123.patch" size="60450" author="dlwh" created="Sat, 8 Aug 2009 04:20:15 +0100"/>
                            <attachment id="12415483" name="MAHOUT-123.patch" size="61162" author="gsingers" created="Tue, 4 Aug 2009 14:53:09 +0100"/>
                            <attachment id="12415388" name="MAHOUT-123.patch" size="59265" author="dlwh" created="Mon, 3 Aug 2009 20:30:13 +0100"/>
                            <attachment id="12415298" name="MAHOUT-123.patch" size="58838" author="dlwh" created="Sun, 2 Aug 2009 20:07:51 +0100"/>
                            <attachment id="12414287" name="MAHOUT-123.patch" size="46850" author="dlwh" created="Thu, 23 Jul 2009 01:21:18 +0100"/>
                            <attachment id="12412344" name="MAHOUT-123.patch" size="43721" author="dlwh" created="Thu, 2 Jul 2009 02:04:12 +0100"/>
                            <attachment id="12412093" name="MAHOUT-123.patch" size="34297" author="dlwh" created="Mon, 29 Jun 2009 19:19:20 +0100"/>
                            <attachment id="12412091" name="MAHOUT-123.patch" size="63" author="dlwh" created="Mon, 29 Jun 2009 19:02:42 +0100"/>
                            <attachment id="12411196" name="MAHOUT-123.patch" size="33965" author="dlwh" created="Fri, 19 Jun 2009 09:00:15 +0100"/>
                            <attachment id="12411141" name="MAHOUT-123.patch" size="26530" author="dlwh" created="Thu, 18 Jun 2009 22:13:38 +0100"/>
                            <attachment id="12410850" name="lda.patch" size="27584" author="dlwh" created="Tue, 16 Jun 2009 21:50:35 +0100"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>11.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Mon, 29 Jun 2009 13:05:59 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9942</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hxy727:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>23295</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                </customfields>
    </item>
</channel>
</rss>