<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 04:22:00 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/MAHOUT-1464/MAHOUT-1464.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[MAHOUT-1464] Cooccurrence Analysis on Spark</title>
                <link>https://issues.apache.org/jira/browse/MAHOUT-1464</link>
                <project id="12310751" key="MAHOUT">Mahout</project>
                    <description>&lt;p&gt;Create a version of Cooccurrence Analysis (RowSimilarityJob with LLR) that runs on Spark. This should be compatible with Mahout Spark DRM DSL so a DRM can be used as input. &lt;/p&gt;

&lt;p&gt;Ideally this would extend to cover &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1422&quot; title=&quot;Make a version of RSJ that uses two inputs&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1422&quot;&gt;&lt;del&gt;MAHOUT-1422&lt;/del&gt;&lt;/a&gt;. This cross-cooccurrence has several applications including cross-action recommendations. &lt;/p&gt;</description>
                <environment>&lt;p&gt;hadoop, spark&lt;/p&gt;</environment>
        <key id="12701768">MAHOUT-1464</key>
            <summary>Cooccurrence Analysis on Spark</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="10">Implemented</resolution>
                                        <assignee username="pferrel">Pat Ferrel</assignee>
                                    <reporter username="pferrel">Pat Ferrel</reporter>
                        <labels>
                            <label>DSL</label>
                            <label>scala</label>
                            <label>spark</label>
                    </labels>
                <created>Sun, 16 Mar 2014 19:43:28 +0000</created>
                <updated>Mon, 13 Apr 2015 10:57:15 +0100</updated>
                            <resolved>Wed, 18 Mar 2015 13:51:54 +0000</resolved>
                                                    <fixVersion>0.10.0</fixVersion>
                                    <component>Collaborative Filtering</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>12</watches>
                                                                <comments>
                            <comment id="13937490" author="ssc" created="Mon, 17 Mar 2014 06:16:13 +0000"  >&lt;p&gt;I&apos;ve started to work on this.&lt;/p&gt;</comment>
                            <comment id="13937925" author="pferrel" created="Mon, 17 Mar 2014 15:32:55 +0000"  >&lt;p&gt;Good news. At the risk of asking for too much, what are your thoughts on the XRSJ,  &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1422&quot; title=&quot;Make a version of RSJ that uses two inputs&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1422&quot;&gt;&lt;del&gt;MAHOUT-1422&lt;/del&gt;&lt;/a&gt;? Speaking for myself I&apos;d rather have &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1464&quot; title=&quot;Cooccurrence Analysis on Spark&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1464&quot;&gt;&lt;del&gt;MAHOUT-1464&lt;/del&gt;&lt;/a&gt; than nothing.&lt;/p&gt;

&lt;p&gt;Also for testing would you recommend Hadoop 2 for Spark? it seems to be their recommended setup but I have 1.2.1 now.&lt;/p&gt;</comment>
                            <comment id="13937951" author="dlyubimov" created="Mon, 17 Mar 2014 16:04:08 +0000"  >&lt;p&gt;I only ever ran spark code with hdfs cluster of cdh 4. Mapreduce api is irrelevant, which is where most of 2.0 vs 1 thing happens, only hdfs is, since spark doesnt need mr cluster. Spark can also run under yarn supervision, which would imply 2.0, but i would strongly recommend against it and use mesos plus zookeeper.&lt;/p&gt;</comment>
                            <comment id="13937954" author="dlyubimov" created="Mon, 17 Mar 2014 16:04:43 +0000"  >&lt;p&gt;Ps spark module has cdh4 maven profile.&lt;/p&gt;</comment>
                            <comment id="13937987" author="ssc" created="Mon, 17 Mar 2014 16:29:27 +0000"  >&lt;p&gt;@Pat I&apos;m pretty busy with non-Mahout stuff until end of April, not sure how far I can get with this until then, unfortunately.&lt;/p&gt;</comment>
                            <comment id="13938020" author="pferrel" created="Mon, 17 Mar 2014 16:52:47 +0000"  >&lt;p&gt;So am I so no problem.&lt;/p&gt;

&lt;p&gt;My plan is to update the Solr-recommender contrib with Spark for calculation of the indicator/similarity matrix. For the one action recommender this only needs RSJ, for the two action recommender it means matrix transpose and multiply OR XRSJ. The PreparePreferenceMatrixJob and its analogy, PrepareActionMatricesJob, will stay in plain old hadoop for now. Not sure there is much benefit moving a dataflow process to Spark&lt;/p&gt;</comment>
                            <comment id="13938254" author="ssc" created="Mon, 17 Mar 2014 19:28:26 +0000"  >&lt;p&gt;I havent tested Spark on Hadoop yet, I ran it standalone in the cluster and had it read data from HDFS.&lt;/p&gt;</comment>
                            <comment id="13938259" author="ssc" created="Mon, 17 Mar 2014 19:31:17 +0000"  >&lt;p&gt;I&apos;d like to rework my prototype from directly using Spark to using Dmitriy&apos;s DSL, but there&apos;s a few operators and mechanics that I need to add.&lt;/p&gt;</comment>
                            <comment id="13938333" author="pferrel" created="Mon, 17 Mar 2014 20:28:01 +0000"  >&lt;p&gt;OK, refreshed the repo and now I see all the Spark/Scala stuff.&lt;/p&gt;

&lt;p&gt;Not sure what you mean by &quot;standalone in the cluster&quot;? Just getting up to speed on Spark and they describe integrating with hadoop. I was asking because I need to set up a clustered environment and really only have one cluster so Spark and Hadoop will coexist on the the same machines. I&apos;ll probably stay on Hadoop 1.2.1 as Dimitriy suggests.&lt;/p&gt;

&lt;p&gt;So you plan to add to D&apos;s PDF doc? If so maybe it would be good to check it in or add it to the Github wiki or cwiki. Even if it&apos;s only a link to the PDF we&apos;d all know where to look for the latest.&lt;/p&gt;</comment>
                            <comment id="13938634" author="ssc" created="Tue, 18 Mar 2014 01:07:45 +0000"  >&lt;p&gt;Luckily, Dmitriy&apos;s latest commit solved most of my problems as it provides the machinery to compute column sums and broadcast drms. Awesome work, Dmitriy.&lt;/p&gt;

&lt;p&gt;Here is a first hacky version of cooccurrence analysis with downsampling and LLR using Dmitriy&apos;s new DSL.&lt;/p&gt;

&lt;p&gt;Successfully tested on the movielens1M dataset on my laptop &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13938799" author="dlyubimov" created="Tue, 18 Mar 2014 04:31:10 +0000"  >&lt;p&gt;What&apos;s the best way to share PDF source? i can put it on the site so committers can re-generate it. otherwise, its source right now in my github doc branch here and pull request is definitely possible way to collaborate too: &lt;a href=&quot;https://github.com/dlyubimov/mahout-commits/tree/ssvd-docs&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/dlyubimov/mahout-commits/tree/ssvd-docs&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13938805" author="dlyubimov" created="Tue, 18 Mar 2014 04:41:21 +0000"  >&lt;p&gt;1.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
val C = A.t %*% A
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt; I don&apos;t remember if i actually put in the physical operator for non-skinny A. There are two distinct algorithms to deal with it. Skinny one (n &amp;lt;= 5000 or something) uses upper-triangular vector-backed accumulator to combine stuff right in map. Of course if accumulator does not realistically fit in memory then another algorithm has to be plugged in for A-squared. See AtA.scala, def at_a_nongraph(). It currently throws UnsupportedOperation (but everything i have done so far only uses slim A&apos;A)&lt;/p&gt;

&lt;p&gt;2. when using partial functions with mapBlock, you actually do not have to use (&lt;/p&gt;
{...}
&lt;p&gt;) but just { }:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
      drmBt = drmBt.mapBlock() {
        &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; (keys, block) =&amp;gt;
&lt;span class=&quot;code-comment&quot;&gt;//...
&lt;/span&gt;          keys -&amp;gt; block
      }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13938810" author="dlyubimov" created="Tue, 18 Mar 2014 04:48:44 +0000"  >&lt;p&gt;Also, just FYI, much as i love to use single-letter capitals for matrices, it turns out Scala is not really  ingesting it in all situtations. For example, &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
val (U, V, s) = ssvd(...)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;doesn&apos;t compile. &lt;/p&gt;

&lt;p&gt;So i ended up using, perhaps verbosely, drmA and inCoreA notations. &lt;br/&gt;
Perhaps we can agree on what&apos;s reasonable .&lt;/p&gt;</comment>
                            <comment id="13938953" author="ssc" created="Tue, 18 Mar 2014 08:23:39 +0000"  >&lt;p&gt;Updated patch to match the coding conventions and use RLikeOps where possible&lt;/p&gt;</comment>
                            <comment id="13938958" author="ssc" created="Tue, 18 Mar 2014 08:25:49 +0000"  >&lt;p&gt;The physical operator for non-skinny A&apos;A is not yet implemented. For my tests with movielens1M, I forced the execution of the in-memory one by setting &lt;b&gt;mahout.math.AtA.maxInMemNCol&lt;/b&gt; to an appropriate value.&lt;/p&gt;</comment>
                            <comment id="13939420" author="pferrel" created="Tue, 18 Mar 2014 16:00:43 +0000"  >&lt;p&gt;PDF in the repo is fine by me.&lt;/p&gt;

&lt;p&gt;Can the patches just be branches in a git repo? That&apos;s really what a branch is after all. When I make changes to Mahout I fork it, make changes in a branch with apache/mahout as the upstream repo (you guys wouldn&apos;t even need to fork Mahout, just stay in the branch). That would make it super easy for everyone to follow changes to the &quot;patch&quot; by just pulling the latest from your branch.&lt;/p&gt;

&lt;p&gt;With any luck these may be the FIRST Mahout jobs people use in a few years so I wouldn&apos;t assume they are already familiar with the in-memory code, hadoop jobs, the math literature, or R for that matter. Just saying that you may want consider the future audience.&lt;/p&gt;</comment>
                            <comment id="13939451" author="dlyubimov" created="Tue, 18 Mar 2014 16:32:35 +0000"  >&lt;p&gt;That&apos;s what i normally do, yes. The scalabindings issue points to a branch in github. Then there&apos;s commit-squash method (described in my blog) i do when pushing to svn. Hopefully we&apos;d see direct git pushes for mahout sooner rather than later.&lt;br/&gt;
However, seeing a combined (&quot;squashed&quot;) patch is pretty useful too as apposed to tons of indivdiual commits.&lt;/p&gt;</comment>
                            <comment id="13939468" author="dlyubimov" created="Tue, 18 Mar 2014 16:56:22 +0000"  >&lt;p&gt;@&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ssc&quot; class=&quot;user-hover&quot; rel=&quot;ssc&quot;&gt;Sebastian Schelter&lt;/a&gt; Looking nice.&lt;/p&gt;

&lt;p&gt;I guess we want non-skinny version of operator A&apos;A still, i may be able to look into it.&lt;/p&gt;</comment>
                            <comment id="13939471" author="pferrel" created="Tue, 18 Mar 2014 16:57:47 +0000"  >&lt;p&gt;Since there are potentially commits by D and S around Spark, what&apos;s the best way to track? I assume only RSJ is an issue since the rest will go to the trunk if changed?&lt;/p&gt;

&lt;p&gt;Sebastian, do you plan to use git or update patches on this issue?&lt;/p&gt;

&lt;p&gt;Dimitriy, can you send me a link to your blog post. I assume you use something like git diff to do the squashed patch, yes very helpful.&lt;/p&gt;</comment>
                            <comment id="13939513" author="dlyubimov" created="Tue, 18 Mar 2014 17:35:47 +0000"  >
&lt;p&gt;&lt;a href=&quot;http://weatheringthrutechdays.blogspot.com/2011/04/git-github-and-committing-to-asf-svn.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://weatheringthrutechdays.blogspot.com/2011/04/git-github-and-committing-to-asf-svn.html&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13941355" author="dlyubimov" created="Thu, 20 Mar 2014 03:29:13 +0000"  >&lt;p&gt;Actually, non-slim A&apos;A operator is practically A&apos;B without need for a zip... So we are almost done, the biggest work here is the test I suppose.&lt;/p&gt;</comment>
                            <comment id="13941500" author="ssc" created="Thu, 20 Mar 2014 07:55:41 +0000"  >&lt;p&gt;replaced &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;matrix.viewRow(i)&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt; calls with nicer looking &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;matrix(i,::)&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt; access&lt;/p&gt;</comment>
                            <comment id="13941511" author="dlyubimov" created="Thu, 20 Mar 2014 08:29:07 +0000"  >&lt;p&gt;yeah .. those views.. i think they create at least 2 objects interim... not so cool for mass iterations. Oh well.&lt;/p&gt;</comment>
                            <comment id="13941516" author="ssc" created="Thu, 20 Mar 2014 08:41:31 +0000"  >&lt;p&gt;In a SparseRowMatrix, this is only an array access, can we intelligently use a SparseRowMatrix instead of SparseMatrix in such cases? &lt;/p&gt;</comment>
                            <comment id="13941517" author="dlyubimov" created="Thu, 20 Mar 2014 08:43:55 +0000"  >&lt;p&gt;I have non-slim A&apos;A. Of course slim operator implementation is upper triangular that cuts outer product computation cost two times in comparison... Significantly wide A&apos;A on the other hand cannot really apply the same cut, since it needs to form rows in distributed way.&lt;/p&gt;

&lt;p&gt;Not surprisingly, slim test takes 17 seconds  and the &quot;fat&quot; one takes 21 seconds on my fairly ancient computer for squaring 400x550 matrix (single thread). Actually, i expected a little more significant gap.&lt;/p&gt;

&lt;p&gt; I wonder if there&apos;s a more interesting way to do this other than forming outer product vertical blocks.&lt;/p&gt;

&lt;p&gt;Maybe I need to use square blocks. In this case i can reuse roughly half of them &amp;#8211; but then there will be significantly more objects with this (albeit smaller in size). and then i will have to have an extra shuffle operation to form the lower triangular part of the matrix still. &lt;/p&gt;

&lt;p&gt;Anyway. i think i will commit what i have.&lt;/p&gt;</comment>
                            <comment id="13941520" author="dlyubimov" created="Thu, 20 Mar 2014 08:47:45 +0000"  >&lt;p&gt;On Thu, Mar 20, 2014 at 1:42 AM, Sebastian Schelter (JIRA)&lt;/p&gt;

&lt;p&gt;For that very reason, i almost always use SRM and almost never SM.&lt;/p&gt;

&lt;p&gt;What i really would probably love is a sparse row and column block (hash&lt;br/&gt;
hanging from hash), this seems like recurring issue in blocking&lt;br/&gt;
calculations such as ALS. SRM does always that, except it uses full size&lt;br/&gt;
vector to hang sprase vectors off.&lt;/p&gt;

</comment>
                            <comment id="13941521" author="ssc" created="Thu, 20 Mar 2014 08:52:46 +0000"  >&lt;p&gt;one possibility would be to allow users to give a &quot;hint&quot; to mapBlock operations, so that the underlying blockify call can choose an appropriate representation. Would that make sense?&lt;/p&gt;</comment>
                            <comment id="13941531" author="dlyubimov" created="Thu, 20 Mar 2014 09:06:39 +0000"  >&lt;p&gt;No, i think blockify is fine. it probably can run a bit faster than it does, but oh well. &lt;/p&gt;

&lt;p&gt;And mapblock doesn&apos;t trigger it (or, rather, it is evaluated lazily; and if previous operator already produced blocks, then blockify is not used). what i was saying is along the lines of A&apos;A computation. There&apos;s a structure that is used to fuse operators, which is sort of &quot;eitherOr&quot; of either DrmRdd or BlockifiedDrmRdd type. I can to conclusion that there are operators that are absolute pain to implement on blocks, and there are that would be pain to implement on row vector bags. But blocks can be presented as row bags via viewing, so conversion to blocks happens only if subsequent operator requires it. What&apos;s more, usually block operator outputs blocks as well and vice versa, so realistically blockify happens not so often at all.&lt;/p&gt;

&lt;p&gt;Another caveat is that one has to be careful with map blocks with side effects on RDD of origin. Even though Spark says all RDDs are immutable, side effects will stay visible to parent RDDs if they are cached as MEMORY_ONLY or MEMORY_AND_DISK (i.e. without mandatory clone-via-serialization in block manager) and then subsequently used as a source again.&lt;/p&gt;</comment>
                            <comment id="13941541" author="dlyubimov" created="Thu, 20 Mar 2014 09:21:35 +0000"  >&lt;p&gt;Oh, you mean in case of sparse row vectors.&lt;br/&gt;
 You are probably right. indeed, there&apos;s currently a SparseMatrix there in this case. I think it should be SparseRowMatrix of course. most of the cases should benefit from it. Problem is, like i said, mapblock doesn&apos;t really form it; nor any other physical operator has any knowledge what formed it. &lt;/p&gt;

&lt;p&gt;It is possible to optimize the entire operator fusion chain based on subsequent operator preferred type, that&apos;s actually a very neat idea for in-core speed optimization; but i have no capacity to pursue this technique at the moment. It needs some digestion anyway (at least on my end). It requires experiments with in-core operations. At the first glance, most non-multiplicative operators would be ok with row-wise matrix, as well as deblockifying views.&lt;/p&gt;</comment>
                            <comment id="13941545" author="dlyubimov" created="Thu, 20 Mar 2014 09:25:11 +0000"  >&lt;p&gt;If anything, at least i see non-negligible speed up in the blockification itself it seems once i use row matrix. I think i will commit that. &lt;/p&gt;</comment>
                            <comment id="13941561" author="hudson" created="Thu, 20 Mar 2014 09:47:01 +0000"  >&lt;p&gt;SUCCESS: Integrated in Mahout-Quality #2530 (See &lt;a href=&quot;https://builds.apache.org/job/Mahout-Quality/2530/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/Mahout-Quality/2530/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1464&quot; title=&quot;Cooccurrence Analysis on Spark&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1464&quot;&gt;&lt;del&gt;MAHOUT-1464&lt;/del&gt;&lt;/a&gt; &apos;Fat&apos; nongraph physical A&apos;A.&lt;br/&gt;
Refactored decompositions package out of drm package.&lt;br/&gt;
Added some kryo and akka related properties to the test setup. (dlyubimov: rev 1579565)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/mahout/trunk/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/AtA.scala&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/AtB.scala&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/spark/src/main/scala/org/apache/mahout/sparkbindings/decompositions&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/spark/src/main/scala/org/apache/mahout/sparkbindings/decompositions/DQR.scala&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/spark/src/main/scala/org/apache/mahout/sparkbindings/decompositions/DSPCA.scala&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/spark/src/main/scala/org/apache/mahout/sparkbindings/decompositions/DSSVD.scala&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/decompositions/DQR.scala&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/decompositions/DSPCA.scala&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/decompositions/DSSVD.scala&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/spark/src/main/scala/org/apache/mahout/sparkbindings/io/WritableKryoSerializer.scala&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/spark/src/main/scala/org/apache/mahout/sparkbindings/package.scala&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/spark/src/test/scala/org/apache/mahout/sparkbindings/decompositions&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/spark/src/test/scala/org/apache/mahout/sparkbindings/decompositions/MathSuite.scala&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/spark/src/test/scala/org/apache/mahout/sparkbindings/drm/RLikeDrmOpsSuite.scala&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/spark/src/test/scala/org/apache/mahout/sparkbindings/drm/decompositions/MathSuite.scala&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/spark/src/test/scala/org/apache/mahout/sparkbindings/test/MahoutLocalContext.scala&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13941604" author="hudson" created="Thu, 20 Mar 2014 10:39:44 +0000"  >&lt;p&gt;SUCCESS: Integrated in Mahout-Quality #2531 (See &lt;a href=&quot;https://builds.apache.org/job/Mahout-Quality/2531/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/Mahout-Quality/2531/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1464&quot; title=&quot;Cooccurrence Analysis on Spark&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1464&quot;&gt;&lt;del&gt;MAHOUT-1464&lt;/del&gt;&lt;/a&gt; using RowMatrix with blockify() (dlyubimov: rev 1579574)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/mahout/trunk/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/package.scala&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13944543" author="kanjilal" created="Sun, 23 Mar 2014 19:17:43 +0000"  >&lt;p&gt;Sebastien,&lt;br/&gt;
Can I help out on this issue or are you actively planning to fix on your own?&lt;/p&gt;</comment>
                            <comment id="13944545" author="ssc" created="Sun, 23 Mar 2014 19:25:06 +0000"  >&lt;p&gt;Would be awesome if you could take the patch and test it on a large dataset on a cluster. So far I have only locally tested it.&lt;/p&gt;</comment>
                            <comment id="13944622" author="kanjilal" created="Sun, 23 Mar 2014 22:58:20 +0000"  >&lt;p&gt;We have a cluster at work , however I&apos;m not sure if I can actually use it, do we have an AWS account for mahout that I can use, I think this might be a good opportunity for us to get one for mahout.&lt;/p&gt;</comment>
                            <comment id="13944673" author="pferrel" created="Mon, 24 Mar 2014 00:39:21 +0000"  >&lt;p&gt;Adding 16 cores to my closet&apos;s cluster next week. Is there a &apos;large&apos; dataset you have in mind? I have one with 4000 rows, 75,000 columns and 700,000 values but that seems smallish. Can&apos;t say when I&apos;ll get to it but it&apos;s on my list. If someone can jump in quicker--have at it.&lt;/p&gt;</comment>
                            <comment id="13944710" author="kanjilal" created="Mon, 24 Mar 2014 03:21:39 +0000"  >&lt;p&gt;+1 on Andrew&apos;s suggestion on using AWS to do this.  Andrew is it possible to have a shared account so mahout contributors can use this, I &apos;d even be willing to chip in donations &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; to have a shared AWS account&lt;/p&gt;</comment>
                            <comment id="13944743" author="andrew.musselman" created="Mon, 24 Mar 2014 04:45:13 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=kanjilal&quot; class=&quot;user-hover&quot; rel=&quot;kanjilal&quot;&gt;Saikat Kanjilal&lt;/a&gt; Not sure how to make a shared account work but do you guys have a scripted test sequence you could share?  I&apos;d be happy to spin up an EMR instance and try it out.&lt;/p&gt;</comment>
                            <comment id="13944755" author="dlyubimov" created="Mon, 24 Mar 2014 05:08:33 +0000"  >&lt;blockquote&gt;&lt;p&gt;Adding 16 cores to my closet&apos;s cluster next week. Is there a &apos;large&apos; dataset you have in mind? I have one with 4000 rows, 75,000 columns and 700,000 values but that seems smallish. Can&apos;t say when I&apos;ll get to it but it&apos;s on my list. If someone can jump in quicker--have at it.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;@Sebastian, actually matrix squaring is incredibly expensive &amp;#8211; size ^1.5 for the flops alone. Did your original version also used matrix squaring? How did it fare?&lt;/p&gt;

&lt;p&gt;Also, since the flops grow power-law w.r.t input size (it is a problem for ssvd, too) we may need to contemplate a technique that creates finer splits for such computations based on input size. It very well may be the case that original hdfs splits may turn out to be too large for adequate load redistribution.&lt;/p&gt;

&lt;p&gt;Technically, it is extremely simple &amp;#8211; we&apos;d just have to insert a physical operator tweaking RDD splits via &quot;shuffless&quot; coalesce() which also costs nothing in Spark. However, i am not sure what would be sensible API for this &amp;#8211; automatic, semi-automatic cost-based...  &lt;/p&gt;

&lt;p&gt;I guess one brainless thing to do is to parameterize optimizer context with desired parallelism (~cluster task capacity) and have optimizer to insert physical opertors that very # of partitions and do automatic shuffless coalesce if the number is too low&lt;/p&gt;

&lt;p&gt;any thoughts?&lt;/p&gt;</comment>
                            <comment id="13944773" author="ssc" created="Mon, 24 Mar 2014 06:06:08 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=pferrel&quot; class=&quot;user-hover&quot; rel=&quot;pferrel&quot;&gt;Pat Ferrel&lt;/a&gt; I planned to test the implementation on the R2 dataset from Yahoo Webscope &lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; with 700M interactions. I regularly use that in the papers I write. You have to do a little paperwork with Yahoo to get access however.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dlyubimov&quot; class=&quot;user-hover&quot; rel=&quot;dlyubimov&quot;&gt;Dmitriy Lyubimov&lt;/a&gt; The original implementation also uses matrix squaring. The input matrix is row-partitioned and mappers compute outer products of rows, emitting the resulting matrices row wise and reducers sum those up. The downsampling which is applied beforehand helps with cutting down the costs of that multiplicatio&lt;/p&gt;


&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; &lt;a href=&quot;http://webscope.sandbox.yahoo.com/catalog.php?datatype=r&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://webscope.sandbox.yahoo.com/catalog.php?datatype=r&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13945306" author="pferrel" created="Mon, 24 Mar 2014 16:26:21 +0000"  >&lt;p&gt;I tried by you have to have a .edu or university email address. If the license allows you can always put it where you put the last data set. In the meantime I&apos;ll see if I still have an alum address. &lt;/p&gt;</comment>
                            <comment id="13945337" author="pferrel" created="Mon, 24 Mar 2014 16:51:06 +0000"  >&lt;p&gt;OK, I do have an alum address but it takes some time to set up.&lt;/p&gt;</comment>
                            <comment id="13966300" author="ssc" created="Fri, 11 Apr 2014 08:13:40 +0100"  >&lt;p&gt;Updated patch that now also contains cross-co-occurrence analysis. Includes an example using the epinions dataset which consists of user-item-ratings and a user-user trust network.&lt;/p&gt;

&lt;p&gt;Has only been tested locally at this point.&lt;/p&gt;</comment>
                            <comment id="13966597" author="pferrel" created="Fri, 11 Apr 2014 15:50:32 +0100"  >&lt;p&gt;These?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://konect.uni-koblenz.de/networks/epinions-rating&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://konect.uni-koblenz.de/networks/epinions-rating&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;http://konect.uni-koblenz.de/networks/epinions&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://konect.uni-koblenz.de/networks/epinions&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13966639" author="ssc" created="Fri, 11 Apr 2014 16:20:19 +0100"  >&lt;p&gt;I used those files here:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.trustlet.org/datasets/downloaded_epinions/ratings_data.txt.bz2&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.trustlet.org/datasets/downloaded_epinions/ratings_data.txt.bz2&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;http://www.trustlet.org/datasets/downloaded_epinions/trust_data.txt.bz2&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.trustlet.org/datasets/downloaded_epinions/trust_data.txt.bz2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;RunCrossCooccurrenceAnalysisOnEpinions in org.apache.mahout.cf.examples.Recommendations.scala shows how to run cross-co-occurrence on these files (code in the patch).&lt;/p&gt;</comment>
                            <comment id="13966817" author="tdunning" created="Fri, 11 Apr 2014 18:18:56 +0100"  >&lt;p&gt;Is there any strong reason to use the similarity form for LLR here?  It obscures what is going one and the only real reason is to invert the sort order and match code that isn&apos;t being used here.&lt;/p&gt;
</comment>
                            <comment id="13967453" author="ssc" created="Sat, 12 Apr 2014 10:18:16 +0100"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tdunning&quot; class=&quot;user-hover&quot; rel=&quot;tdunning&quot;&gt;Ted Dunning&lt;/a&gt; good catch, the similarity form is just an artifact from the old codebase, removed it and updated the patch&lt;/p&gt;</comment>
                            <comment id="13967858" author="ssc" created="Sun, 13 Apr 2014 15:55:27 +0100"  >&lt;p&gt;Updated patch. Removed the similarity form of LLR, removed math specific code that was adressed in &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1508&quot; title=&quot;Performance problems with sparse matrices&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1508&quot;&gt;&lt;del&gt;MAHOUT-1508&lt;/del&gt;&lt;/a&gt;, added nicer output and did a few cosmetic changes.&lt;/p&gt;

&lt;p&gt;I think this code is ready to be tested on a cluster, does anybody have time for that?&lt;/p&gt;</comment>
                            <comment id="13967872" author="pferrel" created="Sun, 13 Apr 2014 16:58:07 +0100"  >&lt;p&gt;I have some time this week so working on it. &lt;/p&gt;</comment>
                            <comment id="13968537" author="pferrel" created="Mon, 14 Apr 2014 18:12:06 +0100"  >&lt;p&gt;OK, I have a cluster set up but first tried locally on my laptop. I installed the latest Spark 0.9.1 (not 0.9.0 called for in the pom assuming this is OK), which uses Scala 2.10. BTW the object RunCrossCooccurrenceAnalysisOnEpinions has an incorrect comment println about usage--wrong object name. I never get the printlns, I assume because I&apos;m not launching from the Spark shell??? &lt;/p&gt;

&lt;p&gt;      println(&quot;Usage: RunCooccurrenceAnalysisOnMovielens1M &amp;lt;path-to-dataset-folder&amp;gt;&quot;)&lt;/p&gt;

&lt;p&gt;This leads me to believe that you launch from the Spark Scala shell?? Anyway I tried the method called out in the Spark docs for CLI execution shown below and execute RunCrossCooccurrenceAnalysisOnEpinions via a bash script. Not sure where to look for output. The code says:&lt;/p&gt;

&lt;p&gt;        RecommendationExamplesHelper.saveIndicatorMatrix(indicatorMatrices(0),&lt;br/&gt;
        &quot;/tmp/co-occurrence-on-epinions/indicators-item-item/&quot;)&lt;br/&gt;
    RecommendationExamplesHelper.saveIndicatorMatrix(indicatorMatrices(1),&lt;br/&gt;
        &quot;/tmp/co-occurrence-on-epinions/indicators-trust-item/&quot;)&lt;/p&gt;

&lt;p&gt;Assume this in localfs since the data came from there? I see the Spark pids there but no temp data.&lt;/p&gt;

&lt;p&gt;Here&apos;s how I ran it.&lt;/p&gt;

&lt;p&gt;Put data in localfs:&lt;br/&gt;
Maclaurin:mahout pat$ ls -al ~/hdfs-mirror/xrsj/&lt;br/&gt;
total 29320&lt;br/&gt;
drwxr-xr-x   4 pat  staff      136 Apr 14 09:01 .&lt;br/&gt;
drwxr-xr-x  10 pat  staff      340 Apr 14 09:00 ..&lt;br/&gt;
&lt;del&gt;rw-r&lt;/del&gt;&lt;del&gt;r&lt;/del&gt;-   1 pat  staff  8650128 Apr 14 09:01 ratings_data.txt&lt;br/&gt;
&lt;del&gt;rw-r&lt;/del&gt;&lt;del&gt;r&lt;/del&gt;-   1 pat  staff  6357397 Apr 14 09:01 trust_data.txt&lt;/p&gt;

&lt;p&gt;Start up Spark on localhost, webUI says all is well.&lt;/p&gt;

&lt;p&gt;Run the xrsj on local data via shell script attached.&lt;/p&gt;

&lt;p&gt;The driver runs and creates a worker, which runs for quite awhile but the log says there was an ERROR.&lt;/p&gt;

&lt;p&gt;Maclaurin:mahout pat$ cat /Users/pat/spark-0.9.1-bin-hadoop1/sbin/../logs/spark-pat-org.apache.spark.deploy.worker.Worker-1-&lt;br/&gt;
spark-pat-org.apache.spark.deploy.worker.Worker-1-Maclaurin.local.out    spark-pat-org.apache.spark.deploy.worker.Worker-1-Maclaurin.local.out.2&lt;br/&gt;
spark-pat-org.apache.spark.deploy.worker.Worker-1-Maclaurin.local.out.1  spark-pat-org.apache.spark.deploy.worker.Worker-1-occam4.out&lt;br/&gt;
Maclaurin:mahout pat$ cat /Users/pat/spark-0.9.1-bin-hadoop1/sbin/../logs/spark-pat-org.apache.spark.deploy.worker.Worker-1-Maclaurin.local.out&lt;br/&gt;
Spark Command: /System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Home/bin/java -cp :/Users/pat/spark-0.9.1-bin-hadoop1/conf:/Users/pat/spark-0.9.1-bin-hadoop1/assembly/target/scala-2.10/spark-assembly_2.10-0.9.1-hadoop1.0.4.jar -Dspark.akka.logLifecycleEvents=true -Djava.library.path= -Xms512m -Xmx512m org.apache.spark.deploy.worker.Worker spark://Maclaurin.local:7077&lt;br/&gt;
========================================&lt;/p&gt;

&lt;p&gt;log4j:WARN No appenders could be found for logger (akka.event.slf4j.Slf4jLogger).&lt;br/&gt;
log4j:WARN Please initialize the log4j system properly.&lt;br/&gt;
log4j:WARN See &lt;a href=&quot;http://logging.apache.org/log4j/1.2/faq.html#noconfig&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://logging.apache.org/log4j/1.2/faq.html#noconfig&lt;/a&gt; for more info.&lt;br/&gt;
14/04/14 09:26:00 INFO Worker: Using Spark&apos;s default log4j profile: org/apache/spark/log4j-defaults.properties&lt;br/&gt;
14/04/14 09:26:00 INFO Worker: Starting Spark worker 192.168.0.2:52068 with 8 cores, 15.0 GB RAM&lt;br/&gt;
14/04/14 09:26:00 INFO Worker: Spark home: /Users/pat/spark-0.9.1-bin-hadoop1&lt;br/&gt;
14/04/14 09:26:00 INFO WorkerWebUI: Started Worker web UI at &lt;a href=&quot;http://192.168.0.2:8081&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://192.168.0.2:8081&lt;/a&gt;&lt;br/&gt;
14/04/14 09:26:00 INFO Worker: Connecting to master spark://Maclaurin.local:7077...&lt;br/&gt;
14/04/14 09:26:00 INFO Worker: Successfully registered with master spark://Maclaurin.local:7077&lt;br/&gt;
14/04/14 09:26:19 INFO Worker: Asked to launch driver driver-20140414092619-0000&lt;br/&gt;
2014-04-14 09:26:19.947 java&lt;span class=&quot;error&quot;&gt;&amp;#91;53509:9407&amp;#93;&lt;/span&gt; Unable to load realm info from SCDynamicStore&lt;br/&gt;
14/04/14 09:26:20 INFO DriverRunner: Copying user jar &lt;a href=&quot;file:/Users/pat/mahout/spark/target/mahout-spark-1.0-SNAPSHOT.jar&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;file:/Users/pat/mahout/spark/target/mahout-spark-1.0-SNAPSHOT.jar&lt;/a&gt; to /Users/pat/spark-0.9.1-bin-hadoop1/work/driver-20140414092619-0000/mahout-spark-1.0-SNAPSHOT.jar&lt;br/&gt;
14/04/14 09:26:20 INFO DriverRunner: Launch Command: &quot;/System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Home/bin/java&quot; &quot;-cp&quot; &quot;:/Users/pat/spark-0.9.1-bin-hadoop1/work/driver-20140414092619-0000/mahout-spark-1.0-SNAPSHOT.jar:/Users/pat/spark-0.9.1-bin-hadoop1/conf:/Users/pat/spark-0.9.1-bin-hadoop1/assembly/target/scala-2.10/spark-assembly_2.10-0.9.1-hadoop1.0.4.jar:/usr/local/hadoop/conf&quot; &quot;-Xms512M&quot; &quot;-Xmx512M&quot; &quot;org.apache.spark.deploy.worker.DriverWrapper&quot; &quot;akka.tcp://sparkWorker@192.168.0.2:52068/user/Worker&quot; &quot;RunCrossCooccurrenceAnalysisOnEpinions&quot; &quot;file://Users/pat/hdfs-mirror/xrsj&quot;&lt;br/&gt;
14/04/14 09:26:21 ERROR OneForOneStrategy: FAILED (of class scala.Enumeration$Val)&lt;br/&gt;
scala.MatchError: FAILED (of class scala.Enumeration$Val)&lt;br/&gt;
	at org.apache.spark.deploy.worker.Worker$$anonfun$receive$1.applyOrElse(Worker.scala:277)&lt;br/&gt;
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)&lt;br/&gt;
	at akka.actor.ActorCell.invoke(ActorCell.scala:456)&lt;br/&gt;
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)&lt;br/&gt;
	at akka.dispatch.Mailbox.run(Mailbox.scala:219)&lt;br/&gt;
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)&lt;br/&gt;
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&lt;br/&gt;
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&lt;br/&gt;
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&lt;br/&gt;
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&lt;br/&gt;
14/04/14 09:26:21 INFO Worker: Starting Spark worker 192.168.0.2:52068 with 8 cores, 15.0 GB RAM&lt;br/&gt;
14/04/14 09:26:21 INFO Worker: Spark home: /Users/pat/spark-0.9.1-bin-hadoop1&lt;br/&gt;
14/04/14 09:26:21 INFO WorkerWebUI: Started Worker web UI at &lt;a href=&quot;http://192.168.0.2:8081&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://192.168.0.2:8081&lt;/a&gt;&lt;br/&gt;
14/04/14 09:26:21 INFO Worker: Connecting to master spark://Maclaurin.local:7077...&lt;br/&gt;
14/04/14 09:26:21 INFO Worker: Successfully registered with master spark://Maclaurin.local:7077&lt;/p&gt;</comment>
                            <comment id="13968539" author="pferrel" created="Mon, 14 Apr 2014 18:16:14 +0100"  >&lt;p&gt;wow, that really screwed up the shell script so I&apos;ve attached it. &lt;/p&gt;</comment>
                            <comment id="13968541" author="pferrel" created="Mon, 14 Apr 2014 18:17:39 +0100"  >&lt;p&gt;script used to execute cross-similarity code on locahost Spark and local filesystem.&lt;/p&gt;</comment>
                            <comment id="13968564" author="ssc" created="Mon, 14 Apr 2014 18:43:04 +0100"  >&lt;p&gt;Currently, the RunCooccurrenceAnalysisOnMovielens1M script only sets up a local spark context and reads and writes from the local fs. Sorry for not mentioning this upfront. Do you want to try to change it yourself or should I update the patch?&lt;/p&gt;</comment>
                            <comment id="13968573" author="pferrel" created="Mon, 14 Apr 2014 18:48:39 +0100"  >&lt;p&gt;I am running it locally, if by that you mean stand alone on localhost and actually running RunCrossCooccurrenceAnalysisOnEpinions&lt;/p&gt;</comment>
                            <comment id="13968587" author="dlyubimov" created="Mon, 14 Apr 2014 18:57:24 +0100"  >&lt;p&gt;Running using Spark Client (inside the cluster) is a new thing in 0.9. Assuming it is stable, it is not supported at this point and going this way will have multiple hurdles. &lt;/p&gt;

&lt;p&gt;for one, mahout spark context requires MAHOUT_HOME to set all mahout binaries properly. The assumption is one needs Mahout&apos;s binaries only on driver&apos;s side, but if driver runs inside remote cluster, this will fail. So our batches should really be started in one of the ways i described in earlier email. &lt;/p&gt;

&lt;p&gt;Second, i don&apos;t think driver can load classes reliably because it includes Mahout dependencies such as mahout-math. That&apos;s another reason why using Client seems problematic to me &amp;#8211; it assumes one has his &lt;em&gt;entire&lt;/em&gt; application within that jar. So not true.&lt;/p&gt;

&lt;p&gt;That said, your attempt doesn&apos;t exhibit any direct ClassNotFounds and looks more like akka communication issues i.e. spark setup issues. One thing about Spark is that requires direct port connectivity not only between cluster nodes but also back to client. In particular it means your client must not firewall incoming calls and must not be behind NAT. (even port forwarding doesn&apos;t really solve networking issues here). So my first bet would be on akka connectivity issues between cluster and back to client.&lt;/p&gt;

</comment>
                            <comment id="13968589" author="pferrel" created="Mon, 14 Apr 2014 18:58:53 +0100"  >&lt;p&gt;not sure what you are saying, local as in local filesystem and only localhost Spark instance? How are you launching RunCrossCooccurrenceAnalysisOnEpinions ?&lt;/p&gt;

&lt;p&gt;I should get the local working first, I see the context setup in the code and will worry about that after local works.&lt;/p&gt;

&lt;p&gt;Is there something wrong with the script?&lt;/p&gt;</comment>
                            <comment id="13968603" author="dlyubimov" created="Mon, 14 Apr 2014 19:09:53 +0100"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=pferrel&quot; class=&quot;user-hover&quot; rel=&quot;pferrel&quot;&gt;Pat Ferrel&lt;/a&gt; if you look inside the Sebastian&apos;s patch, you will find it is hardcoded to use &quot;local&quot; Spark master. The master you specify to Client only tells which cluster to ship code to, not which master for the application to use. Which is why i think this Client thing is a little bit raw idea. Either way, it will not work with Sebastian&apos;s app. Instead, I&apos;d suggest you to run Sebastian script directly from IDEA as a first step, after hacking master url in this line&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
implicit val sc = mahoutSparkContext(masterUrl = &lt;span class=&quot;code-quote&quot;&gt;&quot;local&quot;&lt;/span&gt;, appName = &lt;span class=&quot;code-quote&quot;&gt;&quot;MahoutLocalContext&quot;&lt;/span&gt;,
+      customJars = Traversable.empty[&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;])
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;or making the script to accept it from environment or app params. &lt;/p&gt;

&lt;p&gt;(the convention in Spark example programs is that they accept master url as the first parameter).&lt;/p&gt;</comment>
                            <comment id="13968613" author="pferrel" created="Mon, 14 Apr 2014 19:20:44 +0100"  >&lt;p&gt;@Dmitriy, no clue what email you are talking about, you have written a lot lately. Where is it, on a Jira?&lt;/p&gt;

&lt;p&gt;I did my setup and tried launching with Hadoop and Mahout running locally (MAHOUT_LOCAL=true), One localhost instance of Spark, passing in the &apos;mvn package&apos; mahout spark jar from the localfs and pointing at data on the localfs.  This is per instructions of the Spark site. There is no firewall issue since it is always localhost talking to localhost. &lt;/p&gt;

&lt;p&gt;Anyway if I could find your &quot;running mahout on spark&quot; email it would probably explain what I&apos;m doing wrong.&lt;/p&gt;

&lt;p&gt;You did see I was using Spark 0.9.1?&lt;/p&gt;</comment>
                            <comment id="13968624" author="pferrel" created="Mon, 14 Apr 2014 19:25:48 +0100"  >&lt;p&gt;ok, no spark_client launch, got it.&lt;/p&gt;

&lt;p&gt;A pointer to the email would help.&lt;/p&gt;</comment>
                            <comment id="13968649" author="pferrel" created="Mon, 14 Apr 2014 19:39:59 +0100"  >&lt;p&gt;OK runs fine in IDEA, now I need some pointers for how to launch on the cluster.&lt;/p&gt;

&lt;p&gt;Should I be able to do that from IDEA as well by changing the context? &lt;/p&gt;</comment>
                            <comment id="13968655" author="dlyubimov" created="Mon, 14 Apr 2014 19:43:46 +0100"  >&lt;p&gt;yes, you should be able to both hack the conetxt and launch the driver successfully from IDEA regardless if you are running &quot;local&quot;, &quot;standalone/HA standalone&quot; or &quot;mesos/HA mesos&quot; resource managers &amp;#8211; as long as resource managers are up and running on your cluster.&lt;/p&gt;</comment>
                            <comment id="13968661" author="dlyubimov" created="Mon, 14 Apr 2014 19:48:57 +0100"  >&lt;p&gt;PS it mode other than &quot;local&quot; it will be looking for MAHOUT_HOME or -Dmahout.home= ... to point to latest Mahout directory. this should have latest binaries including RSJ to run in backend. that&apos;s what it ships to Spark app. ( you don&apos;t need to recompile Mahout if all you changed was just context hack). &lt;/p&gt;</comment>
                            <comment id="13968769" author="pferrel" created="Mon, 14 Apr 2014 21:05:30 +0100"  >&lt;p&gt;I&apos;m running on the localhost spark://Maclaurin.local:7077 master now and getting out of heap errors. When I ran locally I just passed in -Xms8000 to the JVM and that was fine.&lt;/p&gt;

&lt;p&gt;Had to hack the mahoutSparkContext code, there doesn&apos;t seem to be a way to pass in or modify the conf? Notice the 4g&lt;/p&gt;

&lt;p&gt;      conf.setAppName(appName).setMaster(masterUrl)&lt;br/&gt;
          .set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;)&lt;br/&gt;
          .set(&quot;spark.kryo.registrator&quot;, &quot;org.apache.mahout.sparkbindings.io.MahoutKryoRegistrator&quot;)&lt;br/&gt;
          .set(&quot;spark.executor.memory&quot;, &quot;4g&quot;)&lt;/p&gt;

&lt;p&gt;This worked fine.&lt;/p&gt;

&lt;p&gt;My dev machine is not part of the cluster and cannot participate because the path to scripts like start-slave.sh is different on the cluster and dev machine (Mac vs Linux). If I try to launch on the dev machine but point to a cluster managed by another machine it eventually tries to look in IDEA&apos;s WORKING_DIRECTORY/_temporary for something that is not there--maybe on the Spark Master?&lt;/p&gt;

&lt;p&gt;I need a way to launch this outside IDEA on a cluster machine, why shouldn&apos;t the spark_client method work?&lt;/p&gt;

&lt;p&gt;Anyway I&apos;ll keep trying to work this out, so far local and &apos;pseudo-cluster&apos; work.&lt;/p&gt;</comment>
                            <comment id="13968774" author="ssc" created="Mon, 14 Apr 2014 21:10:05 +0100"  >&lt;p&gt;Do you run this with the movielens dataset? You shouldn&apos;t need that much memory for that. &lt;/p&gt;</comment>
                            <comment id="13968781" author="pferrel" created="Mon, 14 Apr 2014 21:15:24 +0100"  >&lt;p&gt;Probably don&apos;t here either but there is 16g to 8g on all machines.&lt;/p&gt;</comment>
                            <comment id="13968783" author="dlyubimov" created="Mon, 14 Apr 2014 21:16:19 +0100"  >

&lt;p&gt;That&apos;s odd. Honestly I don&apos;t know and never encountered that.  Maybe it is&lt;br/&gt;
something the program itself does, not Spark? stacktrace or log with some&lt;br/&gt;
sort of complaint would be helpful.&lt;/p&gt;

&lt;p&gt;I know that with Mesos supervision, SPARK_HOME must be the same on all&lt;br/&gt;
nodes (driver including). But i think this is only specfic to mesos setup.&lt;br/&gt;
Standalone back should be able to handle locations.&lt;/p&gt;



&lt;p&gt;I think i gave an explanation for this already. Mostly, because it assumes&lt;br/&gt;
the jar is all it takes to run the program. but it takes entire mahout to&lt;br/&gt;
run a distribution. And because it still doesnt pass master to the program.&lt;br/&gt;
IMO there&apos;s no real advantage of doing  this vs. running a standalone&lt;br/&gt;
application (perhaps with exception when you are running from remote and&lt;br/&gt;
slowly connected client and want to disconnect while task still running).&lt;/p&gt;


</comment>
                            <comment id="13968820" author="pferrel" created="Mon, 14 Apr 2014 21:50:24 +0100"  >&lt;p&gt;Getting input from: hdfs://occam4.local/user/pat/xrsj  the job seems able to complete up to the point where it tries to write the output. Then running inside IDEA I am unable to connect to the cluster HDFS master to write.&lt;/p&gt;

&lt;p&gt;I&apos;ve never been able to have code write to HDFS from inside IDEA. I just run it from a bash script where my dev machine is configured as an HDFS client.&lt;/p&gt;

&lt;p&gt;Shouldn&apos;t using &apos;spark-class org.apache.spark.deploy.Client launch&apos; give us this?&lt;/p&gt;

&lt;p&gt;BTW all the computation is indeed running on the cluster.&lt;/p&gt;</comment>
                            <comment id="13968849" author="dlyubimov" created="Mon, 14 Apr 2014 22:12:19 +0100"  >

&lt;p&gt;IDEA is driver. but output is written by spark workers. Not the same&lt;br/&gt;
environment, and in most cases, not the same machine. Just like it happens&lt;br/&gt;
for MR reducers. Unless it is &quot;local&quot; master url. Which i assume it was not.&lt;/p&gt;


&lt;p&gt;This is strange. I can, was able to and will able to. why wouldn&apos;t it able&lt;br/&gt;
to? unless there are network or security issues. There&apos;s nothing&lt;br/&gt;
fundamentally different between reading/writing hdfs from a worker process&lt;br/&gt;
or any other process.&lt;/p&gt;



&lt;p&gt;No. Spark client is about shipping driver and have it running somewhere&lt;br/&gt;
else. it is as if somebody was running mahout cli command on one of the&lt;br/&gt;
worker nodes. this is it. it knows nothing about hdfs &amp;#8211; and even what the&lt;br/&gt;
driver program is going to do. One might use the Client code to print out&lt;br/&gt;
&quot;Hello, World&quot; and exit on some of the worker nodes, the Client wouldn&apos;t&lt;br/&gt;
know or care. Using a worker to run driver programs, that&apos;s all it does.&lt;/p&gt;

</comment>
                            <comment id="13968864" author="pferrel" created="Mon, 14 Apr 2014 22:27:25 +0100"  >&lt;p&gt;I think IDEA forces some things to run local so it can keep track of threads or something. Seems to work correctly with Spark but not HDFS. There are ways to remote debug with it so it separates processes but I don&apos;t need you to help me with IDEA.&lt;/p&gt;

&lt;p&gt;Seems easier to answer: How do I run this from the CLI? Let&apos;s get IDEA out of the picture. I bet it will just work.&lt;/p&gt;

&lt;p&gt;We need a way to run these from the CLI via cron or scripts anyway, right?&lt;/p&gt;

&lt;p&gt;Using spark-class I get no errors but no output either. It doesn&apos;t create the same Application name so I must be using it wrong. Will look later today.&lt;/p&gt;</comment>
                            <comment id="13968885" author="pferrel" created="Mon, 14 Apr 2014 22:39:20 +0100"  >&lt;p&gt;Hmm, maybe I should ask if anyone has gotten this stuff to read/write to HDFS? I can get read but not write&lt;/p&gt;</comment>
                            <comment id="13968894" author="dlyubimov" created="Mon, 14 Apr 2014 22:44:21 +0100"  >&lt;p&gt;i have been dumping from spark to hdfs, hbase, memory-mapped index&lt;br/&gt;
structures, you name it, for 2 years.&lt;/p&gt;

&lt;p&gt;Pat, something is definitely getting wrong there &amp;#8211; but not by design.&lt;/p&gt;


</comment>
                            <comment id="13969070" author="pferrel" created="Tue, 15 Apr 2014 01:33:47 +0100"  >&lt;p&gt;Ok, something misconfigured maybe, IDEA. Let&apos;s leave IDEA out of the loop.&lt;/p&gt;

&lt;p&gt;If you could indulge me--how do you run this from the CLI? &lt;/p&gt;</comment>
                            <comment id="13969722" author="pferrel" created="Tue, 15 Apr 2014 17:49:15 +0100"  >&lt;p&gt;Silence indicates: you don&apos;t know how to? it can&apos;t be done because jars aren&apos;t created for it yet? you&apos;d rather launch from the Scala shell?  If the later, that&apos;s fine I just want to get IDEA out of the equation so instructions for running in the Scala shell would be helpful.&lt;/p&gt;

&lt;p&gt;I plan to move on to using HDFS for storage but still have a local storage failure below.&lt;/p&gt;

&lt;p&gt;Concentrating on local storage for now I get the following from my dev machine launching in IDEA:&lt;/p&gt;

&lt;p&gt;input,         output,        mahoutSparkContext(masterUrl = ,  Success?&lt;br/&gt;
local path, local path,   &quot;local&quot;,                                               yes&lt;br/&gt;
local path, local path,   &quot;spark://Maclaurin:7077&quot;,                  yes&lt;br/&gt;
local path local path,   &quot;spark://occam4:7077&quot;,                     no, computation finishes correctly but the last stage dump/write&lt;br/&gt;
                                                                                           the DRM fails, the spark master is a remote machine who &lt;br/&gt;
                                                                                           is also the HDFS master and is managing three Spark &lt;br/&gt;
                                                                                           slaves, all is OK in the WebUI, no errors in the Spark logs&lt;br/&gt;
, &lt;br/&gt;
This last case I have tried various forms of the &quot;local path&quot; for output and suspect that using the correct form of the URI may be the problem so if someone sees the mistake please let me know:&lt;br/&gt;
1) &quot;tmp/co-occurrence-on-epinions/indicators-item-item/&quot; relative path to the IDEA working directory, which works for input.&lt;br/&gt;
2) &quot;/Users/pat/hdfs-mirror/tmp/co-occurrence-on-epinions/indicators-item-item/&quot; absolute path so no IDEA working directory&lt;br/&gt;
3) &quot;file:///Users/pat/hdfs-mirror/tmp/co-occurrence-on-epinions/indicators-item-item/&quot; URI form of full local path&lt;/p&gt;

&lt;p&gt;Code for #3 is:&lt;/p&gt;

&lt;p&gt;RecommendationExamplesHelper.saveIndicatorMatrix(indicatorMatrices(0),&lt;br/&gt;
      &quot;file:///Users/pat/hdfs-mirror/tmp/co-occurrence-on-epinions/indicators-item-item/&quot;)&lt;/p&gt;

&lt;p&gt;For #3 I get the following exception message. The _temporary dir does exist, there is just nothing in it:&lt;/p&gt;

&lt;p&gt;14/04/15 09:07:03 INFO scheduler.DAGScheduler: Failed to run saveAsTextFile at Recommendations.scala:178&lt;br/&gt;
Exception in thread &quot;main&quot; org.apache.spark.SparkException: Job aborted: Task 8.0:0 failed 4 times (most recent failure: Exception failure: java.io.IOException: The temporary job-output directory &lt;a href=&quot;file:/Users/pat/hdfs-mirror/tmp/co-occurrence-on-epinions/indicators-item-item/_temporary&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;file:/Users/pat/hdfs-mirror/tmp/co-occurrence-on-epinions/indicators-item-item/_temporary&lt;/a&gt; doesn&apos;t exist!)&lt;br/&gt;
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1028)&lt;br/&gt;
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1026)&lt;br/&gt;
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)&lt;br/&gt;
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)&lt;br/&gt;
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$abortStage(DAGScheduler.scala:1026)&lt;br/&gt;
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:619)&lt;br/&gt;
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:619)&lt;br/&gt;
	at scala.Option.foreach(Option.scala:236)&lt;br/&gt;
	at org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:619)&lt;br/&gt;
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:207)&lt;br/&gt;
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)&lt;br/&gt;
	at akka.actor.ActorCell.invoke(ActorCell.scala:456)&lt;br/&gt;
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)&lt;br/&gt;
	at akka.dispatch.Mailbox.run(Mailbox.scala:219)&lt;br/&gt;
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)&lt;br/&gt;
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&lt;br/&gt;
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&lt;br/&gt;
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&lt;br/&gt;
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&lt;br/&gt;
Disconnected from the target VM, address: &apos;127.0.0.1:58830&apos;, transport: &apos;socket&apos;&lt;/p&gt;</comment>
                            <comment id="13969759" author="pferrel" created="Tue, 15 Apr 2014 18:11:23 +0100"  >&lt;p&gt;Running from my dev machine in IDEA against a remote cluster I can read input from HDFS and the computation on Spark seems to complete correctly but the write fails. &lt;/p&gt;

&lt;p&gt;input,                                      output,                                     mahoutSparkContext(masterUrl = ,  Success?&lt;br/&gt;
HDFS:/occam4/user/pat/xrsj, HDFS:/occam4/user/pat/tmp,  spark://occam4:7077,                        no, computation competes, but output&lt;br/&gt;
                                                                                                                                                        gives a failure to connect message &lt;br/&gt;
                                                                                                                                                        while trying to write to HDFS&lt;/p&gt;

&lt;p&gt;Again this may be a URI error in the output path or some config problem. From the shell I can examine HDFS and all is as expected. Hadoop jobs that I launch from the command line work against the cluster correctly.&lt;/p&gt;

&lt;p&gt;14/04/15 10:04:02 INFO storage.MemoryStore: Block broadcast_3 stored as values to memory (estimated size 385.2 KB, free 4.7 GB)&lt;br/&gt;
14/04/15 10:04:02 INFO rdd.FlatMappedRDD: Removing RDD 16 from persistence list&lt;br/&gt;
14/04/15 10:04:02 INFO storage.BlockManager: Removing RDD 16&lt;br/&gt;
14/04/15 10:04:02 INFO rdd.FlatMappedRDD: Removing RDD 7 from persistence list&lt;br/&gt;
14/04/15 10:04:02 INFO storage.BlockManager: Removing RDD 7&lt;br/&gt;
14/04/15 10:04:09 INFO ipc.Client: Retrying connect to server: occam4/192.168.0.14:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)&lt;br/&gt;
14/04/15 10:04:10 INFO ipc.Client: Retrying connect to server: occam4/192.168.0.14:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)&lt;br/&gt;
14/04/15 10:04:11 INFO ipc.Client: Retrying connect to server: occam4/192.168.0.14:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)&lt;br/&gt;
14/04/15 10:04:12 INFO ipc.Client: Retrying connect to server: occam4/192.168.0.14:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)&lt;br/&gt;
14/04/15 10:04:13 INFO ipc.Client: Retrying connect to server: occam4/192.168.0.14:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)&lt;br/&gt;
14/04/15 10:04:14 INFO ipc.Client: Retrying connect to server: occam4/192.168.0.14:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)&lt;br/&gt;
14/04/15 10:04:15 INFO ipc.Client: Retrying connect to server: occam4/192.168.0.14:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)&lt;br/&gt;
14/04/15 10:04:16 INFO ipc.Client: Retrying connect to server: occam4/192.168.0.14:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)&lt;br/&gt;
14/04/15 10:04:17 INFO ipc.Client: Retrying connect to server: occam4/192.168.0.14:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)&lt;br/&gt;
14/04/15 10:04:18 INFO ipc.Client: Retrying connect to server: occam4/192.168.0.14:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)&lt;br/&gt;
Exception in thread &quot;main&quot; java.net.ConnectException: Call to occam4/192.168.0.14:8020 failed on connection exception: java.net.ConnectException: Connection refused&lt;br/&gt;
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:1142)&lt;br/&gt;
	at org.apache.hadoop.ipc.Client.call(Client.java:1118)&lt;br/&gt;
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)&lt;br/&gt;
	at com.sun.proxy.$Proxy8.getProtocolVersion(Unknown Source)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:597)&lt;br/&gt;
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)&lt;br/&gt;
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)&lt;br/&gt;
	at com.sun.proxy.$Proxy8.getProtocolVersion(Unknown Source)&lt;br/&gt;
	at org.apache.hadoop.ipc.RPC.checkVersion(RPC.java:422)&lt;br/&gt;
	at org.apache.hadoop.hdfs.DFSClient.createNamenode(DFSClient.java:183)&lt;br/&gt;
	at org.apache.hadoop.hdfs.DFSClient.&amp;lt;init&amp;gt;(DFSClient.java:281)&lt;br/&gt;
	at org.apache.hadoop.hdfs.DFSClient.&amp;lt;init&amp;gt;(DFSClient.java:245)&lt;br/&gt;
	at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:100)&lt;br/&gt;
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1446)&lt;br/&gt;
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:67)&lt;br/&gt;
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1464)&lt;br/&gt;
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:263)&lt;br/&gt;
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:187)&lt;br/&gt;
	at org.apache.hadoop.mapred.SparkHadoopWriter$.createPathFromString(SparkHadoopWriter.scala:193)&lt;br/&gt;
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:685)&lt;br/&gt;
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:572)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:894)&lt;br/&gt;
	at org.apache.mahout.cf.examples.RecommendationExamplesHelper$.saveIndicatorMatrix(Recommendations.scala:178)&lt;br/&gt;
	at org.apache.mahout.cf.examples.RunCrossCooccurrenceAnalysisOnEpinions$.main(Recommendations.scala:111)&lt;br/&gt;
	at org.apache.mahout.cf.examples.RunCrossCooccurrenceAnalysisOnEpinions.main(Recommendations.scala)&lt;br/&gt;
Caused by: java.net.ConnectException: Connection refused&lt;br/&gt;
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)&lt;br/&gt;
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:599)&lt;br/&gt;
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)&lt;br/&gt;
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:511)&lt;br/&gt;
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:481)&lt;br/&gt;
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:457)&lt;br/&gt;
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:583)&lt;br/&gt;
	at org.apache.hadoop.ipc.Client$Connection.access$2200(Client.java:205)&lt;br/&gt;
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1249)&lt;br/&gt;
	at org.apache.hadoop.ipc.Client.call(Client.java:1093)&lt;br/&gt;
	... 26 more&lt;br/&gt;
Disconnected from the target VM, address: &apos;127.0.0.1:59483&apos;, transport: &apos;socket&apos;&lt;/p&gt;

</comment>
                            <comment id="13969763" author="dlyubimov" created="Tue, 15 Apr 2014 18:13:40 +0100"  >&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;My&amp;#93;&lt;/span&gt; Silence idicates I&apos;ve been pretty sick &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;/p&gt;

&lt;p&gt;I thought i explained in my email we are not planning CLI. We are planning script shell instead. There is not, nor do i think there will be a way to run this stuff with CLI, just like there&apos;s no way to invoke a particular method in R without writing a short script. &lt;/p&gt;

&lt;p&gt;That said, yes, you can try to run it as a java application, i.e. &lt;span class=&quot;error&quot;&gt;&amp;#91;java|scala&amp;#93;&lt;/span&gt; -cp &amp;lt;cp&amp;gt;. &amp;lt;class name&amp;gt; &lt;/p&gt;

&lt;p&gt;where -cp is what `mahout classpath` returns. &lt;/p&gt;</comment>
                            <comment id="13969766" author="pferrel" created="Tue, 15 Apr 2014 18:15:36 +0100"  >&lt;p&gt;To sum up, Spark Cooccurrence seems to complete correctly on the Spark Cluster in any of the configurations. Writing output has been failing on any case when using the remote Spark cluster for computation. However as far as I can tell input from local filesystem or HDFS seems to work in all cases.&lt;/p&gt;

&lt;p&gt;Next I&apos;ll try running my tests from the Spark master machine by installing IDEA there. There must be some other way than IDEA to run this?&lt;/p&gt;</comment>
                            <comment id="13969775" author="dlyubimov" created="Tue, 15 Apr 2014 18:24:22 +0100"  >&lt;blockquote&gt;&lt;p&gt;Running from my dev machine in IDEA against a remote cluster I can read input from HDFS and the computation on Spark seems to complete correctly but the write fails.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;input, output, mahoutSparkContext(masterUrl = , Success?&lt;br/&gt;
HDFS:/occam4/user/pat/xrsj, HDFS:/occam4/user/pa&lt;/p&gt;

&lt;p&gt;is this a spark task log? or front end log? seems worker process on the cluster is trying to connect to hdfs name node (iirc port 8020 is hadoop namenode service) and fails. Config/networking issues.&lt;/p&gt;</comment>
                            <comment id="13969961" author="dlyubimov" created="Tue, 15 Apr 2014 20:30:16 +0100"  >&lt;blockquote&gt;&lt;p&gt;where -cp is what `mahout classpath` returns.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Actually, scratch that. That generally is still a bad recipe. &quot;mahout classpath&quot; would return installed HADOOP_HOME  dependencies (normally one doesn&apos;t want that because spark managed-libs already exposes whatever version of hadoop it was compiled with), and it neglects to add Spark classpath. So i don&apos;t this &apos;mahout classpath&apos; is plenty useful here. &lt;/p&gt;

&lt;p&gt;BTW that&apos;s another thing here &amp;#8211; you need to compile Spark with correct version of hadoop hdfs you intend to use (at least that&apos;s what i do). By default i think it does a terrible thing.&lt;/p&gt;

&lt;p&gt;The main suggestion stands &amp;#8211; collect &apos;cp&apos; correctly, which idea already does via maven, but the major hurdle is to do it manually &amp;#8211; and user-friendly methods for those are not yet present methinks.&lt;/p&gt;</comment>
                            <comment id="13972210" author="pferrel" created="Thu, 17 Apr 2014 02:45:48 +0100"  >&lt;p&gt;Well here&apos;s something I noticed that may be a clue. First there were some scala 2.10 jars that were built for hadoop 1.0.4 sitting in an assembly/target dir. So even though the managed_lib dir had the correct 1.2.1 version of hadoop I rebuilt and go rid of any 1.0.4 jars I could find.&lt;/p&gt;

&lt;p&gt;Then if I am running hadoop and mahout locally I can launch the Spark shell, where it creates a default context called sc. I can then perform the following:&lt;/p&gt;

&lt;p&gt;Created spark context..&lt;br/&gt;
Spark context available as sc.&lt;/p&gt;

&lt;p&gt;scala&amp;gt;  val textFile = sc.textFile(&quot;xrsj/ratings_data.txt&quot;)&lt;br/&gt;
14/04/16 18:19:52 INFO storage.MemoryStore: ensureFreeSpace(61374) called with curMem=0, maxMem=318111744&lt;br/&gt;
14/04/16 18:19:52 INFO storage.MemoryStore: Block broadcast_0 stored as values to memory (estimated size 59.9 KB, free 303.3 MB)&lt;br/&gt;
textFile: org.apache.spark.rdd.RDD&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt; = MappedRDD&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; at textFile at &amp;lt;console&amp;gt;:12&lt;/p&gt;

&lt;p&gt;scala&amp;gt; textFile.count()&lt;br/&gt;
14/04/16 18:19:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable&lt;br/&gt;
...&lt;br/&gt;
14/04/16 18:20:00 INFO spark.SparkContext: Job finished: count at &amp;lt;console&amp;gt;:15, took 0.995702 s&lt;br/&gt;
res0: Long = 664824&lt;/p&gt;

&lt;p&gt;If I exit the spark shell, start a local pseudo cluster then start the shell the same code works, only reading from the hdfs pseudo-cluster. The same exact code works for the cluster too since the file is at the same location relative to where I start the shell.&lt;/p&gt;

&lt;p&gt;I can also address the file in absolute terms with the line below. Notice I need to use the port # and leaving if off leads to the failure to connect message in a previous comment. &lt;/p&gt;

&lt;p&gt;scala&amp;gt;  val textFile = sc.textFile(&quot;hdfs://occam4:54310/user/pat/xrsj/ratings_data.txt&quot;)&lt;br/&gt;
scala&amp;gt; textFile.count()&lt;/p&gt;

&lt;p&gt;I tried using the port # in the cooccurrence test case but get the same failure to connect message.&lt;/p&gt;

&lt;p&gt;Since the Spark Scala shell is creating the context by detecting the machine&apos;s HDFS setup, could this be the problem in the IDEA running cooccurrence example? The context in the example is setup after the input is read from HDFS is that correct? I know it is not supposed to care about HDFS, only the Spark master but obviously when a context is created by the Spark Shell and is using the context to get the text file it works.  Should we be doing that in the example? I&apos;ll try playing with how the text file is read and where the context is created. &lt;/p&gt;

&lt;p&gt;Perhaps naively I would have thought that the URI I used for read and write would bypass any default settings in the context but this doesn&apos;t seem to be true does it? I suspect something in the context or lack of it is causing Spark to be confused about where to read and write from, no matter how explicit the URI. &lt;/p&gt;
</comment>
                            <comment id="13973012" author="pferrel" created="Thu, 17 Apr 2014 16:06:42 +0100"  >&lt;p&gt;Getting the cooccurrence code to read or write to hdfs is still not working. The cooccurrence code does not seem to use the context that is created though the computation does execute on the cluster and seems to complete properly, I wonder if the context is needed to do the read/write as it is in the above spark-shell example. So the following val is not use afaikt.&lt;/p&gt;

&lt;p&gt;    implicit val sc = mahoutSparkContext(masterUrl = &quot;spark://occam4:7077&quot;, appName = &quot;MahoutClusterContext&quot;,&lt;br/&gt;
      customJars = Traversable.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;)&lt;/p&gt;

&lt;p&gt;I can&apos;t even get this job to complete using the local file system, some strange paths are created for &quot;_temporary&quot; depending on who knows what. one even looked like some version of Linux I don&apos;t own: Exception in thread &quot;main&quot; org.apache.spark.SparkException: Job aborted: Task 8.0:0 failed 4 times (most recent failure: &lt;/p&gt;

&lt;p&gt;Exception failure: java.io.IOException: The temporary job-output directory &lt;a href=&quot;file:/private/tmp/tmp/co-occurrence-on-epinions/indicators-item-item/_temporary&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;file:/private/tmp/tmp/co-occurrence-on-epinions/indicators-item-item/_temporary&lt;/a&gt; doesn&apos;t exist!)&lt;/p&gt;

&lt;p&gt;/private/tmp ??? what is that Centos? I&apos;m using ubuntu 12.04&lt;/p&gt;

&lt;p&gt;Onwards to looking at the Spark config. &lt;/p&gt;

&lt;p&gt;Can you answer the question about why we don&apos;t use the context &apos;sc&apos; to read and write as with the spark-shell example? &lt;/p&gt;</comment>
                            <comment id="13973088" author="pferrel" created="Thu, 17 Apr 2014 16:50:41 +0100"  >&lt;p&gt;Oh, and are you using the spark-shell to execute the cooccurrence examples?&lt;/p&gt;</comment>
                            <comment id="13973288" author="dlyubimov" created="Thu, 17 Apr 2014 20:06:29 +0100"  >&lt;p&gt;the idea most likely willl have screwed hadoop dependency because by default it inherits it from thath of Mahout&apos;s default dependency. I used to run this stuff (or rather our internal variant of this stuff) from my own project which has a very strict control over dependendencies (esp. hadoop dependencies).  I also inserted a CDH4 profile to spark module which overrides Mahout&apos;s default hadoop dependency, and that should help &amp;#8211; but it is still a pain, i gave up on running it from idea with Mahout maven dependencies. Something is screwed there in the end.&lt;/p&gt;

&lt;p&gt;i don&apos;t experiment with RSJ yet &amp;#8211; i guess i will leave it to Sebastian at this point .&lt;/p&gt;

&lt;p&gt;what i do is running the following script on my &quot;shell&quot; branch in github via mahout shell&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeHeader panelHeader&quot; style=&quot;border-bottom-width: 1px;&quot;&gt;&lt;b&gt;&quot;simple.mscala&quot;&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt; 
val a = dense((1,2,3),(3,4,5))
val drmA = drmParallelize(a,numPartitions = 2)
val drmAtA = drmA.t %*% drmA

val r = drmAtA.mapBlock() {
  &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; (keys, block) =&amp;gt;
    block += 1.0
    keys -&amp;gt; block
}.checkpoint(/*StorageLevel.NONE*/)

r.collect

&lt;span class=&quot;code-comment&quot;&gt;// local write
&lt;/span&gt;r.writeDRM(&lt;span class=&quot;code-quote&quot;&gt;&quot;file:&lt;span class=&quot;code-comment&quot;&gt;///home/dmitriy/A&quot;&lt;/span&gt;)
&lt;/span&gt;
&lt;span class=&quot;code-comment&quot;&gt;// hdfs write
&lt;/span&gt;r.writeDRM(&lt;span class=&quot;code-quote&quot;&gt;&quot;hdfs:&lt;span class=&quot;code-comment&quot;&gt;//localhost:11010/A&quot;&lt;/span&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;p&gt;which actually runs totally fine in local mode, and &lt;em&gt;sometimes&lt;/em&gt;  also runs ok in &quot;standalone&quot;/hdfs mode but sometimes there are strange after-effects of hangs and bailing out with OOM when run on remote cluster with &quot;standalone&quot;. &lt;/p&gt;

&lt;p&gt;I am pretty sure it is either dependency issues again in Mahout maven build, or something that has happened to Spark 0.9.x release.  Spark 0.6.x &amp;#8211; 0.8.x releases and earlier had absolutely no trouble working with hdfs sequence files.&lt;/p&gt;</comment>
                            <comment id="13973290" author="dlyubimov" created="Thu, 17 Apr 2014 20:07:13 +0100"  >&lt;p&gt;also just discovered that sbt build in 0.9.1 screws hbase dependency. Not likely to be much of a reason, but who knows. &lt;/p&gt;</comment>
                            <comment id="13973347" author="dlyubimov" created="Thu, 17 Apr 2014 21:21:18 +0100"  >&lt;p&gt;Hm. At home i don&apos;t have any trouble reading/writing from/to hdfs. &lt;/p&gt;

&lt;p&gt;There are some minor differences in configuration plus i am running hdfs cdh 4.3.2 at home vs. 4.3.0 at work computer. That&apos;s the only difference. &lt;/p&gt;

&lt;p&gt;(some patchlevel specific?)&lt;/p&gt;
</comment>
                            <comment id="13974188" author="pferrel" created="Fri, 18 Apr 2014 17:07:21 +0100"  >&lt;p&gt;It looks like our setups are pretty much identical as far as I can tell. The primary difference is in using the IDE to launch and that may be causing the problem. &lt;/p&gt;

&lt;p&gt;Therefore I&apos;ll put the testing aside for awhile and work on getting Dmitriy&apos;s Spark Scala shell working since we know that write from there is working--at least writeDRM is.&lt;/p&gt;

&lt;p&gt;As I&apos;ve said, it looks like the cluster cooccurrence computation (both cross and self similarity) is being executed properly on the epinions data but I&apos;m unable to get a file output. &lt;/p&gt;</comment>
                            <comment id="14005484" author="pferrel" created="Thu, 22 May 2014 02:01:39 +0100"  >&lt;p&gt;Runs correctly on clustered Spark and HDFS.&lt;/p&gt;

&lt;p&gt;Is there more to do here? Are the other similarity types needed?&lt;/p&gt;</comment>
                            <comment id="14005631" author="ssc" created="Thu, 22 May 2014 06:25:56 +0100"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=pferrel&quot; class=&quot;user-hover&quot; rel=&quot;pferrel&quot;&gt;Pat Ferrel&lt;/a&gt; Great, how large was your testdataset?&lt;/p&gt;

&lt;p&gt;I&apos;d vote against other similarity types for sake of similarity, LLR also works best in my experience&lt;/p&gt;</comment>
                            <comment id="14006038" author="pferrel" created="Thu, 22 May 2014 16:41:44 +0100"  >&lt;p&gt;Agreed about LLR. Never saw it underperform the other measures for CF.&lt;/p&gt;

&lt;p&gt;I&apos;m using a very small dataset with minSplits = 2 so I can check the actual values. Then running the epinions, which I can&apos;t really check for value. Testing is going through the ItemSimilarityDriver from &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1541&quot; title=&quot;Create CLI Driver for Spark Cooccurrence Analysis&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1541&quot;&gt;&lt;del&gt;MAHOUT-1541&lt;/del&gt;&lt;/a&gt; so the write is not using your patch. &lt;/p&gt;

&lt;p&gt;Unfortunately I lied. Works with HDFS in and out but on a multi-threaded local&lt;span class=&quot;error&quot;&gt;&amp;#91;4&amp;#93;&lt;/span&gt; standalone Spark. Setting to my cluster master still fails. The error message is about connection refused so there is something is still not configured correctly on my cluster. I had to use fully qualified URIs to the data because Spark was defaulting to the wrong locations. All pointing to a bad Spark build or conf. Spark-shell seems to work fine on the cluster. Anyway, I&apos;ll reinstall Spark and try again. Sorry for the false alarm.&lt;/p&gt;</comment>
                            <comment id="14010278" author="dlyubimov" created="Tue, 27 May 2014 22:08:36 +0100"  >&lt;p&gt;Is there anything else to commit here?&lt;/p&gt;</comment>
                            <comment id="14010653" author="pferrel" created="Wed, 28 May 2014 02:49:14 +0100"  >&lt;p&gt;There have been no commits afaik. The status is for Sebastian to say but I&apos;ve used the cooccurrence analysis and it works correctly. I can&apos;t verify Spark cluster execution with HDFS due to what I think is my own bad setup.&lt;/p&gt;

&lt;p&gt;If someone else could test it on a cluster I&apos;d say it should be committed. If we can wait, I&apos;m trying to get my cluster upgraded to hadoop 2 and reconfigure Spark for that. Then try testing this on the new setup.&lt;/p&gt;

&lt;p&gt;There are no scala tests for this though there are some in the patches. I&apos;m adding some scala tests that will cover this code in doing a CLI in &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1541&quot; title=&quot;Create CLI Driver for Spark Cooccurrence Analysis&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1541&quot;&gt;&lt;del&gt;MAHOUT-1541&lt;/del&gt;&lt;/a&gt;, which is a few weeks from being able to commit.&lt;/p&gt;

&lt;p&gt;Not sure if it&apos;s packaged correctly, the tests supplied here are really examples since they are on large datasets and take a long time to execute.&lt;/p&gt;

&lt;p&gt;Bottom line is it needs to be verified on a Cluster and checked for package structure. I&apos;m happy to do this if we don&apos;t need it committed right away. Both of these things need to be done as part of &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1541&quot; title=&quot;Create CLI Driver for Spark Cooccurrence Analysis&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1541&quot;&gt;&lt;del&gt;MAHOUT-1541&lt;/del&gt;&lt;/a&gt;, which I&apos;m actively working on but is not really ready to review yet.&lt;/p&gt;</comment>
                            <comment id="14015667" author="pferrel" created="Mon, 2 Jun 2014 19:06:05 +0100"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ssc&quot; class=&quot;user-hover&quot; rel=&quot;ssc&quot;&gt;Sebastian Schelter&lt;/a&gt; Should I reassign to me for now so we can get this committed?&lt;/p&gt;</comment>
                            <comment id="14015788" author="pferrel" created="Mon, 2 Jun 2014 20:38:14 +0100"  >&lt;p&gt;Looks like DrmLike may have been refactored since this patch was written.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dlyubimov&quot; class=&quot;user-hover&quot; rel=&quot;dlyubimov&quot;&gt;Dmitriy Lyubimov&lt;/a&gt; The following patch code has an error at &quot;elem&quot; saying &quot;Missing parameter type &apos;elem&apos;&quot; Looking at the scaladocs I tracked back to the DrmLike trait and see no way to .mapBlock on it. Has something been refactored here? The .nonZeroes() is a java sparse vector iterator I think. This worked about a month ago so thought you might have an idea how things have changed?&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unable to find source-code formatter for language: scala.&lt;/span&gt; Available languages are: actionscript, html, java, javascript, none, sql, xhtml, xml&lt;/div&gt;&lt;pre&gt;
  def computeIndicators(drmBtA: DrmLike[Int], numUsers: Int, maxInterestingItemsPerThing: Int,
                        bcastNumInteractionsB: Broadcast[Vector], bcastNumInteractionsA: Broadcast[Vector],
                        crossCooccurrence: &lt;span class=&quot;code-object&quot;&gt;Boolean&lt;/span&gt; = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;) = {
    drmBtA.mapBlock() {
      &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; (keys, block) =&amp;gt;

        val llrBlock = block.like()
        val numInteractionsB: Vector = bcastNumInteractionsB
        val numInteractionsA: Vector = bcastNumInteractionsA

        &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (index &amp;lt;- 0 until keys.size) {

          val thingB = keys(index)

          &lt;span class=&quot;code-comment&quot;&gt;// PriorityQueue to select the top-k items
&lt;/span&gt;          val topItemsPerThing = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; mutable.PriorityQueue[(Int,&lt;span class=&quot;code-object&quot;&gt;Double&lt;/span&gt;)]()(orderByScore)

          block(index, ::).nonZeroes().foreach { elem =&amp;gt; &lt;span class=&quot;code-comment&quot;&gt;//!!!!!!!!!!!!! Error: &lt;span class=&quot;code-quote&quot;&gt;&quot;Missing parameter type &apos;elem&apos;&quot;&lt;/span&gt;
&lt;/span&gt;            val thingA = elem.index
            val cooccurrences = elem.get
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="14015806" author="dlyubimov" created="Mon, 2 Jun 2014 20:54:32 +0100"  >&lt;p&gt;I think this has nothing to do with anything in Spark or scala bindings I think . &lt;/p&gt;

&lt;p&gt;the .nonZeroes() is mahout-math method (java) which produces a java iterator, which then implicitly cast to scala iterator (since .foreach is scala operator). &lt;/p&gt;

&lt;p&gt;is JavaConversions._ still imported?&lt;/p&gt;</comment>
                            <comment id="14015810" author="dlyubimov" created="Mon, 2 Jun 2014 20:59:55 +0100"  >&lt;p&gt;if you want me to verify this, please convert to pull request so i can painlessly sync to exactly what you are testing.&lt;/p&gt;</comment>
                            <comment id="14015824" author="pferrel" created="Mon, 2 Jun 2014 21:08:33 +0100"  >&lt;p&gt;import scala.collection.JavaConversions._&lt;/p&gt;

&lt;p&gt;is included. I&apos;ll pare back to just this ticket and send a PR&lt;/p&gt;</comment>
                            <comment id="14016012" author="githubbot" created="Mon, 2 Jun 2014 23:58:58 +0100"  >&lt;p&gt;Github user dlyubimov closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/8&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/8&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14016013" author="githubbot" created="Mon, 2 Jun 2014 23:58:59 +0100"  >&lt;p&gt;GitHub user dlyubimov reopened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/8&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/8&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1464&quot; title=&quot;Cooccurrence Analysis on Spark&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1464&quot;&gt;&lt;del&gt;MAHOUT-1464&lt;/del&gt;&lt;/a&gt; Cooccurrence Analysis on Spark&lt;/p&gt;

&lt;p&gt;    Grabbed Pat&apos;s branch. submitting as PR (WIP at this point). &lt;/p&gt;

&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/dlyubimov/mahout&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/dlyubimov/mahout&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1464&quot; title=&quot;Cooccurrence Analysis on Spark&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1464&quot;&gt;&lt;del&gt;MAHOUT-1464&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/8.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/8.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #8&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 70654fa58dd4b801c551429945fa2f1377a60b2e&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-02T21:11:55Z&lt;/p&gt;

&lt;p&gt;    starting to merge the cooccurrence stuff, import errors&lt;/p&gt;

&lt;p&gt;commit fc5fb6ac37e4c12d25c35ddb7912a32aac06e449&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-02T21:33:45Z&lt;/p&gt;

&lt;p&gt;    tried changing the imports in CooccurrenceAnalysis.scala to no avail&lt;/p&gt;

&lt;p&gt;commit 242aed0e0921afe9a87ee8973ba8077cbe65fffa&lt;br/&gt;
Author: Dmitriy Lyubimov &amp;lt;dlyubimov@apache.org&amp;gt;&lt;br/&gt;
Date:   2014-06-02T22:42:57Z&lt;/p&gt;

&lt;p&gt;    Compilation fixes, updates for &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1529&quot; title=&quot;Finalize abstraction of distributed logical plans from backend operations&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1529&quot;&gt;&lt;del&gt;MAHOUT-1529&lt;/del&gt;&lt;/a&gt; changes&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="14016102" author="pferrel" created="Tue, 3 Jun 2014 01:53:50 +0100"  >&lt;p&gt;My problem is that my cluster is 1.2.1 and to upgrade everything I run on it has to go to H2. Oh bother.&lt;/p&gt;

&lt;p&gt;I think the best thing is commit this and see it someone will run one of the several included tests on a cluster. It works local and seems to work clustered but the write fails. The write is not part of the core code.&lt;/p&gt;

&lt;p&gt;Anyway unless someone vetos I&apos;ll commit it once I get at least one build integrated test included.&lt;/p&gt;</comment>
                            <comment id="14018494" author="githubbot" created="Thu, 5 Jun 2014 06:43:46 +0100"  >&lt;p&gt;Github user sscdotopen commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/8#discussion_r13425122&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/8#discussion_r13425122&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: spark/src/main/scala/org/apache/mahout/cf/CooccurrenceAnalysis.scala &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,210 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
    + * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
    + * this work for additional information regarding copyright ownership.&lt;br/&gt;
    + * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
    + * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
    + * the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.mahout.cf&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.mahout.math._&lt;br/&gt;
    +import scalabindings._&lt;br/&gt;
    +import RLikeOps._&lt;br/&gt;
    +import drm._&lt;br/&gt;
    +import RLikeDrmOps._&lt;br/&gt;
    +import org.apache.mahout.sparkbindings._&lt;br/&gt;
    +&lt;br/&gt;
    +import scala.collection.JavaConversions._&lt;br/&gt;
    +import org.apache.mahout.math.stats.LogLikelihood&lt;br/&gt;
    +import collection._&lt;br/&gt;
    +// import scala.collection.parallel.mutable&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    we can remove the .parallel. import&lt;/p&gt;</comment>
                            <comment id="14018818" author="githubbot" created="Thu, 5 Jun 2014 15:18:58 +0100"  >&lt;p&gt;Github user pferrel commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/8#issuecomment-45224589&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/8#issuecomment-45224589&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    OK, I tend to use IDEA import optimization, which works about 90% of the time. Notice that the mutable import messes things up so D removed that.&lt;/p&gt;</comment>
                            <comment id="14018870" author="githubbot" created="Thu, 5 Jun 2014 16:28:17 +0100"  >&lt;p&gt;Github user pferrel commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/8#issuecomment-45234064&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/8#issuecomment-45234064&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I could use a little advice here. The epinions and movielens tests in the examples folder. Should they be put into the build?&lt;/p&gt;

&lt;p&gt;    Pros: good example data.&lt;br/&gt;
    Cons: the reading and writing are not parallel and so only work locally. It is easy to change the Spark context to use a cluster but the data still has to be local. These tests would be easier to maintain if they were attached to the ItemSimilarityDriver, which will handle cluster storage and execution and will be maintained better.&lt;/p&gt;

&lt;p&gt;    I&apos;d rather move them out into an ItemSimilarityDriver examples folder and will do this if no one objects. They will not be build tests, obviously, since they take too long.&lt;/p&gt;</comment>
                            <comment id="14018871" author="githubbot" created="Thu, 5 Jun 2014 16:29:43 +0100"  >&lt;p&gt;Github user sscdotopen commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/8#issuecomment-45234269&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/8#issuecomment-45234269&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Its not allowed to redistribute the movielens dataset.&lt;/p&gt;

&lt;p&gt;    On 06/05/2014 05:28 PM, Pat Ferrel wrote:&lt;br/&gt;
    &amp;gt; I could use a little advice here. The epinions and movielens tests in the examples folder. Should they be put into the build?&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt; Pros: good example data.&lt;br/&gt;
    &amp;gt; Cons: the reading and writing are not parallel and so only work locally. It is easy to change the Spark context to use a cluster but the data still has to be local. These tests would be easier to maintain if they were attached to the ItemSimilarityDriver, which will handle cluster storage and execution and will be maintained better.&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt; I&apos;d rather move them out into an ItemSimilarityDriver examples folder and will do this if no one objects. They will not be build tests, obviously, since they take too long.&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt; &amp;#8212;&lt;br/&gt;
    &amp;gt; Reply to this email directly or view it on GitHub:&lt;br/&gt;
    &amp;gt; &lt;a href=&quot;https://github.com/apache/mahout/pull/8#issuecomment-45234064&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/8#issuecomment-45234064&lt;/a&gt;&lt;br/&gt;
    &amp;gt;&lt;/p&gt;</comment>
                            <comment id="14018872" author="githubbot" created="Thu, 5 Jun 2014 16:31:40 +0100"  >&lt;p&gt;Github user pferrel commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/8#issuecomment-45234551&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/8#issuecomment-45234551&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    yes, but downloading is described in the comments&lt;/p&gt;</comment>
                            <comment id="14018878" author="githubbot" created="Thu, 5 Jun 2014 16:35:26 +0100"  >&lt;p&gt;Github user pferrel commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/8#issuecomment-45235053&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/8#issuecomment-45235053&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I guess I&apos;m suggesting that examples like these might be good in the right place. Not as build tests but as usage examples. As long as they use only supported code (read/write for instance)&lt;/p&gt;</comment>
                            <comment id="14018887" author="githubbot" created="Thu, 5 Jun 2014 16:51:18 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/8#issuecomment-45237262&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/8#issuecomment-45237262&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I Like Pat&apos;s github avatar :+1: &lt;/p&gt;</comment>
                            <comment id="14018897" author="githubbot" created="Thu, 5 Jun 2014 17:00:48 +0100"  >&lt;p&gt;Github user pferrel commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/8#issuecomment-45238498&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/8#issuecomment-45238498&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    You a Ren and Stimpy fan or is it just the way you feel sometimes?&lt;/p&gt;</comment>
                            <comment id="14018902" author="githubbot" created="Thu, 5 Jun 2014 17:08:15 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/8#issuecomment-45239528&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/8#issuecomment-45239528&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    i don&apos;t even know that cartoon, just thought is was funny. Yeah, thinking of it, it is how i used to feel most of the time looking at my colleagues&apos; code at work &lt;/p&gt;</comment>
                            <comment id="14018916" author="githubbot" created="Thu, 5 Jun 2014 17:27:10 +0100"  >&lt;p&gt;Github user pferrel commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/8#issuecomment-45241940&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/8#issuecomment-45241940&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Hah, that&apos;s me looking at my own code&lt;/p&gt;</comment>
                            <comment id="14020385" author="githubbot" created="Fri, 6 Jun 2014 22:11:16 +0100"  >&lt;p&gt;GitHub user pferrel opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/12&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/12&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Mahout 1464&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1464&quot; title=&quot;Cooccurrence Analysis on Spark&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1464&quot;&gt;&lt;del&gt;MAHOUT-1464&lt;/del&gt;&lt;/a&gt; looks ready to me but can&apos;t push it yet.&lt;/p&gt;

&lt;p&gt;    My build is broken from an unrelated mr-legacy test so I&apos;ll wait to push it until my local build passes but wanted to get this out for review if anyone cares.&lt;/p&gt;

&lt;p&gt;    I took out the epinions and movielens examples, will add them back in with the CLI driver maybe. &lt;/p&gt;

&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/pferrel/mahout&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/pferrel/mahout&lt;/a&gt; mahout-1464&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/12.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/12.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #12&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 107a0ba9605241653a85b113661a8fa5c055529f&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-04T19:54:22Z&lt;/p&gt;

&lt;p&gt;    added Sebastian&apos;s CooccurrenceAnalysis patch updated it to use current Mahout-DSL&lt;/p&gt;

&lt;p&gt;commit 16c03f7fa73c156859d1dba3a333ef9e8bf922b0&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-04T21:32:18Z&lt;/p&gt;

&lt;p&gt;    added Sebastian&apos;s MurmurHash changes&lt;/p&gt;

&lt;p&gt;    Signed-off-by: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;/p&gt;

&lt;p&gt;commit c6adaa44c80bba99d41600e260bbb1ad5c972e69&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-05T16:52:23Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1464&quot; title=&quot;Cooccurrence Analysis on Spark&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1464&quot;&gt;&lt;del&gt;MAHOUT-1464&lt;/del&gt;&lt;/a&gt; import cleanup, minor changes to examples for running on Spark Cluster&lt;/p&gt;

&lt;p&gt;commit 1d66e5726e71e297ef4a7a27331463ba363098c0&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-06T20:19:32Z&lt;/p&gt;

&lt;p&gt;    scalatest for cooccurrence cross and self along with other CooccurrenceAnalyisi methods&lt;/p&gt;

&lt;p&gt;commit 766db0f9e7feb70520fbd444afcb910788f01e76&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-06T20:20:46Z&lt;/p&gt;

&lt;p&gt;    Merge branch &apos;master&apos; into mahout-1464&lt;/p&gt;

&lt;p&gt;commit e492976688cb8860354bb20a362d370405f560e1&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-06T20:50:07Z&lt;/p&gt;

&lt;p&gt;    cleaned up test comments&lt;/p&gt;

&lt;p&gt;commit a49692eb1664de4b15de1864b95701a6410c80c8&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-06T21:09:55Z&lt;/p&gt;

&lt;p&gt;    got those cursed .DS_Stores out of the branch and put an exclude in .gitignore&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="14025549" author="pferrel" created="Mon, 9 Jun 2014 19:55:43 +0100"  >&lt;p&gt;While I was waiting for the build to settle down I wrote some more tests for different value types. The same row/column is used for each input so all the LLR indicator matrices should be the same and are using the Hadoop code. But using integers of larger than 1 magnitude returns an empty indicator matrix.&lt;/p&gt;

&lt;p&gt;input:&lt;/p&gt;

&lt;p&gt;    val a = dense((1000, 10, 0, 0, 0), (0, 0, 10000, 10, 0), (0, 0, 0, 0, 100), (10000, 0, 0, 1000, 0))&lt;/p&gt;

&lt;p&gt;should produce&lt;/p&gt;

&lt;p&gt;    val matrixLLRCoocAtAControl = dense(&lt;br/&gt;
      (0.0, 1.7260924347106847, 0, 0, 0),&lt;br/&gt;
      (1.7260924347106847, 0, 0, 0, 0),&lt;br/&gt;
      (0, 0, 0, 1.7260924347106847, 0),&lt;br/&gt;
      (0, 0, 1.7260924347106847, 0, 0),&lt;br/&gt;
      (0, 0, 0, 0, 0)&lt;br/&gt;
    )&lt;/p&gt;

&lt;p&gt;however the following gets an empty matrix returned.&lt;/p&gt;

&lt;p&gt;    val drmCooc = CooccurrenceAnalysis.cooccurrences(drmARaw = drmA, drmBs = Array(drmB))&lt;br/&gt;
    //var cp = drmSelfCooc(0).checkpoint()&lt;br/&gt;
    //cp.writeDRM(&quot;/tmp/cooc-spark/&quot;)//to get values written&lt;br/&gt;
    val matrixSelfCooc = drmCooc(0).checkpoint().collect&lt;/p&gt;

&lt;p&gt;matrixSelfCooc is always empty. Took the same input to the Mahout version using LLR and it produces the correct result == matrixLLRCoocAtAControl.&lt;/p&gt;

&lt;p&gt;Still investigating why this happens.&lt;/p&gt;</comment>
                            <comment id="14025655" author="pferrel" created="Mon, 9 Jun 2014 20:55:34 +0100"  >&lt;p&gt;The indicator matrix contains self similarity. The code seems to imply that self similarity values should be excluded. Certainly the mahout itemsimilarity doesn&apos;t return them.&lt;/p&gt;</comment>
                            <comment id="14025893" author="pferrel" created="Tue, 10 Jun 2014 00:02:55 +0100"  >&lt;p&gt;seems like the downsampleAndBinarize method is returning the wrong values. It is actually summing the values where it should be counting the non-zero elements?????&lt;/p&gt;

&lt;p&gt;        // Downsample the interaction vector of each user&lt;br/&gt;
        for (userIndex &amp;lt;- 0 until keys.size) {&lt;/p&gt;

&lt;p&gt;          val interactionsOfUser = block(userIndex, :&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; // this is a Vector&lt;br/&gt;
          // if the values are non-boolean the sum will not be the number of interactions it will be a sum of strength-of-interaction, right?&lt;br/&gt;
          // val numInteractionsOfUser = interactionsOfUser.sum // doesn&apos;t this sum strength of interactions?&lt;br/&gt;
          val numInteractionsOfUser = interactionsOfUser.getNumNonZeroElements()  // should do this I think&lt;/p&gt;

&lt;p&gt;          val perUserSampleRate = math.min(maxNumInteractions, numInteractionsOfUser) / numInteractionsOfUser&lt;/p&gt;

&lt;p&gt;          interactionsOfUser.nonZeroes().foreach { elem =&amp;gt;&lt;br/&gt;
            val numInteractionsWithThing = numInteractions(elem.index)&lt;br/&gt;
            val perThingSampleRate = math.min(maxNumInteractions, numInteractionsWithThing) / numInteractionsWithThing&lt;/p&gt;

&lt;p&gt;            if (random.nextDouble() &amp;lt;= math.min(perUserSampleRate, perThingSampleRate)) &lt;/p&gt;
{
              // We ignore the original interaction value and create a binary 0-1 matrix
              // as we only consider whether interactions happened or did not happen
              downsampledBlock(userIndex, elem.index) = 1
            }
&lt;p&gt;          }&lt;/p&gt;</comment>
                            <comment id="14025933" author="githubbot" created="Tue, 10 Jun 2014 00:33:34 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/8#issuecomment-45557670&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/8#issuecomment-45557670&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Pat, so, we are not going to use this for merging into merging, i take it? I will close it, you can keep working on your other requests.&lt;/p&gt;</comment>
                            <comment id="14025966" author="githubbot" created="Tue, 10 Jun 2014 01:20:22 +0100"  >&lt;p&gt;Github user pferrel commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/8#issuecomment-45560733&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/8#issuecomment-45560733&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    According to the instructions I merge from my branch anyway. I can close it right? The instruction for closing without merging?&lt;/p&gt;

&lt;p&gt;    I assume you got my mail about finding the blocker now there are some questions about the cooccurrence algo itself.&lt;/p&gt;</comment>
                            <comment id="14026021" author="githubbot" created="Tue, 10 Jun 2014 02:47:09 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/8#issuecomment-45565428&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/8#issuecomment-45565428&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    you can close &amp;#8211; but since i originated the PR, it is easier for me (I have&lt;br/&gt;
    access to the &quot;close&quot; button on it while everyone else would have to use&lt;br/&gt;
    &quot;close apache/mahout#8&quot; commit to do the same.)&lt;/p&gt;


&lt;p&gt;    On Mon, Jun 9, 2014 at 5:20 PM, Pat Ferrel &amp;lt;notifications@github.com&amp;gt; wrote:&lt;/p&gt;

&lt;p&gt;    &amp;gt; According to the instructions I merge from my branch anyway. I can close&lt;br/&gt;
    &amp;gt; it right? The instruction for closing without merging?&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt; I assume you got my mail about finding the blocker now there are some&lt;br/&gt;
    &amp;gt; questions about the cooccurrence algo itself.&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt; &#8212;&lt;br/&gt;
    &amp;gt; Reply to this email directly or view it on GitHub&lt;br/&gt;
    &amp;gt; &amp;lt;&lt;a href=&quot;https://github.com/apache/mahout/pull/8#issuecomment-45560733&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/8#issuecomment-45560733&lt;/a&gt;&amp;gt;.&lt;br/&gt;
    &amp;gt;&lt;/p&gt;</comment>
                            <comment id="14026549" author="githubbot" created="Tue, 10 Jun 2014 16:10:52 +0100"  >&lt;p&gt;Github user pferrel commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/8#issuecomment-45626614&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/8#issuecomment-45626614&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Go ahead and hit the butto. Still have a bit more to do here.&lt;/p&gt;


&lt;p&gt;    On Jun 9, 2014, at 6:47 PM, Dmitriy Lyubimov &amp;lt;notifications@github.com&amp;gt; wrote:&lt;/p&gt;

&lt;p&gt;    you can close &amp;#8211; but since i originated the PR, it is easier for me (I have &lt;br/&gt;
    access to the &quot;close&quot; button on it while everyone else would have to use &lt;br/&gt;
    &quot;close apache/mahout#8&quot; commit to do the same.) &lt;/p&gt;


&lt;p&gt;    On Mon, Jun 9, 2014 at 5:20 PM, Pat Ferrel &amp;lt;notifications@github.com&amp;gt; wrote: &lt;/p&gt;

&lt;p&gt;    &amp;gt; According to the instructions I merge from my branch anyway. I can close &lt;br/&gt;
    &amp;gt; it right? The instruction for closing without merging? &lt;br/&gt;
    &amp;gt; &lt;br/&gt;
    &amp;gt; I assume you got my mail about finding the blocker now there are some &lt;br/&gt;
    &amp;gt; questions about the cooccurrence algo itself. &lt;br/&gt;
    &amp;gt; &lt;br/&gt;
    &amp;gt; &#8212; &lt;br/&gt;
    &amp;gt; Reply to this email directly or view it on GitHub &lt;br/&gt;
    &amp;gt; &amp;lt;&lt;a href=&quot;https://github.com/apache/mahout/pull/8#issuecomment-45560733&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/8#issuecomment-45560733&lt;/a&gt;&amp;gt;. &lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &#8212;&lt;br/&gt;
    Reply to this email directly or view it on GitHub.&lt;/p&gt;</comment>
                            <comment id="14026719" author="githubbot" created="Tue, 10 Jun 2014 18:25:27 +0100"  >&lt;p&gt;Github user dlyubimov closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/8&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/8&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14026738" author="githubbot" created="Tue, 10 Jun 2014 18:33:59 +0100"  >&lt;p&gt;Github user sscdotopen commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/8#issuecomment-45646072&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/8#issuecomment-45646072&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    the cooccurrence analysis code should go to the math-scala not spark module, as it is independent of the underlying engine.&lt;/p&gt;</comment>
                            <comment id="14026742" author="githubbot" created="Tue, 10 Jun 2014 18:35:56 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/12#issuecomment-45646307&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/12#issuecomment-45646307&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    i assume this is current PR for &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1464&quot; title=&quot;Cooccurrence Analysis on Spark&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1464&quot;&gt;&lt;del&gt;MAHOUT-1464&lt;/del&gt;&lt;/a&gt;?&lt;/p&gt;</comment>
                            <comment id="14027159" author="pferrel" created="Tue, 10 Jun 2014 23:42:09 +0100"  >&lt;p&gt;I think the same thing is happening with number of item interactions:&lt;/p&gt;

&lt;p&gt;    // Broadcast vector containing the number of interactions with each thing&lt;br/&gt;
    val bcastNumInteractions = drmBroadcast(drmI.colSums)// sums?&lt;/p&gt;

&lt;p&gt;This broadcasts a vector of sums. We need a getNumNonZeroElements() for column vectors,  actually a way to get a Vector of nonZero counts per column? We could get them from rows of the transposed matrix before doing the multiply of At %&lt;b&gt;% A or B.t %&lt;/b&gt;% A in which case we&#8217;d get non-zero counts from the rows. Either way I don&#8217;t see a way to get a vector of these values without doing a mapBlock on the transposed matrix. Am I missing something?&lt;/p&gt;

&lt;p&gt;Currently the IndexedDataset is a very thin wrapper but I could add two vectors, which contain number of non-zero elements for rows and columns. In this case I would have it extend CheckpointedDrm perhaps. Since CheckpointedDrm extends DrmLike it could be used in the DSL algebra directly, in which case it would be simple to do the right thing with these vectors as well as the two id dictionaries for transpose and multiply but it&#8217;s a slippery slope.&lt;/p&gt;

&lt;p&gt;Before I go off in the wrong direction is there an existing way to get a vector of non-zero counts for rows or columns?&lt;/p&gt;</comment>
                            <comment id="14027182" author="tdunning" created="Wed, 11 Jun 2014 00:07:10 +0100"  >&lt;p&gt;I don&apos;t think that numNonZero can be trusted here.  The contract it provides is to return an upper bound on the number of non-zeros, not a precise value.&lt;/p&gt;

&lt;p&gt;Better to write specific code.&lt;/p&gt;
</comment>
                            <comment id="14027202" author="pferrel" created="Wed, 11 Jun 2014 00:22:30 +0100"  >&lt;p&gt;OK, good to know. So the fix above for rows is not good either, oh bother.&lt;/p&gt;

&lt;p&gt;If I have to write specific code might it be better put in the Drm and/or Vector?&lt;/p&gt;</comment>
                            <comment id="14027208" author="tdunning" created="Wed, 11 Jun 2014 00:27:33 +0100"  >&lt;p&gt;Matrix and Vector already have something that can be used:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    Vector counts = x.aggregateColumns(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; VectorFunction() {
      @Override
      &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;double&lt;/span&gt; apply(Vector f) {
        &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; f.aggregate(Functions.PLUS, Functions.greater(0));
      }
    });
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="14027986" author="pferrel" created="Wed, 11 Jun 2014 17:38:35 +0100"  >&lt;p&gt;The algo transposes A (the primary) before self-coocurrence. That gives us a point to look at columns when they are rows, which in turn makes distributed ops on the drm simple. So rather than looking at the counts for columns, my earlier proposal was to look at the same data when it is a row. Might this be better since it can easily be a distributed calculation?&lt;/p&gt;

&lt;p&gt;In other words since A.t * A is calculated, we can split this into transpose and multiply taking column counts from the rows of A.t then doing the multiply after. In the list of calculations: A.t * A, B.t * A, ... each include a state where the columns turn into rows and so the same approach can be used.&lt;/p&gt;

&lt;p&gt;This introduces what was a bug as a significant optimization. If the data is already boolean, use the colSums then no distributed counting is needed.&lt;/p&gt;

&lt;p&gt;Not sure if the above is all true, so read it as a question&lt;/p&gt;
</comment>
                            <comment id="14028342" author="tdunning" created="Wed, 11 Jun 2014 21:27:36 +0100"  >&lt;p&gt;I don&apos;t understand the question.&lt;/p&gt;

&lt;p&gt;In fact, the transpose is never computed explicitly.  There is a special operation that does A&apos; A in a single pass and step.  It is possible to fuse the down-sampling into this multiplication, but not possible to fuse the column counts.  For large sparse A, the value of A&apos;A is computed using a map-reduce style data flow where each row is examined and all cooccurrence counts are emitted to be grouped by row number later.&lt;/p&gt;

&lt;p&gt;In order to save memory, it is probably a good idea to discard the original counts as soon as they are reduced to binary form and down-sampled.&lt;/p&gt;

&lt;p&gt;For computing counts, it is possible to accumulate column sums in a row-wise accumulator at the same time that row sums are accumulated one element at a time.  This avoids a pass over A and probably helps significantly on speed.&lt;/p&gt;</comment>
                            <comment id="14028360" author="ssc" created="Wed, 11 Jun 2014 21:39:02 +0100"  >&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;The computation of A&apos;A is usually done without explicitly forming A&apos;. &lt;br/&gt;
Instead A&apos;A is computed as the sum of outer products of rows of A.&lt;/p&gt;

&lt;p&gt;--sebastian&lt;/p&gt;

</comment>
                            <comment id="14028760" author="pferrel" created="Thu, 12 Jun 2014 03:39:09 +0100"  >&lt;p&gt;Ok, learned something today.&lt;/p&gt;

&lt;p&gt;As to using the Java&apos;s x.aggregateColumns it looks like there are distributed Spark versions of colSums and the rest. They use Spark accumulators to avoid pulling the entire matrix into memory. I followed those models and created &quot;colCounts&quot; in MatrixOps and SparkEngine. Then used it instead of colSums.&lt;/p&gt;

&lt;p&gt;Cooccurrence now passes tests with non-boolean data.&lt;/p&gt;

&lt;p&gt;Scary adding to Dmitriy&apos;s code though so I&apos;ll invite him to look at it. Added a couple tests but I don&apos;t see many for SparkEngine.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/pferrel/mahout/compare/mahout-1464&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/pferrel/mahout/compare/mahout-1464&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Still having problems getting mr-legacy to pass tests spark and match-scala pass tests.&lt;/p&gt;</comment>
                            <comment id="14028773" author="tdunning" created="Thu, 12 Jun 2014 04:02:13 +0100"  >&lt;p&gt;Should there be a dedicated colCounts function, or a more general accumulator?&lt;/p&gt;

&lt;p&gt;Basically, a row-by-row or column-by-column map-reduce aggregator is a common thing to need.  This is different from the aggregateColumns we now have since what we have now doesn&apos;t requires access to the entire row.&lt;/p&gt;

&lt;p&gt;What I would be more interested in would be something like&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
      Vector r = v.aggregateByRows(DoubleDoubleFunction combine, DoubleFunction map)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The virtue here is that iteration by rows is an efficient way to handle row-major arrangements, but iteration by column works as well:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
      &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (MatrixSlice row : m) {
           &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; i = 0; i &amp;lt; columns; i++) {
                r.setQuick(combine.apply(r.getQuick(i), map.apply(row.getQuick(i))));
           }
      }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;or&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
     &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (MatrixSlice col: m.columnIterator()) {
         r.setQuick(col.index(), col.aggregate(combine, map));
    }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;These are approximate and we don&apos;t really have a columnIterator, but you can imagine how some kinds of matrix would have such a thing internally.  You can also see how trivially these would be to parallelize.  Arrangements which have row-wise patches of column-major data would also be easy to handle by combining these patterns.&lt;/p&gt;


</comment>
                            <comment id="14028775" author="tdunning" created="Thu, 12 Jun 2014 04:03:00 +0100"  >&lt;blockquote&gt;
&lt;p&gt;Scary adding to Dmitriy&apos;s code though so I&apos;ll invite him to look at it. Added a couple tests but I don&apos;t see many for SparkEngine.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We don&apos;t have author tags.  This is &lt;b&gt;our&lt;/b&gt; code now and we all have to feel a bit of ownership and entitlement.  Go for it.&lt;/p&gt;</comment>
                            <comment id="14028805" author="pferrel" created="Thu, 12 Jun 2014 05:10:22 +0100"  >&lt;p&gt;Check the code and let me know if there are problems. It uses a spark accumulator Vector keeping track of the non-zero column counts. Accumulators seem like a nice simplification.&lt;/p&gt;

&lt;p&gt;point #2: Still I can only read about 50% of D&apos;s code and can only keep in my brain about 10% of the overlapping traits, and classes. Not concerned with authorship, just correctness.&lt;/p&gt;</comment>
                            <comment id="14028835" author="dlyubimov" created="Thu, 12 Jun 2014 06:23:11 +0100"  >&lt;p&gt;put comments on PR. &lt;/p&gt;

&lt;p&gt;BTW In order for PR comments to mirror in JIRA, you need to use &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1464&quot; title=&quot;Cooccurrence Analysis on Spark&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1464&quot;&gt;&lt;del&gt;MAHOUT-1464&lt;/del&gt;&lt;/a&gt; phrase in PR name, not Mahout 146e or Mahout-1646 (that&apos;s the way ASF bot apparently works)&lt;/p&gt;</comment>
                            <comment id="14029310" author="githubbot" created="Thu, 12 Jun 2014 17:06:04 +0100"  >&lt;p&gt;Github user pferrel commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/12#issuecomment-45912084&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/12#issuecomment-45912084&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Awaiting Sebastian&apos;s take on the naming of &apos;colCounts&apos; to better fit R-Like Semantics&lt;/p&gt;</comment>
                            <comment id="14029315" author="githubbot" created="Thu, 12 Jun 2014 17:16:20 +0100"  >&lt;p&gt;Github user tdunning commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/12#issuecomment-45913357&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/12#issuecomment-45913357&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    This discussion isn&apos;t getting echoed to the mailing list.  I didn&apos;t even know it was happening.&lt;/p&gt;

&lt;p&gt;    I think that a non-zero counter is nice, but it would be better to have a more general general aggregator of somethings.  We have two instances already of this pattern and there will be more (sum of the abs values is common).&lt;/p&gt;

&lt;p&gt;    Why not implement a general aggregator?  THis is different from our current aggregateColumns because that function is not parallelizable.&lt;/p&gt;

&lt;p&gt;    Something like def columnAggregator(combiner, mapper) is what I am aiming for.  Positive counter would be m.columnAggregator(_ + _, _ &amp;gt; 0)&lt;/p&gt;

</comment>
                            <comment id="14029339" author="githubbot" created="Thu, 12 Jun 2014 17:35:24 +0100"  >&lt;p&gt;Github user dlyubimov commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/12#discussion_r13711381&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/12#discussion_r13711381&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: math-scala/src/main/scala/org/apache/mahout/math/scalabindings/MatrixOps.scala &amp;#8212;&lt;br/&gt;
    @@ -188,4 +188,8 @@ object MatrixOps &lt;/p&gt;
{
         def apply(f: Vector): Double = f.sum
       }

&lt;p&gt;    +  private def vectorCountFunc = new VectorFunction &lt;/p&gt;
{
    +    def apply(f: Vector): Double = f.aggregate(Functions.PLUS, Functions.greater(0))
    +  }
&lt;p&gt;    +&lt;br/&gt;
     }&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    it looks like, to me. don&apos;t have time to look in depth. but distributed code definitely counts non-negatives with explicit inline conditional &amp;gt;0&lt;/p&gt;</comment>
                            <comment id="14029344" author="githubbot" created="Thu, 12 Jun 2014 17:36:09 +0100"  >&lt;p&gt;Github user dlyubimov commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/12#discussion_r13711414&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/12#discussion_r13711414&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: math-scala/src/main/scala/org/apache/mahout/math/scalabindings/MatrixOps.scala &amp;#8212;&lt;br/&gt;
    @@ -188,4 +188,8 @@ object MatrixOps &lt;/p&gt;
{
         def apply(f: Vector): Double = f.sum
       }

&lt;p&gt;    +  private def vectorCountFunc = new VectorFunction &lt;/p&gt;
{
    +    def apply(f: Vector): Double = f.aggregate(Functions.PLUS, Functions.greater(0))
    +  }
&lt;p&gt;    +&lt;br/&gt;
     }&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    it is very easy to tweak tests though to check if in doubt&lt;/p&gt;</comment>
                            <comment id="14029345" author="githubbot" created="Thu, 12 Jun 2014 17:37:17 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/12#issuecomment-45915940&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/12#issuecomment-45915940&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    fix header to say &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1464&quot; title=&quot;Cooccurrence Analysis on Spark&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1464&quot;&gt;&lt;del&gt;MAHOUT-1464&lt;/del&gt;&lt;/a&gt;, then hit close and reopen, it will restart the echo.&lt;/p&gt;</comment>
                            <comment id="14029374" author="githubbot" created="Thu, 12 Jun 2014 17:47:14 +0100"  >&lt;p&gt;Github user pferrel commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/12#issuecomment-45917234&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/12#issuecomment-45917234&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I already fixed the header.&lt;/p&gt;

&lt;p&gt;    I agree with Ted, kinda what functional programming is for. The reason I didn&apos;t use the Java aggregate is because it wasn&apos;t distributed. Still probably beyond this ticket. I&apos;ll refactor if a Scala journeyman wants to provide a general mechanism. I&apos;m still on training wheels.&lt;/p&gt;

&lt;p&gt;    This still needs to be tested in a distributed Spark+HDFS environment and &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1561&quot; title=&quot;cluster-syntheticcontrol.sh not running locally with MAHOUT_LOCAL=true&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1561&quot;&gt;&lt;del&gt;MAHOUT-1561&lt;/del&gt;&lt;/a&gt; will make testing easy. I&apos;d be happy to merge this and move on, which will have the side effect of testing larger datasets and clusters.&lt;/p&gt;

&lt;p&gt;    If Someone wants to test this now on a Spark+HDFS cluster, please do!&lt;/p&gt;</comment>
                            <comment id="14029441" author="githubbot" created="Thu, 12 Jun 2014 18:20:24 +0100"  >&lt;p&gt;Github user sscdotopen commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/12#issuecomment-45921381&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/12#issuecomment-45921381&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I think the name &lt;em&gt;colCounts&lt;/em&gt; is misleading, we should stick to something like numNonZeroElementsPerColumn or so, not sure here.&lt;/p&gt;</comment>
                            <comment id="14029445" author="githubbot" created="Thu, 12 Jun 2014 18:21:14 +0100"  >&lt;p&gt;Github user sscdotopen commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/12#discussion_r13713951&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/12#discussion_r13713951&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: spark/src/main/scala/org/apache/mahout/cf/CooccurrenceAnalysis.scala &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,214 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
    + * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
    + * this work for additional information regarding copyright ownership.&lt;br/&gt;
    + * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
    + * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
    + * the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.mahout.cf&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.mahout.math._&lt;br/&gt;
    +import scalabindings._&lt;br/&gt;
    +import RLikeOps._&lt;br/&gt;
    +import drm._&lt;br/&gt;
    +import RLikeDrmOps._&lt;br/&gt;
    +import org.apache.mahout.sparkbindings._&lt;br/&gt;
    +import scala.collection.JavaConversions._&lt;br/&gt;
    +import org.apache.mahout.math.stats.LogLikelihood&lt;br/&gt;
    +import collection._&lt;br/&gt;
    +import org.apache.mahout.common.RandomUtils&lt;br/&gt;
    +import org.apache.mahout.math.function.&lt;/p&gt;
{VectorFunction, Functions}
&lt;p&gt;    +&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * based on &quot;Ted Dunnning &amp;amp; Ellen Friedman: Practical Machine Learning, Innovations in Recommendation&quot;,&lt;br/&gt;
    + * available at &lt;a href=&quot;http://www.mapr.com/practical-machine-learning&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.mapr.com/practical-machine-learning&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * see also &quot;Sebastian Schelter, Christoph Boden, Volker Markl:&lt;br/&gt;
    + * Scalable Similarity-Based Neighborhood Methods with MapReduce&lt;br/&gt;
    + * ACM Conference on Recommender Systems 2012&quot;&lt;br/&gt;
    + */&lt;br/&gt;
    +object CooccurrenceAnalysis extends Serializable {&lt;br/&gt;
    +&lt;br/&gt;
    +  /** Compares (Int,Double) pairs by the second value */&lt;br/&gt;
    +  private val orderByScore = Ordering.fromLessThan&lt;span class=&quot;error&quot;&gt;&amp;#91;(Int, Double)&amp;#93;&lt;/span&gt; &lt;/p&gt;
{ case ((_, score1), (_, score2)) =&amp;gt; score1 &amp;gt; score2}
&lt;p&gt;    +&lt;br/&gt;
    +  def cooccurrences(drmARaw: DrmLike&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;, randomSeed: Int = 0xdeadbeef, maxInterestingItemsPerThing: Int = 50,&lt;br/&gt;
    +                    maxNumInteractions: Int = 500, drmBs: Array[DrmLike&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;] = Array()): List[DrmLike&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;] = {&lt;br/&gt;
    +&lt;br/&gt;
    +    implicit val distributedContext = drmARaw.context&lt;br/&gt;
    +&lt;br/&gt;
    +    // Apply selective downsampling, pin resulting matrix&lt;br/&gt;
    +    val drmA = sampleDownAndBinarize(drmARaw, randomSeed, maxNumInteractions)&lt;br/&gt;
    +&lt;br/&gt;
    +    // num users, which equals the maximum number of interactions per item&lt;br/&gt;
    +    val numUsers = drmA.nrow.toInt&lt;br/&gt;
    +&lt;br/&gt;
    +    // Compute &amp;amp; broadcast the number of interactions per thing in A&lt;br/&gt;
    +    val bcastInteractionsPerItemA = drmBroadcast(drmA.colCounts)&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    drmA is already binary here, so we could use colSums&lt;/p&gt;</comment>
                            <comment id="14029446" author="githubbot" created="Thu, 12 Jun 2014 18:21:39 +0100"  >&lt;p&gt;Github user sscdotopen commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/12#discussion_r13713968&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/12#discussion_r13713968&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: spark/src/main/scala/org/apache/mahout/cf/CooccurrenceAnalysis.scala &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,214 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
    + * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
    + * this work for additional information regarding copyright ownership.&lt;br/&gt;
    + * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
    + * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
    + * the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.mahout.cf&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.mahout.math._&lt;br/&gt;
    +import scalabindings._&lt;br/&gt;
    +import RLikeOps._&lt;br/&gt;
    +import drm._&lt;br/&gt;
    +import RLikeDrmOps._&lt;br/&gt;
    +import org.apache.mahout.sparkbindings._&lt;br/&gt;
    +import scala.collection.JavaConversions._&lt;br/&gt;
    +import org.apache.mahout.math.stats.LogLikelihood&lt;br/&gt;
    +import collection._&lt;br/&gt;
    +import org.apache.mahout.common.RandomUtils&lt;br/&gt;
    +import org.apache.mahout.math.function.&lt;/p&gt;
{VectorFunction, Functions}
&lt;p&gt;    +&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * based on &quot;Ted Dunnning &amp;amp; Ellen Friedman: Practical Machine Learning, Innovations in Recommendation&quot;,&lt;br/&gt;
    + * available at &lt;a href=&quot;http://www.mapr.com/practical-machine-learning&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.mapr.com/practical-machine-learning&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * see also &quot;Sebastian Schelter, Christoph Boden, Volker Markl:&lt;br/&gt;
    + * Scalable Similarity-Based Neighborhood Methods with MapReduce&lt;br/&gt;
    + * ACM Conference on Recommender Systems 2012&quot;&lt;br/&gt;
    + */&lt;br/&gt;
    +object CooccurrenceAnalysis extends Serializable {&lt;br/&gt;
    +&lt;br/&gt;
    +  /** Compares (Int,Double) pairs by the second value */&lt;br/&gt;
    +  private val orderByScore = Ordering.fromLessThan&lt;span class=&quot;error&quot;&gt;&amp;#91;(Int, Double)&amp;#93;&lt;/span&gt; &lt;/p&gt;
{ case ((_, score1), (_, score2)) =&amp;gt; score1 &amp;gt; score2}
&lt;p&gt;    +&lt;br/&gt;
    +  def cooccurrences(drmARaw: DrmLike&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;, randomSeed: Int = 0xdeadbeef, maxInterestingItemsPerThing: Int = 50,&lt;br/&gt;
    +                    maxNumInteractions: Int = 500, drmBs: Array[DrmLike&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;] = Array()): List[DrmLike&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;] = {&lt;br/&gt;
    +&lt;br/&gt;
    +    implicit val distributedContext = drmARaw.context&lt;br/&gt;
    +&lt;br/&gt;
    +    // Apply selective downsampling, pin resulting matrix&lt;br/&gt;
    +    val drmA = sampleDownAndBinarize(drmARaw, randomSeed, maxNumInteractions)&lt;br/&gt;
    +&lt;br/&gt;
    +    // num users, which equals the maximum number of interactions per item&lt;br/&gt;
    +    val numUsers = drmA.nrow.toInt&lt;br/&gt;
    +&lt;br/&gt;
    +    // Compute &amp;amp; broadcast the number of interactions per thing in A&lt;br/&gt;
    +    val bcastInteractionsPerItemA = drmBroadcast(drmA.colCounts)&lt;br/&gt;
    +&lt;br/&gt;
    +    // Compute co-occurrence matrix A&apos;A&lt;br/&gt;
    +    val drmAtA = drmA.t %*% drmA&lt;br/&gt;
    +&lt;br/&gt;
    +    // Compute loglikelihood scores and sparsify the resulting matrix to get the indicator matrix&lt;br/&gt;
    +    val drmIndicatorsAtA = computeIndicators(drmAtA, numUsers, maxInterestingItemsPerThing, bcastInteractionsPerItemA,&lt;br/&gt;
    +      bcastInteractionsPerItemA, crossCooccurrence = false)&lt;br/&gt;
    +&lt;br/&gt;
    +    var indicatorMatrices = List(drmIndicatorsAtA)&lt;br/&gt;
    +&lt;br/&gt;
    +    // Now look at cross-co-occurrences&lt;br/&gt;
    +    for (drmBRaw &amp;lt;- drmBs) {&lt;br/&gt;
    +      // Down-sample and pin other interaction matrix&lt;br/&gt;
    +      val drmB = sampleDownAndBinarize(drmBRaw, randomSeed, maxNumInteractions).checkpoint()&lt;br/&gt;
    +&lt;br/&gt;
    +      // Compute &amp;amp; broadcast the number of interactions per thing in B&lt;br/&gt;
    +      val bcastInteractionsPerThingB = drmBroadcast(drmB.colCounts)&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    drmB is already binary here, so we could use colSums&lt;/p&gt;</comment>
                            <comment id="14029449" author="githubbot" created="Thu, 12 Jun 2014 18:22:36 +0100"  >&lt;p&gt;Github user sscdotopen commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/12#discussion_r13714024&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/12#discussion_r13714024&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: spark/src/main/scala/org/apache/mahout/cf/CooccurrenceAnalysis.scala &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,214 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
    + * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
    + * this work for additional information regarding copyright ownership.&lt;br/&gt;
    + * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
    + * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
    + * the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.mahout.cf&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.mahout.math._&lt;br/&gt;
    +import scalabindings._&lt;br/&gt;
    +import RLikeOps._&lt;br/&gt;
    +import drm._&lt;br/&gt;
    +import RLikeDrmOps._&lt;br/&gt;
    +import org.apache.mahout.sparkbindings._&lt;br/&gt;
    +import scala.collection.JavaConversions._&lt;br/&gt;
    +import org.apache.mahout.math.stats.LogLikelihood&lt;br/&gt;
    +import collection._&lt;br/&gt;
    +import org.apache.mahout.common.RandomUtils&lt;br/&gt;
    +import org.apache.mahout.math.function.&lt;/p&gt;
{VectorFunction, Functions}
&lt;p&gt;    +&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * based on &quot;Ted Dunnning &amp;amp; Ellen Friedman: Practical Machine Learning, Innovations in Recommendation&quot;,&lt;br/&gt;
    + * available at &lt;a href=&quot;http://www.mapr.com/practical-machine-learning&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.mapr.com/practical-machine-learning&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * see also &quot;Sebastian Schelter, Christoph Boden, Volker Markl:&lt;br/&gt;
    + * Scalable Similarity-Based Neighborhood Methods with MapReduce&lt;br/&gt;
    + * ACM Conference on Recommender Systems 2012&quot;&lt;br/&gt;
    + */&lt;br/&gt;
    +object CooccurrenceAnalysis extends Serializable {&lt;br/&gt;
    +&lt;br/&gt;
    +  /** Compares (Int,Double) pairs by the second value */&lt;br/&gt;
    +  private val orderByScore = Ordering.fromLessThan&lt;span class=&quot;error&quot;&gt;&amp;#91;(Int, Double)&amp;#93;&lt;/span&gt; &lt;/p&gt;
{ case ((_, score1), (_, score2)) =&amp;gt; score1 &amp;gt; score2}
&lt;p&gt;    +&lt;br/&gt;
    +  def cooccurrences(drmARaw: DrmLike&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;, randomSeed: Int = 0xdeadbeef, maxInterestingItemsPerThing: Int = 50,&lt;br/&gt;
    +                    maxNumInteractions: Int = 500, drmBs: Array[DrmLike&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;] = Array()): List[DrmLike&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;] = {&lt;br/&gt;
    +&lt;br/&gt;
    +    implicit val distributedContext = drmARaw.context&lt;br/&gt;
    +&lt;br/&gt;
    +    // Apply selective downsampling, pin resulting matrix&lt;br/&gt;
    +    val drmA = sampleDownAndBinarize(drmARaw, randomSeed, maxNumInteractions)&lt;br/&gt;
    +&lt;br/&gt;
    +    // num users, which equals the maximum number of interactions per item&lt;br/&gt;
    +    val numUsers = drmA.nrow.toInt&lt;br/&gt;
    +&lt;br/&gt;
    +    // Compute &amp;amp; broadcast the number of interactions per thing in A&lt;br/&gt;
    +    val bcastInteractionsPerItemA = drmBroadcast(drmA.colCounts)&lt;br/&gt;
    +&lt;br/&gt;
    +    // Compute co-occurrence matrix A&apos;A&lt;br/&gt;
    +    val drmAtA = drmA.t %*% drmA&lt;br/&gt;
    +&lt;br/&gt;
    +    // Compute loglikelihood scores and sparsify the resulting matrix to get the indicator matrix&lt;br/&gt;
    +    val drmIndicatorsAtA = computeIndicators(drmAtA, numUsers, maxInterestingItemsPerThing, bcastInteractionsPerItemA,&lt;br/&gt;
    +      bcastInteractionsPerItemA, crossCooccurrence = false)&lt;br/&gt;
    +&lt;br/&gt;
    +    var indicatorMatrices = List(drmIndicatorsAtA)&lt;br/&gt;
    +&lt;br/&gt;
    +    // Now look at cross-co-occurrences&lt;br/&gt;
    +    for (drmBRaw &amp;lt;- drmBs) &lt;/p&gt;
{
    +      // Down-sample and pin other interaction matrix
    +      val drmB = sampleDownAndBinarize(drmBRaw, randomSeed, maxNumInteractions).checkpoint()
    +
    +      // Compute &amp;amp; broadcast the number of interactions per thing in B
    +      val bcastInteractionsPerThingB = drmBroadcast(drmB.colCounts)
    +
    +      // Compute cross-co-occurrence matrix B&apos;A
    +      val drmBtA = drmB.t %*% drmA
    +
    +      val drmIndicatorsBtA = computeIndicators(drmBtA, numUsers, maxInterestingItemsPerThing,
    +        bcastInteractionsPerThingB, bcastInteractionsPerItemA)
    +
    +      indicatorMatrices = indicatorMatrices :+ drmIndicatorsBtA
    +
    +      drmB.uncache()
    +    }
&lt;p&gt;    +&lt;br/&gt;
    +    // Unpin downsampled interaction matrix&lt;br/&gt;
    +    drmA.uncache()&lt;br/&gt;
    +&lt;br/&gt;
    +    // Return list of indicator matrices&lt;br/&gt;
    +    indicatorMatrices&lt;br/&gt;
    +  }&lt;br/&gt;
    +&lt;br/&gt;
    +  /**&lt;br/&gt;
    +   * Compute loglikelihood ratio&lt;br/&gt;
    +   * see &lt;a href=&quot;http://tdunning.blogspot.de/2008/03/surprise-and-coincidence.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://tdunning.blogspot.de/2008/03/surprise-and-coincidence.html&lt;/a&gt; for details&lt;br/&gt;
    +   **/&lt;br/&gt;
    +  def loglikelihoodRatio(numInteractionsWithA: Long, numInteractionsWithB: Long,&lt;br/&gt;
    +                         numInteractionsWithAandB: Long, numInteractions: Long) = &lt;/p&gt;
{
    +
    +    val k11 = numInteractionsWithAandB
    +    val k12 = numInteractionsWithA - numInteractionsWithAandB
    +    val k21 = numInteractionsWithB - numInteractionsWithAandB
    +    val k22 = numInteractions - numInteractionsWithA - numInteractionsWithB + numInteractionsWithAandB
    +
    +    LogLikelihood.logLikelihoodRatio(k11, k12, k21, k22)
    +  }
&lt;p&gt;    +&lt;br/&gt;
    +  def computeIndicators(drmBtA: DrmLike&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;, numUsers: Int, maxInterestingItemsPerThing: Int,&lt;br/&gt;
    +                        bcastNumInteractionsB: BCast&lt;span class=&quot;error&quot;&gt;&amp;#91;Vector&amp;#93;&lt;/span&gt;, bcastNumInteractionsA: BCast&lt;span class=&quot;error&quot;&gt;&amp;#91;Vector&amp;#93;&lt;/span&gt;,&lt;br/&gt;
    +                        crossCooccurrence: Boolean = true) = {&lt;br/&gt;
    +    drmBtA.mapBlock() {&lt;br/&gt;
    +      case (keys, block) =&amp;gt;&lt;br/&gt;
    +&lt;br/&gt;
    +        val llrBlock = block.like()&lt;br/&gt;
    +        val numInteractionsB: Vector = bcastNumInteractionsB&lt;br/&gt;
    +        val numInteractionsA: Vector = bcastNumInteractionsA&lt;br/&gt;
    +&lt;br/&gt;
    +        for (index &amp;lt;- 0 until keys.size) {&lt;br/&gt;
    +&lt;br/&gt;
    +          val thingB = keys(index)&lt;br/&gt;
    +&lt;br/&gt;
    +          // PriorityQueue to select the top-k items&lt;br/&gt;
    +          val topItemsPerThing = new mutable.PriorityQueue&lt;span class=&quot;error&quot;&gt;&amp;#91;(Int, Double)&amp;#93;&lt;/span&gt;()(orderByScore)&lt;br/&gt;
    +&lt;br/&gt;
    +          block(index, :&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;.nonZeroes().foreach { elem =&amp;gt;&lt;br/&gt;
    +            val thingA = elem.index&lt;br/&gt;
    +            val cooccurrences = elem.get&lt;br/&gt;
    +&lt;br/&gt;
    +            // exclude co-occurrences of the item with itself&lt;br/&gt;
    +            if (crossCooccurrence || thingB != thingA) {&lt;br/&gt;
    +              // Compute loglikelihood ratio&lt;br/&gt;
    +              val llrRatio = loglikelihoodRatio(numInteractionsB(thingB).toLong, numInteractionsA(thingA).toLong,&lt;br/&gt;
    +                cooccurrences.toLong, numUsers)&lt;br/&gt;
    +              val candidate = thingA -&amp;gt; llrRatio&lt;br/&gt;
    +&lt;br/&gt;
    +              // Enqueue item with score, if belonging to the top-k&lt;br/&gt;
    +              if (topItemsPerThing.size &amp;lt; maxInterestingItemsPerThing) &lt;/p&gt;
{
    +                topItemsPerThing.enqueue(candidate)
    +              }
&lt;p&gt; else if (orderByScore.lt(candidate, topItemsPerThing.head)) &lt;/p&gt;
{
    +                topItemsPerThing.dequeue()
    +                topItemsPerThing.enqueue(candidate)
    +              }
&lt;p&gt;    +            }&lt;br/&gt;
    +          }&lt;br/&gt;
    +&lt;br/&gt;
    +          // Add top-k interesting items to the output matrix&lt;br/&gt;
    +          topItemsPerThing.dequeueAll.foreach &lt;/p&gt;
{
    +            case (otherThing, llrScore) =&amp;gt;
    +              llrBlock(index, otherThing) = llrScore
    +          }
&lt;p&gt;    +        }&lt;br/&gt;
    +&lt;br/&gt;
    +        keys -&amp;gt; llrBlock&lt;br/&gt;
    +    }&lt;br/&gt;
    +  }&lt;br/&gt;
    +&lt;br/&gt;
    +  /**&lt;br/&gt;
    +   * Selectively downsample users and things with an anomalous amount of interactions, inspired by&lt;br/&gt;
    +   * &lt;a href=&quot;https://github.com/tdunning/in-memory-cooccurrence/blob/master/src/main/java/com/tdunning/cooc/Analyze.java&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/tdunning/in-memory-cooccurrence/blob/master/src/main/java/com/tdunning/cooc/Analyze.java&lt;/a&gt;&lt;br/&gt;
    +   *&lt;br/&gt;
    +   * additionally binarizes input matrix, as we&apos;re only interesting in knowing whether interactions happened or not&lt;br/&gt;
    +   */&lt;br/&gt;
    +  def sampleDownAndBinarize(drmM: DrmLike&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;, seed: Int, maxNumInteractions: Int) = {&lt;br/&gt;
    +&lt;br/&gt;
    +    implicit val distributedContext = drmM.context&lt;br/&gt;
    +&lt;br/&gt;
    +    // Pin raw interaction matrix&lt;br/&gt;
    +    val drmI = drmM.checkpoint()&lt;br/&gt;
    +&lt;br/&gt;
    +    // Broadcast vector containing the number of interactions with each thing&lt;br/&gt;
    +    val bcastNumInteractions = drmBroadcast(drmI.colCounts)&lt;br/&gt;
    +&lt;br/&gt;
    +    val downSampledDrmI = drmI.mapBlock() {&lt;br/&gt;
    +      case (keys, block) =&amp;gt;&lt;br/&gt;
    +        val numInteractions: Vector = bcastNumInteractions&lt;br/&gt;
    +&lt;br/&gt;
    +        // Use a hash of the unique first key to seed the RNG, makes this computation repeatable in case of failures&lt;br/&gt;
    +        val random = RandomUtils.getRandom(MurmurHash.hash(keys(0), seed))&lt;br/&gt;
    +&lt;br/&gt;
    +        val downsampledBlock = block.like()&lt;br/&gt;
    +&lt;br/&gt;
    +        // Downsample the interaction vector of each user&lt;br/&gt;
    +        for (userIndex &amp;lt;- 0 until keys.size) {&lt;br/&gt;
    +&lt;br/&gt;
    +          val interactionsOfUser = block(userIndex, :&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
    +&lt;br/&gt;
    +          //todo: can we trust getNumNonZeroElements or is this the upper limit? May have to actually count them?&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    you can trust getNumNonZeroElements.&lt;/p&gt;</comment>
                            <comment id="14029464" author="githubbot" created="Thu, 12 Jun 2014 18:28:08 +0100"  >&lt;p&gt;Github user pferrel commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/12#discussion_r13714291&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/12#discussion_r13714291&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: spark/src/main/scala/org/apache/mahout/cf/CooccurrenceAnalysis.scala &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,214 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
    + * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
    + * this work for additional information regarding copyright ownership.&lt;br/&gt;
    + * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
    + * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
    + * the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.mahout.cf&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.mahout.math._&lt;br/&gt;
    +import scalabindings._&lt;br/&gt;
    +import RLikeOps._&lt;br/&gt;
    +import drm._&lt;br/&gt;
    +import RLikeDrmOps._&lt;br/&gt;
    +import org.apache.mahout.sparkbindings._&lt;br/&gt;
    +import scala.collection.JavaConversions._&lt;br/&gt;
    +import org.apache.mahout.math.stats.LogLikelihood&lt;br/&gt;
    +import collection._&lt;br/&gt;
    +import org.apache.mahout.common.RandomUtils&lt;br/&gt;
    +import org.apache.mahout.math.function.&lt;/p&gt;
{VectorFunction, Functions}
&lt;p&gt;    +&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * based on &quot;Ted Dunnning &amp;amp; Ellen Friedman: Practical Machine Learning, Innovations in Recommendation&quot;,&lt;br/&gt;
    + * available at &lt;a href=&quot;http://www.mapr.com/practical-machine-learning&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.mapr.com/practical-machine-learning&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * see also &quot;Sebastian Schelter, Christoph Boden, Volker Markl:&lt;br/&gt;
    + * Scalable Similarity-Based Neighborhood Methods with MapReduce&lt;br/&gt;
    + * ACM Conference on Recommender Systems 2012&quot;&lt;br/&gt;
    + */&lt;br/&gt;
    +object CooccurrenceAnalysis extends Serializable {&lt;br/&gt;
    +&lt;br/&gt;
    +  /** Compares (Int,Double) pairs by the second value */&lt;br/&gt;
    +  private val orderByScore = Ordering.fromLessThan&lt;span class=&quot;error&quot;&gt;&amp;#91;(Int, Double)&amp;#93;&lt;/span&gt; &lt;/p&gt;
{ case ((_, score1), (_, score2)) =&amp;gt; score1 &amp;gt; score2}
&lt;p&gt;    +&lt;br/&gt;
    +  def cooccurrences(drmARaw: DrmLike&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;, randomSeed: Int = 0xdeadbeef, maxInterestingItemsPerThing: Int = 50,&lt;br/&gt;
    +                    maxNumInteractions: Int = 500, drmBs: Array[DrmLike&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;] = Array()): List[DrmLike&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;] = {&lt;br/&gt;
    +&lt;br/&gt;
    +    implicit val distributedContext = drmARaw.context&lt;br/&gt;
    +&lt;br/&gt;
    +    // Apply selective downsampling, pin resulting matrix&lt;br/&gt;
    +    val drmA = sampleDownAndBinarize(drmARaw, randomSeed, maxNumInteractions)&lt;br/&gt;
    +&lt;br/&gt;
    +    // num users, which equals the maximum number of interactions per item&lt;br/&gt;
    +    val numUsers = drmA.nrow.toInt&lt;br/&gt;
    +&lt;br/&gt;
    +    // Compute &amp;amp; broadcast the number of interactions per thing in A&lt;br/&gt;
    +    val bcastInteractionsPerItemA = drmBroadcast(drmA.colCounts)&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    colCounts or whatever we call it is just as efficient, is distributed and tells the reader what is the important value. &lt;/p&gt;</comment>
                            <comment id="14029469" author="githubbot" created="Thu, 12 Jun 2014 18:29:13 +0100"  >&lt;p&gt;Github user pferrel commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/12#discussion_r13714354&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/12#discussion_r13714354&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: spark/src/main/scala/org/apache/mahout/cf/CooccurrenceAnalysis.scala &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,214 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
    + * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
    + * this work for additional information regarding copyright ownership.&lt;br/&gt;
    + * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
    + * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
    + * the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.mahout.cf&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.mahout.math._&lt;br/&gt;
    +import scalabindings._&lt;br/&gt;
    +import RLikeOps._&lt;br/&gt;
    +import drm._&lt;br/&gt;
    +import RLikeDrmOps._&lt;br/&gt;
    +import org.apache.mahout.sparkbindings._&lt;br/&gt;
    +import scala.collection.JavaConversions._&lt;br/&gt;
    +import org.apache.mahout.math.stats.LogLikelihood&lt;br/&gt;
    +import collection._&lt;br/&gt;
    +import org.apache.mahout.common.RandomUtils&lt;br/&gt;
    +import org.apache.mahout.math.function.&lt;/p&gt;
{VectorFunction, Functions}
&lt;p&gt;    +&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * based on &quot;Ted Dunnning &amp;amp; Ellen Friedman: Practical Machine Learning, Innovations in Recommendation&quot;,&lt;br/&gt;
    + * available at &lt;a href=&quot;http://www.mapr.com/practical-machine-learning&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.mapr.com/practical-machine-learning&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * see also &quot;Sebastian Schelter, Christoph Boden, Volker Markl:&lt;br/&gt;
    + * Scalable Similarity-Based Neighborhood Methods with MapReduce&lt;br/&gt;
    + * ACM Conference on Recommender Systems 2012&quot;&lt;br/&gt;
    + */&lt;br/&gt;
    +object CooccurrenceAnalysis extends Serializable {&lt;br/&gt;
    +&lt;br/&gt;
    +  /** Compares (Int,Double) pairs by the second value */&lt;br/&gt;
    +  private val orderByScore = Ordering.fromLessThan&lt;span class=&quot;error&quot;&gt;&amp;#91;(Int, Double)&amp;#93;&lt;/span&gt; &lt;/p&gt;
{ case ((_, score1), (_, score2)) =&amp;gt; score1 &amp;gt; score2}
&lt;p&gt;    +&lt;br/&gt;
    +  def cooccurrences(drmARaw: DrmLike&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;, randomSeed: Int = 0xdeadbeef, maxInterestingItemsPerThing: Int = 50,&lt;br/&gt;
    +                    maxNumInteractions: Int = 500, drmBs: Array[DrmLike&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;] = Array()): List[DrmLike&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;] = {&lt;br/&gt;
    +&lt;br/&gt;
    +    implicit val distributedContext = drmARaw.context&lt;br/&gt;
    +&lt;br/&gt;
    +    // Apply selective downsampling, pin resulting matrix&lt;br/&gt;
    +    val drmA = sampleDownAndBinarize(drmARaw, randomSeed, maxNumInteractions)&lt;br/&gt;
    +&lt;br/&gt;
    +    // num users, which equals the maximum number of interactions per item&lt;br/&gt;
    +    val numUsers = drmA.nrow.toInt&lt;br/&gt;
    +&lt;br/&gt;
    +    // Compute &amp;amp; broadcast the number of interactions per thing in A&lt;br/&gt;
    +    val bcastInteractionsPerItemA = drmBroadcast(drmA.colCounts)&lt;br/&gt;
    +&lt;br/&gt;
    +    // Compute co-occurrence matrix A&apos;A&lt;br/&gt;
    +    val drmAtA = drmA.t %*% drmA&lt;br/&gt;
    +&lt;br/&gt;
    +    // Compute loglikelihood scores and sparsify the resulting matrix to get the indicator matrix&lt;br/&gt;
    +    val drmIndicatorsAtA = computeIndicators(drmAtA, numUsers, maxInterestingItemsPerThing, bcastInteractionsPerItemA,&lt;br/&gt;
    +      bcastInteractionsPerItemA, crossCooccurrence = false)&lt;br/&gt;
    +&lt;br/&gt;
    +    var indicatorMatrices = List(drmIndicatorsAtA)&lt;br/&gt;
    +&lt;br/&gt;
    +    // Now look at cross-co-occurrences&lt;br/&gt;
    +    for (drmBRaw &amp;lt;- drmBs) &lt;/p&gt;
{
    +      // Down-sample and pin other interaction matrix
    +      val drmB = sampleDownAndBinarize(drmBRaw, randomSeed, maxNumInteractions).checkpoint()
    +
    +      // Compute &amp;amp; broadcast the number of interactions per thing in B
    +      val bcastInteractionsPerThingB = drmBroadcast(drmB.colCounts)
    +
    +      // Compute cross-co-occurrence matrix B&apos;A
    +      val drmBtA = drmB.t %*% drmA
    +
    +      val drmIndicatorsBtA = computeIndicators(drmBtA, numUsers, maxInterestingItemsPerThing,
    +        bcastInteractionsPerThingB, bcastInteractionsPerItemA)
    +
    +      indicatorMatrices = indicatorMatrices :+ drmIndicatorsBtA
    +
    +      drmB.uncache()
    +    }
&lt;p&gt;    +&lt;br/&gt;
    +    // Unpin downsampled interaction matrix&lt;br/&gt;
    +    drmA.uncache()&lt;br/&gt;
    +&lt;br/&gt;
    +    // Return list of indicator matrices&lt;br/&gt;
    +    indicatorMatrices&lt;br/&gt;
    +  }&lt;br/&gt;
    +&lt;br/&gt;
    +  /**&lt;br/&gt;
    +   * Compute loglikelihood ratio&lt;br/&gt;
    +   * see &lt;a href=&quot;http://tdunning.blogspot.de/2008/03/surprise-and-coincidence.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://tdunning.blogspot.de/2008/03/surprise-and-coincidence.html&lt;/a&gt; for details&lt;br/&gt;
    +   **/&lt;br/&gt;
    +  def loglikelihoodRatio(numInteractionsWithA: Long, numInteractionsWithB: Long,&lt;br/&gt;
    +                         numInteractionsWithAandB: Long, numInteractions: Long) = &lt;/p&gt;
{
    +
    +    val k11 = numInteractionsWithAandB
    +    val k12 = numInteractionsWithA - numInteractionsWithAandB
    +    val k21 = numInteractionsWithB - numInteractionsWithAandB
    +    val k22 = numInteractions - numInteractionsWithA - numInteractionsWithB + numInteractionsWithAandB
    +
    +    LogLikelihood.logLikelihoodRatio(k11, k12, k21, k22)
    +  }
&lt;p&gt;    +&lt;br/&gt;
    +  def computeIndicators(drmBtA: DrmLike&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;, numUsers: Int, maxInterestingItemsPerThing: Int,&lt;br/&gt;
    +                        bcastNumInteractionsB: BCast&lt;span class=&quot;error&quot;&gt;&amp;#91;Vector&amp;#93;&lt;/span&gt;, bcastNumInteractionsA: BCast&lt;span class=&quot;error&quot;&gt;&amp;#91;Vector&amp;#93;&lt;/span&gt;,&lt;br/&gt;
    +                        crossCooccurrence: Boolean = true) = {&lt;br/&gt;
    +    drmBtA.mapBlock() {&lt;br/&gt;
    +      case (keys, block) =&amp;gt;&lt;br/&gt;
    +&lt;br/&gt;
    +        val llrBlock = block.like()&lt;br/&gt;
    +        val numInteractionsB: Vector = bcastNumInteractionsB&lt;br/&gt;
    +        val numInteractionsA: Vector = bcastNumInteractionsA&lt;br/&gt;
    +&lt;br/&gt;
    +        for (index &amp;lt;- 0 until keys.size) {&lt;br/&gt;
    +&lt;br/&gt;
    +          val thingB = keys(index)&lt;br/&gt;
    +&lt;br/&gt;
    +          // PriorityQueue to select the top-k items&lt;br/&gt;
    +          val topItemsPerThing = new mutable.PriorityQueue&lt;span class=&quot;error&quot;&gt;&amp;#91;(Int, Double)&amp;#93;&lt;/span&gt;()(orderByScore)&lt;br/&gt;
    +&lt;br/&gt;
    +          block(index, :&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;.nonZeroes().foreach { elem =&amp;gt;&lt;br/&gt;
    +            val thingA = elem.index&lt;br/&gt;
    +            val cooccurrences = elem.get&lt;br/&gt;
    +&lt;br/&gt;
    +            // exclude co-occurrences of the item with itself&lt;br/&gt;
    +            if (crossCooccurrence || thingB != thingA) {&lt;br/&gt;
    +              // Compute loglikelihood ratio&lt;br/&gt;
    +              val llrRatio = loglikelihoodRatio(numInteractionsB(thingB).toLong, numInteractionsA(thingA).toLong,&lt;br/&gt;
    +                cooccurrences.toLong, numUsers)&lt;br/&gt;
    +              val candidate = thingA -&amp;gt; llrRatio&lt;br/&gt;
    +&lt;br/&gt;
    +              // Enqueue item with score, if belonging to the top-k&lt;br/&gt;
    +              if (topItemsPerThing.size &amp;lt; maxInterestingItemsPerThing) &lt;/p&gt;
{
    +                topItemsPerThing.enqueue(candidate)
    +              }
&lt;p&gt; else if (orderByScore.lt(candidate, topItemsPerThing.head)) &lt;/p&gt;
{
    +                topItemsPerThing.dequeue()
    +                topItemsPerThing.enqueue(candidate)
    +              }
&lt;p&gt;    +            }&lt;br/&gt;
    +          }&lt;br/&gt;
    +&lt;br/&gt;
    +          // Add top-k interesting items to the output matrix&lt;br/&gt;
    +          topItemsPerThing.dequeueAll.foreach &lt;/p&gt;
{
    +            case (otherThing, llrScore) =&amp;gt;
    +              llrBlock(index, otherThing) = llrScore
    +          }
&lt;p&gt;    +        }&lt;br/&gt;
    +&lt;br/&gt;
    +        keys -&amp;gt; llrBlock&lt;br/&gt;
    +    }&lt;br/&gt;
    +  }&lt;br/&gt;
    +&lt;br/&gt;
    +  /**&lt;br/&gt;
    +   * Selectively downsample users and things with an anomalous amount of interactions, inspired by&lt;br/&gt;
    +   * &lt;a href=&quot;https://github.com/tdunning/in-memory-cooccurrence/blob/master/src/main/java/com/tdunning/cooc/Analyze.java&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/tdunning/in-memory-cooccurrence/blob/master/src/main/java/com/tdunning/cooc/Analyze.java&lt;/a&gt;&lt;br/&gt;
    +   *&lt;br/&gt;
    +   * additionally binarizes input matrix, as we&apos;re only interesting in knowing whether interactions happened or not&lt;br/&gt;
    +   */&lt;br/&gt;
    +  def sampleDownAndBinarize(drmM: DrmLike&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;, seed: Int, maxNumInteractions: Int) = {&lt;br/&gt;
    +&lt;br/&gt;
    +    implicit val distributedContext = drmM.context&lt;br/&gt;
    +&lt;br/&gt;
    +    // Pin raw interaction matrix&lt;br/&gt;
    +    val drmI = drmM.checkpoint()&lt;br/&gt;
    +&lt;br/&gt;
    +    // Broadcast vector containing the number of interactions with each thing&lt;br/&gt;
    +    val bcastNumInteractions = drmBroadcast(drmI.colCounts)&lt;br/&gt;
    +&lt;br/&gt;
    +    val downSampledDrmI = drmI.mapBlock() {&lt;br/&gt;
    +      case (keys, block) =&amp;gt;&lt;br/&gt;
    +        val numInteractions: Vector = bcastNumInteractions&lt;br/&gt;
    +&lt;br/&gt;
    +        // Use a hash of the unique first key to seed the RNG, makes this computation repeatable in case of failures&lt;br/&gt;
    +        val random = RandomUtils.getRandom(MurmurHash.hash(keys(0), seed))&lt;br/&gt;
    +&lt;br/&gt;
    +        val downsampledBlock = block.like()&lt;br/&gt;
    +&lt;br/&gt;
    +        // Downsample the interaction vector of each user&lt;br/&gt;
    +        for (userIndex &amp;lt;- 0 until keys.size) {&lt;br/&gt;
    +&lt;br/&gt;
    +          val interactionsOfUser = block(userIndex, :&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
    +&lt;br/&gt;
    +          //todo: can we trust getNumNonZeroElements or is this the upper limit? May have to actually count them?&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    got it, I&apos;ll remove the comment since we do rely on it in the code&lt;/p&gt;</comment>
                            <comment id="14029474" author="githubbot" created="Thu, 12 Jun 2014 18:34:25 +0100"  >&lt;p&gt;Github user pferrel commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/12#issuecomment-45923020&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/12#issuecomment-45923020&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    numNonZeroElementsPerColumn? vs colSums?&lt;/p&gt;

&lt;p&gt;    OK&lt;/p&gt;</comment>
                            <comment id="14029631" author="githubbot" created="Thu, 12 Jun 2014 20:05:49 +0100"  >&lt;p&gt;Github user tdunning commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/12#issuecomment-45934491&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/12#issuecomment-45934491&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I hate abbreviations.  If you are asking about naming, use the long name.&lt;/p&gt;

&lt;p&gt;    If you can assure binary, then going with what we already have would be&lt;br/&gt;
    nice.&lt;/p&gt;



&lt;p&gt;    On Thu, Jun 12, 2014 at 10:34 AM, Pat Ferrel &amp;lt;notifications@github.com&amp;gt;&lt;br/&gt;
    wrote:&lt;/p&gt;

&lt;p&gt;    &amp;gt; numNonZeroElementsPerColumn? vs colSums?&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt; OK&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt; &#8212;&lt;br/&gt;
    &amp;gt; Reply to this email directly or view it on GitHub&lt;br/&gt;
    &amp;gt; &amp;lt;&lt;a href=&quot;https://github.com/apache/mahout/pull/12#issuecomment-45923020&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/12#issuecomment-45923020&lt;/a&gt;&amp;gt;.&lt;br/&gt;
    &amp;gt;&lt;/p&gt;</comment>
                            <comment id="14030109" author="githubbot" created="Fri, 13 Jun 2014 01:46:05 +0100"  >&lt;p&gt;Github user asfgit closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/12&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/12&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14030125" author="hudson" created="Fri, 13 Jun 2014 02:16:37 +0100"  >&lt;p&gt;SUCCESS: Integrated in Mahout-Quality #2653 (See &lt;a href=&quot;https://builds.apache.org/job/Mahout-Quality/2653/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/Mahout-Quality/2653/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1464&quot; title=&quot;Cooccurrence Analysis on Spark&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1464&quot;&gt;&lt;del&gt;MAHOUT-1464&lt;/del&gt;&lt;/a&gt; Cooccurrence Analysis on Spark (pat) closes apache/mahout#12 (pat: rev c1ca30872c622e513e49fc1bb111bc4b8a527d3b)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;spark/src/test/scala/org/apache/mahout/cf/CooccurrenceAnalysisSuite.scala&lt;/li&gt;
	&lt;li&gt;math-scala/src/main/scala/org/apache/mahout/math/drm/DistributedEngine.scala&lt;/li&gt;
	&lt;li&gt;spark/src/main/scala/org/apache/mahout/cf/CooccurrenceAnalysis.scala&lt;/li&gt;
	&lt;li&gt;spark/src/main/scala/org/apache/mahout/sparkbindings/SparkEngine.scala&lt;/li&gt;
	&lt;li&gt;math-scala/src/main/scala/org/apache/mahout/math/scalabindings/MatrixOps.scala&lt;/li&gt;
	&lt;li&gt;math/src/main/java/org/apache/mahout/math/MurmurHash.java&lt;/li&gt;
	&lt;li&gt;math-scala/src/main/scala/org/apache/mahout/math/drm/CheckpointedOps.scala&lt;/li&gt;
	&lt;li&gt;spark/src/test/scala/org/apache/mahout/sparkbindings/drm/RLikeDrmOpsSuite.scala&lt;/li&gt;
	&lt;li&gt;spark/src/main/scala/org/apache/mahout/sparkbindings/package.scala&lt;/li&gt;
	&lt;li&gt;math-scala/src/test/scala/org/apache/mahout/math/scalabindings/MatrixOpsSuite.scala&lt;/li&gt;
	&lt;li&gt;CHANGELOG&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14030843" author="githubbot" created="Fri, 13 Jun 2014 18:02:30 +0100"  >&lt;p&gt;GitHub user pferrel opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/18&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/18&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1464&quot; title=&quot;Cooccurrence Analysis on Spark&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1464&quot;&gt;&lt;del&gt;MAHOUT-1464&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    The numNonZeroElementsPerColumn additions did not account for negative values, only counted the positive non-zero values. Fixed this in the in core and distributed case.&lt;/p&gt;

&lt;p&gt;    I added to Functions.java to create a Functions.notEqual. It may be possible to do this with the other functions but it wasn&apos;t obvious so I wrote one. The test is in MatrixOpsSuite, where is it used.&lt;/p&gt;

&lt;p&gt;    The distributed case was much simpler.&lt;/p&gt;

&lt;p&gt;    Changed tests to include negative values.&lt;/p&gt;

&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/pferrel/mahout&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/pferrel/mahout&lt;/a&gt; mahout-1464&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/18.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/18.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #18&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 107a0ba9605241653a85b113661a8fa5c055529f&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-04T19:54:22Z&lt;/p&gt;

&lt;p&gt;    added Sebastian&apos;s CooccurrenceAnalysis patch updated it to use current Mahout-DSL&lt;/p&gt;

&lt;p&gt;commit 16c03f7fa73c156859d1dba3a333ef9e8bf922b0&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-04T21:32:18Z&lt;/p&gt;

&lt;p&gt;    added Sebastian&apos;s MurmurHash changes&lt;/p&gt;

&lt;p&gt;    Signed-off-by: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;/p&gt;

&lt;p&gt;commit c6adaa44c80bba99d41600e260bbb1ad5c972e69&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-05T16:52:23Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1464&quot; title=&quot;Cooccurrence Analysis on Spark&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1464&quot;&gt;&lt;del&gt;MAHOUT-1464&lt;/del&gt;&lt;/a&gt; import cleanup, minor changes to examples for running on Spark Cluster&lt;/p&gt;

&lt;p&gt;commit 1d66e5726e71e297ef4a7a27331463ba363098c0&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-06T20:19:32Z&lt;/p&gt;

&lt;p&gt;    scalatest for cooccurrence cross and self along with other CooccurrenceAnalyisi methods&lt;/p&gt;

&lt;p&gt;commit 766db0f9e7feb70520fbd444afcb910788f01e76&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-06T20:20:46Z&lt;/p&gt;

&lt;p&gt;    Merge branch &apos;master&apos; into mahout-1464&lt;/p&gt;

&lt;p&gt;commit e492976688cb8860354bb20a362d370405f560e1&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-06T20:50:07Z&lt;/p&gt;

&lt;p&gt;    cleaned up test comments&lt;/p&gt;

&lt;p&gt;commit a49692eb1664de4b15de1864b95701a6410c80c8&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-06T21:09:55Z&lt;/p&gt;

&lt;p&gt;    got those cursed .DS_Stores out of the branch and put an exclude in .gitignore&lt;/p&gt;

&lt;p&gt;commit 268290d28d4f83cc47a7e6baebc5eb4c53d7c8da&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-07T21:50:04Z&lt;/p&gt;

&lt;p&gt;    Merge branch &apos;master&apos; into mahout-1464&lt;/p&gt;

&lt;p&gt;commit 63b10704390e18f513cca30596b1d25e146a6edd&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-08T15:26:36Z&lt;/p&gt;

&lt;p&gt;    Merge branch &apos;master&apos; into mahout-1464&lt;/p&gt;

&lt;p&gt;commit ac00d7655c4cba5f6c6dcb4882be95656b17a834&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-09T14:11:43Z&lt;/p&gt;

&lt;p&gt;    Merge branch &apos;master&apos; into mahout-1464&lt;/p&gt;

&lt;p&gt;commit fb008efeae3d5f6f6ba350fbc2ef3944da1dcaef&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-12T02:17:27Z&lt;/p&gt;

&lt;p&gt;    added &apos;colCounts&apos; to a drm using the SparkEngine and MatrixOps, which, when used in cooccurrence, fixes the problem with non-boolean preference values&lt;/p&gt;

&lt;p&gt;commit 5b04cb31403e2521d9874ad5e14f28cd0af26c26&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-12T02:18:29Z&lt;/p&gt;

&lt;p&gt;    Merge branch &apos;master&apos; into mahout-1464&lt;/p&gt;

&lt;p&gt;commit e451a2a596f5ceda8d1b4990e97ad3d5673fdb5f&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-12T16:02:26Z&lt;/p&gt;

&lt;p&gt;    fixed some things from Dmitiy&apos;s comments, primary being the SparkEngine accumulator was doing &amp;gt;= 0 instead of &amp;gt; 0&lt;/p&gt;

&lt;p&gt;commit 411e0e92b4721626b736d66c292926fa4fdbb530&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-12T17:43:21Z&lt;/p&gt;

&lt;p&gt;    changing the name of drm.colCounts to drm.getNumNonZeroElements&lt;/p&gt;

&lt;p&gt;commit 9655fd70f69ed97eb2d6765928a0a1f7dd760281&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-12T18:32:03Z&lt;/p&gt;

&lt;p&gt;    meant to say changing drm.colCounts to drm.numNonZeroElementsPerColumn&lt;/p&gt;

&lt;p&gt;commit a2001375d46c5946b671f89f5a7cff2e6a094ea8&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-12T18:34:32Z&lt;/p&gt;

&lt;p&gt;    Merge branch &apos;master&apos; into mahout-1464&lt;/p&gt;

&lt;p&gt;commit 2db06b5566c8dcccb382733613b2fab6c223b5de&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-12T18:51:54Z&lt;/p&gt;

&lt;p&gt;    typo&lt;/p&gt;

&lt;p&gt;commit 0b689b8b879c4ac03b71cf504a9d0d78ffa6bfa5&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-12T20:03:45Z&lt;/p&gt;

&lt;p&gt;    clean up test&lt;/p&gt;

&lt;p&gt;commit 32afbe5e552ab94979dd545d14cda17ebc9c018e&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-12T23:42:08Z&lt;/p&gt;

&lt;p&gt;    one more fat finger error&lt;/p&gt;

&lt;p&gt;commit b91e5e98c47829a5cc099289f83e99e6bf317dd6&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-13T16:18:33Z&lt;/p&gt;

&lt;p&gt;    did not account for negative values in the purely mathematical MatrixOps and SparkEngine version of numNonZeroElementsPerColumn so fixed this and added to tests&lt;/p&gt;

&lt;p&gt;commit 9f6fd902f95c7daf687ecb59698f78217dbf6b6b&lt;br/&gt;
Author: pferrel &amp;lt;pat@occamsmachete.com&amp;gt;&lt;br/&gt;
Date:   2014-06-13T16:43:46Z&lt;/p&gt;

&lt;p&gt;    merging master to run new tests&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="14030848" author="githubbot" created="Fri, 13 Jun 2014 18:06:04 +0100"  >&lt;p&gt;Github user pferrel commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/18#issuecomment-46035704&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/18#issuecomment-46035704&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Accounting for possible negative values in matrix of drm columns.&lt;/p&gt;

&lt;p&gt;    drm case was a simple fix but in core Functions.java was modified to include a &quot;notEqual(value)&quot; function. There may be some other way to do this but it is a trivial function and now rather obvious. &lt;/p&gt;</comment>
                            <comment id="14031133" author="githubbot" created="Fri, 13 Jun 2014 21:43:02 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/18#issuecomment-46057614&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/18#issuecomment-46057614&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Bringing up a second PR from same branch. You really need just to rebase the changes over current master. This may not merge well.&lt;/p&gt;</comment>
                            <comment id="14031135" author="githubbot" created="Fri, 13 Jun 2014 21:44:21 +0100"  >&lt;p&gt;Github user dlyubimov commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/18#discussion_r13770844&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/18#discussion_r13770844&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: math/src/main/java/org/apache/mahout/math/function/Functions.java &amp;#8212;&lt;br/&gt;
    @@ -1393,6 +1393,17 @@ public double apply(double a) {&lt;br/&gt;
         };&lt;br/&gt;
       }&lt;/p&gt;

&lt;p&gt;    +  /** Constructs a function that returns &amp;lt;tt&amp;gt;a != b ? 1 : 0&amp;lt;/tt&amp;gt;. &amp;lt;tt&amp;gt;a&amp;lt;/tt&amp;gt; is a variable, &amp;lt;tt&amp;gt;b&amp;lt;/tt&amp;gt; is fixed. */&lt;br/&gt;
    +  public static DoubleFunction notEqual(final double b) {&lt;br/&gt;
    +    return new DoubleFunction() {&lt;br/&gt;
    +&lt;br/&gt;
    +      @Override&lt;br/&gt;
    +      public double apply(double a) {&lt;br/&gt;
    +        return a != b ? 1 : 0;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Since you are returning doubles, correct style is to say 1.0 or 0.0 not 1 or 0&lt;/p&gt;</comment>
                            <comment id="14031138" author="githubbot" created="Fri, 13 Jun 2014 21:46:29 +0100"  >&lt;p&gt;Github user dlyubimov commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/18#discussion_r13770924&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/18#discussion_r13770924&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: math-scala/src/test/scala/org/apache/mahout/math/scalabindings/MatrixOpsSuite.scala &amp;#8212;&lt;br/&gt;
    @@ -123,4 +123,16 @@ class MatrixOpsSuite extends FunSuite with MahoutSuite {&lt;/p&gt;

&lt;p&gt;       }&lt;/p&gt;

&lt;p&gt;    +  test(&quot;numNonZeroElementsPerColumn&quot;) {&lt;br/&gt;
    +    val a = dense(&lt;br/&gt;
    +      (2, 3, 4),&lt;br/&gt;
    +      (3, 4, 5),&lt;br/&gt;
    +      (-5, 0, -1),&lt;br/&gt;
    +      (0, 0, 1)&lt;br/&gt;
    +    )&lt;br/&gt;
    +&lt;br/&gt;
    +    a.numNonZeroElementsPerColumn() should equal(dvec(3,2,4))&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    style: spacing?&lt;/p&gt;</comment>
                            <comment id="14031145" author="githubbot" created="Fri, 13 Jun 2014 21:49:23 +0100"  >&lt;p&gt;Github user dlyubimov commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/18#discussion_r13771023&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/18#discussion_r13771023&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: math-scala/src/main/scala/org/apache/mahout/math/scalabindings/MatrixOps.scala &amp;#8212;&lt;br/&gt;
    @@ -188,8 +188,8 @@ object MatrixOps &lt;/p&gt;
{
         def apply(f: Vector): Double = f.sum
       }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private def vectorCountFunc = new VectorFunction {&lt;/li&gt;
	&lt;li&gt;def apply(f: Vector): Double = f.aggregate(Functions.PLUS, Functions.greater(0))&lt;br/&gt;
    +  private def vectorCountNonZeroElementsFunc = new VectorFunction {&lt;br/&gt;
    +    def apply(f: Vector): Double = f.aggregate(Functions.PLUS, Functions.notEqual(0))
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Hm. isn&apos;t that is made obsolete with Sebastian&apos;s PR #17 ?&lt;/p&gt;</comment>
                            <comment id="14031157" author="githubbot" created="Fri, 13 Jun 2014 21:59:39 +0100"  >&lt;p&gt;Github user dlyubimov commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/18#discussion_r13771395&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/18#discussion_r13771395&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: spark/src/test/scala/org/apache/mahout/cf/CooccurrenceAnalysisSuite.scala &amp;#8212;&lt;br/&gt;
    @@ -118,8 +118,8 @@ class CooccurrenceAnalysisSuite extends FunSuite with MahoutSuite with MahoutLoc&lt;br/&gt;
       }&lt;/p&gt;

&lt;p&gt;       test(&quot;cooccurrence &lt;span class=&quot;error&quot;&gt;&amp;#91;A&amp;#39;A&amp;#93;&lt;/span&gt;, &lt;span class=&quot;error&quot;&gt;&amp;#91;B&amp;#39;A&amp;#93;&lt;/span&gt; integer data using LLR&quot;) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val a = dense((1000, 10, 0, 0, 0), (0, 0, 10000, 10, 0), (0, 0, 0, 0, 100), (10000, 0, 0, 1000, 0))&lt;/li&gt;
	&lt;li&gt;val b = dense((100, 1000, 10000, 10000, 0), (10000, 1000, 100, 10, 0), (0, 0, 10, 0, 100), (10, 100, 0, 1000, 0))&lt;br/&gt;
    +    val a = dense((1000, 10, 0, 0, 0), (0, 0, -10000, 10, 0), (0, 0, 0, 0, 100), (10000, 0, 0, 1000, 0))
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    not sure if coocurrence test changes like that are necessary&lt;/p&gt;</comment>
                            <comment id="14031165" author="githubbot" created="Fri, 13 Jun 2014 22:05:30 +0100"  >&lt;p&gt;Github user pferrel commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/18#discussion_r13771632&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/18#discussion_r13771632&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: math-scala/src/main/scala/org/apache/mahout/math/scalabindings/MatrixOps.scala &amp;#8212;&lt;br/&gt;
    @@ -188,8 +188,8 @@ object MatrixOps &lt;/p&gt;
{
         def apply(f: Vector): Double = f.sum
       }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private def vectorCountFunc = new VectorFunction {&lt;/li&gt;
	&lt;li&gt;def apply(f: Vector): Double = f.aggregate(Functions.PLUS, Functions.greater(0))&lt;br/&gt;
    +  private def vectorCountNonZeroElementsFunc = new VectorFunction {&lt;br/&gt;
    +    def apply(f: Vector): Double = f.aggregate(Functions.PLUS, Functions.notEqual(0))
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    This is creating a Vector of non-zero counts per column, just like colSums is summing the column&apos;s values. The function is simply needed in the aggregateColumns. If you are suggesting another way to do this you&apos;ll have to be more explicit.&lt;/p&gt;

&lt;p&gt;    #17 has nothing to do with that afaict. It is about finding non-zero elements in a Vector.&lt;/p&gt;</comment>
                            <comment id="14031166" author="githubbot" created="Fri, 13 Jun 2014 22:06:12 +0100"  >&lt;p&gt;Github user pferrel commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/18#discussion_r13771652&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/18#discussion_r13771652&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: math/src/main/java/org/apache/mahout/math/function/Functions.java &amp;#8212;&lt;br/&gt;
    @@ -1393,6 +1393,17 @@ public double apply(double a) {&lt;br/&gt;
         };&lt;br/&gt;
       }&lt;/p&gt;

&lt;p&gt;    +  /** Constructs a function that returns &amp;lt;tt&amp;gt;a != b ? 1 : 0&amp;lt;/tt&amp;gt;. &amp;lt;tt&amp;gt;a&amp;lt;/tt&amp;gt; is a variable, &amp;lt;tt&amp;gt;b&amp;lt;/tt&amp;gt; is fixed. */&lt;br/&gt;
    +  public static DoubleFunction notEqual(final double b) {&lt;br/&gt;
    +    return new DoubleFunction() {&lt;br/&gt;
    +&lt;br/&gt;
    +      @Override&lt;br/&gt;
    +      public double apply(double a) {&lt;br/&gt;
    +        return a != b ? 1 : 0;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Taken from &quot;equal&quot; in the same file. Changed one character. But I&apos;ll note the point.&lt;/p&gt;</comment>
                            <comment id="14031177" author="githubbot" created="Fri, 13 Jun 2014 22:13:13 +0100"  >&lt;p&gt;Github user pferrel commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/18#discussion_r13771890&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/18#discussion_r13771890&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: spark/src/test/scala/org/apache/mahout/cf/CooccurrenceAnalysisSuite.scala &amp;#8212;&lt;br/&gt;
    @@ -118,8 +118,8 @@ class CooccurrenceAnalysisSuite extends FunSuite with MahoutSuite with MahoutLoc&lt;br/&gt;
       }&lt;/p&gt;

&lt;p&gt;       test(&quot;cooccurrence &lt;span class=&quot;error&quot;&gt;&amp;#91;A&amp;#39;A&amp;#93;&lt;/span&gt;, &lt;span class=&quot;error&quot;&gt;&amp;#91;B&amp;#39;A&amp;#93;&lt;/span&gt; integer data using LLR&quot;) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val a = dense((1000, 10, 0, 0, 0), (0, 0, 10000, 10, 0), (0, 0, 0, 0, 100), (10000, 0, 0, 1000, 0))&lt;/li&gt;
	&lt;li&gt;val b = dense((100, 1000, 10000, 10000, 0), (10000, 1000, 100, 10, 0), (0, 0, 10, 0, 100), (10, 100, 0, 1000, 0))&lt;br/&gt;
    +    val a = dense((1000, 10, 0, 0, 0), (0, 0, -10000, 10, 0), (0, 0, 0, 0, 100), (10000, 0, 0, 1000, 0))
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    We may want to check for illegal values at some place in the pipeline. This is here so I don&apos;t forget. At present a negative value is legal. If we make it illegal I want this to fail.&lt;/p&gt;</comment>
                            <comment id="14031187" author="githubbot" created="Fri, 13 Jun 2014 22:19:16 +0100"  >&lt;p&gt;Github user dlyubimov commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/18#discussion_r13772128&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/18#discussion_r13772128&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: math/src/main/java/org/apache/mahout/math/function/Functions.java &amp;#8212;&lt;br/&gt;
    @@ -1393,6 +1393,17 @@ public double apply(double a) {&lt;br/&gt;
         };&lt;br/&gt;
       }&lt;/p&gt;

&lt;p&gt;    +  /** Constructs a function that returns &amp;lt;tt&amp;gt;a != b ? 1 : 0&amp;lt;/tt&amp;gt;. &amp;lt;tt&amp;gt;a&amp;lt;/tt&amp;gt; is a variable, &amp;lt;tt&amp;gt;b&amp;lt;/tt&amp;gt; is fixed. */&lt;br/&gt;
    +  public static DoubleFunction notEqual(final double b) {&lt;br/&gt;
    +    return new DoubleFunction() {&lt;br/&gt;
    +&lt;br/&gt;
    +      @Override&lt;br/&gt;
    +      public double apply(double a) {&lt;br/&gt;
    +        return a != b ? 1 : 0;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    just relaying some historical discussion in Mahout. It was known to create a bug in my own Mahout commit once. Which had sparked the discussion about constant formatting.&lt;/p&gt;</comment>
                            <comment id="14031292" author="githubbot" created="Fri, 13 Jun 2014 23:44:29 +0100"  >&lt;p&gt;Github user dlyubimov commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/18#discussion_r13775208&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/18#discussion_r13775208&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: math-scala/src/main/scala/org/apache/mahout/math/scalabindings/MatrixOps.scala &amp;#8212;&lt;br/&gt;
    @@ -188,8 +188,8 @@ object MatrixOps &lt;/p&gt;
{
         def apply(f: Vector): Double = f.sum
       }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private def vectorCountFunc = new VectorFunction {&lt;/li&gt;
	&lt;li&gt;def apply(f: Vector): Double = f.aggregate(Functions.PLUS, Functions.greater(0))&lt;br/&gt;
    +  private def vectorCountNonZeroElementsFunc = new VectorFunction {&lt;br/&gt;
    +    def apply(f: Vector): Double = f.aggregate(Functions.PLUS, Functions.notEqual(0))
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    i mean, shouldn&apos;t this specialized version be more effective than an aggregate: &lt;/p&gt;

&lt;p&gt;        def apply(f: Vector): Double = f.getnumNonZeroElements().toDouble&lt;/p&gt;</comment>
                            <comment id="14031608" author="githubbot" created="Sat, 14 Jun 2014 16:55:46 +0100"  >&lt;p&gt;Github user pferrel commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/18#discussion_r13781567&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/18#discussion_r13781567&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: math-scala/src/main/scala/org/apache/mahout/math/scalabindings/MatrixOps.scala &amp;#8212;&lt;br/&gt;
    @@ -188,8 +188,8 @@ object MatrixOps &lt;/p&gt;
{
         def apply(f: Vector): Double = f.sum
       }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private def vectorCountFunc = new VectorFunction {&lt;/li&gt;
	&lt;li&gt;def apply(f: Vector): Double = f.aggregate(Functions.PLUS, Functions.greater(0))&lt;br/&gt;
    +  private def vectorCountNonZeroElementsFunc = new VectorFunction {&lt;br/&gt;
    +    def apply(f: Vector): Double = f.aggregate(Functions.PLUS, Functions.notEqual(0))
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Nice. I didn&apos;t look deep enough to see that f is the column vector. I&apos;ll change that.&lt;/p&gt;

&lt;p&gt;    While we are at it, I now know about A&apos;A (which is the slim calc?) that doesn&apos;t really compute A&apos;. If you do similar for two different matrices:``` B.t %*% A``` does B.t ever get checkpointed?&lt;/p&gt;</comment>
                            <comment id="14031613" author="githubbot" created="Sat, 14 Jun 2014 17:34:36 +0100"  >&lt;p&gt;Github user pferrel commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/18#discussion_r13781773&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/18#discussion_r13781773&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: math/src/main/java/org/apache/mahout/math/function/Functions.java &amp;#8212;&lt;br/&gt;
    @@ -1393,6 +1393,17 @@ public double apply(double a) {&lt;br/&gt;
         };&lt;br/&gt;
       }&lt;/p&gt;

&lt;p&gt;    +  /** Constructs a function that returns &amp;lt;tt&amp;gt;a != b ? 1 : 0&amp;lt;/tt&amp;gt;. &amp;lt;tt&amp;gt;a&amp;lt;/tt&amp;gt; is a variable, &amp;lt;tt&amp;gt;b&amp;lt;/tt&amp;gt; is fixed. */&lt;br/&gt;
    +  public static DoubleFunction notEqual(final double b) {&lt;br/&gt;
    +    return new DoubleFunction() {&lt;br/&gt;
    +&lt;br/&gt;
    +      @Override&lt;br/&gt;
    +      public double apply(double a) {&lt;br/&gt;
    +        return a != b ? 1 : 0;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Whenever I modify a mature file that someone else has created, my general rule is to stay with the style of the collective authors. Here I agree that the 1.0, 0.0 is better I&apos;m hesitant to change it here when 1, and 0 are used throughout the file and I don&apos;t want to change it everywhere. There is probably more chance of me messing something up accidentally than actually fixing something if I change the whole file. If this seem wrong let me know but in past jobs we did this to avoid constant thrash over minor style disagreements.&lt;/p&gt;</comment>
                            <comment id="14031619" author="githubbot" created="Sat, 14 Jun 2014 17:43:46 +0100"  >&lt;p&gt;Github user asfgit closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/18&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/18&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14031621" author="hudson" created="Sat, 14 Jun 2014 17:48:18 +0100"  >&lt;p&gt;FAILURE: Integrated in Mahout-Quality #2655 (See &lt;a href=&quot;https://builds.apache.org/job/Mahout-Quality/2655/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/Mahout-Quality/2655/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1464&quot; title=&quot;Cooccurrence Analysis on Spark&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1464&quot;&gt;&lt;del&gt;MAHOUT-1464&lt;/del&gt;&lt;/a&gt; fixed bug counting only positive column elements, now counts all non-zero (pat) closes apache/mahout#18 (pat: rev c20eee89c6cc669494cf7edbb80255a83e194a15)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;math/src/main/java/org/apache/mahout/math/function/Functions.java&lt;/li&gt;
	&lt;li&gt;spark/src/test/scala/org/apache/mahout/sparkbindings/drm/RLikeDrmOpsSuite.scala&lt;/li&gt;
	&lt;li&gt;math-scala/src/main/scala/org/apache/mahout/math/scalabindings/MatrixOps.scala&lt;/li&gt;
	&lt;li&gt;spark/src/main/scala/org/apache/mahout/sparkbindings/SparkEngine.scala&lt;/li&gt;
	&lt;li&gt;spark/src/test/scala/org/apache/mahout/cf/CooccurrenceAnalysisSuite.scala&lt;/li&gt;
	&lt;li&gt;math-scala/src/test/scala/org/apache/mahout/math/scalabindings/MatrixOpsSuite.scala&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14031632" author="githubbot" created="Sat, 14 Jun 2014 18:50:50 +0100"  >&lt;p&gt;Github user tdunning commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/18#discussion_r13782027&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/18#discussion_r13782027&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: math-scala/src/main/scala/org/apache/mahout/math/scalabindings/MatrixOps.scala &amp;#8212;&lt;br/&gt;
    @@ -188,8 +188,8 @@ object MatrixOps &lt;/p&gt;
{
         def apply(f: Vector): Double = f.sum
       }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private def vectorCountFunc = new VectorFunction {&lt;/li&gt;
	&lt;li&gt;def apply(f: Vector): Double = f.aggregate(Functions.PLUS, Functions.greater(0))&lt;br/&gt;
    +  private def vectorCountNonZeroElementsFunc = new VectorFunction {&lt;br/&gt;
    +    def apply(f: Vector): Double = f.aggregate(Functions.PLUS, Functions.notEqual(0))
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Using the vector aggregation framework will be very inefficient here.  We should either use Seb&apos;s suggestion or add properly scalable aggregation that doesn&apos;t depend on getting a vector view of a column.&lt;/p&gt;</comment>
                            <comment id="14031707" author="githubbot" created="Sat, 14 Jun 2014 22:32:52 +0100"  >&lt;p&gt;Github user pferrel commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/18#discussion_r13782871&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/18#discussion_r13782871&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: math-scala/src/main/scala/org/apache/mahout/math/scalabindings/MatrixOps.scala &amp;#8212;&lt;br/&gt;
    @@ -188,8 +188,8 @@ object MatrixOps &lt;/p&gt;
{
         def apply(f: Vector): Double = f.sum
       }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private def vectorCountFunc = new VectorFunction {&lt;/li&gt;
	&lt;li&gt;def apply(f: Vector): Double = f.aggregate(Functions.PLUS, Functions.greater(0))&lt;br/&gt;
    +  private def vectorCountNonZeroElementsFunc = new VectorFunction {&lt;br/&gt;
    +    def apply(f: Vector): Double = f.aggregate(Functions.PLUS, Functions.notEqual(0))
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    used Sebastian&apos;s getNumNonZeroElements here. Is your issue with Dmitriy&apos;s suggestion? This is only for in core matrices, the code used for drms is the stuff in SparkEngine, which accumulates using the nonZero iterator on row Vectors.&lt;/p&gt;</comment>
                            <comment id="14031751" author="githubbot" created="Sun, 15 Jun 2014 02:45:38 +0100"  >&lt;p&gt;Github user tdunning commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/18#discussion_r13783816&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/18#discussion_r13783816&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: math-scala/src/main/scala/org/apache/mahout/math/scalabindings/MatrixOps.scala &amp;#8212;&lt;br/&gt;
    @@ -188,8 +188,8 @@ object MatrixOps &lt;/p&gt;
{
         def apply(f: Vector): Double = f.sum
       }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private def vectorCountFunc = new VectorFunction {&lt;/li&gt;
	&lt;li&gt;def apply(f: Vector): Double = f.aggregate(Functions.PLUS, Functions.greater(0))&lt;br/&gt;
    +  private def vectorCountNonZeroElementsFunc = new VectorFunction {&lt;br/&gt;
    +    def apply(f: Vector): Double = f.aggregate(Functions.PLUS, Functions.notEqual(0))
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    The issue I have is with the rowAggregation and columnAggregation API.  It enforces row by row evaluation.  A map-reduce API could evaluate in many different orders and could iterate by rows or by columns for either aggregation and wouldn&apos;t require the a custom VectorFunction for simple aggregations.&lt;/p&gt;</comment>
                            <comment id="14046423" author="pferrel" created="Fri, 27 Jun 2014 22:35:38 +0100"  >&lt;p&gt;finally got a Spark cluster working and this does run on it. Also using HDFS for I/O. &lt;/p&gt;

&lt;p&gt;I think there is more to do here but will treat as enhancements and close now.&lt;/p&gt;</comment>
                            <comment id="14046424" author="pferrel" created="Fri, 27 Jun 2014 22:38:11 +0100"  >&lt;p&gt;WFM on HDFS 1.2.1 Spark compiled to match.&lt;/p&gt;

&lt;p&gt;The problem that made this untestable on HDFS seems to be that the read code was broadcasting something that Kryo did not know how to handle. Fixed this in the driver/ I/O code and this now works. &lt;/p&gt;</comment>
                            <comment id="14058025" author="pferrel" created="Thu, 10 Jul 2014 23:02:39 +0100"  >&lt;p&gt;Argh, I finally got to looking at cross-cooccurrence with matrices of different dimensions. This made a rather easy to fix but nasty error obvious. &lt;/p&gt;

&lt;p&gt;If A is the primary self-similairty matrix we want to do A&apos;A with, and B is the secondary matrix, we want to do A&apos;B with it since for use in a recommender we want rows of the cooccurrence to be IDed by the columns of A (items in A), right?&lt;/p&gt;

&lt;p&gt;The code does:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
      &lt;span class=&quot;code-comment&quot;&gt;// Compute cross-co-occurrence matrix B&apos;A
&lt;/span&gt;      val drmBtA = drmB.t %*% drmA
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;it should do:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
      &lt;span class=&quot;code-comment&quot;&gt;// Compute cross-co-occurrence matrix A&apos;B
&lt;/span&gt;      val drmAtB = drmA.t %*% drmB
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Ted, can you confirm this? It&apos;s the way I&apos;ve got it burned into my brain and applying ID translation with completely different IDs in B seems to confirm it.&lt;/p&gt;</comment>
                            <comment id="14060237" author="tdunning" created="Mon, 14 Jul 2014 00:39:07 +0100"  >&lt;blockquote&gt;
&lt;p&gt;If A is the primary self-similarity matrix we want to do A&apos;A with, and B is the secondary matrix, we want to do A&apos;B with it since for use in a recommender we want rows of the cooccurrence to be IDed by the columns of A (items in A), right?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes.  I think I understand the question.&lt;/p&gt;

&lt;p&gt;A&apos;B should have row id&apos;s which are column id&apos;s of A.  It should have column id&apos;s which are the column id&apos;s of B.&lt;/p&gt;
</comment>
                            <comment id="14349780" author="andrew_palumbo" created="Fri, 6 Mar 2015 01:53:09 +0000"  >&lt;p&gt;We can close this, right?&lt;/p&gt;</comment>
                            <comment id="14367142" author="pferrel" created="Wed, 18 Mar 2015 13:51:54 +0000"  >&lt;p&gt;SimilarityAnalysis has cooccurrence for columns, cross-cooccurrence too and row similarity with LLR filtering. No other &quot;similarity&quot; measure is implemented. Something like Cosine will require a different approach than A&apos;A of AA&apos; since it requires vector dot products while LLR only looks at occurrence counts--maybe something like the legacy mapreduce code. Deferring other similarity measures.&lt;/p&gt;</comment>
                            <comment id="14492169" author="sslavic" created="Mon, 13 Apr 2015 10:57:15 +0100"  >&lt;p&gt;Bulk closing all 0.10.0 resolved issues&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12639983" name="MAHOUT-1464.patch" size="16487" author="ssc" created="Sun, 13 Apr 2014 15:55:27 +0100"/>
                            <attachment id="12639925" name="MAHOUT-1464.patch" size="18183" author="ssc" created="Sat, 12 Apr 2014 10:18:16 +0100"/>
                            <attachment id="12639747" name="MAHOUT-1464.patch" size="18862" author="ssc" created="Fri, 11 Apr 2014 08:13:40 +0100"/>
                            <attachment id="12635739" name="MAHOUT-1464.patch" size="8526" author="ssc" created="Thu, 20 Mar 2014 07:55:41 +0000"/>
                            <attachment id="12635258" name="MAHOUT-1464.patch" size="8547" author="ssc" created="Tue, 18 Mar 2014 08:23:39 +0000"/>
                            <attachment id="12635215" name="MAHOUT-1464.patch" size="8019" author="ssc" created="Tue, 18 Mar 2014 01:07:45 +0000"/>
                            <attachment id="12640101" name="run-spark-xrsj.sh" size="1172" author="pferrel" created="Mon, 14 Apr 2014 18:17:39 +0100"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>7.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Mon, 17 Mar 2014 06:16:13 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>380113</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hznchr:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>380397</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                </customfields>
    </item>
</channel>
</rss>