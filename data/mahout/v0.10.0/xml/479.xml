<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 04:28:38 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/MAHOUT-479/MAHOUT-479.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[MAHOUT-479] Streamline classification/ clustering data structures</title>
                <link>https://issues.apache.org/jira/browse/MAHOUT-479</link>
                <project id="12310751" key="MAHOUT">Mahout</project>
                    <description>&lt;p&gt;Opening this JIRA issue to collect ideas on how to streamline our classification and clustering algorithms to make integration for users easier as per mailing list thread &lt;a href=&quot;http://markmail.org/message/pnzvrqpv5226twfs&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://markmail.org/message/pnzvrqpv5226twfs&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Jake and Robin and I were talking the other evening and a common lament was that our classification (and clustering) stuff was all over the map in terms of data structures.  Driving that to rest and getting those comments even vaguely as plug and play as our much more advanced recommendation components would be very, very helpful.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This issue probably also realates to &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-287&quot; title=&quot;Bayes Classifier should use Vector as input&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-287&quot;&gt;&lt;del&gt;MAHOUT-287&lt;/del&gt;&lt;/a&gt; (intention there is to make naive bayes run on vectors as input).&lt;/p&gt;

&lt;p&gt;Ted, Jake, Robin: Would be great if someone of you could add a comment on some of the issues you discussed &quot;the other evening&quot; and (if applicable) any minor or major changes you think could help solve this issue.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12471556">MAHOUT-479</key>
            <summary>Streamline classification/ clustering data structures</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="isabel">Isabel Drost-Fromm</assignee>
                                    <reporter username="isabel">Isabel Drost-Fromm</reporter>
                        <labels>
                    </labels>
                <created>Fri, 13 Aug 2010 15:52:45 +0100</created>
                <updated>Thu, 9 Feb 2012 14:01:05 +0000</updated>
                            <resolved>Sat, 15 Oct 2011 11:21:57 +0100</resolved>
                                    <version>0.1</version>
                    <version>0.2</version>
                    <version>0.3</version>
                    <version>0.4</version>
                                    <fixVersion>0.6</fixVersion>
                                    <component>Classification</component>
                    <component>Clustering</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                <comments>
                            <comment id="12898267" author="isabel" created="Fri, 13 Aug 2010 16:06:18 +0100"  >&lt;p&gt;Some thoughts that come to my mind:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Algorithm implementations should be able to rely on getting their input as vectors+.&lt;/li&gt;
	&lt;li&gt;To make plug&apos;n&apos;play of algorithms easy we need to provide sensible default parameters for each implementation (for spectral clustering that would include adding a default strategy for computing an affinity matrix from a set of item vectors).&lt;/li&gt;
	&lt;li&gt;Parameters must be easy to override.&lt;/li&gt;
	&lt;li&gt;Integrate with the vector generation classes in mahout.util - should we move anything feature related that is still in core there?&lt;/li&gt;
	&lt;li&gt;Need a set of common interfaces for classification algorithms (methods train, classify etc. come to mind) so implementations of these can be exchanged easily.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Probably have forgotten like dozens of other open questions - any input welcome.&lt;/p&gt;

&lt;p&gt;+ Could potentially help with &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-287&quot; title=&quot;Bayes Classifier should use Vector as input&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-287&quot;&gt;&lt;del&gt;MAHOUT-287&lt;/del&gt;&lt;/a&gt;, however I need some help understanding the existing code.&lt;/p&gt;</comment>
                            <comment id="12898429" author="drew.farris" created="Fri, 13 Aug 2010 22:43:13 +0100"  >&lt;p&gt;Thanks for getting the ball rolling Isabel&lt;/p&gt;

&lt;p&gt;More discussion from the mailing list can be found here &lt;a href=&quot;http://markmail.org/thread/tarn6f4ump5zn67n&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://markmail.org/thread/tarn6f4ump5zn67n&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;One thought I remember reading somewhere was instead of using a common datatype as input for classifiers and clusterers, provide something like an InputSource interface &amp;#8211; which defines how things are read and can be implemented for any number of physical input types, e.g: text files, serialized vectors in sequence files, various types of distributed data stores)...  we provide a number of input sources with the appropriate methods and allow users of Mahout to implement their own.&lt;/p&gt;

&lt;p&gt;Some other discussion I&apos;m particularly interested in is how we might unify models across the classification and clustering code.&lt;/p&gt;</comment>
                            <comment id="12898504" author="tdunning" created="Sat, 14 Aug 2010 03:04:08 +0100"  >&lt;p&gt;Unification of the resulting models is probably much easier than the unification of the model building process itself.&lt;/p&gt;

&lt;p&gt;Some of the problems I have seen include:&lt;/p&gt;

&lt;p&gt;a) all of our clustering and classification models should be able to accept vectors and produce either a &quot;most-likely&quot; category or a vector of scores for all possible categories.  Unfortunately, there is no uniform way to load a model from a file and no uniform object structure for these models and no consistent way to call them.&lt;/p&gt;

&lt;p&gt;b) most of our learning algorithms would be happy with vectors, but there is a pretty fundamental difference between good ways to call hadoop-based and sequential training algorithms.  The sequential stuff is traditional java so the interface is very easy.  The parallel stuff is considerably harder to make into a really good interface.  We may learn some tricks with Plume or we may be able to use the Distributed Row Matrix, but it isn&apos;t an obvious answer.&lt;/p&gt;

&lt;p&gt;c) in some cases, the vectors are noticeably larger than the original data.  This occurs when the original data is very sparse and we are looking at lots of interaction variables.  Again, for sequential algorithms, this is pretty easy to deal with, but for parallel ones, it really might be better to store the original data and pass in a function that handles the vectorization on the fly.&lt;/p&gt;
</comment>
                            <comment id="12898505" author="tdunning" created="Sat, 14 Aug 2010 03:12:46 +0100"  >&lt;p&gt;Regarding vectorization strategies in general, I know of three that we currently use:&lt;/p&gt;

&lt;p&gt;a) full-size vector model.  Each continuous variable and each unique term for each text-like variable and each unique interaction term is assigned a unique location in the feature vector.  This requires that we know the vocabulary and the number of interaction combinations that exist.  This is the strategy used by the Lucene to vector converter.&lt;/p&gt;

&lt;p&gt;b) pass around the text.  This is what Naive Bayes currently does.  The result is kind of like a duplicated vector implementation with strings as the indices.  The current implementation has difficult with different fields containing text and with continuous variables.  This avoids a dictionary building pass, but leads to other difficulties unless we figure out a way to inject a text vectorizer.&lt;/p&gt;

&lt;p&gt;c) feature hashing.  This is what SGD supports.  Here, we pick the size of the vectors a priori.  Words are assigned a location based on a hash of the variable name and the word value.  Continuous variables are assigned locations based on the variable name.  It can be moderately difficult to reverse engineer a vector back to features since there can be ambiguity with very large feature spaces, but it isn&apos;t necessary to build a dictionary in order to make vectors.&lt;/p&gt;</comment>
                            <comment id="12898942" author="tdunning" created="Mon, 16 Aug 2010 15:11:45 +0100"  >&lt;p&gt;I am about to put up a patch related to &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-228&quot; title=&quot;Need sequential logistic regression implementation using SGD techniques&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-228&quot;&gt;&lt;del&gt;MAHOUT-228&lt;/del&gt;&lt;/a&gt; that proposes some interfaces for on-line learning and vector classification.&lt;/p&gt;

&lt;p&gt;These interfaces are probably too special purpose to be considered an answer to &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-479&quot; title=&quot;Streamline classification/ clustering data structures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-479&quot;&gt;&lt;del&gt;MAHOUT-479&lt;/del&gt;&lt;/a&gt;, but they indicate where my own work has been leading me lately.&lt;/p&gt;</comment>
                            <comment id="12899452" author="tdunning" created="Tue, 17 Aug 2010 17:25:55 +0100"  >&lt;p&gt;I just moved the encoding objects associated with &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-228&quot; title=&quot;Need sequential logistic regression implementation using SGD techniques&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-228&quot;&gt;&lt;del&gt;MAHOUT-228&lt;/del&gt;&lt;/a&gt; to org.apache.mahout.vectors to provide a nucleus for feature encoding.&lt;/p&gt;

&lt;p&gt;There are also a fair number of things in oam.text and oam.utils that are related.  Since those are in the utils module, however, I couldn&apos;t leverage them.  We may want to consider moving some of them to core to allow wider use.&lt;/p&gt;</comment>
                            <comment id="12899710" author="jeastman" created="Wed, 18 Aug 2010 03:49:17 +0100"  >&lt;p&gt;I&apos;m making some progress on integrating the clustering data structures. Here&apos;s how the top levels are shaping up:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-comment&quot;&gt;// &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; defines the minimal Dirichlet model
&lt;/span&gt;&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;interface&lt;/span&gt; Model&amp;lt;O&amp;gt; &lt;span class=&quot;code-keyword&quot;&gt;extends&lt;/span&gt; Writable {
  &lt;span class=&quot;code-object&quot;&gt;double&lt;/span&gt; pdf(O x);
  void observe(O x);
  &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; count();
  void computeParameters();
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; Model&amp;lt;VectorWritable&amp;gt; sampleFromPosterior();
}

&lt;span class=&quot;code-comment&quot;&gt;// &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; puts a face on a cluster
&lt;/span&gt;&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;interface&lt;/span&gt; Cluster {
  &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; getId();
  Vector getCenter();
  Vector getRadius();
  &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; getNumPoints();
  &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; asFormatString(&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;[] bindings);
  &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; asJsonString();
}

&lt;span class=&quot;code-comment&quot;&gt;// here&apos;s the resulting Cluster class hierarchy
&lt;/span&gt;&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;abstract&lt;/span&gt; class AbstractCluster &lt;span class=&quot;code-keyword&quot;&gt;implements&lt;/span&gt; Cluster, Model&amp;lt;VectorWritable&amp;gt; {}
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;abstract&lt;/span&gt; class DistanceMeasureCluster &lt;span class=&quot;code-keyword&quot;&gt;extends&lt;/span&gt; AbstractCluster {}
    &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; class Canopy &lt;span class=&quot;code-keyword&quot;&gt;extends&lt;/span&gt; DistanceMeasureCluster {}
    &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; class Cluster &lt;span class=&quot;code-keyword&quot;&gt;extends&lt;/span&gt; DistanceMeasureCluster {}
      &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; class MeanShiftCanopy &lt;span class=&quot;code-keyword&quot;&gt;extends&lt;/span&gt; Cluster {}
      &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; class SoftCluster extendsCluster {}
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; class GaussianCluster &lt;span class=&quot;code-keyword&quot;&gt;extends&lt;/span&gt; AbstractCluster {}

&lt;span class=&quot;code-comment&quot;&gt;// Note: all the current Dirichlet models can be subsumed by GaussianCluster or simple subclasses&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There&apos;s a fair amount of cleanup to tests etc to do but I thought I&apos;d post this for visibility.&lt;/p&gt;</comment>
                            <comment id="12899751" author="tdunning" created="Wed, 18 Aug 2010 07:13:46 +0100"  >&lt;p&gt;Jeff,&lt;/p&gt;

&lt;p&gt;How do you think that these relate to AbstractVectorClassifier?&lt;/p&gt;

&lt;p&gt;Does it fit reasonably as a super-class of the models?&lt;/p&gt;</comment>
                            <comment id="12899874" author="jeastman" created="Wed, 18 Aug 2010 16:34:39 +0100"  >&lt;p&gt;I don&apos;t see AbstractVectorClassifier as a super-class of these models, since it needs to operate on a set of models rather than an individual model. What I see is something more like below. I realize classify() does not return the right sized vector, but how else to normalize it properly given that an arbitrary set of model pdfs won&apos;t sum to 1? Also, what about making AbstractVectorClassifier work over VectorWritables instead of Vectors? All the clustering code uses VWs.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; class VectorModelClassifier &lt;span class=&quot;code-keyword&quot;&gt;extends&lt;/span&gt; AbstractVectorClassifier {

  List&amp;lt;Model&amp;lt;VectorWritable&amp;gt;&amp;gt; models;

  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; VectorModelClassifier(List&amp;lt;Model&amp;lt;VectorWritable&amp;gt;&amp;gt; models) {
    &lt;span class=&quot;code-keyword&quot;&gt;super&lt;/span&gt;();
    &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.models = models;
  }

  @Override
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; Vector classify(Vector instance) {
    Vector pdfs = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; DenseVector(models.size());
    &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; i = 0;
    &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (Model&amp;lt;VectorWritable&amp;gt; model : models) {
      pdfs.set(i++, model.pdf(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; VectorWritable(instance)));
    }
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; pdfs.assign(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; TimesFunction(), 1.0 / pdfs.zSum());
  }

  @Override
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;double&lt;/span&gt; classifyScalar(Vector instance) {
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (models.size() == 2) {
      &lt;span class=&quot;code-object&quot;&gt;double&lt;/span&gt; pdf0 = models.get(0).pdf(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; VectorWritable(instance));
      &lt;span class=&quot;code-object&quot;&gt;double&lt;/span&gt; pdf1 = models.get(1).pdf(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; VectorWritable(instance));
      &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; pdf0 / (pdf0 + pdf1);
    }
    &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; IllegalStateException();
  }

  @Override
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; numCategories() {
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; models.size();
  }
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12900071" author="hudson" created="Wed, 18 Aug 2010 23:23:35 +0100"  >&lt;p&gt;Integrated in Mahout-Quality #202 (See &lt;a href=&quot;https://hudson.apache.org/hudson/job/Mahout-Quality/202/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://hudson.apache.org/hudson/job/Mahout-Quality/202/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-479&quot; title=&quot;Streamline classification/ clustering data structures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-479&quot;&gt;&lt;del&gt;MAHOUT-479&lt;/del&gt;&lt;/a&gt;: Refactored Model and Cluster hierarchies to include DistanceMeasure in cluster state. All unit tests run&lt;/p&gt;</comment>
                            <comment id="12900393" author="hudson" created="Thu, 19 Aug 2010 19:36:09 +0100"  >&lt;p&gt;Integrated in Mahout-Quality #208 (See &lt;a href=&quot;https://hudson.apache.org/hudson/job/Mahout-Quality/208/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://hudson.apache.org/hudson/job/Mahout-Quality/208/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-479&quot; title=&quot;Streamline classification/ clustering data structures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-479&quot;&gt;&lt;del&gt;MAHOUT-479&lt;/del&gt;&lt;/a&gt;: This commit refactors Cluster to inherit from Model&amp;lt;VectorWritable&amp;gt; instead of AbstractCluster&lt;br/&gt;
which now inherits just Cluster. The existing Dirichlet models also now inherit from Cluster, simplifying the use&lt;br/&gt;
of generics and cleaning up a lot of the code.&lt;/p&gt;

&lt;p&gt;Since Dirichlet now can iterate over arbitrary Clusters as its models, this opens up the entire set of DistanceMeasure&lt;br/&gt;
based clusters for Dirichlet processing. This should allow the output of e.g. Canopy to become the prior model&lt;br/&gt;
distribution in a subsequent Dirichlet step. Is this a feature?&lt;/p&gt;

&lt;p&gt;The AbstractCluster hierarchy has been adjusted allowing GaussianClusterDistribution and &lt;br/&gt;
DistanceMeasureClusterDistribution to instantiate its subclasses. Both have tests and seem to work as expected.&lt;/p&gt;

&lt;p&gt;All unit tests run. More to follow&lt;/p&gt;</comment>
                            <comment id="12900968" author="hudson" created="Sat, 21 Aug 2010 04:38:31 +0100"  >&lt;p&gt;Integrated in Mahout-Quality #213 (See &lt;a href=&quot;https://hudson.apache.org/hudson/job/Mahout-Quality/213/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://hudson.apache.org/hudson/job/Mahout-Quality/213/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-479&quot; title=&quot;Streamline classification/ clustering data structures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-479&quot;&gt;&lt;del&gt;MAHOUT-479&lt;/del&gt;&lt;/a&gt;: Fixed a bug in TestVectorModelClassifier and messed around with pdf() in GaussianCluster and ASNModel. Synthetic control seems to work better on Dirichlet now but I&apos;m troubled by the impact of the two pdf() implementations on the outcome.&lt;/p&gt;</comment>
                            <comment id="12906277" author="hudson" created="Sat, 4 Sep 2010 20:28:30 +0100"  >&lt;p&gt;Integrated in Mahout-Quality #241 (See &lt;a href=&quot;https://hudson.apache.org/hudson/job/Mahout-Quality/241/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://hudson.apache.org/hudson/job/Mahout-Quality/241/&lt;/a&gt;)&lt;/p&gt;
</comment>
                            <comment id="12912284" author="tdunning" created="Sun, 19 Sep 2010 21:20:56 +0100"  >
&lt;p&gt;I did a commit recently that introduced ModelDissector.  This is useful for reverse engineering feature hashed models.&lt;/p&gt;

&lt;p&gt;The idea is that the hashed encoders have the option of having a trace dictionary.  This tells us where each feature is hashed to, or each feature/value combination in the case of word-like values.  Using this dictionary, we can put values into a synthetic feature vector in just the locations specified by a single feature or interaction.  Then we can push this through a linear model to see the contribution of that input. For any generalized linear model like logistic regression, there is a linear part of the model that allows this.   &lt;/p&gt;

&lt;p&gt;What the ModelDissector does is to accept a trace dictionary and a model in an update method.  Then in a flush method, the biggest weights are returned.  This update/flush style is used so that the trace dictionary doesn&apos;t have to grow to enormous levels, but instead can be cleared&lt;br/&gt;
between updates.&lt;/p&gt;</comment>
                            <comment id="12912292" author="hudson" created="Sun, 19 Sep 2010 22:50:19 +0100"  >&lt;p&gt;Integrated in Mahout-Quality #300 (See &lt;a href=&quot;https://hudson.apache.org/hudson/job/Mahout-Quality/300/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://hudson.apache.org/hudson/job/Mahout-Quality/300/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-479&quot; title=&quot;Streamline classification/ clustering data structures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-479&quot;&gt;&lt;del&gt;MAHOUT-479&lt;/del&gt;&lt;/a&gt; - Added javadoc explanations.&lt;/p&gt;</comment>
                            <comment id="12913625" author="jeastman" created="Wed, 22 Sep 2010 16:38:36 +0100"  >&lt;p&gt;Moving this from neverland to 0.5. A lot has been accomplished in 0.4 in the clustering jobs but more work is needed to converge with classification, SVD and perhaps recommenders&lt;/p&gt;</comment>
                            <comment id="13003870" author="srowen" created="Tue, 8 Mar 2011 09:05:53 +0000"  >&lt;p&gt;I like the idea of getting this in for 0.5, building on the work already done. Is there a reasonable &quot;stopping point&quot; in sight that would be a good place to leave it for 0.5? I&apos;m supposing we&apos;ll start heading to release within a month.&lt;/p&gt;</comment>
                            <comment id="13015225" author="srowen" created="Sun, 3 Apr 2011 22:26:15 +0100"  >&lt;p&gt;Are there any near-term changes that are worth getting in for 0.5? It is looking like this won&apos;t be fully resolved for 0.5&lt;/p&gt;</comment>
                            <comment id="13015248" author="tdunning" created="Mon, 4 Apr 2011 01:25:06 +0100"  >&lt;p&gt;I think we have bits of this already in, but the grand unified theory is&lt;br/&gt;
still lacking.&lt;/p&gt;

&lt;p&gt;I wouldn&apos;t expect it for 0.5.&lt;/p&gt;

</comment>
                            <comment id="13020336" author="jeastman" created="Fri, 15 Apr 2011 16:58:48 +0100"  >&lt;p&gt;Ted: Yeah... this is what I had in mind when I said grand unified theory.&lt;/p&gt;

&lt;p&gt;On Wed, Apr 13, 2011 at 9:24 PM, Jeff Eastman &amp;lt;jdog@windwardsolutions.com&amp;gt;wrote:&lt;/p&gt;

&lt;p&gt;&amp;gt; This could potentially collapse kmeans, fuzzyk and Dirichlet into a single implementation too:&lt;br/&gt;
&amp;gt;&lt;br/&gt;
&amp;gt; - Begin with a prior ClusterClassifier containing the appropriate sort of Cluster, in clusters-n&lt;br/&gt;
&amp;gt; - For each input Vector, compute the pdf vector using CC.classify()&lt;br/&gt;
&amp;gt; &amp;#8211; For kmeans, train the most likely model from the pdf vector&lt;br/&gt;
&amp;gt; &amp;#8211; For Dirichlet, train the model selected by the multinomial of the pfd vector * mixture vector&lt;br/&gt;
&amp;gt; &amp;#8211; For fuzzyk, train each model by its normalized pdf (would need a new classify method for this)&lt;br/&gt;
&amp;gt; - Close the CC, computing all posterior model parameters&lt;br/&gt;
&amp;gt; - Serialize the CC into clusters-n+1&lt;br/&gt;
&amp;gt;&lt;br/&gt;
&amp;gt; Now that would really be cool&lt;br/&gt;
&amp;gt;&lt;br/&gt;
&amp;gt;&lt;br/&gt;
&amp;gt; On 4/13/11 9:00 PM, Jeff Eastman wrote:&lt;br/&gt;
&amp;gt;&lt;br/&gt;
&amp;gt;&amp;gt; Here&apos;s how I got there:&lt;br/&gt;
&amp;gt;&amp;gt;&lt;br/&gt;
&amp;gt;&amp;gt; - ClusterClassifier holds a &quot;List&amp;lt;Cluster&amp;gt; models;&quot; field as its only state just like VectorModelClassifier does&lt;br/&gt;
&amp;gt;&amp;gt; - Started with ModelSerializerTest since you suggested being compatible with ModelSerializer&lt;br/&gt;
&amp;gt;&amp;gt; - This tests OnlineLogisticRegression, CrossFoldLearner and AdaptiveLogisticRegression&lt;br/&gt;
&amp;gt;&amp;gt; - The first two are also subclasses of AbstractVectorClassifier just like ClusterClassifier&lt;br/&gt;
&amp;gt;&amp;gt; - The tests pass OLR and CFL learners to train(OnlineLearner) so it made sense for a CC to be an OL too&lt;br/&gt;
&amp;gt;&amp;gt; - The new CC.train(...) methods map to &quot;models.get(actual).observe()&quot; in Cluster.observe(V)&lt;br/&gt;
&amp;gt;&amp;gt; - CC.close() maps to cluster.computeParameters() for each model which computes the posterior cluster parameters&lt;br/&gt;
&amp;gt;&amp;gt; - Now the CC is ready for another iteration or to classify, etc.&lt;br/&gt;
&amp;gt;&amp;gt;&lt;br/&gt;
&amp;gt;&amp;gt; So, the cluster iteration process starts with a prior List&amp;lt;Cluster&amp;gt; &lt;br/&gt;
&amp;gt;&amp;gt; which is used to construct the ClusterClassifier. Then in each &lt;br/&gt;
&amp;gt;&amp;gt; iteration each point is passed to CC.classify() and the maximum &lt;br/&gt;
&amp;gt;&amp;gt; probability element index in the returned Vector is used to train() &lt;br/&gt;
&amp;gt;&amp;gt; the CC. Since all the DistanceMeasureClusters contain their &lt;br/&gt;
&amp;gt;&amp;gt; appropriate DistanceMeasure, the one with the maximum pdf() is the &lt;br/&gt;
&amp;gt;&amp;gt; closest. Just what kmeans already does but done less efficiently (it &lt;br/&gt;
&amp;gt;&amp;gt; uses just the minimum distance, but pdf() = e^-distance so the closest cluster has the largest pdf()).&lt;br/&gt;
&amp;gt;&amp;gt;&lt;br/&gt;
&amp;gt;&amp;gt; Finally, instead of passing in a List&amp;lt;Cluster&amp;gt; in the KMeansClusterer &lt;br/&gt;
&amp;gt;&amp;gt; I can just carry around a CC which wraps it. Instead of serializing a &lt;br/&gt;
&amp;gt;&amp;gt; List&amp;lt;Cluster&amp;gt; at the end of each iteration I can just serialize the &lt;br/&gt;
&amp;gt;&amp;gt; CC. At the beginning of the next iteration, I just deserialize it and go.&lt;/p&gt;</comment>
                            <comment id="13020727" author="jeastman" created="Sun, 17 Apr 2011 06:54:00 +0100"  >&lt;p&gt;Here&apos;s an interesting piece of code. It nets out the clustering process in terms of the ClusterClassifier, which is an AbstractVectorClassifier which implements OnlineLearner and Writable. The policy for kmeans just returns the index of the max pdf element and the policy for dirichlet returns the multinomial of the mixture times the pdf. The really exciting thing to me is how, for clustering, we classify and then train whereas for classification it is the opposite order. Very symmetric and, uh, now rather obvious.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; ClusteringPolicy policy;

  /**
   * Iterate over data using a prior-trained classifier, &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; a number of iterations
   * @param data a List&amp;lt;Vector&amp;gt; of input vectors
   * @param prior the prior-trained ClusterClassifier
   * @param numIterations the &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; number of iterations to perform
   * @&lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; the posterior ClusterClassifier
   */
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; ClusterClassifier iterate(List&amp;lt;Vector&amp;gt; data, ClusterClassifier prior, &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; numIterations) {
    &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; iteration = 1; iteration &amp;lt;= numIterations; iteration++) {
      &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (Vector vector : data) {
        &lt;span class=&quot;code-comment&quot;&gt;// classification yields probabilities
&lt;/span&gt;        Vector pdfs = prior.classify(vector);
        &lt;span class=&quot;code-comment&quot;&gt;// policy selects a model given those probabilities
&lt;/span&gt;        &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; selected = policy.select(pdfs);
        &lt;span class=&quot;code-comment&quot;&gt;// training causes all models to observe data
&lt;/span&gt;        prior.train(selected, vector);
      }
      &lt;span class=&quot;code-comment&quot;&gt;// compute the posterior models
&lt;/span&gt;      prior.close();
      &lt;span class=&quot;code-comment&quot;&gt;// update the policy
&lt;/span&gt;      policy.update(prior);
      }
    }
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; prior;
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13023160" author="hudson" created="Fri, 22 Apr 2011 09:47:27 +0100"  >&lt;p&gt;Integrated in Mahout-Quality #769 (See &lt;a href=&quot;https://builds.apache.org/hudson/job/Mahout-Quality/769/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/hudson/job/Mahout-Quality/769/&lt;/a&gt;)&lt;/p&gt;
</comment>
                            <comment id="13030592" author="hudson" created="Mon, 9 May 2011 04:32:53 +0100"  >&lt;p&gt;Integrated in Mahout-Quality #805 (See &lt;a href=&quot;https://builds.apache.org/hudson/job/Mahout-Quality/805/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/hudson/job/Mahout-Quality/805/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-479&quot; title=&quot;Streamline classification/ clustering data structures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-479&quot;&gt;&lt;del&gt;MAHOUT-479&lt;/del&gt;&lt;/a&gt;: added a new iterate method to ClusterIterator. Method accepts 3&lt;br/&gt;
hadoop Paths for input, prior and output information plus number of desired iterations. All algorithm data is pulled-from/pushed-to SequenceFiles. Added a unit test and improved the example DisplayKMeans, DisplayFuzzyKMeans and DisplayDirichlet to use the new file-based implementation. Check out Dirichlet.&lt;/p&gt;</comment>
                            <comment id="13050302" author="vavasilev" created="Thu, 16 Jun 2011 10:05:45 +0100"  >&lt;p&gt;Hi Jeff,&lt;/p&gt;

&lt;p&gt;I am running Dirichlet clustering over the Reuters data set. Due to issues with the resulting clusters I found that there is a problem with the function calculating the pdf in org.apache.mahout.clustering.dirichlet.models.GaussianCluster and with the change you made recently:&lt;br/&gt;
1. Part of the clusters have very small radius &amp;lt;&amp;lt; 0. This leads to UncommonDistributions.dNorm returning 0.0 in case the point is at a bigger distance from the mean&lt;br/&gt;
2. dNorm returns probability density, not probability, which means that for the cases where radius &amp;lt;&amp;lt; 0 and the number of dimensions of the feature vectors is very big (~50000) the pdf goes quickly to infinity.&lt;br/&gt;
3. In case 1 and 2 happen the result for the pdf is NaN&lt;/p&gt;</comment>
                            <comment id="13128125" author="srowen" created="Sat, 15 Oct 2011 11:21:57 +0100"  >&lt;p&gt;Am I right that this is done? Looks so from the JIRA commits. Or at least, is it better to continue further work in a new JIRA?&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12444135">MAHOUT-228</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Fri, 13 Aug 2010 21:43:13 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9584</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hxy4v3:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>22939</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                </customfields>
    </item>
</channel>
</rss>