<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 04:21:01 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/MAHOUT-1615/MAHOUT-1615.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[MAHOUT-1615] SparkEngine drmFromHDFS returning the same Key for all Key,Vec Pairs for Text-Keyed SequenceFiles</title>
                <link>https://issues.apache.org/jira/browse/MAHOUT-1615</link>
                <project id="12310751" key="MAHOUT">Mahout</project>
                    <description>&lt;p&gt;When reading in seq2sparse output from HDFS in the spark-shell of form &amp;lt;Text,VectorWriteable&amp;gt;  SparkEngine&apos;s drmFromHDFS method is creating rdds with the same Key for all Pairs:  &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
mahout&amp;gt; val drmTFIDF= drmFromHDFS( path = &lt;span class=&quot;code-quote&quot;&gt;&quot;/tmp/mahout-work-andy/20news-test-vectors/part-r-00000&quot;&lt;/span&gt;)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Has keys:&lt;/p&gt;
{...} &lt;br/&gt;
    key: /talk.religion.misc/84570&lt;br/&gt;
    key: /talk.religion.misc/84570&lt;br/&gt;
    key: /talk.religion.misc/84570&lt;br/&gt;
{...}

&lt;p&gt;for the entire set.  This is the last Key in the set.&lt;/p&gt;

&lt;p&gt;The problem can be traced to the first line of drmFromHDFS(...) in SparkEngine.scala: &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
 val rdd = sc.sequenceFile(path, classOf[Writable], classOf[VectorWritable], minPartitions = parMin)
        &lt;span class=&quot;code-comment&quot;&gt;// Get rid of VectorWritable
&lt;/span&gt;        .map(t =&amp;gt; (t._1, t._2.get()))
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;which gives the same key for all t._1.&lt;/p&gt;



</description>
                <environment></environment>
        <key id="12741289">MAHOUT-1615</key>
            <summary>SparkEngine drmFromHDFS returning the same Key for all Key,Vec Pairs for Text-Keyed SequenceFiles</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="Andrew_Palumbo">Andrew Palumbo</assignee>
                                    <reporter username="Andrew_Palumbo">Andrew Palumbo</reporter>
                        <labels>
                    </labels>
                <created>Sat, 13 Sep 2014 23:58:45 +0100</created>
                <updated>Mon, 13 Apr 2015 11:20:41 +0100</updated>
                            <resolved>Fri, 10 Oct 2014 22:41:00 +0100</resolved>
                                                    <fixVersion>0.10.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                <comments>
                            <comment id="14133249" author="andrew_palumbo" created="Sun, 14 Sep 2014 16:31:54 +0100"  >&lt;p&gt;In the mahout spark-shell I can get several other Keys (though not all):&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
mahout&amp;gt; val rdd = sdc.sequenceFile(path = &lt;span class=&quot;code-quote&quot;&gt;&quot;/tmp/mahout-work-andy/20news-test-vectors/part-r-00000&quot;&lt;/span&gt;, classOf[Writable], classOf[VectorWritable], minPartitions = 10).map(t =&amp;gt; (t._1, t._2.get()))

mahout&amp;gt; val keyVec = rdd.map(_._1).collect.distinct
keyVec: Array[org.apache.hadoop.io.Writable] = Array(/comp.os.ms-windows.misc/9141, /comp.sys.mac.hardware/52007, /rec.autos/101620, /rec.sport.baseball/104334, /sci.crypt/15200, /sci.electronics/54486, /sci.space/61469, /talk.politics.guns/54503, /talk.politics.mideast/77353, /talk.religion.misc/84570)
mahout&amp;gt; keyVec.size
res1: Int = 10
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;however I&apos;m expecting several more disticnt values for each category eg.:&lt;/p&gt;

&lt;p&gt;/comp.os.ms-windows.misc/9141&lt;br/&gt;
/comp.os.ms-windows.misc/9142&lt;/p&gt;
{...}

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
mahout seqdumper -i /tmp/mahout-work-andy/20news-test-vectors/part-r-00000 | less
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;shows the first entry is for: Key: /alt.atheism/51119 which doesn&apos;t seem be showing up at all in the keys read in from the SparkContext.&lt;/p&gt;

</comment>
                            <comment id="14133255" author="andrew_palumbo" created="Sun, 14 Sep 2014 16:54:04 +0100"  >&lt;p&gt;It looks like it has something to do with Spark reusing Objects in partitions.  If I change the minPartitions to 1 I only get a single Key:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt; 
mahout&amp;gt; val rdd = sdc.sequenceFile(path = &lt;span class=&quot;code-quote&quot;&gt;&quot;/tmp/mahout-work-andy/20news-test-vectors/part-r-00000&quot;&lt;/span&gt;, classOf[Writable], classOf[Writable], minPartitions = 1)

mahout&amp;gt; val keyVec = rdd.map(_._1).collect.distinct
keyVec: Array[org.apache.hadoop.io.Writable] = Array(/talk.religion.misc/84570)

mahout&amp;gt; keyVec.size
res1: Int = 1
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and increasing minPartitions to 1000 gives the missing /alt.atheism/*:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
mahout&amp;gt; val rdd = sdc.sequenceFile(path = &lt;span class=&quot;code-quote&quot;&gt;&quot;/tmp/mahout-work-andy/20news-test-vectors/part-r-00000&quot;&lt;/span&gt;, classOf[Writable], classOf[Writable], minPartitions = 1000)
rdd: org.apache.spark.rdd.RDD[(org.apache.hadoop.io.Writable, org.apache.hadoop.io.Writable)] = HadoopRDD[9] at sequenceFile at &amp;lt;console&amp;gt;:35

mahout&amp;gt; val keyVec = rdd.map(_._1).collect.distinct
keyVec: Array[org.apache.hadoop.io.Writable] = Array(/alt.atheism/51146, /alt.atheism/51157, /alt.atheism/51188, /alt.atheism/51212, /alt.atheism/51222, /alt.atheism/51239, /alt.atheism/51244, /alt.atheism/51275, /alt.atheism/51284, /alt.atheism/51298, /alt.atheism/53055, /alt.atheism/53090, /alt.atheism/53113, /alt.atheism/53140, /alt.atheism/53178, /alt.atheism/53203, /alt.atheism/53221, /alt.atheism/53248, /alt.atheism/53274, /alt.atheism/53305, /alt.atheism/53325, /alt.atheism/53333, /alt.atheism/53347, /alt.atheism/53376, /alt.atheism/53412, /alt.atheism/53436, /alt.atheism/53458, /alt.atheism/53487, /alt.atheism/53515, /alt.atheism/53541, /alt.atheism/53579, /alt.atheism/53600, /alt.atheism/53632, /alt.atheism/53673, /alt.atheism/53759, /alt.atheism/53792, /alt.atheism/54127, /alt...

mahout&amp;gt; keyVec.size
res2: Int = 995
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;the suggested fix is:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
rdd.map(_.clone).cache
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;though i&apos;m getting an error:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Access to &lt;span class=&quot;code-keyword&quot;&gt;protected&lt;/span&gt; method clone not permitted because
 prefix type (org.apache.hadoop.io.Writable, org.apache.hadoop.io.Writable) does not conform to
 class $iwC where the access take place
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt; 

&lt;p&gt;when I try this.&lt;/p&gt;
</comment>
                            <comment id="14133379" author="githubbot" created="Sun, 14 Sep 2014 22:01:30 +0100"  >&lt;p&gt;GitHub user andrewpalumbo opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1615&quot; title=&quot;SparkEngine drmFromHDFS returning the same Key for all Key,Vec Pairs for Text-Keyed SequenceFiles&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1615&quot;&gt;&lt;del&gt;MAHOUT-1615&lt;/del&gt;&lt;/a&gt;: drmFromHDFS returning the same Key for all Key,Vec Pairs for Text-Keyed SequenceFiles&lt;/p&gt;

&lt;p&gt;    SparkContext.sequenceFile(...) will yield the same key per partition for Text-Keyed Sequence files if the key a new copy of the key is not created when mapping to an RDD.  This patch checks for Text Keys and creates a copy of each Key if necessary.    &lt;/p&gt;

&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/andrewpalumbo/mahout&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/andrewpalumbo/mahout&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1615&quot; title=&quot;SparkEngine drmFromHDFS returning the same Key for all Key,Vec Pairs for Text-Keyed SequenceFiles&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1615&quot;&gt;&lt;del&gt;MAHOUT-1615&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #52&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 6adb01ee53ce591962b97a4ed474c111635f7c47&lt;br/&gt;
Author: Andrew Palumbo &amp;lt;ap.dev@outlook.com&amp;gt;&lt;br/&gt;
Date:   2014-09-14T20:50:50Z&lt;/p&gt;

&lt;p&gt;    Create copy of Key for Text Keys&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="14133552" author="githubbot" created="Mon, 15 Sep 2014 04:51:17 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-55550711&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-55550711&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @andrewpalumbo I have to disagree with this patch. drmFromHDFS actually does exactly what it is supposed to. Here is the test script (assuming you are running in local mode you should see mappers outputs directly to your console and see distinct keys there.: &lt;/p&gt;


&lt;p&gt;    // hdfs write &amp;#8211; uncomment to test&lt;br/&gt;
    // r.writeDRM(&quot;hdfs://localhost:11010/A&quot;)&lt;/p&gt;

&lt;p&gt;    val drmA = drmParallelizeEmpty(30, 40)&lt;/p&gt;

&lt;p&gt;    val drmB = drmA.mapBlock() { case (keys, block) =&amp;gt;&lt;br/&gt;
      keys.map { key =&amp;gt; s&quot;key-$&lt;/p&gt;
{key}
&lt;p&gt;&quot;} -&amp;gt; block&lt;br/&gt;
    }&lt;/p&gt;


&lt;p&gt;    // in local mode we can see printouts to console so we&lt;br/&gt;
    // can check if the keys are actually there as strings&lt;/p&gt;

&lt;p&gt;    drmB.mapBlock() &lt;/p&gt;
{ case (keys,block) =&amp;gt;
      keys.map(println)
      keys -&amp;gt; block
    }.collect&lt;br/&gt;
    &lt;br/&gt;
    // save&lt;br/&gt;
    drmB.writeDRM(&quot;B-with-test-keys&quot;)&lt;br/&gt;
    &lt;br/&gt;
    // load back&lt;br/&gt;
    val drmC = drmFromHDFS(&quot;B-with-test-keys&quot;)&lt;br/&gt;
    &lt;br/&gt;
    // check the keys are still text there&lt;br/&gt;
    drmB.mapBlock() { case (keys,block) =&amp;gt;
      keys.map(println)
      keys -&amp;gt; block
    }
&lt;p&gt;.collect&lt;/p&gt;
</comment>
                            <comment id="14133557" author="githubbot" created="Mon, 15 Sep 2014 04:59:22 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-55551176&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-55551176&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Oops. the script has error. sorry i do see some duplicates in drm C&lt;/p&gt;

&lt;p&gt;    val inCoreC = drmC.mapBlock() &lt;/p&gt;
{ case (keys,block) =&amp;gt;
      keys.map(println)
      keys -&amp;gt; block
    }
&lt;p&gt;.collect&lt;/p&gt;


&lt;p&gt;    key-2&lt;br/&gt;
    key-2&lt;br/&gt;
    key-2&lt;br/&gt;
    key-17&lt;br/&gt;
    key-17&lt;br/&gt;
    key-17&lt;br/&gt;
    key-29&lt;br/&gt;
    key-29&lt;br/&gt;
    key-29&lt;br/&gt;
    key-5&lt;br/&gt;
    key-5&lt;br/&gt;
    key-5&lt;br/&gt;
    key-8&lt;br/&gt;
    key-8&lt;br/&gt;
    key-8&lt;br/&gt;
    key-20&lt;br/&gt;
    key-20&lt;br/&gt;
    key-20&lt;br/&gt;
    key-23&lt;br/&gt;
    key-23&lt;br/&gt;
    key-23&lt;br/&gt;
    key-14&lt;br/&gt;
    key-14&lt;br/&gt;
    key-14&lt;br/&gt;
    key-11&lt;br/&gt;
    key-11&lt;br/&gt;
    key-11&lt;br/&gt;
    key-26&lt;br/&gt;
    key-26&lt;br/&gt;
    key-26&lt;/p&gt;


&lt;p&gt;    I did check row bindings on drmB and they are correct, but after save-reload cycle they are no more correct. Which means collect() is not the reason, it is either save or drmFromHdfs. I still think the patch is kludgy and there has to be a simpler way to fix this though. Let me consider this for a moment. &lt;/p&gt;


</comment>
                            <comment id="14133567" author="githubbot" created="Mon, 15 Sep 2014 05:13:52 +0100"  >&lt;p&gt;Github user andrewpalumbo commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-55551628&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-55551628&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    thanks for lookling at it Dmitriy.  I&apos;m sure the patch can be written more elegantly.  There is some information here from the spark Mailing list about what is causing the problem:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;http://apache-spark-user-list.1001560.n3.nabble.com/Spark-SequenceFile-Java-API-Repeat-Key-Values-td353.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://apache-spark-user-list.1001560.n3.nabble.com/Spark-SequenceFile-Java-API-Repeat-Key-Values-td353.html&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14133581" author="githubbot" created="Mon, 15 Sep 2014 05:52:39 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-55552838&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-55552838&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    yeah i dont think cloning is needed, since we unwrapping the actual object (string, int etc). So by the time it is cached, it is a collection of ints or stings but not writables. So what Matei is saying there, should not already be applicable.&lt;/p&gt;

&lt;p&gt;    More likely, i was thinking, spark is creating a strict collection when it fuses two maps, so more likely fix would be simply removing extra map, something along the lines &lt;/p&gt;

&lt;p&gt;         val rdd = sc.sequenceFile(path, classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Writable&amp;#93;&lt;/span&gt;, classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;VectorWritable&amp;#93;&lt;/span&gt;, minPartitions = parMin)&lt;/p&gt;

&lt;p&gt;        ... &lt;/p&gt;

&lt;p&gt;          val drmRdd = rdd.map &lt;/p&gt;
{ t =&amp;gt; key2valFunc(t._1) -&amp;gt; t._2.get}
&lt;p&gt;        ...&lt;/p&gt;


&lt;p&gt;    (so there&apos;s no forced map fusion). But it doesn&apos;t seem to work in my tests either.  Weird.&lt;/p&gt;

&lt;p&gt;    Ok i think i want to stop spending my neurons on this for now and  to have a look at it tomorrow again. &lt;/p&gt;

&lt;p&gt;    But i don&apos;t think we should clone anything. we just need to transform away from writable before attempting any caches.&lt;/p&gt;
</comment>
                            <comment id="14133583" author="githubbot" created="Mon, 15 Sep 2014 05:54:05 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-55552882&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-55552882&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Actually i am  starting to think that caching to &quot;memory only&quot; is also a mistake here. If people want to cache immediately after load, they can do it with drm.checkpoint(), i am not sure why it should be trying to cache it right away. i&apos;d remove that line as well.&lt;/p&gt;</comment>
                            <comment id="14133584" author="githubbot" created="Mon, 15 Sep 2014 05:58:19 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-55553012&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-55553012&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Also it puzzles me that the test `DRM DFS i/o (local)` in `DrmLikeSuiteBase` should have been catching this, or modified to catch this. but it wasn&apos;t catching it, most likely because by default matrices parallelize into 10 partitions, so in this case any partition would have at most 1 row, at which point Writable problem of course will not manifest. &lt;/p&gt;

&lt;p&gt;    That&apos;s what i&apos;d start with, modify this test to reproduce this.&lt;/p&gt;</comment>
                            <comment id="14133597" author="githubbot" created="Mon, 15 Sep 2014 06:29:57 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-55554028&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-55554028&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    ok yes i know why it is. Spark doesn&apos;t of course read the header file and set proper class tag evidence on the loaded rdd. hence, none of the conversions happen and drm is loaded as simply DrmLike&lt;span class=&quot;error&quot;&gt;&amp;#91;Writable&amp;#93;&lt;/span&gt;. which is of course not what should ideally happen since information about key type is already avalable in the sequence file header. &lt;/p&gt;

&lt;p&gt;    This would require some thinking since correct conversion away from writable to concrete key type does need to happen. &lt;/p&gt;

&lt;p&gt;    bummer.&lt;/p&gt;</comment>
                            <comment id="14134174" author="githubbot" created="Mon, 15 Sep 2014 18:39:42 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-55627685&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-55627685&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Well, there are three solutions to the problem I see. One is easy, another is pretty easy but perhaps more expensive operationally.&lt;/p&gt;

&lt;p&gt;    First, let&apos;s clarify what is going on there. &lt;/p&gt;

&lt;p&gt;    (1) since the key type is having a ContextTag as context bound: DrmLike type is defined as DrmLike&lt;span class=&quot;error&quot;&gt;&amp;#91;K:ClassTag&amp;#93;&lt;/span&gt;, it means that returning CheckpointedDrm&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt; (which is subtype) from drmFromHDFS in fact implicitly means returning two things: CheckpointedDrm and evidence:ContextTag&lt;span class=&quot;error&quot;&gt;&amp;#91;K&amp;#93;&lt;/span&gt;. The evidence is essentially a run time class information (including all generics hierarchies possibly attached to it).&lt;/p&gt;

&lt;p&gt;    Thinking is, since key class name is stored in the sequence file headers, it makes sense to expect for this routine to read out the header and infer ContextTag&lt;span class=&quot;error&quot;&gt;&amp;#91;K&amp;#93;&lt;/span&gt; on its own. As comment indicates there, it actually expected Spark to do that. But simple examination of sequenceFile() reveals that Spark actually does not do that. Instead, it is just propagates the evidence it already got when this method is called, and doesn&apos;t read any headers (or anything for that matter) until actual execution action occurs. As a result, intended conversion from writable to its wrapped type (String, Int or Long) does not really happen as it stands; instead, Writable is just propagated up the food chain, and we of course already know that writable instances are reused and would not make it thru strict Scala collections. &lt;/p&gt;

&lt;p&gt;    So, based on that, there are two ways to address that. &lt;/p&gt;

&lt;p&gt;    (1) One, simple way, is to escape this problem the same way Spark has escaped it: namely, require actual key type evidence from the user. That would require changing method signature to &lt;/p&gt;

&lt;p&gt;       def drmFromHDFS&lt;span class=&quot;error&quot;&gt;&amp;#91;K:ClassTag&amp;#93;&lt;/span&gt; (path: String, parMin:Int = 0)(implicit sc: DistributedContext): CheckpointedDrm&lt;span class=&quot;error&quot;&gt;&amp;#91;K&amp;#93;&lt;/span&gt; &lt;/p&gt;


&lt;p&gt;    Then you&apos;d have to implicitly supply evidence by doing something like this:&lt;/p&gt;

&lt;p&gt;        val drmA:DrmLike&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt; = drmFromHdfs(...)&lt;/p&gt;

&lt;p&gt;    I don&apos;t like this too much. &lt;/p&gt;

&lt;p&gt;    First, you have to remember to explicitly give it a type like above instead of implicit type inference which is expected in Scala &lt;/p&gt;

&lt;p&gt;         val drmA=...&lt;/p&gt;

&lt;p&gt;    Another reason i don&apos;t like it is that you potentially are supplying a redundant information (since this information is already available form sequence file headers). Not only that, you &lt;em&gt;have&lt;/em&gt; to be right, or your delayed execution will fail in the tasks (so it is late error catching, which is fairly hairy for most users, so i&apos;d expect a wall of questions of the type &quot;why my tasks fail&quot; for this very basic operation. &lt;/p&gt;

&lt;p&gt;    So, not so user-friendly (although those who know will cope). &lt;/p&gt;

&lt;p&gt;    (2) Another way to fix it is to keep current method contract, and fill in this functionality that I believe is missing in Spark, namely, examine input headers and load in key class type and thus infer proper ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt;. &lt;/p&gt;

&lt;p&gt;    There practically no downsides to this, except for one. Up until now we pretty much avoided direct dependencies on any Hadoop libraries. Spark goes to great length (mostly, reflection and pluggable Strategies based on Hadoop version) in order to retain compatibility with various HDFS flavors and versions. I just don&apos;t want to join this game. Very much so.&lt;/p&gt;

&lt;p&gt;    (3) Another way is similar to (2) in the sense that we keep the signature and idea of filling the gap of inferring ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt; ourselves, but we do it by examining first Writeable key instance in the dataset. &lt;/p&gt;

&lt;p&gt;    This is even better than (2) since it keeps us from dealing with HDFS apis directly. But obvious downside is that we probably now will have to load the entire partition in spark instead of just reading sequence file header. So much for delayed action and memory allocation. &lt;/p&gt;


&lt;hr /&gt;
&lt;p&gt;    my personal preference is probably going with (2) while exploring (3) in terms of how bad this practice actually may get. Of course (1) is also an option since it is the easiest to implement.&lt;/p&gt;</comment>
                            <comment id="14134219" author="githubbot" created="Mon, 15 Sep 2014 19:05:18 +0100"  >&lt;p&gt;Github user andrewpalumbo commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-55631586&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-55631586&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    (2) and (3) seem best to me,  I should be able to look more closely at it  tonight to try to get a better understing of what it entails.&lt;/p&gt;</comment>
                            <comment id="14134331" author="githubbot" created="Mon, 15 Sep 2014 20:04:20 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-55641649&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-55641649&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @andrewpalumbo  ok &amp;#8211; thank you for taking initiative on this! much appreciated, this seems like a most very cruel bug to me that ever happened to date on the spark stuff. &lt;/p&gt;</comment>
                            <comment id="14143378" author="githubbot" created="Mon, 22 Sep 2014 17:36:37 +0100"  >&lt;p&gt;Github user andrewpalumbo commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-56401176&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-56401176&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Still a work in progress, (and still in need of some cleanup). The latest commits now solve the original key object reuse problem by method (2) - reading key type in from the SequenceFile  Headers and then matching on it:&lt;/p&gt;

&lt;p&gt;        mahout&amp;gt; val drmTFIDF= drmFromHDFS( path = &quot;/tmp/mahout-work-andy/20news-test-vectors/part-r-00000&quot;)&lt;br/&gt;
        14/09/22 11:20:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable&lt;br/&gt;
        drmTFIDF: org.apache.mahout.math.drm.CheckpointedDrm&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt; = org.apache.mahout.sparkbindings.drm.CheckpointedDrmSpark@adf7236 &lt;br/&gt;
        mahout&amp;gt; val rowLabels=drmTFIDF.getRowLabelBindings&lt;br/&gt;
        rowLabels: java.util.Map&lt;span class=&quot;error&quot;&gt;&amp;#91;String,Integer&amp;#93;&lt;/span&gt; = {/soc.religion.christian/21427=6141, /comp.graphics/38427=422, /comp.sys.ibm.pc.hardware/60526=1281, /misc.forsale/76295=2495, /soc.religion.christian/21332=6103, /sci.med/59045=5265, /sci.electronics/54343=5096, /comp.sys.ibm.pc.hardware/60928=1404, /rec.sport.hockey/54173=4205, /rec.motorcycles/104596=3282, /rec.autos/103326=2968, /talk.politics.misc/179110=7333, /comp.windows.x/66966=1944, /rec.autos/103707=3053, /comp.windows.x/67474=2146, /rec.sport.baseball/105011=3850, /talk.religion.misc/83812=7424, /comp.graphics/38707=522, /comp.graphics/38597=484, /sci.electronics/54317=5083, /rec.motorcycles/104708=3322, /rec.sport.hockey/53627=3994, /comp.sys.mac.hardware/51633=1601, /sci.crypt/16088=4686, /sci.electronics/53714=4840, /rec.sport.ho...&lt;br/&gt;
        mahout&amp;gt; rowLabels.size&lt;br/&gt;
        res15: Int = 7598&lt;/p&gt;


&lt;p&gt;    Which is what I am expecting.  &lt;/p&gt;

&lt;p&gt;    Two problems that I am still having:&lt;/p&gt;

&lt;p&gt;    (1) Its not yet solving the problem of setting the DrmLike&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt; ClassTag yet.&lt;/p&gt;


&lt;p&gt;        mahout&amp;gt; def getKeyClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;K: ClassTag&amp;#93;&lt;/span&gt;(drm:DrmLike&lt;span class=&quot;error&quot;&gt;&amp;#91;K&amp;#93;&lt;/span&gt;) = implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;K&amp;#93;&lt;/span&gt;]&lt;/p&gt;

&lt;p&gt;        mahout&amp;gt; getKeyClassTag(drmTFIDF)&lt;br/&gt;
        res13: scala.reflect.ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt; = Object &lt;/p&gt;

&lt;p&gt;     I believe that this is just because I&apos;m not setting it correctly due to my limited scala abilities.&lt;/p&gt;

&lt;p&gt;    (2) DRM DFS i/o (local) is failing.  I believe that this may  downside to integrating HDFS I/O code into the spark module. I&apos;m not positive I&apos;m setting the configuration correctly inside of drmFromHDFS(...).  I have no problem reading in the files from within the spark-shell, but the spark `DRM DFS i/o (local)` test is failing with:&lt;/p&gt;

&lt;p&gt;        DRM DFS i/o (local) *** FAILED ***&lt;br/&gt;
        java.io.FileNotFoundException: /home/andy/sandbox/mahout/spark/tmp/UploadedDRM (Is a directory)&lt;/p&gt;


&lt;p&gt;    I believe may be because SequenceFile.readHeader(...) is trying to read from HDFS and the test is writing locally.  I will continue to look into this.   &lt;/p&gt;
</comment>
                            <comment id="14143419" author="githubbot" created="Mon, 22 Sep 2014 17:58:53 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-56404290&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-56404290&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    on (1), it doesn&apos;t work because it takes classTag from the method bound,&lt;br/&gt;
    not from actual evidence in the class.&lt;/p&gt;

&lt;p&gt;    in order for this to work, i suggest to add&lt;/p&gt;

&lt;p&gt;    def keyClassTag:ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;K&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;    to CheckpointedDrm trait and implement it in concrete checkpoined&lt;br/&gt;
    implementations as simply `implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;K&amp;#93;&lt;/span&gt;]`. Unfortunately you&lt;br/&gt;
    cannot implement it in a trait (like inside DrmLike or CheckpointedDRM)&lt;br/&gt;
    because as it stands, traits do not support access to concrete class&lt;br/&gt;
    evidence (as our workaround demonstrates, it is theoretically possible to&lt;br/&gt;
    support it thru virtual query to implementation, but as it stands, scala is&lt;br/&gt;
    not really there).&lt;/p&gt;

&lt;p&gt;    on (2), you need to ask to load not the directory, by any partition file&lt;br/&gt;
    inside that directory. Obviously you need to require that source directory&lt;br/&gt;
    contains at least on partion file with a header.&lt;/p&gt;

&lt;p&gt;    Also keep in mind that SequenceFile api changed A LOT between hadoop 2 and&lt;br/&gt;
    1 and spark works with both, but naive (non-reflection) implementation can&lt;br/&gt;
    only work with whatever currently declared as Mahout dependency. This is&lt;br/&gt;
    why i am saying implementing it with full cross-version hadoop&lt;br/&gt;
    compatibility the way Spark does is extremely hairy.&lt;/p&gt;





&lt;p&gt;    On Mon, Sep 22, 2014 at 9:36 AM, Andrew Palumbo &amp;lt;notifications@github.com&amp;gt;&lt;br/&gt;
    wrote:&lt;/p&gt;

&lt;p&gt;    &amp;gt; Still a work in progress, (and still in need of some cleanup). The latest&lt;br/&gt;
    &amp;gt; commits now solve the original key object reuse problem by method (2) -&lt;br/&gt;
    &amp;gt; reading key type in from the SequenceFile Headers and then matching on it:&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt; mahout&amp;gt; val drmTFIDF= drmFromHDFS( path = &quot;/tmp/mahout-work-andy/20news-test-vectors/part-r-00000&quot;)&lt;br/&gt;
    &amp;gt; 14/09/22 11:20:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable&lt;br/&gt;
    &amp;gt; drmTFIDF: org.apache.mahout.math.drm.CheckpointedDrm&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt; = org.apache.mahout.sparkbindings.drm.CheckpointedDrmSpark@adf7236&lt;br/&gt;
    &amp;gt; mahout&amp;gt; val rowLabels=drmTFIDF.getRowLabelBindings&lt;br/&gt;
    &amp;gt; rowLabels: java.util.Map&lt;span class=&quot;error&quot;&gt;&amp;#91;String,Integer&amp;#93;&lt;/span&gt; = {/soc.religion.christian/21427=6141, /comp.graphics/38427=422, /comp.sys.ibm.pc.hardware/60526=1281, /misc.forsale/76295=2495, /soc.religion.christian/21332=6103, /sci.med/59045=5265, /sci.electronics/54343=5096, /comp.sys.ibm.pc.hardware/60928=1404, /rec.sport.hockey/54173=4205, /rec.motorcycles/104596=3282, /rec.autos/103326=2968, /talk.politics.misc/179110=7333, /comp.windows.x/66966=1944, /rec.autos/103707=3053, /comp.windows.x/67474=2146, /rec.sport.baseball/105011=3850, /talk.religion.misc/83812=7424, /comp.graphics/38707=522, /comp.graphics/38597=484, /sci.electronics/54317=5083, /rec.motorcycles/104708=3322, /rec.sport.hockey/53627=3994, /comp.sys.mac.hardware/51633=1601, /sci.crypt/16088=4686, /sci.electronics/53714=4840, /rec.sport.ho...&lt;br/&gt;
    &amp;gt; mahout&amp;gt; rowLabels.size&lt;br/&gt;
    &amp;gt; res15: Int = 7598&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt; Which is what I am expecting.&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt; Two problems that I am still having:&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt; (1) Its not yet solving the problem of setting the DrmLike&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt; ClassTag yet.&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt; mahout&amp;gt; def getKeyClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;K: ClassTag&amp;#93;&lt;/span&gt;(drm:DrmLike&lt;span class=&quot;error&quot;&gt;&amp;#91;K&amp;#93;&lt;/span&gt;) = implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;K&amp;#93;&lt;/span&gt;]&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt; mahout&amp;gt; getKeyClassTag(drmTFIDF)&lt;br/&gt;
    &amp;gt; res13: scala.reflect.ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt; = Object&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt; I believe that this is just because I&apos;m not setting it correctly due to my&lt;br/&gt;
    &amp;gt; limited scala abilities.&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt; (2) DRM DFS i/o (local) is failing. I believe that this may downside to&lt;br/&gt;
    &amp;gt; integrating HDFS I/O code into the spark module. I&apos;m not positive I&apos;m&lt;br/&gt;
    &amp;gt; setting the configuration correctly inside of drmFromHDFS(...). I have no&lt;br/&gt;
    &amp;gt; problem reading in the files from within the spark-shell, but the spark DRM&lt;br/&gt;
    &amp;gt; DFS i/o (local) test is failing with:&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt; DRM DFS i/o (local) *** FAILED ***&lt;br/&gt;
    &amp;gt; java.io.FileNotFoundException: /home/andy/sandbox/mahout/spark/tmp/UploadedDRM (Is a directory)&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt; I believe may be because SequenceFile.readHeader(...) is trying to read&lt;br/&gt;
    &amp;gt; from HDFS and the test is writing locally. I will continue to look into&lt;br/&gt;
    &amp;gt; this.&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt; &#8212;&lt;br/&gt;
    &amp;gt; Reply to this email directly or view it on GitHub&lt;br/&gt;
    &amp;gt; &amp;lt;&lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-56401176&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-56401176&lt;/a&gt;&amp;gt;.&lt;br/&gt;
    &amp;gt;&lt;/p&gt;</comment>
                            <comment id="14143432" author="githubbot" created="Mon, 22 Sep 2014 18:07:15 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-56405511&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-56405511&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    to make scala things  a little bit simpler, declaring context bound is like&lt;br/&gt;
    creating an implicit value inside a class or implicit parameter call to a&lt;br/&gt;
    method. Examples of what it means :&lt;/p&gt;


&lt;p&gt;         def method&lt;span class=&quot;error&quot;&gt;&amp;#91;K:ContextType&amp;#93;&lt;/span&gt;( ...params)&lt;/p&gt;


&lt;p&gt;    is equivalent to declaring&lt;/p&gt;

&lt;p&gt;        def method(...params)(implicit evidence$1:ContextType)&lt;/p&gt;


&lt;p&gt;    similarly,&lt;/p&gt;

&lt;p&gt;        class Clazz&lt;span class=&quot;error&quot;&gt;&amp;#91;K:ContextType&amp;#93;&lt;/span&gt; (...params...)&lt;/p&gt;


&lt;p&gt;    is equivalent to writing&lt;/p&gt;

&lt;p&gt;        class Clazz(...params...)(implicit val evidence$1:ContextType)  ....&lt;/p&gt;


&lt;p&gt;    so when you do what you did it is equivalent to returning implicitly passed&lt;br/&gt;
    parameter to the mehod, not implicit value extraction from implementation&lt;br/&gt;
    class you&apos;ve passed.&lt;/p&gt;



&lt;p&gt;    On Mon, Sep 22, 2014 at 9:58 AM, Dmitriy Lyubimov &amp;lt;dlieu.7@gmail.com&amp;gt; wrote:&lt;/p&gt;

&lt;p&gt;    &amp;gt; on (1), it doesn&apos;t work because it takes classTag from the method bound,&lt;br/&gt;
    &amp;gt; not from actual evidence in the class.&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt; in order for this to work, i suggest to add&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt; def keyClassTag:ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;K&amp;#93;&lt;/span&gt;&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt; to CheckpointedDrm trait and implement it in concrete checkpoined&lt;br/&gt;
    &amp;gt; implementations as simply `implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;K&amp;#93;&lt;/span&gt;]`. Unfortunately you&lt;br/&gt;
    &amp;gt; cannot implement it in a trait (like inside DrmLike or CheckpointedDRM)&lt;br/&gt;
    &amp;gt; because as it stands, traits do not support access to concrete class&lt;br/&gt;
    &amp;gt; evidence (as our workaround demonstrates, it is theoretically possible to&lt;br/&gt;
    &amp;gt; support it thru virtual query to implementation, but as it stands, scala is&lt;br/&gt;
    &amp;gt; not really there).&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt; on (2), you need to ask to load not the directory, by any partition file&lt;br/&gt;
    &amp;gt; inside that directory. Obviously you need to require that source directory&lt;br/&gt;
    &amp;gt; contains at least on partion file with a header.&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt; Also keep in mind that SequenceFile api changed A LOT between hadoop 2 and&lt;br/&gt;
    &amp;gt; 1 and spark works with both, but naive (non-reflection) implementation can&lt;br/&gt;
    &amp;gt; only work with whatever currently declared as Mahout dependency. This is&lt;br/&gt;
    &amp;gt; why i am saying implementing it with full cross-version hadoop&lt;br/&gt;
    &amp;gt; compatibility the way Spark does is extremely hairy.&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt; On Mon, Sep 22, 2014 at 9:36 AM, Andrew Palumbo &amp;lt;notifications@github.com&amp;gt;&lt;br/&gt;
    &amp;gt; wrote:&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt;&amp;gt; Still a work in progress, (and still in need of some cleanup). The latest&lt;br/&gt;
    &amp;gt;&amp;gt; commits now solve the original key object reuse problem by method (2) -&lt;br/&gt;
    &amp;gt;&amp;gt; reading key type in from the SequenceFile Headers and then matching on it:&lt;br/&gt;
    &amp;gt;&amp;gt;&lt;br/&gt;
    &amp;gt;&amp;gt; mahout&amp;gt; val drmTFIDF= drmFromHDFS( path = &quot;/tmp/mahout-work-andy/20news-test-vectors/part-r-00000&quot;)&lt;br/&gt;
    &amp;gt;&amp;gt; 14/09/22 11:20:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable&lt;br/&gt;
    &amp;gt;&amp;gt; drmTFIDF: org.apache.mahout.math.drm.CheckpointedDrm&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt; = org.apache.mahout.sparkbindings.drm.CheckpointedDrmSpark@adf7236&lt;br/&gt;
    &amp;gt;&amp;gt; mahout&amp;gt; val rowLabels=drmTFIDF.getRowLabelBindings&lt;br/&gt;
    &amp;gt;&amp;gt; rowLabels: java.util.Map&lt;span class=&quot;error&quot;&gt;&amp;#91;String,Integer&amp;#93;&lt;/span&gt; = {/soc.religion.christian/21427=6141, /comp.graphics/38427=422, /comp.sys.ibm.pc.hardware/60526=1281, /misc.forsale/76295=2495, /soc.religion.christian/21332=6103, /sci.med/59045=5265, /sci.electronics/54343=5096, /comp.sys.ibm.pc.hardware/60928=1404, /rec.sport.hockey/54173=4205, /rec.motorcycles/104596=3282, /rec.autos/103326=2968, /talk.politics.misc/179110=7333, /comp.windows.x/66966=1944, /rec.autos/103707=3053, /comp.windows.x/67474=2146, /rec.sport.baseball/105011=3850, /talk.religion.misc/83812=7424, /comp.graphics/38707=522, /comp.graphics/38597=484, /sci.electronics/54317=5083, /rec.motorcycles/104708=3322, /rec.sport.hockey/53627=3994, /comp.sys.mac.hardware/51633=1601, /sci.crypt/16088=4686, /sci.electronics/53714=4840, /rec.sport.ho...&lt;br/&gt;
    &amp;gt;&amp;gt; mahout&amp;gt; rowLabels.size&lt;br/&gt;
    &amp;gt;&amp;gt; res15: Int = 7598&lt;br/&gt;
    &amp;gt;&amp;gt;&lt;br/&gt;
    &amp;gt;&amp;gt; Which is what I am expecting.&lt;br/&gt;
    &amp;gt;&amp;gt;&lt;br/&gt;
    &amp;gt;&amp;gt; Two problems that I am still having:&lt;br/&gt;
    &amp;gt;&amp;gt;&lt;br/&gt;
    &amp;gt;&amp;gt; (1) Its not yet solving the problem of setting the DrmLike&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt; ClassTag&lt;br/&gt;
    &amp;gt;&amp;gt; yet.&lt;br/&gt;
    &amp;gt;&amp;gt;&lt;br/&gt;
    &amp;gt;&amp;gt; mahout&amp;gt; def getKeyClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;K: ClassTag&amp;#93;&lt;/span&gt;(drm:DrmLike&lt;span class=&quot;error&quot;&gt;&amp;#91;K&amp;#93;&lt;/span&gt;) = implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;K&amp;#93;&lt;/span&gt;]&lt;br/&gt;
    &amp;gt;&amp;gt;&lt;br/&gt;
    &amp;gt;&amp;gt; mahout&amp;gt; getKeyClassTag(drmTFIDF)&lt;br/&gt;
    &amp;gt;&amp;gt; res13: scala.reflect.ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt; = Object&lt;br/&gt;
    &amp;gt;&amp;gt;&lt;br/&gt;
    &amp;gt;&amp;gt; I believe that this is just because I&apos;m not setting it correctly due to&lt;br/&gt;
    &amp;gt;&amp;gt; my limited scala abilities.&lt;br/&gt;
    &amp;gt;&amp;gt;&lt;br/&gt;
    &amp;gt;&amp;gt; (2) DRM DFS i/o (local) is failing. I believe that this may downside to&lt;br/&gt;
    &amp;gt;&amp;gt; integrating HDFS I/O code into the spark module. I&apos;m not positive I&apos;m&lt;br/&gt;
    &amp;gt;&amp;gt; setting the configuration correctly inside of drmFromHDFS(...). I have no&lt;br/&gt;
    &amp;gt;&amp;gt; problem reading in the files from within the spark-shell, but the spark DRM&lt;br/&gt;
    &amp;gt;&amp;gt; DFS i/o (local) test is failing with:&lt;br/&gt;
    &amp;gt;&amp;gt;&lt;br/&gt;
    &amp;gt;&amp;gt; DRM DFS i/o (local) *** FAILED ***&lt;br/&gt;
    &amp;gt;&amp;gt; java.io.FileNotFoundException: /home/andy/sandbox/mahout/spark/tmp/UploadedDRM (Is a directory)&lt;br/&gt;
    &amp;gt;&amp;gt;&lt;br/&gt;
    &amp;gt;&amp;gt; I believe may be because SequenceFile.readHeader(...) is trying to read&lt;br/&gt;
    &amp;gt;&amp;gt; from HDFS and the test is writing locally. I will continue to look into&lt;br/&gt;
    &amp;gt;&amp;gt; this.&lt;br/&gt;
    &amp;gt;&amp;gt;&lt;br/&gt;
    &amp;gt;&amp;gt; &#8212;&lt;br/&gt;
    &amp;gt;&amp;gt; Reply to this email directly or view it on GitHub&lt;br/&gt;
    &amp;gt;&amp;gt; &amp;lt;&lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-56401176&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-56401176&lt;/a&gt;&amp;gt;.&lt;br/&gt;
    &amp;gt;&amp;gt;&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt;&lt;/p&gt;</comment>
                            <comment id="14143449" author="githubbot" created="Mon, 22 Sep 2014 18:17:57 +0100"  >&lt;p&gt;GitHub user dlyubimov opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/53&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/53&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1615&quot; title=&quot;SparkEngine drmFromHDFS returning the same Key for all Key,Vec Pairs for Text-Keyed SequenceFiles&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1615&quot;&gt;&lt;del&gt;MAHOUT-1615&lt;/del&gt;&lt;/a&gt; test and keyclass helper property on checkpoints&lt;/p&gt;

&lt;p&gt;    adding assert to `DRM DFS i/o (local)` test to fail if key class tag is incorrectly loaded.&lt;/p&gt;

&lt;p&gt;    temporary commended h20 module since it doesn&apos;t compile&lt;/p&gt;

&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/dlyubimov/mahout&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/dlyubimov/mahout&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1615&quot; title=&quot;SparkEngine drmFromHDFS returning the same Key for all Key,Vec Pairs for Text-Keyed SequenceFiles&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1615&quot;&gt;&lt;del&gt;MAHOUT-1615&lt;/del&gt;&lt;/a&gt;-drmFromHdfs&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/53.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/53.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #53&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 0d64b0be427998e74cc6d57bf653573b235e0a31&lt;br/&gt;
Author: Dmitriy Lyubimov &amp;lt;dlyubimov@apache.org&amp;gt;&lt;br/&gt;
Date:   2014-09-22T17:13:15Z&lt;/p&gt;

&lt;p&gt;    Adding key class tag extraction from checkpointed class;&lt;br/&gt;
    adding assert to `DRM DFS i/o (local)` test to fail if key class tag is incorrectly loaded.&lt;/p&gt;

&lt;p&gt;    temporary commended h20 module since it doesn&apos;t compile&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="14143455" author="githubbot" created="Mon, 22 Sep 2014 18:19:56 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-56407286&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-56407286&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @andrewpalumbo see #53 for added test claim of correct key class tag. (warning, #53 disables h20 compilation for the time being). you can merge it in your #52 work branch for the test to verify the solution correctly.&lt;/p&gt;</comment>
                            <comment id="14143458" author="githubbot" created="Mon, 22 Sep 2014 18:22:29 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-56407640&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-56407640&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Obviously, #53 assert inside the i/o test fails right now  with &lt;/p&gt;

&lt;p&gt;         org.apache.hadoop.io.Writable was not equal to Int&lt;/p&gt;

&lt;p&gt;    because it doesn&apos;t deduce correct key type.&lt;/p&gt;</comment>
                            <comment id="14143468" author="githubbot" created="Mon, 22 Sep 2014 18:25:49 +0100"  >&lt;p&gt;Github user andrewpalumbo commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-56408133&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-56408133&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    ok great- thanks!&lt;/p&gt;</comment>
                            <comment id="14143476" author="githubbot" created="Mon, 22 Sep 2014 18:29:11 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-56408611&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-56408611&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I think I made a mistake in this sence by not defining DrmLike or at least CheckpointedDrm as an abstract class instead of a trait. That way, classtag evidence would&apos;ve been accessible to abstract class. &lt;/p&gt;</comment>
                            <comment id="14143491" author="githubbot" created="Mon, 22 Sep 2014 18:37:27 +0100"  >&lt;p&gt;Github user andrewpalumbo commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/53#issuecomment-56409754&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/53#issuecomment-56409754&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    thats weird- I can&apos;t seem to pull this branch github is asking me for authentication.&lt;/p&gt;</comment>
                            <comment id="14143504" author="githubbot" created="Mon, 22 Sep 2014 18:42:15 +0100"  >&lt;p&gt;Github user andrewpalumbo commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/53#issuecomment-56410489&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/53#issuecomment-56410489&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    sorry- had a typo when trying to pull&lt;/p&gt;</comment>
                            <comment id="14143505" author="githubbot" created="Mon, 22 Sep 2014 18:42:19 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/53#issuecomment-56410501&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/53#issuecomment-56410501&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    `git pull git@github.com:dlyubimov/mahout &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1615&quot; title=&quot;SparkEngine drmFromHDFS returning the same Key for all Key,Vec Pairs for Text-Keyed SequenceFiles&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1615&quot;&gt;&lt;del&gt;MAHOUT-1615&lt;/del&gt;&lt;/a&gt;-drmFromHdfs`?&lt;/p&gt;</comment>
                            <comment id="14143510" author="githubbot" created="Mon, 22 Sep 2014 18:44:12 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/53#issuecomment-56410749&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/53#issuecomment-56410749&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    or i guess `git pull &lt;a href=&quot;https://github.com.dlyubimov/mahout&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com.dlyubimov/mahout&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1615&quot; title=&quot;SparkEngine drmFromHDFS returning the same Key for all Key,Vec Pairs for Text-Keyed SequenceFiles&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1615&quot;&gt;&lt;del&gt;MAHOUT-1615&lt;/del&gt;&lt;/a&gt;-drmFromHdfs`?&lt;/p&gt;</comment>
                            <comment id="14143515" author="githubbot" created="Mon, 22 Sep 2014 18:46:56 +0100"  >&lt;p&gt;Github user andrewpalumbo commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/53#issuecomment-56411169&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/53#issuecomment-56411169&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    yeah- sorry had your name spelled wrong....  thx&lt;/p&gt;</comment>
                            <comment id="14143557" author="githubbot" created="Mon, 22 Sep 2014 19:15:20 +0100"  >&lt;p&gt;Github user andrewpalumbo commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-56415515&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-56415515&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Ahh- great!:&lt;/p&gt;

&lt;p&gt;        mahout&amp;gt; drmTFIDF.checkpoint(CacheHint.NONE).keyClassTag &lt;br/&gt;
        res3: scala.reflect.ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt; = java.lang.String&lt;/p&gt;
</comment>
                            <comment id="14143923" author="githubbot" created="Mon, 22 Sep 2014 23:12:49 +0100"  >&lt;p&gt;Github user andrewpalumbo commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-56451708&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-56451708&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    The New Classtag verification is failing will have to come back to this.&lt;/p&gt;</comment>
                            <comment id="14143924" author="githubbot" created="Mon, 22 Sep 2014 23:12:49 +0100"  >&lt;p&gt;Github user andrewpalumbo closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14143926" author="githubbot" created="Mon, 22 Sep 2014 23:13:47 +0100"  >&lt;p&gt;Github user andrewpalumbo commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-56451829&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-56451829&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    oops.. somehow accidentally closed this.&lt;/p&gt;</comment>
                            <comment id="14143927" author="githubbot" created="Mon, 22 Sep 2014 23:13:49 +0100"  >&lt;p&gt;GitHub user andrewpalumbo reopened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1615&quot; title=&quot;SparkEngine drmFromHDFS returning the same Key for all Key,Vec Pairs for Text-Keyed SequenceFiles&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1615&quot;&gt;&lt;del&gt;MAHOUT-1615&lt;/del&gt;&lt;/a&gt;: drmFromHDFS returning the same Key for all Key,Vec Pairs for Text-Keyed SequenceFiles&lt;/p&gt;

&lt;p&gt;    SparkContext.sequenceFile(...) will yield the same key per partition for Text-Keyed Sequence files if the key a new copy of the key is not created when mapping to an RDD.  This patch checks for Text Keys and creates a copy of each Key if necessary.    &lt;/p&gt;

&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/andrewpalumbo/mahout&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/andrewpalumbo/mahout&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1615&quot; title=&quot;SparkEngine drmFromHDFS returning the same Key for all Key,Vec Pairs for Text-Keyed SequenceFiles&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1615&quot;&gt;&lt;del&gt;MAHOUT-1615&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #52&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 6adb01ee53ce591962b97a4ed474c111635f7c47&lt;br/&gt;
Author: Andrew Palumbo &amp;lt;ap.dev@outlook.com&amp;gt;&lt;br/&gt;
Date:   2014-09-14T20:50:50Z&lt;/p&gt;

&lt;p&gt;    Create copy of Key for Text Keys&lt;/p&gt;

&lt;p&gt;commit a81c34000e0152a7ad7349afdba8a9e854380653&lt;br/&gt;
Author: Andrew Palumbo &amp;lt;ap.dev@outlook.com&amp;gt;&lt;br/&gt;
Date:   2014-09-14T21:11:45Z&lt;/p&gt;

&lt;p&gt;    use y: Writable instead of y: VectorWritable&lt;/p&gt;

&lt;p&gt;commit 2d431a40bd7a62a117487451b3e255c2e56c7d1a&lt;br/&gt;
Author: Andrew Palumbo &amp;lt;ap.dev@outlook.com&amp;gt;&lt;br/&gt;
Date:   2014-09-14T22:55:34Z&lt;/p&gt;

&lt;p&gt;    scala not java&lt;/p&gt;

&lt;p&gt;commit 5541d500ef8cfb53c9e18da1c760ea8c39dd5409&lt;br/&gt;
Author: Andrew Palumbo &amp;lt;ap.dev@outlook.com&amp;gt;&lt;br/&gt;
Date:   2014-09-17T19:57:35Z&lt;/p&gt;

&lt;p&gt;    Read SequenceFile Header to get Key/Value Classes&lt;/p&gt;

&lt;p&gt;commit 79468e7a83e7af1f9ab10689b46a900bd463aa38&lt;br/&gt;
Author: Andrew Palumbo &amp;lt;ap.dev@outlook.com&amp;gt;&lt;br/&gt;
Date:   2014-09-17T19:59:34Z&lt;/p&gt;

&lt;p&gt;    Use our new method to get the ClassTag&lt;/p&gt;

&lt;p&gt;commit ffb40161173df145041cf60d9b2878af3cab911b&lt;br/&gt;
Author: Andrew Palumbo &amp;lt;ap.dev@outlook.com&amp;gt;&lt;br/&gt;
Date:   2014-09-18T23:07:33Z&lt;/p&gt;

&lt;p&gt;    Very Clunky use of getKeyClass. This commit solves inital problems.  Needs to be gutted.  Hadoop configuration needs to be set up correctly.  Spark I/O DFS tests fail because reader doesnt know weather to read locally or from HDFS&lt;/p&gt;

&lt;p&gt;commit 56b5db1b2ee5cdef3700fb9199c75637d5e3b570&lt;br/&gt;
Author: Andrew Palumbo &amp;lt;ap.dev@outlook.com&amp;gt;&lt;br/&gt;
Date:   2014-09-19T19:10:00Z&lt;/p&gt;

&lt;p&gt;    Use case matching and val2keyFunc to map rdd&lt;/p&gt;

&lt;p&gt;commit 0edeffe30c64e78297335f06516d8cb3ff6b36a3&lt;br/&gt;
Author: Andrew Palumbo &amp;lt;ap.dev@outlook.com&amp;gt;&lt;br/&gt;
Date:   2014-09-21T21:58:29Z&lt;/p&gt;

&lt;p&gt;    Cleanup/temporarily hardcode spark kryo buffer property&lt;/p&gt;

&lt;p&gt;commit 0d64b0be427998e74cc6d57bf653573b235e0a31&lt;br/&gt;
Author: Dmitriy Lyubimov &amp;lt;dlyubimov@apache.org&amp;gt;&lt;br/&gt;
Date:   2014-09-22T17:13:15Z&lt;/p&gt;

&lt;p&gt;    Adding key class tag extraction from checkpointed class;&lt;br/&gt;
    adding assert to `DRM DFS i/o (local)` test to fail if key class tag is incorrectly loaded.&lt;/p&gt;

&lt;p&gt;    temporary commended h20 module since it doesn&apos;t compile&lt;/p&gt;

&lt;p&gt;commit c63468e10b9ae7e7b0b50728b0d2b71883f894b8&lt;br/&gt;
Author: Andrew Palumbo &amp;lt;ap.dev@outlook.com&amp;gt;&lt;br/&gt;
Date:   2014-09-22T17:40:20Z&lt;/p&gt;

&lt;p&gt;    Merge branch &apos;&lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1615&quot; title=&quot;SparkEngine drmFromHDFS returning the same Key for all Key,Vec Pairs for Text-Keyed SequenceFiles&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1615&quot;&gt;&lt;del&gt;MAHOUT-1615&lt;/del&gt;&lt;/a&gt;-drmFromHdfs&apos; of &lt;a href=&quot;http://github.com/dlyubimov/mahout&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://github.com/dlyubimov/mahout&lt;/a&gt; into &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1615&quot; title=&quot;SparkEngine drmFromHDFS returning the same Key for all Key,Vec Pairs for Text-Keyed SequenceFiles&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1615&quot;&gt;&lt;del&gt;MAHOUT-1615&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;commit 36b40615619c12dcd7820e7628b3bbf86329bd76&lt;br/&gt;
Author: Andrew Palumbo &amp;lt;ap.dev@outlook.com&amp;gt;&lt;br/&gt;
Date:   2014-09-22T21:42:29Z&lt;/p&gt;

&lt;p&gt;    Read a sequence FILE not a directory- like the error says&lt;/p&gt;

&lt;p&gt;commit da014759404db1398f8e334e2801b5159e816d56&lt;br/&gt;
Author: Andrew Palumbo &amp;lt;ap.dev@outlook.com&amp;gt;&lt;br/&gt;
Date:   2014-09-22T21:50:46Z&lt;/p&gt;

&lt;p&gt;    cleanup&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="14145351" author="githubbot" created="Tue, 23 Sep 2014 21:10:25 +0100"  >&lt;p&gt;Github user andrewpalumbo commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-56581834&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-56581834&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Spark tests are all passing now&lt;/p&gt;</comment>
                            <comment id="14145898" author="githubbot" created="Wed, 24 Sep 2014 06:09:03 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/53#issuecomment-56625478&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/53#issuecomment-56625478&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;     it is incorporated into #52, closing.&lt;/p&gt;</comment>
                            <comment id="14145899" author="githubbot" created="Wed, 24 Sep 2014 06:09:03 +0100"  >&lt;p&gt;Github user dlyubimov closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/53&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/53&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14145904" author="githubbot" created="Wed, 24 Sep 2014 06:11:01 +0100"  >&lt;p&gt;Github user dlyubimov commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#discussion_r17954398&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#discussion_r17954398&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: pom.xml &amp;#8212;&lt;br/&gt;
    @@ -701,7 +701,7 @@&lt;br/&gt;
         &amp;lt;module&amp;gt;math-scala&amp;lt;/module&amp;gt;&lt;br/&gt;
         &amp;lt;module&amp;gt;spark&amp;lt;/module&amp;gt;&lt;br/&gt;
         &amp;lt;module&amp;gt;spark-shell&amp;lt;/module&amp;gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&amp;lt;module&amp;gt;h2o&amp;lt;/module&amp;gt;&lt;br/&gt;
    +    &amp;lt;!-- module&amp;gt;h2o&amp;lt;/module --&amp;gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    we want of course to fix it. unfortunately i am still unable to compile h20. I read somewhere that the error i am getting means there&apos;s a broken jar in classpath, but i am still getting this error even after i cleaned out my entire local maven repo.&lt;/p&gt;</comment>
                            <comment id="14145908" author="githubbot" created="Wed, 24 Sep 2014 06:13:08 +0100"  >&lt;p&gt;Github user dlyubimov commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#discussion_r17954424&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#discussion_r17954424&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: spark/src/main/scala/org/apache/mahout/sparkbindings/SparkEngine.scala &amp;#8212;&lt;br/&gt;
    @@ -127,33 +131,41 @@ object SparkEngine extends DistributedEngine {&lt;br/&gt;
        */&lt;br/&gt;
       def drmFromHDFS (path: String, parMin:Int = 0)(implicit sc: DistributedContext): CheckpointedDrm&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt; = {&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val rdd = sc.sequenceFile(path, classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Writable&amp;#93;&lt;/span&gt;, classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;VectorWritable&amp;#93;&lt;/span&gt;, minPartitions = parMin)&lt;/li&gt;
	&lt;li&gt;// Get rid of VectorWritable&lt;/li&gt;
	&lt;li&gt;.map(t =&amp;gt; (t._1, t._2.get()))&lt;br/&gt;
    +    // HDFS Paramaters&lt;br/&gt;
    +    val hConf= new Configuration()&lt;br/&gt;
    +    val hPath= new Path(path)&lt;br/&gt;
    +    val fs= FileSystem.get(hConf)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def getKeyClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;K: ClassTag, V&amp;#93;&lt;/span&gt;(rdd: RDD&lt;span class=&quot;error&quot;&gt;&amp;#91;(K, V)&amp;#93;&lt;/span&gt;) = implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;K&amp;#93;&lt;/span&gt;]&lt;br/&gt;
    +    /** Get the Key Class For the Sequence File */&lt;br/&gt;
    +    def getKeyClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;K:ClassTag&amp;#93;&lt;/span&gt; = ClassTag(new SequenceFile.Reader(fs, hPath, hConf).getKeyClass)&lt;br/&gt;
    +    /** Get the Value Class For the Sequence File */&lt;br/&gt;
    +//    def getValueClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;V:ClassTag&amp;#93;&lt;/span&gt; = ClassTag(new SequenceFile.Reader(fs, hPath, hConf).getValueClass)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// Spark should&apos;ve loaded the type info from the header, right?&lt;/li&gt;
	&lt;li&gt;val keyTag = getKeyClassTag(rdd)&lt;br/&gt;
    +    // Spark doesn&apos;t check the Sequence File Header so we have to.&lt;br/&gt;
    +    val keyTag = getKeyClassTag&lt;br/&gt;
    +//    val ct= ClassTag(keyTag.getClass)&lt;br/&gt;
    +&lt;br/&gt;
    +    // ClassTag to match on not lost by erasure&lt;br/&gt;
    +    val ct= ClassTag(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Writable&amp;#93;&lt;/span&gt;)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         val (key2valFunc, val2keyFunc, unwrappedKeyTag) = keyTag match {&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;case xx: ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;Writable&amp;#93;&lt;/span&gt; if (xx == implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;IntWritable&amp;#93;&lt;/span&gt;]) =&amp;gt; (&lt;br/&gt;
    +      case ct if (keyTag == implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;IntWritable&amp;#93;&lt;/span&gt;]) =&amp;gt; (&lt;br/&gt;
               (v: AnyRef) =&amp;gt; v.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;IntWritable&amp;#93;&lt;/span&gt;.get,&lt;/li&gt;
	&lt;li&gt;(x: Any) =&amp;gt; new IntWritable(x.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;),&lt;br/&gt;
    +          (x: Any) =&amp;gt; new Integer(x.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;IntWritable&amp;#93;&lt;/span&gt;.get),&lt;br/&gt;
               implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;])&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;case xx: ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;Writable&amp;#93;&lt;/span&gt; if (xx == implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;Text&amp;#93;&lt;/span&gt;]) =&amp;gt; (&lt;br/&gt;
    +      case ct if (keyTag == implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;Text&amp;#93;&lt;/span&gt;]) =&amp;gt; (&lt;br/&gt;
               (v: AnyRef) =&amp;gt; v.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Text&amp;#93;&lt;/span&gt;.toString,&lt;br/&gt;
               (x: Any) =&amp;gt; new Text(x.toString),&lt;br/&gt;
               implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;])&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;case xx: ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;Writable&amp;#93;&lt;/span&gt; if (xx == implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;LongWritable&amp;#93;&lt;/span&gt;]) =&amp;gt; (&lt;br/&gt;
    +      case ct if (keyTag == implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;LongWritable&amp;#93;&lt;/span&gt;]) =&amp;gt; (&lt;br/&gt;
               (v: AnyRef) =&amp;gt; v.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LongWritable&amp;#93;&lt;/span&gt;.get,&lt;/li&gt;
	&lt;li&gt;(x: Any) =&amp;gt; new LongWritable(x.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;),&lt;br/&gt;
    +          (x: Any) =&amp;gt; new LongWritable(x.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LongWritable&amp;#93;&lt;/span&gt;.get),
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    this can be removed entirely. use key2val instead.&lt;/p&gt;</comment>
                            <comment id="14146908" author="githubbot" created="Wed, 24 Sep 2014 22:23:12 +0100"  >&lt;p&gt;Github user andrewpalumbo commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#discussion_r18001536&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#discussion_r18001536&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: spark/src/main/scala/org/apache/mahout/sparkbindings/SparkEngine.scala &amp;#8212;&lt;br/&gt;
    @@ -127,33 +131,41 @@ object SparkEngine extends DistributedEngine {&lt;br/&gt;
        */&lt;br/&gt;
       def drmFromHDFS (path: String, parMin:Int = 0)(implicit sc: DistributedContext): CheckpointedDrm&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt; = {&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val rdd = sc.sequenceFile(path, classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Writable&amp;#93;&lt;/span&gt;, classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;VectorWritable&amp;#93;&lt;/span&gt;, minPartitions = parMin)&lt;/li&gt;
	&lt;li&gt;// Get rid of VectorWritable&lt;/li&gt;
	&lt;li&gt;.map(t =&amp;gt; (t._1, t._2.get()))&lt;br/&gt;
    +    // HDFS Paramaters&lt;br/&gt;
    +    val hConf= new Configuration()&lt;br/&gt;
    +    val hPath= new Path(path)&lt;br/&gt;
    +    val fs= FileSystem.get(hConf)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def getKeyClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;K: ClassTag, V&amp;#93;&lt;/span&gt;(rdd: RDD&lt;span class=&quot;error&quot;&gt;&amp;#91;(K, V)&amp;#93;&lt;/span&gt;) = implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;K&amp;#93;&lt;/span&gt;]&lt;br/&gt;
    +    /** Get the Key Class For the Sequence File */&lt;br/&gt;
    +    def getKeyClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;K:ClassTag&amp;#93;&lt;/span&gt; = ClassTag(new SequenceFile.Reader(fs, hPath, hConf).getKeyClass)&lt;br/&gt;
    +    /** Get the Value Class For the Sequence File */&lt;br/&gt;
    +//    def getValueClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;V:ClassTag&amp;#93;&lt;/span&gt; = ClassTag(new SequenceFile.Reader(fs, hPath, hConf).getValueClass)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// Spark should&apos;ve loaded the type info from the header, right?&lt;/li&gt;
	&lt;li&gt;val keyTag = getKeyClassTag(rdd)&lt;br/&gt;
    +    // Spark doesn&apos;t check the Sequence File Header so we have to.&lt;br/&gt;
    +    val keyTag = getKeyClassTag&lt;br/&gt;
    +//    val ct= ClassTag(keyTag.getClass)&lt;br/&gt;
    +&lt;br/&gt;
    +    // ClassTag to match on not lost by erasure&lt;br/&gt;
    +    val ct= ClassTag(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Writable&amp;#93;&lt;/span&gt;)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         val (key2valFunc, val2keyFunc, unwrappedKeyTag) = keyTag match {&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;case xx: ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;Writable&amp;#93;&lt;/span&gt; if (xx == implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;IntWritable&amp;#93;&lt;/span&gt;]) =&amp;gt; (&lt;br/&gt;
    +      case ct if (keyTag == implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;IntWritable&amp;#93;&lt;/span&gt;]) =&amp;gt; (&lt;br/&gt;
               (v: AnyRef) =&amp;gt; v.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;IntWritable&amp;#93;&lt;/span&gt;.get,&lt;/li&gt;
	&lt;li&gt;(x: Any) =&amp;gt; new IntWritable(x.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;),&lt;br/&gt;
    +          (x: Any) =&amp;gt; new Integer(x.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;IntWritable&amp;#93;&lt;/span&gt;.get),&lt;br/&gt;
               implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;])&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;case xx: ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;Writable&amp;#93;&lt;/span&gt; if (xx == implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;Text&amp;#93;&lt;/span&gt;]) =&amp;gt; (&lt;br/&gt;
    +      case ct if (keyTag == implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;Text&amp;#93;&lt;/span&gt;]) =&amp;gt; (&lt;br/&gt;
               (v: AnyRef) =&amp;gt; v.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Text&amp;#93;&lt;/span&gt;.toString,&lt;br/&gt;
               (x: Any) =&amp;gt; new Text(x.toString),&lt;br/&gt;
               implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;])&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;case xx: ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;Writable&amp;#93;&lt;/span&gt; if (xx == implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;LongWritable&amp;#93;&lt;/span&gt;]) =&amp;gt; (&lt;br/&gt;
    +      case ct if (keyTag == implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;LongWritable&amp;#93;&lt;/span&gt;]) =&amp;gt; (&lt;br/&gt;
               (v: AnyRef) =&amp;gt; v.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LongWritable&amp;#93;&lt;/span&gt;.get,&lt;/li&gt;
	&lt;li&gt;(x: Any) =&amp;gt; new LongWritable(x.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;),&lt;br/&gt;
    +          (x: Any) =&amp;gt; new LongWritable(x.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LongWritable&amp;#93;&lt;/span&gt;.get),
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    I&apos;m not sure here- we need to remove one of the functions them but I think we should be using val2key here not key2val correct?&lt;br/&gt;
    ```scala&lt;br/&gt;
         key2val(v: AnyRef) =&amp;gt; v.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;IntWritable&amp;#93;&lt;/span&gt;.get&lt;br/&gt;
         val2key(x: Any) =&amp;gt; new Integer(x.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;IntWritable&amp;#93;&lt;/span&gt;.get)&lt;br/&gt;
     ```&lt;br/&gt;
      so later when we map to the RDD:&lt;br/&gt;
    ```scala&lt;br/&gt;
    val drmRdd = rdd.map &lt;/p&gt;
{ t =&amp;gt; val2keyFunc(t._1) -&amp;gt; t._2.get()}
&lt;p&gt;    ``` &lt;br/&gt;
    they will be of form `&lt;span class=&quot;error&quot;&gt;&amp;#91;Integer&amp;#93;&lt;/span&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;Vector&amp;#93;&lt;/span&gt;`rather than `&lt;span class=&quot;error&quot;&gt;&amp;#91;IntWritable&amp;#93;&lt;/span&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;Vector&amp;#93;&lt;/span&gt;` &lt;/p&gt;
</comment>
                            <comment id="14146924" author="githubbot" created="Wed, 24 Sep 2014 22:35:33 +0100"  >&lt;p&gt;Github user andrewpalumbo commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#discussion_r18002168&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#discussion_r18002168&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: pom.xml &amp;#8212;&lt;br/&gt;
    @@ -701,7 +701,7 @@&lt;br/&gt;
         &amp;lt;module&amp;gt;math-scala&amp;lt;/module&amp;gt;&lt;br/&gt;
         &amp;lt;module&amp;gt;spark&amp;lt;/module&amp;gt;&lt;br/&gt;
         &amp;lt;module&amp;gt;spark-shell&amp;lt;/module&amp;gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&amp;lt;module&amp;gt;h2o&amp;lt;/module&amp;gt;&lt;br/&gt;
    +    &amp;lt;!-- module&amp;gt;h2o&amp;lt;/module --&amp;gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Yeah- i left it commented for now because there&apos;s some work to be done in h2o also.  Introducing the field to the `CheckpointedDrm` trait at math-scala required an implementation in h2o.  So after adding &lt;br/&gt;
    ```scala&lt;br/&gt;
      /** Explicit extraction of key class Tag   */&lt;br/&gt;
      def keyClassTag: ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;K&amp;#93;&lt;/span&gt; = implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;K&amp;#93;&lt;/span&gt;]&lt;br/&gt;
    ```&lt;br/&gt;
     to `CheckpointedDrmH20.scala`&lt;/p&gt;

&lt;p&gt;    The tests are failing.  Unfortunately I&apos;ve not had a lot of uninterupted time to work on this over the last week so I havent really looked at the h2o side yet- not sure yet but i thik we need to do some similar class matching in h2o.&lt;/p&gt;</comment>
                            <comment id="14147373" author="githubbot" created="Thu, 25 Sep 2014 06:06:55 +0100"  >&lt;p&gt;Github user dlyubimov commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#discussion_r18014902&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#discussion_r18014902&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: spark/src/main/scala/org/apache/mahout/sparkbindings/SparkEngine.scala &amp;#8212;&lt;br/&gt;
    @@ -127,33 +131,41 @@ object SparkEngine extends DistributedEngine {&lt;br/&gt;
        */&lt;br/&gt;
       def drmFromHDFS (path: String, parMin:Int = 0)(implicit sc: DistributedContext): CheckpointedDrm&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt; = {&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val rdd = sc.sequenceFile(path, classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Writable&amp;#93;&lt;/span&gt;, classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;VectorWritable&amp;#93;&lt;/span&gt;, minPartitions = parMin)&lt;/li&gt;
	&lt;li&gt;// Get rid of VectorWritable&lt;/li&gt;
	&lt;li&gt;.map(t =&amp;gt; (t._1, t._2.get()))&lt;br/&gt;
    +    // HDFS Paramaters&lt;br/&gt;
    +    val hConf= new Configuration()&lt;br/&gt;
    +    val hPath= new Path(path)&lt;br/&gt;
    +    val fs= FileSystem.get(hConf)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def getKeyClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;K: ClassTag, V&amp;#93;&lt;/span&gt;(rdd: RDD&lt;span class=&quot;error&quot;&gt;&amp;#91;(K, V)&amp;#93;&lt;/span&gt;) = implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;K&amp;#93;&lt;/span&gt;]&lt;br/&gt;
    +    /** Get the Key Class For the Sequence File */&lt;br/&gt;
    +    def getKeyClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;K:ClassTag&amp;#93;&lt;/span&gt; = ClassTag(new SequenceFile.Reader(fs, hPath, hConf).getKeyClass)&lt;br/&gt;
    +    /** Get the Value Class For the Sequence File */&lt;br/&gt;
    +//    def getValueClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;V:ClassTag&amp;#93;&lt;/span&gt; = ClassTag(new SequenceFile.Reader(fs, hPath, hConf).getValueClass)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// Spark should&apos;ve loaded the type info from the header, right?&lt;/li&gt;
	&lt;li&gt;val keyTag = getKeyClassTag(rdd)&lt;br/&gt;
    +    // Spark doesn&apos;t check the Sequence File Header so we have to.&lt;br/&gt;
    +    val keyTag = getKeyClassTag&lt;br/&gt;
    +//    val ct= ClassTag(keyTag.getClass)&lt;br/&gt;
    +&lt;br/&gt;
    +    // ClassTag to match on not lost by erasure&lt;br/&gt;
    +    val ct= ClassTag(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Writable&amp;#93;&lt;/span&gt;)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         val (key2valFunc, val2keyFunc, unwrappedKeyTag) = keyTag match {&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;case xx: ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;Writable&amp;#93;&lt;/span&gt; if (xx == implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;IntWritable&amp;#93;&lt;/span&gt;]) =&amp;gt; (&lt;br/&gt;
    +      case ct if (keyTag == implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;IntWritable&amp;#93;&lt;/span&gt;]) =&amp;gt; (&lt;br/&gt;
               (v: AnyRef) =&amp;gt; v.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;IntWritable&amp;#93;&lt;/span&gt;.get,&lt;/li&gt;
	&lt;li&gt;(x: Any) =&amp;gt; new IntWritable(x.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;),&lt;br/&gt;
    +          (x: Any) =&amp;gt; new Integer(x.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;IntWritable&amp;#93;&lt;/span&gt;.get),&lt;br/&gt;
               implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;])&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;case xx: ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;Writable&amp;#93;&lt;/span&gt; if (xx == implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;Text&amp;#93;&lt;/span&gt;]) =&amp;gt; (&lt;br/&gt;
    +      case ct if (keyTag == implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;Text&amp;#93;&lt;/span&gt;]) =&amp;gt; (&lt;br/&gt;
               (v: AnyRef) =&amp;gt; v.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Text&amp;#93;&lt;/span&gt;.toString,&lt;br/&gt;
               (x: Any) =&amp;gt; new Text(x.toString),&lt;br/&gt;
               implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;])&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;case xx: ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;Writable&amp;#93;&lt;/span&gt; if (xx == implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;LongWritable&amp;#93;&lt;/span&gt;]) =&amp;gt; (&lt;br/&gt;
    +      case ct if (keyTag == implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;LongWritable&amp;#93;&lt;/span&gt;]) =&amp;gt; (&lt;br/&gt;
               (v: AnyRef) =&amp;gt; v.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LongWritable&amp;#93;&lt;/span&gt;.get,&lt;/li&gt;
	&lt;li&gt;(x: Any) =&amp;gt; new LongWritable(x.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;),&lt;br/&gt;
    +          (x: Any) =&amp;gt; new LongWritable(x.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LongWritable&amp;#93;&lt;/span&gt;.get),
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    perhaps naming is to blame. key2val was meant to be the transformation from&lt;br/&gt;
    file key (i.e. writable) to actual non-reused type such as Int etc. val2key&lt;br/&gt;
    was meant to be inverse (and in the non-edited code it is), but it is not&lt;br/&gt;
    used in context of this method and therefore should be omited. i.e it&lt;br/&gt;
    should be simply&lt;/p&gt;

&lt;p&gt;           val (key2val, exactTag) = .... match ...&lt;/p&gt;




&lt;p&gt;    On Wed, Sep 24, 2014 at 2:23 PM, Andrew Palumbo &amp;lt;notifications@github.com&amp;gt;&lt;br/&gt;
    wrote:&lt;/p&gt;

&lt;p&gt;    &amp;gt; In spark/src/main/scala/org/apache/mahout/sparkbindings/SparkEngine.scala:&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt; &amp;gt;            (v: AnyRef) =&amp;gt; v.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LongWritable&amp;#93;&lt;/span&gt;.get,&lt;br/&gt;
    &amp;gt; &amp;gt; -          (x: Any) =&amp;gt; new LongWritable(x.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;),&lt;br/&gt;
    &amp;gt; &amp;gt; +          (x: Any) =&amp;gt; new LongWritable(x.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LongWritable&amp;#93;&lt;/span&gt;.get),&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt; I&apos;m not sure here- we need to remove one of the functions them but I think&lt;br/&gt;
    &amp;gt; we should be using val2key here not key2val correct?&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt;      key2val(v: AnyRef) =&amp;gt; v.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;IntWritable&amp;#93;&lt;/span&gt;.get&lt;br/&gt;
    &amp;gt;      val2key(x: Any) =&amp;gt; new Integer(x.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;IntWritable&amp;#93;&lt;/span&gt;.get)&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt; so later when we map to the RDD:&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt; val drmRdd = rdd.map &lt;/p&gt;
{ t =&amp;gt; val2keyFunc(t._1) -&amp;gt; t._2.get()}
&lt;p&gt;    &amp;gt;&lt;br/&gt;
    &amp;gt; they will be of form &lt;span class=&quot;error&quot;&gt;&amp;#91;Integer&amp;#93;&lt;/span&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;Vector&amp;#93;&lt;/span&gt;rather than &lt;span class=&quot;error&quot;&gt;&amp;#91;IntWritable&amp;#93;&lt;/span&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;Vector&amp;#93;&lt;/span&gt;&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt; &#8212;&lt;br/&gt;
    &amp;gt; Reply to this email directly or view it on GitHub&lt;br/&gt;
    &amp;gt; &amp;lt;&lt;a href=&quot;https://github.com/apache/mahout/pull/52/files#r18001536&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52/files#r18001536&lt;/a&gt;&amp;gt;.&lt;br/&gt;
    &amp;gt;&lt;/p&gt;</comment>
                            <comment id="14147376" author="githubbot" created="Thu, 25 Sep 2014 06:08:44 +0100"  >&lt;p&gt;Github user dlyubimov commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#discussion_r18014938&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#discussion_r18014938&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: spark/src/main/scala/org/apache/mahout/sparkbindings/SparkEngine.scala &amp;#8212;&lt;br/&gt;
    @@ -127,33 +131,41 @@ object SparkEngine extends DistributedEngine {&lt;br/&gt;
        */&lt;br/&gt;
       def drmFromHDFS (path: String, parMin:Int = 0)(implicit sc: DistributedContext): CheckpointedDrm&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt; = {&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val rdd = sc.sequenceFile(path, classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Writable&amp;#93;&lt;/span&gt;, classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;VectorWritable&amp;#93;&lt;/span&gt;, minPartitions = parMin)&lt;/li&gt;
	&lt;li&gt;// Get rid of VectorWritable&lt;/li&gt;
	&lt;li&gt;.map(t =&amp;gt; (t._1, t._2.get()))&lt;br/&gt;
    +    // HDFS Paramaters&lt;br/&gt;
    +    val hConf= new Configuration()&lt;br/&gt;
    +    val hPath= new Path(path)&lt;br/&gt;
    +    val fs= FileSystem.get(hConf)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def getKeyClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;K: ClassTag, V&amp;#93;&lt;/span&gt;(rdd: RDD&lt;span class=&quot;error&quot;&gt;&amp;#91;(K, V)&amp;#93;&lt;/span&gt;) = implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;K&amp;#93;&lt;/span&gt;]&lt;br/&gt;
    +    /** Get the Key Class For the Sequence File */&lt;br/&gt;
    +    def getKeyClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;K:ClassTag&amp;#93;&lt;/span&gt; = ClassTag(new SequenceFile.Reader(fs, hPath, hConf).getKeyClass)&lt;br/&gt;
    +    /** Get the Value Class For the Sequence File */&lt;br/&gt;
    +//    def getValueClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;V:ClassTag&amp;#93;&lt;/span&gt; = ClassTag(new SequenceFile.Reader(fs, hPath, hConf).getValueClass)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// Spark should&apos;ve loaded the type info from the header, right?&lt;/li&gt;
	&lt;li&gt;val keyTag = getKeyClassTag(rdd)&lt;br/&gt;
    +    // Spark doesn&apos;t check the Sequence File Header so we have to.&lt;br/&gt;
    +    val keyTag = getKeyClassTag&lt;br/&gt;
    +//    val ct= ClassTag(keyTag.getClass)&lt;br/&gt;
    +&lt;br/&gt;
    +    // ClassTag to match on not lost by erasure&lt;br/&gt;
    +    val ct= ClassTag(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Writable&amp;#93;&lt;/span&gt;)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         val (key2valFunc, val2keyFunc, unwrappedKeyTag) = keyTag match {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    i mean, this should just be &lt;/p&gt;

&lt;p&gt;         val (key2valFunc, unwrappedKeyTag) = keyTag match {....&lt;/p&gt;


&lt;p&gt;    and `key2valFunc` should eventually be used for transformation.&lt;/p&gt;
</comment>
                            <comment id="14147379" author="githubbot" created="Thu, 25 Sep 2014 06:09:36 +0100"  >&lt;p&gt;Github user dlyubimov commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#discussion_r18014952&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#discussion_r18014952&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: spark/src/main/scala/org/apache/mahout/sparkbindings/SparkEngine.scala &amp;#8212;&lt;br/&gt;
    @@ -162,9 +174,12 @@ object SparkEngine extends DistributedEngine {&lt;br/&gt;
         {&lt;br/&gt;
           implicit def getWritable(x: Any): Writable = val2keyFunc()&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val drmRdd = rdd.map 
{ t =&amp;gt; (key2valFunc(t._1), t._2)}
&lt;p&gt;    +      val rdd = sc.sequenceFile(path, classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Writable&amp;#93;&lt;/span&gt;, classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;VectorWritable&amp;#93;&lt;/span&gt;, minPartitions = parMin)&lt;br/&gt;
    +&lt;br/&gt;
    +      val drmRdd = rdd.map &lt;/p&gt;
{ t =&amp;gt; val2keyFunc(t._1) -&amp;gt; t._2.get()}&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;drmWrap(rdd = drmRdd, cacheHint = CacheHint.MEMORY_ONLY)(unwrappedKeyTag.asInstanceOf[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;Any&amp;#93;&lt;/span&gt;])&lt;br/&gt;
    +//      drmWrap(rdd = drmRdd, cacheHint = CacheHint.MEMORY_ONLY)(unwrappedKeyTag.asInstanceOf[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;Writable&amp;#93;&lt;/span&gt;])&lt;br/&gt;
    +      drmWrap(rdd = drmRdd, cacheHint = CacheHint.MEMORY_ONLY)(unwrappedKeyTag.asInstanceOf[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;Object&amp;#93;&lt;/span&gt;])&lt;br/&gt;
         }
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    While we at this, we probably should use cacheHint &apos;NONE&apos; here. Spark automatically disables HadoopRDD&apos;s caching anyway.&lt;/p&gt;</comment>
                            <comment id="14147387" author="githubbot" created="Thu, 25 Sep 2014 06:15:59 +0100"  >&lt;p&gt;Github user dlyubimov commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#discussion_r18015155&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#discussion_r18015155&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: spark/src/main/scala/org/apache/mahout/sparkbindings/SparkEngine.scala &amp;#8212;&lt;br/&gt;
    @@ -127,33 +131,41 @@ object SparkEngine extends DistributedEngine {&lt;br/&gt;
        */&lt;br/&gt;
       def drmFromHDFS (path: String, parMin:Int = 0)(implicit sc: DistributedContext): CheckpointedDrm&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt; = {&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val rdd = sc.sequenceFile(path, classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Writable&amp;#93;&lt;/span&gt;, classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;VectorWritable&amp;#93;&lt;/span&gt;, minPartitions = parMin)&lt;/li&gt;
	&lt;li&gt;// Get rid of VectorWritable&lt;/li&gt;
	&lt;li&gt;.map(t =&amp;gt; (t._1, t._2.get()))&lt;br/&gt;
    +    // HDFS Paramaters&lt;br/&gt;
    +    val hConf= new Configuration()&lt;br/&gt;
    +    val hPath= new Path(path)&lt;br/&gt;
    +    val fs= FileSystem.get(hConf)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def getKeyClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;K: ClassTag, V&amp;#93;&lt;/span&gt;(rdd: RDD&lt;span class=&quot;error&quot;&gt;&amp;#91;(K, V)&amp;#93;&lt;/span&gt;) = implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;K&amp;#93;&lt;/span&gt;]&lt;br/&gt;
    +    /** Get the Key Class For the Sequence File */&lt;br/&gt;
    +    def getKeyClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;K:ClassTag&amp;#93;&lt;/span&gt; = ClassTag(new SequenceFile.Reader(fs, hPath, hConf).getKeyClass)&lt;br/&gt;
    +    /** Get the Value Class For the Sequence File */&lt;br/&gt;
    +//    def getValueClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;V:ClassTag&amp;#93;&lt;/span&gt; = ClassTag(new SequenceFile.Reader(fs, hPath, hConf).getValueClass)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// Spark should&apos;ve loaded the type info from the header, right?&lt;/li&gt;
	&lt;li&gt;val keyTag = getKeyClassTag(rdd)&lt;br/&gt;
    +    // Spark doesn&apos;t check the Sequence File Header so we have to.&lt;br/&gt;
    +    val keyTag = getKeyClassTag&lt;br/&gt;
    +//    val ct= ClassTag(keyTag.getClass)&lt;br/&gt;
    +&lt;br/&gt;
    +    // ClassTag to match on not lost by erasure&lt;br/&gt;
    +    val ct= ClassTag(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Writable&amp;#93;&lt;/span&gt;)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         val (key2valFunc, val2keyFunc, unwrappedKeyTag) = keyTag match {&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;case xx: ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;Writable&amp;#93;&lt;/span&gt; if (xx == implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;IntWritable&amp;#93;&lt;/span&gt;]) =&amp;gt; (&lt;br/&gt;
    +      case ct if (keyTag == implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;IntWritable&amp;#93;&lt;/span&gt;]) =&amp;gt; (&lt;br/&gt;
               (v: AnyRef) =&amp;gt; v.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;IntWritable&amp;#93;&lt;/span&gt;.get,&lt;/li&gt;
	&lt;li&gt;(x: Any) =&amp;gt; new IntWritable(x.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;),&lt;br/&gt;
    +          (x: Any) =&amp;gt; new Integer(x.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;IntWritable&amp;#93;&lt;/span&gt;.get),&lt;br/&gt;
               implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;])&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;case xx: ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;Writable&amp;#93;&lt;/span&gt; if (xx == implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;Text&amp;#93;&lt;/span&gt;]) =&amp;gt; (&lt;br/&gt;
    +      case ct if (keyTag == implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;Text&amp;#93;&lt;/span&gt;]) =&amp;gt; (&lt;br/&gt;
               (v: AnyRef) =&amp;gt; v.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Text&amp;#93;&lt;/span&gt;.toString,&lt;br/&gt;
               (x: Any) =&amp;gt; new Text(x.toString),&lt;br/&gt;
               implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;])&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;case xx: ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;Writable&amp;#93;&lt;/span&gt; if (xx == implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;LongWritable&amp;#93;&lt;/span&gt;]) =&amp;gt; (&lt;br/&gt;
    +      case ct if (keyTag == implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;LongWritable&amp;#93;&lt;/span&gt;]) =&amp;gt; (&lt;br/&gt;
               (v: AnyRef) =&amp;gt; v.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LongWritable&amp;#93;&lt;/span&gt;.get,&lt;/li&gt;
	&lt;li&gt;(x: Any) =&amp;gt; new LongWritable(x.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;),&lt;br/&gt;
    +          (x: Any) =&amp;gt; new LongWritable(x.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LongWritable&amp;#93;&lt;/span&gt;.get),&lt;br/&gt;
               implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt;])&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;case xx: ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;Writable&amp;#93;&lt;/span&gt; =&amp;gt; (&lt;br/&gt;
    +      case ct =&amp;gt; (&lt;br/&gt;
               (v: AnyRef) =&amp;gt; v,
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    .. and since we know that Writables are not be useable since they are reused, we probably should block this case completely out with an error?&lt;/p&gt;

&lt;p&gt;    There&apos;s another piece of information to consider. Spark itself definex implicit conversions from some well-known Writables to their payload types. Perhaps we should support everything that&apos;s there; and maybe even figure a way to automatically apply everything that Spark exports, without even doing cases. I tried to figure how to do that (i remember that) but still haven&apos;t figured. it may not be possible; at least, i remember i haven&apos;t figured how that might be done. But we are at least 1.5 years past that moment, so perhaps we could revisist this from a fresh perspective. It would require eyeballing Spark&apos;s implicit Writable conversions again.&lt;/p&gt;
</comment>
                            <comment id="14148377" author="githubbot" created="Thu, 25 Sep 2014 23:00:25 +0100"  >&lt;p&gt;Github user andrewpalumbo commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-56890678&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-56890678&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;gt;There&apos;s another piece of information to consider. Spark itself definex implicit conversions from some well-known Writables to their payload types. Perhaps we should support everything that&apos;s there; and maybe even figure a way to automatically apply everything that Spark exports, without even doing cases.&lt;/p&gt;

&lt;p&gt;    Looking at SparkContext.scala there are implicit converters for converting to: ArrayWritable, BooleanWritable, BytesWritable, DoubleWritable, FloatWritable, IntWritable, LongWritable, NullWritable, Text, Writable&lt;/p&gt;

&lt;p&gt;    It should be  simple to do the conversion by matching since its all set up.  is there a reason that you wouldn&apos;t want to use `match`?  &lt;/p&gt;

</comment>
                            <comment id="14148797" author="githubbot" created="Fri, 26 Sep 2014 06:27:22 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-56920785&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-56920785&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I think the more standard types we support the better. Original Mahout DRM concept did not assume any sort of limitations on key type. It could be a tree, for all we know. The limitation stems from trying to build higher-level subroutines like drmFromHDFS for the algebraic optimizer, where user is not assumed to supply writeable to value converter on its own; at least, not by default. Maybe we just need an additional version of drmFromHDFS where such conversion function could be supplied explicitly by the user in order to handle compound annotation key types such as trees or  collections.&lt;/p&gt;

&lt;p&gt;    The reason why int, long and String were chosen as necessary minimum conversion set of types  is mostly because there are known algorithms that rely on these types. String is used in seq2sparse output, Int has a special meaning for transposition, and Long... Long happens from time to time in legacy Mahout code too.&lt;/p&gt;

&lt;p&gt;     The idea with implicit converters was to support anything that is already defined in implicit scope, without match. Think about it. Implicit stuff is just a special case of a scope. Like a package. as such, it might be possible to enumerate those conversions at runtime. Unfortunately I haven&apos;t found a scala construct that would allow to do that and apply found conversion elegantly. so this point is probably just a distraction. Let&apos;s forget about it and perhaps assume a match for a set of basic types, as defined in Spark. We should require hard stop if we don&apos;t know the converter (and user hasn&apos;t supplied one).&lt;/p&gt;

</comment>
                            <comment id="14149734" author="githubbot" created="Fri, 26 Sep 2014 19:04:31 +0100"  >&lt;p&gt;Github user pferrel commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-56998268&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-56998268&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    A slightly off topic question about naming; doesn&apos;t the drmFromHDFS actually read sequence files? &lt;/p&gt;

&lt;p&gt;    Might it better named drmFromSeqFile to distinguish from a text formatted DRM, also read from HDFS? The text readers and writers read from HDFS but expect text files with something like an &quot;extended-DRM&quot; that uses IDs for row and column. Currently maps them to CheckpointedDrm&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt; inside an IndexedDataset&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;    Maybe something like this...&lt;/p&gt;

&lt;p&gt;    Sequence file I/O&lt;br/&gt;
    drmFromHDFS becomes drmFromSeqFile&lt;br/&gt;
    drm.writeDrm becomes drm.writeSeqFile&lt;/p&gt;

&lt;p&gt;    I&apos;ll do these to fit into whatever naming we come with.&lt;/p&gt;

&lt;p&gt;    text I/O&lt;br/&gt;
    drmFromTextFile( src: String, schema: Schema = DefaultDrmTextSchema): IndexedDataset&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;&lt;br/&gt;
    drm.writeTextFile( destDir: String, schema: Schema = DefaultDrmTextSchema)&lt;/p&gt;
</comment>
                            <comment id="14149760" author="githubbot" created="Fri, 26 Sep 2014 19:22:06 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-57000893&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-57000893&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Let&apos;s not pile all things together. DRM is DRM and sequence file is sequence file (not DRM). &lt;/p&gt;

&lt;p&gt;    There is such a thing as DRM persistence. Since hadoop times and to date, such persistence on (H)DFS has been only defined via persistence file. So saving to hdfs can only mean one thing in order for data to stay DRM. &lt;/p&gt;

&lt;p&gt;    Corrollary to that are few things :(1) any sequence file is not DRM. Only a sequence file with o.a.m.VectorWritable as value is. (2) DRM data saved to anything but sequence file cannot be DRM.&lt;/p&gt;

&lt;p&gt;    That said, custom input/output adapters  are possible. But i am against making no distinction between text and sequence files, as one continues to be DRM while the other is just a bunch of comma separated numbers.&lt;/p&gt;</comment>
                            <comment id="14149771" author="githubbot" created="Fri, 26 Sep 2014 19:30:32 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-57002012&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-57002012&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    to be a bit more concrete, there&apos;s indeed slight discrepancy between write and read names, but semantically they are what they say they are, i.e. they are persisting drm to hdfs. &lt;/p&gt;

&lt;p&gt;    To be even more concrete, i am probably for simply package-level `drmDfsRead()` and method-level `dfsWrite()` names.&lt;/p&gt;

&lt;p&gt;    The convention here is that all drm-related package-level routines start with `drm` prefix so we don&apos;t easily mix these things with other things in global scope. &lt;/p&gt;

&lt;p&gt;    Now, everything else, including reading/writing CSV formats, is an &lt;em&gt;export&lt;/em&gt; operation (as opposed to persistence). Consequently, proper names are perhaps along the lines `drmImportCSV` and `exportCSV` respectively. Import and export emphasizes the fact that format is not native, loses a lot of coherency enforcement, and requires a lot of validation while parsing back. &lt;/p&gt;</comment>
                            <comment id="14149817" author="githubbot" created="Fri, 26 Sep 2014 20:07:33 +0100"  >&lt;p&gt;Github user andrewpalumbo commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-57006717&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-57006717&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    The last commit adds support for other Writable Key Types.  We now have support all Key Types that spark supports for saveAsSequenceFile with the Exception of NullWritable,  ArrayWritable and a generic Writable.  &lt;/p&gt;

&lt;p&gt;    also +1 to a good reader/writer naming convention &lt;/p&gt;</comment>
                            <comment id="14149849" author="githubbot" created="Fri, 26 Sep 2014 20:30:36 +0100"  >&lt;p&gt;Github user dlyubimov commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#discussion_r18108922&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#discussion_r18108922&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: spark/src/main/scala/org/apache/mahout/sparkbindings/SparkEngine.scala &amp;#8212;&lt;br/&gt;
    @@ -127,45 +133,64 @@ object SparkEngine extends DistributedEngine {&lt;br/&gt;
        */&lt;br/&gt;
       def drmFromHDFS (path: String, parMin:Int = 0)(implicit sc: DistributedContext): CheckpointedDrm&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt; = {&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val rdd = sc.sequenceFile(path, classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Writable&amp;#93;&lt;/span&gt;, classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;VectorWritable&amp;#93;&lt;/span&gt;, minPartitions = parMin)&lt;/li&gt;
	&lt;li&gt;// Get rid of VectorWritable&lt;/li&gt;
	&lt;li&gt;.map(t =&amp;gt; (t._1, t._2.get()))&lt;br/&gt;
    +    // HDFS Paramaters&lt;br/&gt;
    +    val hConf= new Configuration()&lt;br/&gt;
    +    val hPath= new Path(path)&lt;br/&gt;
    +    val fs= FileSystem.get(hConf)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def getKeyClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;K: ClassTag, V&amp;#93;&lt;/span&gt;(rdd: RDD&lt;span class=&quot;error&quot;&gt;&amp;#91;(K, V)&amp;#93;&lt;/span&gt;) = implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;K&amp;#93;&lt;/span&gt;]&lt;br/&gt;
    +    /** Get the Key Class For the Sequence File */&lt;br/&gt;
    +    def getKeyClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;K:ClassTag&amp;#93;&lt;/span&gt; = ClassTag(new SequenceFile.Reader(fs, hPath, hConf).getKeyClass)&lt;br/&gt;
    +   
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Can we please squirrel away lines 136-143 and all direct hadoop imports into a separate function, separate util helper object please? I am still very wary of direct hadoop dependencies and i predict this will not work on &lt;em&gt;all&lt;/em&gt; CDH /public hadoop releases.&lt;/p&gt;</comment>
                            <comment id="14149854" author="githubbot" created="Fri, 26 Sep 2014 20:33:37 +0100"  >&lt;p&gt;Github user dlyubimov commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#discussion_r18109074&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#discussion_r18109074&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: spark/src/main/scala/org/apache/mahout/sparkbindings/SparkEngine.scala &amp;#8212;&lt;br/&gt;
    @@ -127,45 +133,64 @@ object SparkEngine extends DistributedEngine {&lt;br/&gt;
        */&lt;br/&gt;
       def drmFromHDFS (path: String, parMin:Int = 0)(implicit sc: DistributedContext): CheckpointedDrm&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt; = {&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val rdd = sc.sequenceFile(path, classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Writable&amp;#93;&lt;/span&gt;, classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;VectorWritable&amp;#93;&lt;/span&gt;, minPartitions = parMin)&lt;/li&gt;
	&lt;li&gt;// Get rid of VectorWritable&lt;/li&gt;
	&lt;li&gt;.map(t =&amp;gt; (t._1, t._2.get()))&lt;br/&gt;
    +    // HDFS Paramaters&lt;br/&gt;
    +    val hConf= new Configuration()&lt;br/&gt;
    +    val hPath= new Path(path)&lt;br/&gt;
    +    val fs= FileSystem.get(hConf)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def getKeyClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;K: ClassTag, V&amp;#93;&lt;/span&gt;(rdd: RDD&lt;span class=&quot;error&quot;&gt;&amp;#91;(K, V)&amp;#93;&lt;/span&gt;) = implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;K&amp;#93;&lt;/span&gt;]&lt;br/&gt;
    +    /** Get the Key Class For the Sequence File */&lt;br/&gt;
    +    def getKeyClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;K:ClassTag&amp;#93;&lt;/span&gt; = ClassTag(new SequenceFile.Reader(fs, hPath, hConf).getKeyClass)
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    I think we don&apos;t need a key tag for writable. We only need key tag for value type (i.e. ClassTag.Int, etc.). This is not used anywhere. I suspect just matching on the class should be o.k.&lt;/p&gt;</comment>
                            <comment id="14149874" author="githubbot" created="Fri, 26 Sep 2014 20:48:21 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-57011309&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-57011309&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    ok @andrewpalumbo let me do a quick PR to your branch.&lt;/p&gt;</comment>
                            <comment id="14149877" author="githubbot" created="Fri, 26 Sep 2014 20:49:40 +0100"  >&lt;p&gt;Github user andrewpalumbo commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-57011460&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-57011460&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    ok- great! thx.. &lt;/p&gt;</comment>
                            <comment id="14150111" author="githubbot" created="Fri, 26 Sep 2014 23:30:54 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-57028930&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-57028930&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    ok so my commits are now here. Then you our regular merge-to-head commit routine applies. (don&apos;t forget to --squash the history please on the master! )&lt;/p&gt;</comment>
                            <comment id="14150119" author="githubbot" created="Fri, 26 Sep 2014 23:33:28 +0100"  >&lt;p&gt;Github user andrewpalumbo commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-57029159&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-57029159&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Great- thanks!  So I&apos;ll just disable the h2o Build for now by commenting it out in the pom.xml like it was before right?   &lt;/p&gt;</comment>
                            <comment id="14150148" author="githubbot" created="Fri, 26 Sep 2014 23:52:07 +0100"  >&lt;p&gt;Github user andrewpalumbo commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-57030591&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-57030591&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    just got an couple error building:&lt;/p&gt;

&lt;p&gt;        &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; /home/andy/sandbox/mahout/spark/src/main/scala:-1: info: compiling&lt;br/&gt;
        &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Compiling 36 source files to /home/andy/sandbox/mahout/spark/target/classes at 1411771604185&lt;br/&gt;
        &lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; /home/andy/sandbox/mahout/spark/src/main/scala/org/apache/mahout/common/DrmMetadata.scala:53: error: value copyBytes is not a member of org.apache.hadoop.io.BytesWritable&lt;br/&gt;
        &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt;   private&lt;span class=&quot;error&quot;&gt;&amp;#91;common&amp;#93;&lt;/span&gt; def w2bytes(w: Writable) = w.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;BytesWritable&amp;#93;&lt;/span&gt;.copyBytes()&lt;br/&gt;
        &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt;                                                                            ^&lt;br/&gt;
        &lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; /home/andy/sandbox/mahout/spark/src/main/scala/org/apache/mahout/common/Hadoop1HDFSUtil.scala:40: error: value isFile is not a member of org.apache.hadoop.fs.FileStatus&lt;br/&gt;
        &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt;         .filter &lt;/p&gt;
{ s =&amp;gt; !s.getPath.getName.startsWith(&quot;\\.&quot;) &amp;amp;&amp;amp; s.isFile}
&lt;p&gt;        &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; &lt;/p&gt;
</comment>
                            <comment id="14150160" author="githubbot" created="Sat, 27 Sep 2014 00:01:35 +0100"  >&lt;p&gt;Github user andrewpalumbo commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-57031300&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-57031300&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    changed to &lt;/p&gt;

&lt;p&gt;        private&lt;span class=&quot;error&quot;&gt;&amp;#91;common&amp;#93;&lt;/span&gt; def w2bytes(w: Writable) = w.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;BytesWritable&amp;#93;&lt;/span&gt;.getBytes&lt;br/&gt;
    and&lt;/p&gt;

&lt;p&gt;        .filter &lt;/p&gt;
{ s =&amp;gt; !s.getPath.getName.startsWith(&quot;\\.&quot;) &amp;amp;&amp;amp; !s.isDir}</comment>
                            <comment id="14150163" author="githubbot" created="Sat, 27 Sep 2014 00:05:03 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-57031556&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-57031556&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    hm i don&apos;t get that build error .&lt;/p&gt;

&lt;p&gt;    The error is clearly hadoop version thing. looks like my compiler figures later hadoop jar than yours. Spark module hadoop dependencies are coming from spark, not mr-legacy but it looks like you are picking the mr-legacy deps . &lt;/p&gt;</comment>
                            <comment id="14150164" author="githubbot" created="Sat, 27 Sep 2014 00:05:43 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-57031596&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-57031596&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    yeah i guess we can disable h20 module build for now&lt;/p&gt;</comment>
                            <comment id="14150165" author="githubbot" created="Sat, 27 Sep 2014 00:07:30 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-57031711&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-57031711&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    after clean &lt;/p&gt;

&lt;p&gt;    INFO] Reactor Summary:&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; &lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Mahout Build Tools ................................ SUCCESS &lt;span class=&quot;error&quot;&gt;&amp;#91;0.974s&amp;#93;&lt;/span&gt;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Apache Mahout ..................................... SUCCESS &lt;span class=&quot;error&quot;&gt;&amp;#91;0.432s&amp;#93;&lt;/span&gt;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Mahout Math ....................................... SUCCESS &lt;span class=&quot;error&quot;&gt;&amp;#91;8.988s&amp;#93;&lt;/span&gt;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Mahout MapReduce Legacy ........................... SUCCESS &lt;span class=&quot;error&quot;&gt;&amp;#91;7.276s&amp;#93;&lt;/span&gt;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Mahout Integration ................................ SUCCESS &lt;span class=&quot;error&quot;&gt;&amp;#91;1.184s&amp;#93;&lt;/span&gt;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Mahout Examples ................................... SUCCESS &lt;span class=&quot;error&quot;&gt;&amp;#91;6.126s&amp;#93;&lt;/span&gt;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Mahout Release Package ............................ SUCCESS &lt;span class=&quot;error&quot;&gt;&amp;#91;0.011s&amp;#93;&lt;/span&gt;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Mahout Math Scala bindings ........................ SUCCESS &lt;span class=&quot;error&quot;&gt;&amp;#91;24.802s&amp;#93;&lt;/span&gt;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Mahout Spark bindings ............................. SUCCESS &lt;span class=&quot;error&quot;&gt;&amp;#91;46.054s&amp;#93;&lt;/span&gt;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Mahout Spark bindings shell ....................... SUCCESS &lt;span class=&quot;error&quot;&gt;&amp;#91;5.288s&amp;#93;&lt;/span&gt;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Mahout H2O backend ................................ SUCCESS &lt;span class=&quot;error&quot;&gt;&amp;#91;18.002s&amp;#93;&lt;/span&gt;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; ------------------------------------------------------------------------&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; BUILD SUCCESS&lt;/p&gt;

</comment>
                            <comment id="14150166" author="githubbot" created="Sat, 27 Sep 2014 00:10:17 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-57031879&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-57031879&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    btw `getBytes`, iirc, is returning reusable buffer, so it needs to be copied with accounting for getLength() (actual value could be shorter than array returned). That&apos;s why copy method is necessary.&lt;/p&gt;</comment>
                            <comment id="14150167" author="githubbot" created="Sat, 27 Sep 2014 00:12:03 +0100"  >&lt;p&gt;Github user andrewpalumbo commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-57031986&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-57031986&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I&apos;ll clean and build again with yours.  They&apos;re both showing red in my IDEA.    &lt;/p&gt;</comment>
                            <comment id="14150170" author="githubbot" created="Sat, 27 Sep 2014 00:13:02 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-57032037&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-57032037&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    the only thing that matters is maven build. Idea may be wrong (do &quot;maven re-import&quot; to pick correct deps)&lt;/p&gt;
</comment>
                            <comment id="14150209" author="githubbot" created="Sat, 27 Sep 2014 00:36:40 +0100"  >&lt;p&gt;Github user andrewpalumbo commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-57033433&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-57033433&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Yeah everything always comes up red in idea.. usually still works...   Anyways Tried re-importing and then cleaned out my repo.  Its stil not building.  I think that its because our default Hadoop 1 artifact is 1.2.1 which doesnt seem to support copyBytes() &lt;/p&gt;</comment>
                            <comment id="14150282" author="githubbot" created="Sat, 27 Sep 2014 01:41:46 +0100"  >&lt;p&gt;Github user andrewpalumbo commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-57036558&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-57036558&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Doesn&apos;t look like my workarounds pass the tests:&lt;/p&gt;

&lt;p&gt;    ```scala&lt;br/&gt;
        private&lt;span class=&quot;error&quot;&gt;&amp;#91;common&amp;#93;&lt;/span&gt; def w2bytes(w: Writable) =&lt;br/&gt;
                            Arrays.copyOf(w.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;BytesWritable&amp;#93;&lt;/span&gt;.getBytes(),&lt;br/&gt;
                            w.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;BytesWritable&amp;#93;&lt;/span&gt;.getLength())&lt;br/&gt;
    ```&lt;br/&gt;
    should be ok but: &lt;/p&gt;

&lt;p&gt;    ```scala&lt;br/&gt;
        .filter &lt;/p&gt;
{ s =&amp;gt; !s.getPath.getName.startsWith(&quot;\\.&quot;) &amp;amp;&amp;amp; !s.isDir}
&lt;p&gt;    ```&lt;br/&gt;
    doesn&apos;t work in DRM DFS test: &lt;/p&gt;

&lt;p&gt;    ```&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;DRM DFS i/o (local) *** FAILED ***&lt;br/&gt;
      java.io.EOFException:&lt;br/&gt;
      at java.io.DataInputStream.readFully(DataInputStream.java:197)&lt;br/&gt;
      at java.io.DataInputStream.readFully(DataInputStream.java:169)&lt;br/&gt;
      at org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:1512)&lt;br/&gt;
      at org.apache.hadoop.io.SequenceFile$Reader.&amp;lt;init&amp;gt;(SequenceFile.java:1490)&lt;br/&gt;
      at org.apache.hadoop.io.SequenceFile$Reader.&amp;lt;init&amp;gt;(SequenceFile.java:1479)&lt;br/&gt;
      at org.apache.hadoop.io.SequenceFile$Reader.&amp;lt;init&amp;gt;(SequenceFile.java:1474)&lt;br/&gt;
      at org.apache.mahout.common.Hadoop1HDFSUtil$.readDrmHeader(Hadoop1HDFSUtil.scala:53)&lt;br/&gt;
    ```&lt;/li&gt;
&lt;/ul&gt;

</comment>
                            <comment id="14150401" author="githubbot" created="Sat, 27 Sep 2014 05:47:06 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-57042362&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-57042362&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    well, it compiles for me and doesn&apos;t compile for you. one way is to commit&lt;br/&gt;
    and see what jenkins says. Worst case, it is extremely easy to issue a&lt;br/&gt;
    revert the next day.&lt;/p&gt;

&lt;p&gt;    On Fri, Sep 26, 2014 at 5:41 PM, Andrew Palumbo &amp;lt;notifications@github.com&amp;gt;&lt;br/&gt;
    wrote:&lt;/p&gt;

&lt;p&gt;    &amp;gt; Doesn&apos;t look like my workarounds pass the tests:&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt;     private&lt;span class=&quot;error&quot;&gt;&amp;#91;common&amp;#93;&lt;/span&gt; def w2bytes(w: Writable) =&lt;br/&gt;
    &amp;gt;                         Arrays.copyOf(w.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;BytesWritable&amp;#93;&lt;/span&gt;.getBytes(),&lt;br/&gt;
    &amp;gt;                         w.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;BytesWritable&amp;#93;&lt;/span&gt;.getLength())&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt; should be ok but:&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt;     .filter &lt;/p&gt;
{ s =&amp;gt; !s.getPath.getName.startsWith(&quot;\\.&quot;) &amp;amp;&amp;amp; !s.isDir}
&lt;p&gt;    &amp;gt;&lt;br/&gt;
    &amp;gt; doesn&apos;t work in DRM DFS test:&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt; - DRM DFS i/o (local) *** FAILED ***&lt;br/&gt;
    &amp;gt;   java.io.EOFException:&lt;br/&gt;
    &amp;gt;   at java.io.DataInputStream.readFully(DataInputStream.java:197)&lt;br/&gt;
    &amp;gt;   at java.io.DataInputStream.readFully(DataInputStream.java:169)&lt;br/&gt;
    &amp;gt;   at org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:1512)&lt;br/&gt;
    &amp;gt;   at org.apache.hadoop.io.SequenceFile$Reader.&amp;lt;init&amp;gt;(SequenceFile.java:1490)&lt;br/&gt;
    &amp;gt;   at org.apache.hadoop.io.SequenceFile$Reader.&amp;lt;init&amp;gt;(SequenceFile.java:1479)&lt;br/&gt;
    &amp;gt;   at org.apache.hadoop.io.SequenceFile$Reader.&amp;lt;init&amp;gt;(SequenceFile.java:1474)&lt;br/&gt;
    &amp;gt;   at org.apache.mahout.common.Hadoop1HDFSUtil$.readDrmHeader(Hadoop1HDFSUtil.scala:53)&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt; &#8212;&lt;br/&gt;
    &amp;gt; Reply to this email directly or view it on GitHub&lt;br/&gt;
    &amp;gt; &amp;lt;&lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-57036558&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-57036558&lt;/a&gt;&amp;gt;.&lt;br/&gt;
    &amp;gt;&lt;/p&gt;</comment>
                            <comment id="14150583" author="githubbot" created="Sat, 27 Sep 2014 14:07:47 +0100"  >&lt;p&gt;Github user andrewpalumbo commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-57052150&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-57052150&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Ok thx- I&apos;ll push it shortly.  &lt;/p&gt;</comment>
                            <comment id="14150645" author="githubbot" created="Sat, 27 Sep 2014 16:49:35 +0100"  >&lt;p&gt;Github user andrewpalumbo commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-57056658&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-57056658&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    oops... wrote the commit message incorrectly with a space btwn apache/mahout and #52  so did not auto close.&lt;/p&gt;</comment>
                            <comment id="14150647" author="hudson" created="Sat, 27 Sep 2014 17:06:34 +0100"  >&lt;p&gt;FAILURE: Integrated in Mahout-Quality #2803 (See &lt;a href=&quot;https://builds.apache.org/job/Mahout-Quality/2803/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/Mahout-Quality/2803/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1615&quot; title=&quot;SparkEngine drmFromHDFS returning the same Key for all Key,Vec Pairs for Text-Keyed SequenceFiles&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1615&quot;&gt;&lt;del&gt;MAHOUT-1615&lt;/del&gt;&lt;/a&gt;: SparkEngine drmFromHDFS returning the same Key for all Key,Vec Pairs for Text-Keyed SequenceFiles. this closes apache/mahout #52 (ap.dev: rev 034afd61126660b6f44b232fdf3e1dd9d1a79708)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;spark/src/main/scala/org/apache/mahout/sparkbindings/drm/CheckpointedDrmSpark.scala&lt;/li&gt;
	&lt;li&gt;h2o/src/main/scala/org/apache/mahout/h2obindings/H2OEngine.scala&lt;/li&gt;
	&lt;li&gt;math-scala/src/test/scala/org/apache/mahout/math/drm/DrmLikeSuiteBase.scala&lt;/li&gt;
	&lt;li&gt;spark/src/main/scala/org/apache/mahout/common/DrmMetadata.scala&lt;/li&gt;
	&lt;li&gt;spark/src/main/scala/org/apache/mahout/common/HDFSUtil.scala&lt;/li&gt;
	&lt;li&gt;spark/src/main/scala/org/apache/mahout/common/Hadoop1HDFSUtil.scala&lt;/li&gt;
	&lt;li&gt;pom.xml&lt;/li&gt;
	&lt;li&gt;math-scala/src/main/scala/org/apache/mahout/math/drm/package.scala&lt;/li&gt;
	&lt;li&gt;spark-shell/src/main/scala/org/apache/mahout/sparkbindings/shell/MahoutSparkILoop.scala&lt;/li&gt;
	&lt;li&gt;math-scala/src/main/scala/org/apache/mahout/math/drm/DistributedEngine.scala&lt;/li&gt;
	&lt;li&gt;math-scala/src/main/scala/org/apache/mahout/math/drm/DrmLike.scala&lt;/li&gt;
	&lt;li&gt;math-scala/src/main/scala/org/apache/mahout/math/drm/CheckpointedDrm.scala&lt;/li&gt;
	&lt;li&gt;CHANGELOG&lt;/li&gt;
	&lt;li&gt;h2o/src/main/scala/org/apache/mahout/h2obindings/drm/CheckpointedDrmH2O.scala&lt;/li&gt;
	&lt;li&gt;spark/src/main/scala/org/apache/mahout/sparkbindings/SparkEngine.scala&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14150679" author="githubbot" created="Sat, 27 Sep 2014 18:56:05 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-57060526&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-57060526&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Bummer... doesn&apos;t build... needs revert... i&apos;ll issue the revert. &lt;/p&gt;

&lt;p&gt;    weird.&lt;/p&gt;</comment>
                            <comment id="14150683" author="githubbot" created="Sat, 27 Sep 2014 18:58:45 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-57060595&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-57060595&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    issued revert. we should continue on this PR. it&apos;s a good thing you did not close it.&lt;/p&gt;</comment>
                            <comment id="14150684" author="githubbot" created="Sat, 27 Sep 2014 19:00:27 +0100"  >&lt;p&gt;Github user andrewpalumbo commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-57060645&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-57060645&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    cool- thx.  Am begining to understand why you didn&apos;t want to go down the road of supporting multiple hadoop version for i/o..  &lt;/p&gt;</comment>
                            <comment id="14150686" author="githubbot" created="Sat, 27 Sep 2014 19:07:50 +0100"  >&lt;p&gt;Github user andrewpalumbo commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-57060848&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-57060848&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I will be able to come back to this tomorrow night or Monday afternoon.. I think some version the  workarounds i was trying last night should take care of the those 2 errors.&lt;/p&gt;</comment>
                            <comment id="14150688" author="githubbot" created="Sat, 27 Sep 2014 19:08:33 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-57060869&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-57060869&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    exactly &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;, or, rather, i didn&apos;t want to do anything with hadoop directly&lt;br/&gt;
    .. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &quot;let someone else do it&quot; &amp;#8211; and in our case it&apos;s Spark... bummer.&lt;/p&gt;


&lt;p&gt;    On Sat, Sep 27, 2014 at 11:00 AM, Andrew Palumbo &amp;lt;notifications@github.com&amp;gt;&lt;br/&gt;
    wrote:&lt;/p&gt;

&lt;p&gt;    &amp;gt; cool- thx. Am begining to understand why you didn&apos;t want to go down the&lt;br/&gt;
    &amp;gt; road of supporting multiple hadoop version for i/o..&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt; &#8212;&lt;br/&gt;
    &amp;gt; Reply to this email directly or view it on GitHub&lt;br/&gt;
    &amp;gt; &amp;lt;&lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-57060645&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-57060645&lt;/a&gt;&amp;gt;.&lt;br/&gt;
    &amp;gt;&lt;/p&gt;</comment>
                            <comment id="14150694" author="hudson" created="Sat, 27 Sep 2014 19:35:10 +0100"  >&lt;p&gt;SUCCESS: Integrated in Mahout-Quality #2805 (See &lt;a href=&quot;https://builds.apache.org/job/Mahout-Quality/2805/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/Mahout-Quality/2805/&lt;/a&gt;)&lt;br/&gt;
Revert &quot;&lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1615&quot; title=&quot;SparkEngine drmFromHDFS returning the same Key for all Key,Vec Pairs for Text-Keyed SequenceFiles&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1615&quot;&gt;&lt;del&gt;MAHOUT-1615&lt;/del&gt;&lt;/a&gt;: SparkEngine drmFromHDFS returning the same Key for all Key,Vec Pairs for Text-Keyed SequenceFiles. this closes apache/mahout #52&quot; (dlyubimov: rev 3c88f50432dc8dd70ab7eb03e1bbf419f3f7ab81)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;math-scala/src/main/scala/org/apache/mahout/math/drm/package.scala&lt;/li&gt;
	&lt;li&gt;spark-shell/src/main/scala/org/apache/mahout/sparkbindings/shell/MahoutSparkILoop.scala&lt;/li&gt;
	&lt;li&gt;math-scala/src/main/scala/org/apache/mahout/math/drm/CheckpointedDrm.scala&lt;/li&gt;
	&lt;li&gt;spark/src/main/scala/org/apache/mahout/sparkbindings/SparkEngine.scala&lt;/li&gt;
	&lt;li&gt;math-scala/src/main/scala/org/apache/mahout/math/drm/DistributedEngine.scala&lt;/li&gt;
	&lt;li&gt;math-scala/src/main/scala/org/apache/mahout/math/drm/DrmLike.scala&lt;/li&gt;
	&lt;li&gt;spark/src/main/scala/org/apache/mahout/common/Hadoop1HDFSUtil.scala&lt;/li&gt;
	&lt;li&gt;math-scala/src/test/scala/org/apache/mahout/math/drm/DrmLikeSuiteBase.scala&lt;/li&gt;
	&lt;li&gt;h2o/src/main/scala/org/apache/mahout/h2obindings/H2OEngine.scala&lt;/li&gt;
	&lt;li&gt;spark/src/main/scala/org/apache/mahout/sparkbindings/drm/CheckpointedDrmSpark.scala&lt;/li&gt;
	&lt;li&gt;CHANGELOG&lt;/li&gt;
	&lt;li&gt;pom.xml&lt;/li&gt;
	&lt;li&gt;spark/src/main/scala/org/apache/mahout/common/HDFSUtil.scala&lt;/li&gt;
	&lt;li&gt;h2o/src/main/scala/org/apache/mahout/h2obindings/drm/CheckpointedDrmH2O.scala&lt;/li&gt;
	&lt;li&gt;spark/src/main/scala/org/apache/mahout/common/DrmMetadata.scala&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14157573" author="githubbot" created="Fri, 3 Oct 2014 02:45:19 +0100"  >&lt;p&gt;Github user andrewpalumbo commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-57739029&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-57739029&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I&apos;ve been a bit bogged down here this and haven&apos;t had much of a chance to look at this but this last commit adds some minor edits for Hadoop 1.2.1 support.  It builds and passes tests on my machine.&lt;/p&gt;

&lt;p&gt;    I&apos;m not sure if we wanted to go this way, and I&apos;d hoped to put a quick branch up with option (1) from above- the simplest but the option which changes the method signature and allows for more user error, but havent had a chance and am going to be out of town early next week.  I can come back to this then and investigate something like:&lt;/p&gt;

&lt;p&gt;    ```scala&lt;br/&gt;
    def drmFromHDFS&lt;span class=&quot;error&quot;&gt;&amp;#91;K:ClassTag&amp;#93;&lt;/span&gt; (path: String, parMin:Int = 0)(implicit sc: DistributedContext): CheckpointedDrm&lt;span class=&quot;error&quot;&gt;&amp;#91;K&amp;#93;&lt;/span&gt; &lt;br/&gt;
    ```&lt;br/&gt;
    but thinking of it, it will still have a similar isue with:&lt;br/&gt;
    ```scala&lt;br/&gt;
    private&lt;span class=&quot;error&quot;&gt;&amp;#91;common&amp;#93;&lt;/span&gt; def w2bytes(w: Writable) = w.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;BytesWritable&amp;#93;&lt;/span&gt;.getBytes&lt;br/&gt;
    ```&lt;/p&gt;
</comment>
                            <comment id="14157574" author="githubbot" created="Fri, 3 Oct 2014 02:45:19 +0100"  >&lt;p&gt;Github user andrewpalumbo closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14157576" author="githubbot" created="Fri, 3 Oct 2014 02:46:45 +0100"  >&lt;p&gt;Github user andrewpalumbo commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-57739114&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-57739114&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    sorry about that- closed accidentaly agin&lt;/p&gt;</comment>
                            <comment id="14157577" author="githubbot" created="Fri, 3 Oct 2014 02:46:46 +0100"  >&lt;p&gt;GitHub user andrewpalumbo reopened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1615&quot; title=&quot;SparkEngine drmFromHDFS returning the same Key for all Key,Vec Pairs for Text-Keyed SequenceFiles&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1615&quot;&gt;&lt;del&gt;MAHOUT-1615&lt;/del&gt;&lt;/a&gt;: drmFromHDFS returning the same Key for all Key,Vec Pairs for Text-Keyed SequenceFiles&lt;/p&gt;

&lt;p&gt;    SparkContext.sequenceFile(...) will yield the same key per partition for Text-Keyed Sequence files if the key a new copy of the key is not created when mapping to an RDD.  This patch checks for Text Keys and creates a copy of each Key if necessary.    &lt;/p&gt;

&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/andrewpalumbo/mahout&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/andrewpalumbo/mahout&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1615&quot; title=&quot;SparkEngine drmFromHDFS returning the same Key for all Key,Vec Pairs for Text-Keyed SequenceFiles&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1615&quot;&gt;&lt;del&gt;MAHOUT-1615&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #52&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 6adb01ee53ce591962b97a4ed474c111635f7c47&lt;br/&gt;
Author: Andrew Palumbo &amp;lt;ap.dev@outlook.com&amp;gt;&lt;br/&gt;
Date:   2014-09-14T20:50:50Z&lt;/p&gt;

&lt;p&gt;    Create copy of Key for Text Keys&lt;/p&gt;

&lt;p&gt;commit a81c34000e0152a7ad7349afdba8a9e854380653&lt;br/&gt;
Author: Andrew Palumbo &amp;lt;ap.dev@outlook.com&amp;gt;&lt;br/&gt;
Date:   2014-09-14T21:11:45Z&lt;/p&gt;

&lt;p&gt;    use y: Writable instead of y: VectorWritable&lt;/p&gt;

&lt;p&gt;commit 2d431a40bd7a62a117487451b3e255c2e56c7d1a&lt;br/&gt;
Author: Andrew Palumbo &amp;lt;ap.dev@outlook.com&amp;gt;&lt;br/&gt;
Date:   2014-09-14T22:55:34Z&lt;/p&gt;

&lt;p&gt;    scala not java&lt;/p&gt;

&lt;p&gt;commit 5541d500ef8cfb53c9e18da1c760ea8c39dd5409&lt;br/&gt;
Author: Andrew Palumbo &amp;lt;ap.dev@outlook.com&amp;gt;&lt;br/&gt;
Date:   2014-09-17T19:57:35Z&lt;/p&gt;

&lt;p&gt;    Read SequenceFile Header to get Key/Value Classes&lt;/p&gt;

&lt;p&gt;commit 79468e7a83e7af1f9ab10689b46a900bd463aa38&lt;br/&gt;
Author: Andrew Palumbo &amp;lt;ap.dev@outlook.com&amp;gt;&lt;br/&gt;
Date:   2014-09-17T19:59:34Z&lt;/p&gt;

&lt;p&gt;    Use our new method to get the ClassTag&lt;/p&gt;

&lt;p&gt;commit ffb40161173df145041cf60d9b2878af3cab911b&lt;br/&gt;
Author: Andrew Palumbo &amp;lt;ap.dev@outlook.com&amp;gt;&lt;br/&gt;
Date:   2014-09-18T23:07:33Z&lt;/p&gt;

&lt;p&gt;    Very Clunky use of getKeyClass. This commit solves inital problems.  Needs to be gutted.  Hadoop configuration needs to be set up correctly.  Spark I/O DFS tests fail because reader doesnt know weather to read locally or from HDFS&lt;/p&gt;

&lt;p&gt;commit 56b5db1b2ee5cdef3700fb9199c75637d5e3b570&lt;br/&gt;
Author: Andrew Palumbo &amp;lt;ap.dev@outlook.com&amp;gt;&lt;br/&gt;
Date:   2014-09-19T19:10:00Z&lt;/p&gt;

&lt;p&gt;    Use case matching and val2keyFunc to map rdd&lt;/p&gt;

&lt;p&gt;commit 0edeffe30c64e78297335f06516d8cb3ff6b36a3&lt;br/&gt;
Author: Andrew Palumbo &amp;lt;ap.dev@outlook.com&amp;gt;&lt;br/&gt;
Date:   2014-09-21T21:58:29Z&lt;/p&gt;

&lt;p&gt;    Cleanup/temporarily hardcode spark kryo buffer property&lt;/p&gt;

&lt;p&gt;commit 0d64b0be427998e74cc6d57bf653573b235e0a31&lt;br/&gt;
Author: Dmitriy Lyubimov &amp;lt;dlyubimov@apache.org&amp;gt;&lt;br/&gt;
Date:   2014-09-22T17:13:15Z&lt;/p&gt;

&lt;p&gt;    Adding key class tag extraction from checkpointed class;&lt;br/&gt;
    adding assert to `DRM DFS i/o (local)` test to fail if key class tag is incorrectly loaded.&lt;/p&gt;

&lt;p&gt;    temporary commended h20 module since it doesn&apos;t compile&lt;/p&gt;

&lt;p&gt;commit c63468e10b9ae7e7b0b50728b0d2b71883f894b8&lt;br/&gt;
Author: Andrew Palumbo &amp;lt;ap.dev@outlook.com&amp;gt;&lt;br/&gt;
Date:   2014-09-22T17:40:20Z&lt;/p&gt;

&lt;p&gt;    Merge branch &apos;&lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1615&quot; title=&quot;SparkEngine drmFromHDFS returning the same Key for all Key,Vec Pairs for Text-Keyed SequenceFiles&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1615&quot;&gt;&lt;del&gt;MAHOUT-1615&lt;/del&gt;&lt;/a&gt;-drmFromHdfs&apos; of &lt;a href=&quot;http://github.com/dlyubimov/mahout&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://github.com/dlyubimov/mahout&lt;/a&gt; into &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1615&quot; title=&quot;SparkEngine drmFromHDFS returning the same Key for all Key,Vec Pairs for Text-Keyed SequenceFiles&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1615&quot;&gt;&lt;del&gt;MAHOUT-1615&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;commit 36b40615619c12dcd7820e7628b3bbf86329bd76&lt;br/&gt;
Author: Andrew Palumbo &amp;lt;ap.dev@outlook.com&amp;gt;&lt;br/&gt;
Date:   2014-09-22T21:42:29Z&lt;/p&gt;

&lt;p&gt;    Read a sequence FILE not a directory- like the error says&lt;/p&gt;

&lt;p&gt;commit da014759404db1398f8e334e2801b5159e816d56&lt;br/&gt;
Author: Andrew Palumbo &amp;lt;ap.dev@outlook.com&amp;gt;&lt;br/&gt;
Date:   2014-09-22T21:50:46Z&lt;/p&gt;

&lt;p&gt;    cleanup&lt;/p&gt;

&lt;p&gt;commit b5c3575c7575bc6b1b7b22881895f1a6a21c74ed&lt;br/&gt;
Author: Andrew Palumbo &amp;lt;ap.dev@outlook.com&amp;gt;&lt;br/&gt;
Date:   2014-09-23T15:37:54Z&lt;/p&gt;

&lt;p&gt;    Change DFS I/O test to read a specified part-00000 file&lt;/p&gt;

&lt;p&gt;commit 4fff4eea3c257ae03f1061eae0cde72013b0aff5&lt;br/&gt;
Author: Andrew Palumbo &amp;lt;ap.dev@outlook.com&amp;gt;&lt;br/&gt;
Date:   2014-09-23T17:26:02Z&lt;/p&gt;

&lt;p&gt;    Fix Int and Long key casting test failure&lt;/p&gt;

&lt;p&gt;commit 57c0964d0852409e11e6c6ae376ff261db19d3d6&lt;br/&gt;
Author: Andrew Palumbo &amp;lt;ap.dev@outlook.com&amp;gt;&lt;br/&gt;
Date:   2014-09-24T21:40:02Z&lt;/p&gt;

&lt;p&gt;    define keyClassTag in h2o&lt;/p&gt;

&lt;p&gt;commit 8152bdba666b690249f4cfada7bddf9089fedd11&lt;br/&gt;
Author: Andrew Palumbo &amp;lt;ap.dev@outlook.com&amp;gt;&lt;br/&gt;
Date:   2014-09-25T15:33:47Z&lt;/p&gt;

&lt;p&gt;    Switch val2key and key2val, remove val2key&lt;/p&gt;

&lt;p&gt;commit a9da184eaa0dc768abf397e97d540d92eaf6fb5b&lt;br/&gt;
Author: Andrew Palumbo &amp;lt;ap.dev@outlook.com&amp;gt;&lt;br/&gt;
Date:   2014-09-25T20:43:47Z&lt;/p&gt;

&lt;p&gt;    Throw Exception if not Int, Text or Long Keyed&lt;/p&gt;

&lt;p&gt;commit 2a3aaad5ee4319a9a7c5a0d2ee408f704aa603af&lt;br/&gt;
Author: Andrew Palumbo &amp;lt;ap.dev@outlook.com&amp;gt;&lt;br/&gt;
Date:   2014-09-25T21:19:53Z&lt;/p&gt;

&lt;p&gt;    Cleanup&lt;/p&gt;

&lt;p&gt;commit 8dfa83e0dd72c4dfb86121caf5d6ff0725bc23ee&lt;br/&gt;
Author: Andrew Palumbo &amp;lt;ap.dev@outlook.com&amp;gt;&lt;br/&gt;
Date:   2014-09-26T18:49:35Z&lt;/p&gt;

&lt;p&gt;    Add support for Keys of other Writable Types supported By Spark&lt;/p&gt;

&lt;p&gt;commit ffc4469c6282b2a303a49032b4b51e4da1c86584&lt;br/&gt;
Author: Dmitriy Lyubimov &amp;lt;dlyubimov@apache.org&amp;gt;&lt;br/&gt;
Date:   2014-09-26T21:58:08Z&lt;/p&gt;

&lt;p&gt;    Refactoring hdfs ops&lt;/p&gt;

&lt;p&gt;commit 91dc481b45efafdf0c12fdfabcd21be085e6a17e&lt;br/&gt;
Author: Dmitriy Lyubimov &amp;lt;dlyubimov@apache.org&amp;gt;&lt;br/&gt;
Date:   2014-09-26T22:09:00Z&lt;/p&gt;

&lt;p&gt;    Fixing h20 build&lt;/p&gt;

&lt;p&gt;commit c8d9a93f556d34149598b8b56203110506064f5f&lt;br/&gt;
Author: Dmitriy Lyubimov &amp;lt;dlyubimov@apache.org&amp;gt;&lt;br/&gt;
Date:   2014-09-26T22:17:17Z&lt;/p&gt;

&lt;p&gt;    Actually load and write paths should be identical (parent dfs folder, not a particular partition file)&lt;/p&gt;

&lt;p&gt;commit 42296b4bfb327ebd30152299d5ae264f2cd4279b&lt;br/&gt;
Author: Dmitriy Lyubimov &amp;lt;dlyubimov@apache.org&amp;gt;&lt;br/&gt;
Date:   2014-09-26T22:19:21Z&lt;/p&gt;

&lt;p&gt;    cosmetic&lt;/p&gt;

&lt;p&gt;commit ed46efc36027c2340b2749257266741c26871cf8&lt;br/&gt;
Author: Andrew Palumbo &amp;lt;ap.dev@outlook.com&amp;gt;&lt;br/&gt;
Date:   2014-09-26T22:28:49Z&lt;/p&gt;

&lt;p&gt;    Merge pull request #1 from dlyubimov/&lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1615&quot; title=&quot;SparkEngine drmFromHDFS returning the same Key for all Key,Vec Pairs for Text-Keyed SequenceFiles&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1615&quot;&gt;&lt;del&gt;MAHOUT-1615&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Refactoring hdfs ops&lt;/p&gt;

&lt;p&gt;commit 7ab50293a409d30313093d070f538d39a0ffa919&lt;br/&gt;
Author: Andrew Palumbo &amp;lt;ap.dev@outlook.com&amp;gt;&lt;br/&gt;
Date:   2014-09-26T22:37:29Z&lt;/p&gt;

&lt;p&gt;    Temporarily disable h2o build&lt;/p&gt;

&lt;p&gt;commit 9a4d7d2077a5c78b13d06e9bbffce3a78a3ea829&lt;br/&gt;
Author: Andrew Palumbo &amp;lt;ap.dev@outlook.com&amp;gt;&lt;br/&gt;
Date:   2014-10-02T23:33:30Z&lt;/p&gt;

&lt;p&gt;    Minor edits for Hadoop 1.2.1 support&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="14157589" author="githubbot" created="Fri, 3 Oct 2014 03:06:38 +0100"  >&lt;p&gt;Github user andrewpalumbo commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-57740168&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-57740168&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    To clarify my above comment (sorry- completely burned out)  I still think option (2) is the way to go.. and was looking a little more at option (1) and see that we will still run into hadoop versioning issues there.  (one less in this case- though it is a big one- reading the file header).   I was just talking about putting up an other branch for comparison.  &lt;/p&gt;</comment>
                            <comment id="14157603" author="githubbot" created="Fri, 3 Oct 2014 03:29:06 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-57741359&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-57741359&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I agree with your fixes except for getBytes() since it will return longer values. If copyBytes is not supported in previous versions, you&apos;d have to allocate new buffer and copy it yourself, via something like &lt;/p&gt;


&lt;p&gt;        java.util.Arrays.copyOf(w.getBytes, w.getLength)&lt;/p&gt;


&lt;p&gt;    .&lt;/p&gt;

</comment>
                            <comment id="14159319" author="githubbot" created="Sat, 4 Oct 2014 23:41:21 +0100"  >&lt;p&gt;Github user dlyubimov commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#discussion_r18432276&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#discussion_r18432276&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: spark/src/main/scala/org/apache/mahout/common/DrmMetadata.scala &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,56 @@&lt;br/&gt;
    +package org.apache.mahout.common&lt;br/&gt;
    +&lt;br/&gt;
    +import scala.reflect.ClassTag&lt;br/&gt;
    +import org.apache.hadoop.io._&lt;br/&gt;
    +import java.util.Arrays&lt;br/&gt;
    +&lt;br/&gt;
    +class DrmMetadata(&lt;br/&gt;
    +&lt;br/&gt;
    +    /** Writable  key type as a sub-type of Writable */&lt;br/&gt;
    +    val keyTypeWritable: Class&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt;,&lt;br/&gt;
    +&lt;br/&gt;
    +    /** Value writable type, as a sub-type of Writable */&lt;br/&gt;
    +    val valueTypeWritable: Class&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt;&lt;br/&gt;
    +&lt;br/&gt;
    +    ) {&lt;br/&gt;
    +&lt;br/&gt;
    +  import DrmMetadata._&lt;br/&gt;
    +&lt;br/&gt;
    +  val (&lt;br/&gt;
    +&lt;br/&gt;
    +      /** Actual drm key class tag once converted out of writable */&lt;br/&gt;
    +      keyClassTag: ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt;,&lt;br/&gt;
    +&lt;br/&gt;
    +      /** Conversion from Writable to value type of the DRM key */&lt;br/&gt;
    +      keyW2ValFunc: ((Writable) =&amp;gt; Any)&lt;br/&gt;
    +&lt;br/&gt;
    +      ) = keyTypeWritable match {&lt;br/&gt;
    +    case cz if (cz == classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;IntWritable&amp;#93;&lt;/span&gt;) =&amp;gt; ClassTag.Int -&amp;gt; w2int _&lt;br/&gt;
    +    case cz if (cz == classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LongWritable&amp;#93;&lt;/span&gt;) =&amp;gt; ClassTag.Long -&amp;gt; w2long _&lt;br/&gt;
    +    case cz if (cz == classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;DoubleWritable&amp;#93;&lt;/span&gt;) =&amp;gt; ClassTag.Double -&amp;gt; w2double _&lt;br/&gt;
    +    case cz if (cz == classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;FloatWritable&amp;#93;&lt;/span&gt;) =&amp;gt; ClassTag.Float -&amp;gt; w2float _&lt;br/&gt;
    +    case cz if (cz == classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Text&amp;#93;&lt;/span&gt;) =&amp;gt; ClassTag(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;) -&amp;gt; w2string _&lt;br/&gt;
    +    case cz if (cz == classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;BooleanWritable&amp;#93;&lt;/span&gt;) =&amp;gt; ClassTag(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Boolean&amp;#93;&lt;/span&gt;) -&amp;gt; w2bool _&lt;br/&gt;
    +    case cz if (cz == classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;BytesWritable&amp;#93;&lt;/span&gt;) =&amp;gt; ClassTag(classOf[Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;]) -&amp;gt; w2bytes _&lt;br/&gt;
    +    case _ =&amp;gt; throw new IllegalArgumentException(s&quot;Unsupported DRM key type:$&lt;/p&gt;
{keyTypeWritable.getName}
&lt;p&gt;&quot;)&lt;br/&gt;
    +  }&lt;br/&gt;
    +&lt;br/&gt;
    +}&lt;br/&gt;
    +&lt;br/&gt;
    +object DrmMetadata &lt;/p&gt;
{
    +
    +  private[common] def w2int(w: Writable) = w.asInstanceOf[IntWritable].get()
    +
    +  private[common] def w2long(w: Writable) = w.asInstanceOf[LongWritable].get()
    +
    +  private[common] def w2double(w: Writable) = w.asInstanceOf[DoubleWritable].get()
    +
    +  private[common] def w2float(w: Writable) = w.asInstanceOf[FloatWritable].get()
    +
    +  private[common] def w2string(w: Writable) = w.asInstanceOf[Text].toString()
    +
    +  private[common] def w2bool(w: Writable) = w.asInstanceOf[BooleanWritable].get()
    +
    +  private[common] def w2bytes(w: Writable) = Arrays.copyOf(w.asInstanceOf[BytesWritable].getBytes(),
    +                                                           w.asInstanceOf[BytesWritable].getLength())
    +}
&lt;p&gt;    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    yes this looks right now&lt;/p&gt;</comment>
                            <comment id="14165661" author="githubbot" created="Thu, 9 Oct 2014 21:11:18 +0100"  >&lt;p&gt;Github user andrewpalumbo commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52#issuecomment-58569759&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52#issuecomment-58569759&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    ok, great- I&apos;ll push this now then.&lt;/p&gt;</comment>
                            <comment id="14165672" author="githubbot" created="Thu, 9 Oct 2014 21:19:25 +0100"  >&lt;p&gt;Github user asfgit closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/52&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/52&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14165743" author="hudson" created="Thu, 9 Oct 2014 22:06:25 +0100"  >&lt;p&gt;SUCCESS: Integrated in Mahout-Quality #2818 (See &lt;a href=&quot;https://builds.apache.org/job/Mahout-Quality/2818/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/Mahout-Quality/2818/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1615&quot; title=&quot;SparkEngine drmFromHDFS returning the same Key for all Key,Vec Pairs for Text-Keyed SequenceFiles&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1615&quot;&gt;&lt;del&gt;MAHOUT-1615&lt;/del&gt;&lt;/a&gt;: SparkEngine drmFromHDFS returning the same Key for all Key,Vec Pairs for Text-Keyed SequenceFiles.  This closes apache/mahout#52 (ap.dev: rev c34e8a84c39b7bd2175cdbcb8583d487548a34b4)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;spark/src/main/scala/org/apache/mahout/common/Hadoop1HDFSUtil.scala&lt;/li&gt;
	&lt;li&gt;h2o/src/main/scala/org/apache/mahout/h2obindings/drm/CheckpointedDrmH2O.scala&lt;/li&gt;
	&lt;li&gt;math-scala/src/main/scala/org/apache/mahout/math/drm/DistributedEngine.scala&lt;/li&gt;
	&lt;li&gt;spark/src/main/scala/org/apache/mahout/common/HDFSUtil.scala&lt;/li&gt;
	&lt;li&gt;spark/src/main/scala/org/apache/mahout/sparkbindings/SparkEngine.scala&lt;/li&gt;
	&lt;li&gt;math-scala/src/main/scala/org/apache/mahout/math/drm/package.scala&lt;/li&gt;
	&lt;li&gt;math-scala/src/test/scala/org/apache/mahout/math/drm/DrmLikeSuiteBase.scala&lt;/li&gt;
	&lt;li&gt;spark/src/main/scala/org/apache/mahout/sparkbindings/drm/CheckpointedDrmSpark.scala&lt;/li&gt;
	&lt;li&gt;math-scala/src/main/scala/org/apache/mahout/math/drm/DrmLike.scala&lt;/li&gt;
	&lt;li&gt;pom.xml&lt;/li&gt;
	&lt;li&gt;spark-shell/src/main/scala/org/apache/mahout/sparkbindings/shell/MahoutSparkILoop.scala&lt;/li&gt;
	&lt;li&gt;CHANGELOG&lt;/li&gt;
	&lt;li&gt;math-scala/src/main/scala/org/apache/mahout/math/drm/CheckpointedDrm.scala&lt;/li&gt;
	&lt;li&gt;spark/src/main/scala/org/apache/mahout/common/DrmMetadata.scala&lt;/li&gt;
	&lt;li&gt;h2o/src/main/scala/org/apache/mahout/h2obindings/H2OEngine.scala&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14165956" author="githubbot" created="Fri, 10 Oct 2014 00:10:33 +0100"  >&lt;p&gt;GitHub user avati opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/58&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/58&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1615&quot; title=&quot;SparkEngine drmFromHDFS returning the same Key for all Key,Vec Pairs for Text-Keyed SequenceFiles&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1615&quot;&gt;&lt;del&gt;MAHOUT-1615&lt;/del&gt;&lt;/a&gt;: fix up HDFS ClassTag reading from H2O bindings&lt;/p&gt;

&lt;p&gt;    Use the same approach taken by spark.&lt;/p&gt;

&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/avati/mahout&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/avati/mahout&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1615&quot; title=&quot;SparkEngine drmFromHDFS returning the same Key for all Key,Vec Pairs for Text-Keyed SequenceFiles&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1615&quot;&gt;&lt;del&gt;MAHOUT-1615&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/58.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/58.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #58&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit ab3bd213fd3e325ec9163b1630fdda6265763be9&lt;br/&gt;
Author: Anand Avati &amp;lt;avati@redhat.com&amp;gt;&lt;br/&gt;
Date:   2014-10-09T22:58:13Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1615&quot; title=&quot;SparkEngine drmFromHDFS returning the same Key for all Key,Vec Pairs for Text-Keyed SequenceFiles&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1615&quot;&gt;&lt;del&gt;MAHOUT-1615&lt;/del&gt;&lt;/a&gt;: Fix H2O bindings to read proper ClassTag from HDFS&lt;/p&gt;

&lt;p&gt;    Signed-off-by: Anand Avati &amp;lt;avati@redhat.com&amp;gt;&lt;/p&gt;

&lt;p&gt;commit 7d9a3d908598e7ebfc9280171107031ceb91d04c&lt;br/&gt;
Author: Anand Avati &amp;lt;avati@redhat.com&amp;gt;&lt;br/&gt;
Date:   2014-10-09T23:01:14Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1615&quot; title=&quot;SparkEngine drmFromHDFS returning the same Key for all Key,Vec Pairs for Text-Keyed SequenceFiles&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1615&quot;&gt;&lt;del&gt;MAHOUT-1615&lt;/del&gt;&lt;/a&gt;: Move common helpers into math-scala from spark&lt;/p&gt;

&lt;p&gt;    Signed-off-by: Anand Avati &amp;lt;avati@redhat.com&amp;gt;&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="14165961" author="githubbot" created="Fri, 10 Oct 2014 00:11:36 +0100"  >&lt;p&gt;Github user avati commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/58#issuecomment-58592238&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/58#issuecomment-58592238&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @andrewpalumbo - this is passing all tests on my laptop now&lt;/p&gt;</comment>
                            <comment id="14165984" author="githubbot" created="Fri, 10 Oct 2014 00:31:14 +0100"  >&lt;p&gt;Github user andrewpalumbo commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/58#issuecomment-58593954&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/58#issuecomment-58593954&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Thanks Anand!&lt;/p&gt;</comment>
                            <comment id="14165989" author="githubbot" created="Fri, 10 Oct 2014 00:33:52 +0100"  >&lt;p&gt;Github user andrewpalumbo commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/58#issuecomment-58594183&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/58#issuecomment-58594183&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @dlyubimov , @pferrel - any objections to moving common/HDFSUtils into math-scala?&lt;/p&gt;</comment>
                            <comment id="14166991" author="githubbot" created="Fri, 10 Oct 2014 16:35:49 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/58#issuecomment-58672862&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/58#issuecomment-58672862&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    i already mentioned that i don&apos;t want any whiff of hadoop stuff in math-scala. For most part, because of impossibility to pinpoint exact hadoop api version a third party wants to use. There will always be applications claiming incompatbility with this or that in Hadoop with that approach. &lt;/p&gt;

&lt;p&gt;    it might make sense to create another module for &quot;all things Hadoop&quot;, and make engine specific modules depend on that, but i am not sure if amount of current code really justifies it yet. mrLegacy is kind of that, but it is legacy. maybe it makes sense to move some things from legacy to that &quot;all things hadoop&quot; module (e.g. sequence file iterators and such), although that is not used right now anywhere beyond mrlegacy.&lt;/p&gt;</comment>
                            <comment id="14166999" author="githubbot" created="Fri, 10 Oct 2014 16:40:55 +0100"  >&lt;p&gt;Github user dlyubimov commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/58#issuecomment-58673564&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/58#issuecomment-58673564&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    To clarify, the &quot;math&quot; module in Mahout is reserved for all things math in-core (Colt etc.). &lt;/p&gt;

&lt;p&gt;    &quot;math-scala&quot; module just provides convenience DSL and adaptation for scala. Maybe some new in-core math which is pure scala too. &lt;/p&gt;

&lt;p&gt;    These modules were not meant to work with distributed data  from the start.&lt;/p&gt;</comment>
                            <comment id="14167059" author="githubbot" created="Fri, 10 Oct 2014 17:19:06 +0100"  >&lt;p&gt;Github user pferrel commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/58#issuecomment-58678815&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/58#issuecomment-58678815&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    In some refactoring not yet checked in I left the Hadoop stuff in the spark module since Spark must be build with a specific version of Hadoop in mind. It also leaves one more dep out of the math-scala pom. I have an HDFS utils class too (and I&apos;m not very attached to it) so maybe we should compare notes.&lt;/p&gt;</comment>
                            <comment id="14167067" author="githubbot" created="Fri, 10 Oct 2014 17:25:07 +0100"  >&lt;p&gt;Github user pferrel commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/58#issuecomment-58679598&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/58#issuecomment-58679598&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    BTW the utils are probably not engine specific but are likely engine version specific. Not sure what that means to H2O.&lt;/p&gt;</comment>
                            <comment id="14167078" author="githubbot" created="Fri, 10 Oct 2014 17:33:38 +0100"  >&lt;p&gt;Github user andrewpalumbo commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/58#issuecomment-58680749&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/58#issuecomment-58680749&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @dlyubimov &lt;del&gt;That makes sense&lt;/del&gt; I&apos;d not noticed until late last night that there were currently no Hadoop dependencies in math-scala. &lt;/p&gt;

&lt;p&gt;    So maybe a &quot;hadoop-util&quot; module?&lt;/p&gt;

&lt;p&gt;    @pferrel I&apos;m not sure if you saw these- I just pushed it last night- the classes in the o.a.m.common package in the spark module:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/tree/master/spark/src/main/scala/org/apache/mahout/common&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/tree/master/spark/src/main/scala/org/apache/mahout/common&lt;/a&gt;&lt;/p&gt;

</comment>
                            <comment id="14167090" author="githubbot" created="Fri, 10 Oct 2014 17:43:07 +0100"  >&lt;p&gt;Github user andrewpalumbo commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/58#issuecomment-58682030&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/58#issuecomment-58682030&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    or simply leave each engine to deal with the hadoop api in its own module?&lt;/p&gt;</comment>
                            <comment id="14167104" author="githubbot" created="Fri, 10 Oct 2014 17:51:43 +0100"  >&lt;p&gt;Github user pferrel commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/58#issuecomment-58683117&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/58#issuecomment-58683117&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @andrewpalumbo pretty orthogonal to mine but a better place so I&apos;ll probably move my stuff there.&lt;/p&gt;

&lt;p&gt;    Since Spark is coupled to the hadoop version leaving the engine to deal with hadoop seems easiest. &lt;/p&gt;</comment>
                            <comment id="14167147" author="githubbot" created="Fri, 10 Oct 2014 18:21:17 +0100"  >&lt;p&gt;Github user andrewpalumbo commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/58#issuecomment-58687065&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/58#issuecomment-58687065&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I think that makes sense- Since we don&apos;t have enough code to call for a new module now, and the code may be version specific, which would be something for the engine module experts to determine.  we could just leave it as is and create a new package in h2o.  We can always create a &quot;hadoop-util&quot; module if it comes up enough and move it up to that.&lt;/p&gt;

&lt;p&gt;    btw- not trying to take credit for Dmitriy&apos;s work- I only pushed that package as part of M-1615 &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; .&lt;/p&gt;</comment>
                            <comment id="14167281" author="githubbot" created="Fri, 10 Oct 2014 19:40:38 +0100"  >&lt;p&gt;Github user avati commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/58#issuecomment-58698762&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/58#issuecomment-58698762&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    As an immediate step, I will copy those classes into h2o module. Refactoring them into a common separate module can be a new effort.&lt;/p&gt;</comment>
                            <comment id="14167461" author="githubbot" created="Fri, 10 Oct 2014 21:35:01 +0100"  >&lt;p&gt;Github user andrewpalumbo commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/58#issuecomment-58712637&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/58#issuecomment-58712637&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    looks good- thanks @avati &lt;/p&gt;</comment>
                            <comment id="14167559" author="githubbot" created="Fri, 10 Oct 2014 22:35:25 +0100"  >&lt;p&gt;Github user asfgit closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/mahout/pull/58&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/mahout/pull/58&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14167625" author="hudson" created="Fri, 10 Oct 2014 23:04:40 +0100"  >&lt;p&gt;SUCCESS: Integrated in Mahout-Quality #2821 (See &lt;a href=&quot;https://builds.apache.org/job/Mahout-Quality/2821/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/Mahout-Quality/2821/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1615&quot; title=&quot;SparkEngine drmFromHDFS returning the same Key for all Key,Vec Pairs for Text-Keyed SequenceFiles&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1615&quot;&gt;&lt;del&gt;MAHOUT-1615&lt;/del&gt;&lt;/a&gt;: fix up HDFS ClassTag reading from H2O bindings. this closes apache/mahout#58 (ap.dev: rev 61c9e1a92975de5891f082068fb708dc30cfafa1)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;h2o/src/main/scala/org/apache/mahout/common/DrmMetadata.scala&lt;/li&gt;
	&lt;li&gt;pom.xml&lt;/li&gt;
	&lt;li&gt;h2o/src/main/scala/org/apache/mahout/h2obindings/H2OEngine.scala&lt;/li&gt;
	&lt;li&gt;h2o/src/main/scala/org/apache/mahout/common/Hadoop1HDFSUtil.scala&lt;/li&gt;
	&lt;li&gt;h2o/src/main/scala/org/apache/mahout/common/HDFSUtil.scala&lt;/li&gt;
	&lt;li&gt;CHANGELOG&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14167842" author="hudson" created="Sat, 11 Oct 2014 01:19:04 +0100"  >&lt;p&gt;SUCCESS: Integrated in Mahout-Quality #2822 (See &lt;a href=&quot;https://builds.apache.org/job/Mahout-Quality/2822/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/Mahout-Quality/2822/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1615&quot; title=&quot;SparkEngine drmFromHDFS returning the same Key for all Key,Vec Pairs for Text-Keyed SequenceFiles&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1615&quot;&gt;&lt;del&gt;MAHOUT-1615&lt;/del&gt;&lt;/a&gt;: remove temporary hard coded settings from spark-shell (ap.dev: rev 4e6577d140c203eae0b634764971bf2444438f41)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;spark-shell/src/main/scala/org/apache/mahout/sparkbindings/shell/MahoutSparkILoop.scala&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Sun, 14 Sep 2014 21:01:30 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hztvkn:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                </customfields>
    </item>
</channel>
</rss>