<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 04:23:16 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/MAHOUT-60/MAHOUT-60.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[MAHOUT-60] Complementary Naive Bayes</title>
                <link>https://issues.apache.org/jira/browse/MAHOUT-60</link>
                <project id="12310751" key="MAHOUT">Mahout</project>
                    <description>&lt;p&gt;The focus is to implement an improved text classifier based on this paper &lt;a href=&quot;http://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf&lt;/a&gt;.&lt;/p&gt;
</description>
                <environment></environment>
        <key id="12397232">MAHOUT-60</key>
            <summary>Complementary Naive Bayes</summary>
                <type id="7" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/subtask_alternate.png">Sub-task</type>
                            <parent id="12389810">MAHOUT-9</parent>
                                    <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="gsingers">Grant Ingersoll</assignee>
                                    <reporter username="robinanil">Robin Anil</reporter>
                        <labels>
                    </labels>
                <created>Sat, 31 May 2008 23:28:58 +0100</created>
                <updated>Sat, 21 May 2011 04:24:11 +0100</updated>
                            <resolved>Tue, 26 Aug 2008 13:58:49 +0100</resolved>
                                                    <fixVersion>0.1</fixVersion>
                                    <component>Classification</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="12601420" author="robinanil" created="Sat, 31 May 2008 23:50:45 +0100"  >&lt;p&gt;Before using this patch please use &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-9&quot; title=&quot;Implement MapReduce BayesianClassifier&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-9&quot;&gt;&lt;del&gt;MAHOUT-9&lt;/del&gt;&lt;/a&gt; (Implement MapReduce BayesianClassifier) patch and the instructions given there.&lt;/p&gt;

&lt;p&gt;the 20Newsgroups Trainer requires the collapsed version as given in &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-9&quot; title=&quot;Implement MapReduce BayesianClassifier&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-9&quot;&gt;&lt;del&gt;MAHOUT-9&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Steps to get it running&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ant extract-20news-18828&lt;br/&gt;
ant examples-job&lt;/p&gt;

&lt;p&gt;bin/start-all.sh //Start Hadoop&lt;br/&gt;
bin/hadoop dfs -put &amp;lt;MAHOUT_HOME&amp;gt;/work/20news-18828-collapse 20newsInput &lt;br/&gt;
bin/hadoop jar &amp;lt;MAHOUT_HOME&amp;gt;/build/apache-mahout-0.1-dev-ex.jar org.apache.mahout.examples.classifiers.cbayes.TwentyNewsgroups -t -i 20newsinput/20news-18828-collapse -o 20newsoutput //This will train a classifier and write the model in a file named model in the folder 20newsoutput&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Copy the model file from the DFS to the local filesystem&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;bin/hadoop dfs -get 20newsoutput 20newsoutput&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Test on the 20newsgroups data to check how well it is able to classify the train set. Accuracy is around 98.4% on the train set. But only way to check the implementation is correct is by doing some cross validation which is yet to be done.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;java -Xmx1024M org.apache.mahout.examples.classifiers.cbayes.Test20Newsgroups -p 20newsoutput/model -t  work/20news-18828/  &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;TODO: Option to Split the 20newsgroups dataset into a train and a test set. Meanwhile if you have a set of test and train set on the 20newsgroups data you can build model on one of them and test on the other.&lt;/p&gt;

</comment>
                            <comment id="12608251" author="gsingers" created="Thu, 26 Jun 2008 03:25:56 +0100"  >&lt;p&gt;Hi Robin,&lt;/p&gt;

&lt;p&gt;Can you work on building up some Junit tests now?  Also, can you generate the patch again against the latest trunk?&lt;/p&gt;

&lt;p&gt;Thanks,&lt;br/&gt;
Grant&lt;/p&gt;</comment>
                            <comment id="12608806" author="robinanil" created="Fri, 27 Jun 2008 16:30:46 +0100"  >&lt;p&gt;Hi i have been working on the output statistics generation. I will update it&lt;br/&gt;
shortly&lt;/p&gt;</comment>
                            <comment id="12609144" author="robinanil" created="Mon, 30 Jun 2008 02:30:40 +0100"  >&lt;p&gt;This is the latest diff against the trunk&lt;/p&gt;

&lt;p&gt;Changes:&lt;br/&gt;
*Added a Result Analyzer Class to generate Classification Statistics. &lt;br/&gt;
*Currently generates Confusion Matrix and Percentage accuracy. &lt;br/&gt;
*It will be extended to include (Precison, Recall, RMSE , Relative Absolute Error, Kappa Statistic)&lt;br/&gt;
*All such instances extends a Summarizable Interface &lt;/p&gt;

&lt;p&gt;Before using this patch please use &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-9&quot; title=&quot;Implement MapReduce BayesianClassifier&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-9&quot;&gt;&lt;del&gt;MAHOUT-9&lt;/del&gt;&lt;/a&gt; (Implement MapReduce BayesianClassifier) patch and the instructions given above.&lt;/p&gt;

&lt;p&gt;The number of reducers are limited to 1 at the moment. Will need to figure out a way to read intermediate result&lt;/p&gt;

&lt;p&gt;You can directly run the TestTwentyNewsgroups from the dfs as follows&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt; 
$bin/hadoop jar &amp;lt;MAHOUT_HOME&amp;gt;/build/apache-mahout-0.1-dev-ex.jar org.apache.mahout.examples.classifiers.cbayes.TestTwentyNewsgroups -p 20newsoutput/model -t work/20news-18828
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt; 


</comment>
                            <comment id="12610887" author="robinanil" created="Mon, 7 Jul 2008 05:22:04 +0100"  >&lt;p&gt;  There are a lot of changes in this patch. Most of the Files have been renamed. The trainer is now a bunch of &lt;b&gt;5 Map Reduce jobs&lt;/b&gt;. The exact functionality of each job is as follows. The trainer can support &lt;b&gt;any number of maps and any number of reduces&lt;/b&gt;.  Also i am using Apache Lang library commons-lang-2.4.jar ( which should be put in the classpath)&lt;/p&gt;


 &lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt; 
      //Read the features in each document normalized by length of each document
      CBayesFeatureDriver.runJob(input, output);
      

      //Calculate the TfIdf for each word in each label
      CBayesTfIdfDriver.runJob(input, output);
      

      //Calculate the Sums of weights for each label, for each feature and for each feature and for each label
      CBayesWeightSummerDriver.runJob(input, output);
      

      //Calculate the W_ij = log(Theta) for each label, feature. This step actually generates the complement class
      CBayesThetaDriver.runJob(input, output);
      
   
      //Calculate the normalization factor Sigma_W_ij for each complement class. 
      CBayesThetaNormalizerDriver.runJob(input, output);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt; 

&lt;p&gt;  I have tested it on a 6 system cluster. On 20 newsgroups dataset, it takes around 4 minutes to train. It just used to take  20-30 seconds when creating the CNB model in-memory. But the design is based on the assumption that the datasets are going to be too huge to fit into memory.&lt;/p&gt;

&lt;p&gt;There can be a lot of speed improvement if the Map-Reduce operations can be somehow chained.&lt;br/&gt;
So Instead of Map1 -&amp;gt; Reduce1 - &amp;gt; Map1 -&amp;gt; Reduce2....&lt;br/&gt;
if it is possible to do. Map1 -&amp;gt; Reduce1 - &amp;gt; Reduce2 -&amp;gt; Reduce3 -&amp;gt;... then we could save a lot of time on IO. I am not sure if such a functionality exists in hadoop&lt;/p&gt;

&lt;p&gt; I will test it out on Dmoz or Wikipedia dataset (if i can preprocess it within a reasonable amount of time)&lt;/p&gt;

&lt;p&gt;  The other changes are that there is no longer a model file. The model is stored in multiple part files in  the folders trainer-theta and trainer-thetaNormalizer&lt;/p&gt;

&lt;p&gt;To Train&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt; 
$bin/hadoop jar &amp;lt;MAHOUT_HOME&amp;gt;/build/apache-mahout-0.1-dev-ex.jar org.apache.mahout.examples.classifiers.cbayes.TrainTwentyNewsgroups -t -i 20newsinput -o 20newsoutput
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To Test&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt; 
$bin/hadoop jar &amp;lt;MAHOUT_HOME&amp;gt;/build/apache-mahout-0.1-dev-ex.jar org.apache.mahout.examples.classifiers.cbayes.TestTwentyNewsgroups -p 20newsoutput  -t work/20news-18828
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt; 

&lt;p&gt;Next Step, to make the Classifier and the Testing completely Map Reduce.&lt;/p&gt;
</comment>
                            <comment id="12611429" author="robinanil" created="Tue, 8 Jul 2008 02:45:23 +0100"  >&lt;p&gt;FYI. This Transformed Weight Normalized Complementary Naive Bayes. Steps 1 -8. &lt;br/&gt;
This is being shrunk into 5 Map-Red Jobs&lt;/p&gt;</comment>
                            <comment id="12612163" author="stevenkhanderson" created="Wed, 9 Jul 2008 17:01:39 +0100"  >&lt;p&gt;I&apos;m still a newbie at this, just running your code, but I noticed that I was&lt;br/&gt;
able to get high utilization (30-40%) on the early map-reduce tasks,&lt;br/&gt;
but down to 3% on the fourth map-reduce pass.&lt;/p&gt;

&lt;p&gt;Any idea why this would be?  Obviously I&apos;m still playing with the machine&lt;br/&gt;
hadoop installation parameters (this is an 8-core 1-chip Sun box).&lt;/p&gt;</comment>
                            <comment id="12612173" author="stevenkhanderson" created="Wed, 9 Jul 2008 17:15:19 +0100"  >&lt;p&gt;Maybe one thing you need is a configurable number of reduce tasks &amp;#8211;&lt;br/&gt;
mapping is high-util, but even the first reduce drops way down&lt;br/&gt;
(only 1 reduce?).&lt;/p&gt;</comment>
                            <comment id="12612225" author="robinanil" created="Wed, 9 Jul 2008 19:34:32 +0100"  >&lt;p&gt;The number of reduce tasks are automatically picked up from your hadoop&lt;br/&gt;
configuration. Just set it in your hadoop-site.xml. BTW the 5th map reduce&lt;br/&gt;
pass outputs the data and calculates the sum for length normalization. 4th&lt;br/&gt;
one ought to have high CPU utilization(this is the stage where weight is&lt;br/&gt;
calculated as per the CNB formulation)&lt;/p&gt;

&lt;p&gt;On Wed, Jul 9, 2008 at 9:33 PM, Steven Handerson (JIRA) &amp;lt;jira@apache.org&amp;gt;&lt;/p&gt;



&lt;p&gt;&amp;#8211; &lt;br/&gt;
Robin Anil&lt;br/&gt;
Senior Dual Degree Student&lt;br/&gt;
Department of Computer Science &amp;amp; Engineering&lt;br/&gt;
IIT Kharagpur&lt;/p&gt;

&lt;p&gt;--------------------------------------------------------------------------------------------&lt;br/&gt;
techdigger.wordpress.com&lt;br/&gt;
A discursive take on the world around us&lt;/p&gt;

&lt;p&gt;www.minekey.com&lt;br/&gt;
You Might Like This&lt;/p&gt;

&lt;p&gt;www.ithink.com&lt;br/&gt;
Express Yourself&lt;/p&gt;</comment>
                            <comment id="12614847" author="stevenkhanderson" created="Fri, 18 Jul 2008 20:47:32 +0100"  >&lt;p&gt;Robin,&lt;/p&gt;

&lt;p&gt;I can get the training working very well &amp;#8211; I&apos;ve even started working with a very &lt;br/&gt;
large file (700+ Meg, and not done creating yet, the old slow way).  No problem.&lt;br/&gt;
But I&apos;d say the judgment / application of a model maybe needs a better map-reduce&lt;br/&gt;
treatment now &amp;#8211; at least I &lt;b&gt;think&lt;/b&gt; it&apos;s working (I&apos;ve seen it work on smaller&lt;br/&gt;
training data) but with my larger task it&apos;s getting bogged down.&lt;/p&gt;

&lt;p&gt;Maybe I&apos;ll think about it / try it, but I&apos;m very new to map-reduce, but it seems&lt;br/&gt;
like you should be able to do something clever with throwing the&lt;br/&gt;
test data (feature|doc) and model data (feature|category, increment) together,&lt;br/&gt;
reducing and emitting category increments / decrements for each&lt;br/&gt;
(doc, category) pair, and then summing them up in a reduce.&lt;br/&gt;
Or just emitting (doc|category,increment) for all features, and then you&lt;br/&gt;
can easily also in the reduce find the maximal category.&lt;/p&gt;

&lt;p&gt;I don&apos;t think this is what you&apos;re doing yet &amp;#8211; you&apos;re thinking of loading&lt;br/&gt;
the model, rather than shoving it through a map/reduce sequence.  I think.&lt;/p&gt;</comment>
                            <comment id="12615035" author="robinanil" created="Sat, 19 Jul 2008 19:35:33 +0100"  >&lt;p&gt;I thought of that. But i wasnt sure. If classifying one document requires&lt;br/&gt;
one map-reduce over the whole model. Then its more or less a waste of&lt;br/&gt;
resource utilization. But if i do batch classification. This is what i would&lt;br/&gt;
do. I want to know whether there is some  tweak that can be done.&lt;/p&gt;

&lt;p&gt;For the model Map:output (docid:label:featureid, weight) will lead to (docNo&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;number of label,features) keys (too huge)&lt;br/&gt;
For each document Map:output (docid:label:featureid, featureFrequency)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Reducer: each reducer will get 2 values or 1 value. if its one value&lt;br/&gt;
ignore.&lt;br/&gt;
if its 2 value then multiply and output(docid:label:featureid, weight)&lt;/p&gt;

&lt;p&gt;Start a second map reduce on this Map:output (docid:label, weight)&lt;br/&gt;
then reducer sums up the probabilities for each docid:label pair&lt;/p&gt;

&lt;p&gt;Third Map:reduce can take the doc,label emit docid =&amp;gt; label:weight&lt;br/&gt;
then Reduce takes the min weight label and output the result.&lt;/p&gt;

&lt;p&gt;Any thoughts&lt;/p&gt;

&lt;p&gt;Robin&lt;/p&gt;

</comment>
                            <comment id="12615040" author="tdunning@veoh.com" created="Sat, 19 Jul 2008 20:18:38 +0100"  >&lt;p&gt;Classifying a single document isn&apos;t particularly an interesting task to parallelize since it is already so fast.&lt;/p&gt;

&lt;p&gt;The interesting parallel tasks are training and batch classification.  This is pretty much as you say.  For batch classification, I would find it tempting to have each map do a single document classification and emit the result.  At that point, you have trivial parallelism and no need for a reduce.  You need to have a bit of a lookup table on each mapper, but this isn&apos;t usually all that big (typically only thousands of interesting term weights, possibly hundreds of thousands for some kinds of application).  Not only do you not need the reduce, but you don&apos;t need three phases of map-reduce either.&lt;/p&gt;

&lt;p&gt;Training is a different matter since it involves data that is found in one way (terms in documents) that needs to be aggregated another way (terms for different categories).  That is natural for map-reduce as well.&lt;/p&gt;
</comment>
                            <comment id="12615042" author="robinanil" created="Sat, 19 Jul 2008 20:27:42 +0100"  >&lt;p&gt;If you remember my other post about creating Server to hold the matrix to lookup the term:label weight. Such a server can enable you to classify one document from anywhere. The Mapper just requests the the weights and passes it to the reducer. Maybe HBase can help? I will have to experiment with it a bit&lt;/p&gt;</comment>
                            <comment id="12615302" author="stevenkhanderson" created="Mon, 21 Jul 2008 17:44:05 +0100"  >&lt;p&gt;Well, yes, I was thinking of batch classification (like, constructing&lt;br/&gt;
confusion matrices, or running the training data back through the model to test the model).&lt;br/&gt;
But the problem I&apos;m running into with the code is that the model&lt;br/&gt;
is too large to load in a single process, let alone multiple mappers.&lt;br/&gt;
So classifying fast doesn&apos;t help if simply loading the model is very slow&lt;br/&gt;
(and I mean very slow, and then doesn&apos;t necessarily succeed anyway &amp;#8211; out of mem).&lt;/p&gt;

&lt;p&gt;I also admit that &quot;batch classification&quot; &amp;#8211; in the sense that there is overlap&lt;br/&gt;
in the different feature sets from different documents &amp;#8211;&lt;br/&gt;
makes it more interesting / saves some work perhaps, but you can&apos;t count on that anyway.&lt;br/&gt;
Yes, you might want something fast for single-document classification,&lt;br/&gt;
but map-reduce isn&apos;t the right tool for that.  Indexed structures are better.&lt;/p&gt;

&lt;p&gt;The choices are either some indexed structure (like HBase) which can&lt;br/&gt;
handle large datasets / models, or just use map-reduce to join&lt;br/&gt;
the model to the data.  The latter is definitely not useless &amp;#8211;&lt;br/&gt;
usages similiarly divide into people who have a lot of data / docs to classify,&lt;br/&gt;
versus people who are building some kind of online system.&lt;br/&gt;
Throughput versus round-trip time.&lt;br/&gt;
Also, note that with an indexed solution, you might have contention for the indexed data &amp;#8211;&lt;br/&gt;
if there&apos;s only one copy (which should probably be the case, for large models).&lt;/p&gt;

&lt;p&gt;So I&apos;d suggest implementing both, and to consider the cases where the models are very large&lt;br/&gt;
(which is where map-reduce shines anyway).  I might be the only person commenting&lt;br/&gt;
who has tried a lot of data (800Meg input document file), and as I said it would&lt;br/&gt;
be nice to have some results (confusion matrices) &lt;br/&gt;
to see if the method is working for me and my particular data.&lt;/p&gt;

&lt;p&gt;If nobody else agrees, I might have to try it myself, but I&apos;m new at this&lt;br/&gt;
and sometimes get pulled away for other work.&lt;/p&gt;
</comment>
                            <comment id="12615308" author="stevenkhanderson" created="Mon, 21 Jul 2008 18:07:00 +0100"  >&lt;p&gt;Hmm &amp;#8211; well, in a sense I agree that all you really need is the model&lt;br/&gt;
sorted by feature (being able to find all information about a particular feature).&lt;br/&gt;
So maybe a final sorting step, and something that can find &lt;br/&gt;
everything about a particular feature would make sense.&lt;br/&gt;
But again without the map-reduce, there will be contention for that structure.&lt;/p&gt;</comment>
                            <comment id="12615324" author="stevenkhanderson" created="Mon, 21 Jul 2008 18:44:21 +0100"  >&lt;p&gt;I&apos;ll add one more thing so you see where I&apos;m coming from.&lt;/p&gt;

&lt;p&gt;Map-reduce is basically a hash join (database term),&lt;br/&gt;
which are actually slow in a database in my experience&lt;br/&gt;
(because of the space allocation required, but also just not using an index).&lt;br/&gt;
Map-reduce makes hash joins relatively fast,&lt;br/&gt;
by using multiple processors and networking.&lt;br/&gt;
You could do other kinds of joins in map-reduce, use indexes, use ordered data,&lt;br/&gt;
things like &quot;partitions&quot; in database parlance,&lt;br/&gt;
or you could just use a database.&lt;br/&gt;
Databases have a problem that the round-trip to the database&lt;br/&gt;
sometimes makes your application much slower than necessary,&lt;br/&gt;
for doing lots of individual queries in sequence (randomly accessing&lt;br/&gt;
but doing so using an index) &amp;#8211;&lt;br/&gt;
they are good at streaming the results of a join out (which use indexes).&lt;br/&gt;
Of course, some applications (like web-based) are slower in &lt;br/&gt;
aggregate, in order to answer individual queries relatively quickly&lt;br/&gt;
(faster round-trip time).&lt;br/&gt;
There may be similar issues with respect to map-reduce,&lt;br/&gt;
but you can see there&apos;s a kind of connection between what&lt;br/&gt;
databases do and map-reduce does: join data sources on some field or computed value.&lt;/p&gt;

&lt;p&gt;Hmm &amp;#8211; also, it&apos;s not that the data is too large (yet) &amp;#8211;&lt;br/&gt;
my model is about 1.5 Gig, and I&apos;m (as of now) trying using the code&lt;br/&gt;
in a single process rather than hadoop, but maybe the model&lt;br/&gt;
size isn&apos;t the max process size (-Xmx4g didn&apos;t work), so I&apos;m trying larger and larger -Xmx&lt;br/&gt;
(and I do have 64 bit java available &amp;#8211; right now trying 8 Gig).&lt;/p&gt;
</comment>
                            <comment id="12618854" author="robinanil" created="Thu, 31 Jul 2008 22:09:52 +0100"  >&lt;p&gt;To Split Wikipedia xml dump into small XML chunks&lt;/p&gt;
 &lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt; 
hadoop jar build/apache-mahout-0.1-dev-ex.jar org.apache.mahout.examples.classifiers.cbayes.WikipediaXmlSplitter -d /home/robin/data/wikipedia/articles/enwiki-latest-pages-articles.xml -o  /home/robin/data/wikipedia/chunks/ -c 64
  &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt; 

&lt;p&gt;Put the chunks into the dfs&lt;/p&gt;
 &lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt; 
 hadoop dfs -put /home/robin/data/wikipedia/chunks/ wikipediadump
 &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt; 

&lt;p&gt;Create the countries based Split of wikipedia dataset.(See the attached country.txt file)&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt; hadoop jar build/apache-mahout-0.1-dev-ex.jar org.apache.mahout.examples.classifiers.cbayes.WikipediaDatasetCreator -i wikipediadump -o wikipediainput -c pathto/country.txt
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Train the Classifier on the Countries bases split of wikipedia&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;$bin/hadoop jar &amp;lt;MAHOUT_HOME&amp;gt;/build/apache-mahout-0.1-dev-ex.jar org.apache.mahout.examples.classifiers.cbayes.TrainTwentyNewsgroups -t -i wikipediainput -o wikipediamodel
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;p&gt;Fetch the Input Files for Testing&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt; hadoop dfs -get wikipediainput wikipediainput 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test the Classifier&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;$bin/hadoop jar &amp;lt;MAHOUT_HOME&amp;gt;/build/apache-mahout-0.1-dev-ex.jar org.apache.mahout.examples.classifiers.cbayes.TestTwentyNewsgroups -p wikipediamodel -t  wikipediainput
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
</comment>
                            <comment id="12619129" author="robinanil" created="Fri, 1 Aug 2008 21:58:48 +0100"  >&lt;p&gt;Fixed some bugs in the previous patch&lt;/p&gt;</comment>
                            <comment id="12621796" author="gsingers" created="Tue, 12 Aug 2008 13:37:23 +0100"  >&lt;p&gt;Hi Robin,&lt;/p&gt;

&lt;p&gt;What&apos;s the Summarizable interface for?  I don&apos;t see any uses or implementations.&lt;/p&gt;</comment>
                            <comment id="12621801" author="robinanil" created="Tue, 12 Aug 2008 13:54:46 +0100"  >&lt;p&gt;It is implementd by ConfusionMatrix and ResultAnalyzer&lt;/p&gt;

</comment>
                            <comment id="12622040" author="robinanil" created="Wed, 13 Aug 2008 01:10:03 +0100"  >&lt;p&gt;I have merged the BayesClassifier and CBayesClassifier. Now both use some common Map reduce operation. The specific Map-Reduce operations are factored out. &lt;br/&gt;
The Model is also factored out. &lt;/p&gt;

&lt;p&gt;The new feature in this patch is a n-gram generator using the cli parameter  -ng &amp;lt;gram-size&amp;gt;&lt;br/&gt;
If a model is made using a 3-gram then you can use 1/2/3 gram to classify.&lt;/p&gt;

&lt;p&gt;Try increasing n-gram and see how the classification accuracy grow with it.&lt;/p&gt;

&lt;p&gt;cbayes.TestTwentyNewsgroups is renamed to bayes.TestClassifier&lt;br/&gt;
cbayes.TrainTwentyNewsgrousp is renamed to bayes.TrainClassifier&lt;/p&gt;

&lt;p&gt;The Tests will fail when using this patch. So dont worry. New tests will be put up shortly.&lt;/p&gt;

 &lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt; 
     //To Train a Bayes Classifier using tri-grams
      hadoop jar build/apache-mahout-0.1-dev-ex.jar org.apache.mahout.examples.classifiers.bayes.TrainClassifier -t -i newstrain -o newsmodel -ng 3 -type bayes
     //To Test a Bayes Classifier using tri-grams
      hadoop jar build/apache-mahout-0.1-dev-ex.jar org.apache.mahout.examples.classifiers.bayes.TestClassifier -p newsmodel -t work/newstest -ng 3 -type bayes

     //To Train a CBayes Classifier using bi-grams
      hadoop jar build/apache-mahout-0.1-dev-ex.jar org.apache.mahout.examples.classifiers.bayes.TrainClassifier -t -i newstrain -o newsmodel -ng 2 -type cbayes
     //To Test a CBayes Classifier using bi-grams
      hadoop jar build/apache-mahout-0.1-dev-ex.jar org.apache.mahout.examples.classifiers.bayes.TestClassifier -p newsmodel -t work/newstest -ng 2 -type cbayes
 &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt; 

&lt;p&gt;Hope you will enjoy using this patch.&lt;/p&gt;</comment>
                            <comment id="12622863" author="gsingers" created="Fri, 15 Aug 2008 13:51:16 +0100"  >&lt;p&gt;Hi Robin,&lt;/p&gt;

&lt;p&gt;Things are coming along.  Looks like you forgot to add the Classifier class to common.  I think instead of putting it under the Common package, it probably should go at the top of the classifier package.  Also, not sure which version of commons-lang you are using, so either upload that jar here, or just let me know which one you are using.&lt;/p&gt;

&lt;p&gt;-Grant&lt;/p&gt;</comment>
                            <comment id="12622864" author="gsingers" created="Fri, 15 Aug 2008 13:53:25 +0100"  >&lt;p&gt;Also, you&apos;ll need to update the patch slightly as the examples have now been separated out.&lt;/p&gt;</comment>
                            <comment id="12622868" author="robinanil" created="Fri, 15 Aug 2008 14:43:10 +0100"  >&lt;p&gt;Use apache commons-lang-2.4.jar in the lib path&lt;/p&gt;

&lt;p&gt;Refactored as per the new core examples build file. &lt;/p&gt;</comment>
                            <comment id="12622887" author="gsingers" created="Fri, 15 Aug 2008 16:05:33 +0100"  >&lt;p&gt;I think we should move Classifier and Model from the common package to the classifier package.   Classifier should also be an abstract class instead of an interface for future &quot;back-compatibility&quot;.  The other question is whether the Classifier and Model classes are &quot;generic&quot; enough to be considered usable by any classifier that is implemented, or are the Bayes specific.  If it is the latter, than perhaps we should just create a single bayes package, and put them all in there, including the CBayes classes.  Not a big deal, but something to think about.&lt;/p&gt;

&lt;p&gt;I committed commons-lang to lib.&lt;/p&gt;</comment>
                            <comment id="12622889" author="robinanil" created="Fri, 15 Aug 2008 16:18:45 +0100"  >&lt;p&gt;Right now the Model (Abstract class is not completely bayes specific)&lt;br/&gt;
Certain things like storing a score for a label, feature matrix, storing the&lt;br/&gt;
sum score of a label and also of a feature. Storing the total sum score.&lt;br/&gt;
Storing label normalizatin factors These are reused in other kind of&lt;br/&gt;
classifiers also. These can indeed be taken out and put as a base class&lt;br/&gt;
which stores it on the dfs or in a Distributed Matrix Storage/Retrieval&lt;br/&gt;
system(later). The other bayes specific Datastructs can be taken out  and&lt;br/&gt;
put in a BayesBaseModel class ?&lt;/p&gt;



</comment>
                            <comment id="12622987" author="gsingers" created="Fri, 15 Aug 2008 20:15:09 +0100"  >&lt;p&gt;Sounds reasonable, besides we are only on 0.1, we needn&apos;t fret over it too much yet.  If you can finish it up, then I can put on a final review and commit.&lt;/p&gt;</comment>
                            <comment id="12623242" author="robinanil" created="Mon, 18 Aug 2008 01:43:11 +0100"  >&lt;p&gt;Added some tests for the Model. and Some code tidy ups. No functionality Change&lt;/p&gt;</comment>
                            <comment id="12623397" author="gsingers" created="Mon, 18 Aug 2008 17:32:53 +0100"  >&lt;p&gt;I&apos;m getting failures in the BayesFileFormatterTest.  Namely due to the change to \t, which is an easy fix.  However, I wonder why the check to the &quot;seen&quot; CharSet was removed?  I seem to recall that we only want unique words for training, otherwise the calculations get screwed up, at least in the NB implementation (not sure what you want in CNB)&lt;/p&gt;

&lt;p&gt;The loop used to look like:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; ((token = ts.next(token)) != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
      &lt;span class=&quot;code-object&quot;&gt;char&lt;/span&gt;[] termBuffer = token.termBuffer();
      &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; termLen = token.termLength();
      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (seen.contains(termBuffer, 0, termLen) == &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;) {
        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (numTokens &amp;gt; 0) {
          writer.write(&apos; &apos;);
        }
        writer.write(termBuffer, 0, termLen);
        &lt;span class=&quot;code-object&quot;&gt;char&lt;/span&gt; [] tmp = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;char&lt;/span&gt;[termLen];
        &lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.arraycopy(termBuffer, 0, tmp, 0, termLen);
        seen.add(tmp);&lt;span class=&quot;code-comment&quot;&gt;//&lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; b/c CharArraySet doesn&apos;t allow offsets
&lt;/span&gt;      }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12623415" author="robinanil" created="Mon, 18 Aug 2008 18:42:45 +0100"  >&lt;p&gt;I am generating the bigrams. So if you keep only unique words then bigrams&lt;br/&gt;
dont get generated correctly.&lt;/p&gt;

</comment>
                            <comment id="12623650" author="gsingers" created="Tue, 19 Aug 2008 13:16:27 +0100"  >&lt;p&gt;Do we always want to use ngrams, though?  If n == 1, do we have a way of filtering out duplicates?  Seems like even if n &amp;gt; 1, you could still have duplicates.  Not sure how this is supposed to be handled, will have to look into the code more.&lt;/p&gt;</comment>
                            <comment id="12623659" author="gsingers" created="Tue, 19 Aug 2008 13:58:50 +0100"  >&lt;p&gt;OK, I committed this.  I think we can leave this open for some more patches.  I&apos;d also like to see some more docs on the interplay between the various drivers, although it seems like some of them should just be package protected if they are not intended for use by the public.&lt;/p&gt;</comment>
                            <comment id="12644314" author="gsingers" created="Fri, 31 Oct 2008 15:07:36 +0000"  >&lt;p&gt;Hey Robin,&lt;/p&gt;

&lt;p&gt;On the country.txt, where did that come from?  Is it something that can be checked in?&lt;/p&gt;</comment>
                            <comment id="12644316" author="gsingers" created="Fri, 31 Oct 2008 15:08:54 +0000"  >&lt;p&gt;Never mind, it&apos;s just a list of countries.  If that isn&apos;t public domain, I don&apos;t know what is.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12388094" name="MAHOUT-60-13082008.patch" size="241290" author="robinanil" created="Wed, 13 Aug 2008 01:10:03 +0100"/>
                            <attachment id="12388312" name="MAHOUT-60-15082008.patch" size="243629" author="robinanil" created="Fri, 15 Aug 2008 14:43:10 +0100"/>
                            <attachment id="12388400" name="MAHOUT-60-17082008.patch" size="251896" author="robinanil" created="Mon, 18 Aug 2008 01:43:11 +0100"/>
                            <attachment id="12387370" name="MAHOUT-60.patch" size="67479" author="robinanil" created="Fri, 1 Aug 2008 21:58:48 +0100"/>
                            <attachment id="12387312" name="MAHOUT-60.patch" size="67479" author="robinanil" created="Thu, 31 Jul 2008 22:09:52 +0100"/>
                            <attachment id="12385366" name="MAHOUT-60.patch" size="130215" author="robinanil" created="Mon, 7 Jul 2008 05:22:04 +0100"/>
                            <attachment id="12384930" name="MAHOUT-60.patch" size="67248" author="robinanil" created="Mon, 30 Jun 2008 02:30:40 +0100"/>
                            <attachment id="12383176" name="MAHOUT-60.patch" size="50322" author="robinanil" created="Sat, 31 May 2008 23:50:45 +0100"/>
                            <attachment id="12387313" name="country.txt" size="2463" author="robinanil" created="Thu, 31 Jul 2008 22:10:51 +0100"/>
                            <attachment id="12385460" name="twcnb.jpg" size="49542" author="robinanil" created="Tue, 8 Jul 2008 02:45:23 +0100"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>10.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 26 Jun 2008 02:25:56 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>10006</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hxy7g7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>23358</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                </customfields>
    </item>
</channel>
</rss>