<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 04:21:54 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/MAHOUT-542/MAHOUT-542.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[MAHOUT-542] MapReduce implementation of ALS-WR</title>
                <link>https://issues.apache.org/jira/browse/MAHOUT-542</link>
                <project id="12310751" key="MAHOUT">Mahout</project>
                    <description>&lt;p&gt;As Mahout is currently lacking a distributed collaborative filtering algorithm that uses matrix factorization, I spent some time reading through a couple of the Netflix papers and stumbled upon the &quot;Large-scale Parallel Collaborative Filtering for the Net&#64258;ix Prize&quot; available at &lt;a href=&quot;http://www.hpl.hp.com/personal/Robert_Schreiber/papers/2008%20AAIM%20Netflix/netflix_aaim08(submitted).pdf&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.hpl.hp.com/personal/Robert_Schreiber/papers/2008%20AAIM%20Netflix/netflix_aaim08(submitted).pdf&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It describes a parallel algorithm that uses &quot;Alternating-Least-Squares with Weighted-&#955;-Regularization&quot; to factorize the preference-matrix and gives some insights on how the authors distributed the computation using Matlab.&lt;/p&gt;

&lt;p&gt;It seemed to me that this approach could also easily be parallelized using Map/Reduce, so I sat down and created a prototype version. I&apos;m not really sure I got the mathematical details correct (they need some optimization anyway), but I wanna put up my prototype implementation here per Yonik&apos;s law of patches.&lt;/p&gt;

&lt;p&gt;Maybe someone has the time and motivation to work a little on this with me. It would be great if someone could validate the approach taken (I&apos;m willing to help as the code might not be intuitive to read) and could try to factorize some test data and give feedback then.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12479887">MAHOUT-542</key>
            <summary>MapReduce implementation of ALS-WR</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="ssc">Sebastian Schelter</assignee>
                                    <reporter username="ssc">Sebastian Schelter</reporter>
                        <labels>
                    </labels>
                <created>Sat, 13 Nov 2010 22:16:42 +0000</created>
                <updated>Tue, 15 Nov 2011 13:18:23 +0000</updated>
                            <resolved>Wed, 23 Mar 2011 22:34:08 +0000</resolved>
                                    <version>0.5</version>
                                    <fixVersion>0.5</fixVersion>
                                    <component>Collaborative Filtering</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                <comments>
                            <comment id="12931758" author="tdunning" created="Sun, 14 Nov 2010 04:48:10 +0000"  >&lt;p&gt;Sebastian, &lt;/p&gt;

&lt;p&gt;Nice idea.  Can you say how it relates to Elkan and Menon&apos;s paper at &lt;a href=&quot;http://arxiv.org/abs/1006.2156&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://arxiv.org/abs/1006.2156&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The core of their algorithm is a matrix factorization, but they seem to get away with very few latent factors per user and are able to handle side information very nicely.&lt;/p&gt;

&lt;p&gt;So it seems on the surface that these methods should be related.  One interesting difference is the use of a link function and the handling of ordinal target variables in Elkan and Menon&apos;s work.&lt;/p&gt;</comment>
                            <comment id="12931787" author="ssc" created="Sun, 14 Nov 2010 07:59:41 +0000"  >&lt;p&gt;Hi Ted,&lt;/p&gt;

&lt;p&gt;I read through that paper a while ago when we exchanged ideas for Mahout 0.5 on the mailing list and to be honest I didn&apos;t really get the mathematical details. Nevertheless I understood that its possibilities are for superior to what we currently have and also to the approach described in the Netflix paper mentioned above, mainly because of the ability to handle side information and nominal values as you already mentioned. I think the paper does not describe a parallelization approach to the algorithm, though I&apos;m not sure whether this is even necessary for it.&lt;/p&gt;

&lt;p&gt;But I had the prototype code attached in the patch ready before we had that discussion and I have the hope that it could be finished with only a little input from someone else so I decided to put it up here. I&apos;d have no problem dropping this here though when &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-525&quot; title=&quot;Implement LatentFactorLogLinear models&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-525&quot;&gt;&lt;del&gt;MAHOUT-525&lt;/del&gt;&lt;/a&gt; is done and it turns out that putting work in there would give a much nicer recommender for Mahout. &lt;/p&gt;

</comment>
                            <comment id="12931894" author="tdunning" created="Sun, 14 Nov 2010 21:34:13 +0000"  >&lt;p&gt;Sebastian, &lt;/p&gt;

&lt;p&gt;I very much did not want to decrease your very commendable momentum.  Getting a parallel version in place for comparison is very valuable.&lt;/p&gt;

&lt;p&gt;I won&apos;t be able to look at your code due to my current constraints, but I think your general direction is very good.&lt;/p&gt;</comment>
                            <comment id="12973411" author="ssc" created="Mon, 20 Dec 2010 23:27:02 +0000"  >&lt;p&gt;An updated version of the patch. I fixed a small bug, added more tests and polished the code a little.&lt;/p&gt;

&lt;p&gt;The distributed matrix factorization works fine now on a toy example. The next steps will be to use real data and do some holdout tests.&lt;/p&gt;</comment>
                            <comment id="12974402" author="ssc" created="Wed, 22 Dec 2010 21:41:54 +0000"  >&lt;p&gt;Thanks for the input so far Ted and Dimitriy.&lt;/p&gt;

&lt;p&gt;Here is an updated patch that does not address the issue of automatically learning lambda but provides some simple tools to evaluate the predicition quality of the factorization manually. &lt;/p&gt;

&lt;p&gt;I ran some local tests against the Movielens 1M dataset on my notebook:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;# downloaded and converted the movielens 1M dataset to mahout&apos;s common format for ratings
cat /path/to/ratings.dat |sed -e s/::/,/g| cut -d, -f1,2,3 &amp;gt; /path/to/ratings.csv

# create a 90% percent training set and a 10% probe set
bin/mahout splitDataset --input /path/to/ratings.csv --output /tmp/dataset --trainingPercentage 0.9 --probePercentage 0.1

# run distributed ALS-WR to factorize the rating matrix based on the training set 
bin/mahout parallelALS --input /tmp/dataset/trainingSet/ --output /tmp/als/out --tempDir /tmp/als/tmp --numFeatures 20 --numIterations 10 --lambda 0.065

# measure the error of the predictions against the probe set
bin/mahout evaluateALS --probes /tmp/dataset/probeSet/ --userFeatures /tmp/als/out/U/ --itemFeatures /tmp/als/out/M/
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Gave RMSE of 0.8564062387241173 and MAE of 0.6791075767551951 in a testrun. &lt;/p&gt;

&lt;p&gt;Unfortunately I don&apos;t have a cluster available currently to test this so I couldn&apos;t use the Netflix dataset.... &lt;/p&gt;

&lt;p&gt;I still don&apos;t see how to automatically learn lambda yet without running lot&apos;s of subsequent M/R jobs...&lt;/p&gt;</comment>
                            <comment id="12974436" author="tdunning" created="Wed, 22 Dec 2010 23:35:25 +0000"  >&lt;blockquote&gt;
&lt;p&gt;I still don&apos;t see how to automatically learn lambda yet without running lot&apos;s of subsequent M/R jobs...&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Can you compute factorizations for multiple values of lambda in one go and then evaluate all of them in one pass?&lt;/p&gt;

&lt;p&gt;This would require that parallelALS accept a list of lambdas and produce multiple outputs&lt;br/&gt;
It would also require that evaluateALS accept multiple models as well.&lt;/p&gt;

&lt;p&gt;It should take about the same amount of time to test against lots of models as it does to test against a single model.  The distributed ALS might exhibit the same properties.&lt;/p&gt;</comment>
                            <comment id="12976238" author="ssc" created="Fri, 31 Dec 2010 11:49:21 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Can you compute factorizations for multiple values of lambda in one go and then evaluate all of them in one pass?&lt;/p&gt;

&lt;p&gt;This would require that parallelALS accept a list of lambdas and produce multiple outputs&lt;br/&gt;
It would also require that evaluateALS accept multiple models as well.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don&apos;t think this could work with the distributed implementation as each iteration needs to see the feature vectors that have been computed in previous iteration with the same lambda value, so I don&apos;t see how to concurrently compute the values for several lambdas. My evaluation code currently depends on the feature matrices fitting into memory which might not always be the case, which could be another bottleneck. &lt;/p&gt;

&lt;p&gt;However in a non-distributed implementation this approach could work, would it help to pick a sample from the input data, try to find a near-optimal lambda on that in a non-distributed way and use that for the distributed computation too? Not sure on this.&lt;/p&gt;

&lt;p&gt;Another issue I&apos;m currently stuck on is how to compute the recommendations after the factorization. The rating matrix is factorized in the feature matrices U and M for users and items and a single rating can easily predicted by multiplying the feature vectors of the user and the item. If we wanna compute batch recommendations for all users we need to find an intelligent way to select &quot;candidate items&quot; to recommend for each users as we can&apos;t simply compute t(U) * M because those are dense matrices.&lt;/p&gt;

&lt;p&gt;Our non-distributed SVDRecommender uses simple cooccurrence to identify those candidate items, so one way I could think of would be to use RowSimilarityJob to find cooccurring items and foreach user to only compute his ratings for items that cooccur with preferred ones. Not sure either if this is the best way to do this.&lt;/p&gt;</comment>
                            <comment id="12979182" author="srowen" created="Sat, 8 Jan 2011 19:24:02 +0000"  >&lt;p&gt;Sebastian are you done with this? I know you added ALSWRFactorizer for example. Is that what this issue covered?&lt;/p&gt;</comment>
                            <comment id="12979194" author="ssc" created="Sat, 8 Jan 2011 20:18:11 +0000"  >&lt;p&gt;No, unfortunately there&apos;s still a lot of open work here. ALSWRFactorizer is just the non-distributed implementation of this algorithm.&lt;/p&gt;

&lt;p&gt;In this issue I&apos;m reaching for a distributed implementation.  The actual matrix factorization part is working, but there are some open problems:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;the factorization needs a regularization parameter called lambda, which heavily influences the quality of the result. I don&apos;t see how to automatically find a near optimal lambda (which would be key requirement for providing a certain ease of use of this algorithm). I have some code in the works that can find a near optimal lambda in a non-distributed way, but I&apos;m not sure whether my approach is mathematically correct, I will put it up for review when I&apos;m done.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;When we have the factorization we can easily estimate single preferences by computing the dot product of the user and item vectors from the factorization. However if this job here should produce recommendations for all users, we cannot naively multiply the transpose of the user features matrix with the item features matrix to estimate all possible preferences as these are dense matrices. We need to find a way to isolate a few candidate items per user, maybe by utilizing item cooccurrence. I&apos;m not sure what&apos;s the best approach here either as this problem is not covered in the paper.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12979265" author="lancenorskog" created="Sun, 9 Jan 2011 04:25:16 +0000"  >&lt;blockquote&gt;&lt;p&gt;the factorization needs a regularization parameter called lambda, which heavily influences the quality of the result.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If you distribute the matrix portions, and run the lambda calculator in each job separately, how closely do they match? If they correlate well, would it work to have a bunch of similar lambdas?&lt;/p&gt;</comment>
                            <comment id="12986825" author="danny.bickson" created="Wed, 26 Jan 2011 03:00:46 +0000"  >&lt;p&gt;Hi!&lt;br/&gt;
I tried to install patch &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-542&quot; title=&quot;MapReduce implementation of ALS-WR&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-542&quot;&gt;&lt;del&gt;MAHOUT-542&lt;/del&gt;&lt;/a&gt;-3 against a clean svn checkout from Mahout trunk.&lt;br/&gt;
it seems that the file AlternateLeastSquaresSolver.java was already inserted into svn, so the code got doubled.&lt;br/&gt;
I tried to edit manually changes. I got several compilation errors. I attach a patch I created (named unifiedpatch)&lt;br/&gt;
after fixing the compilation errors.&lt;/p&gt;

&lt;p&gt;The code compiles, but fails some tests.&lt;/p&gt;

&lt;p&gt;-------------------------------------------------------&lt;br/&gt;
 T E S T S&lt;br/&gt;
-------------------------------------------------------&lt;br/&gt;
Running org.apache.mahout.math.jet.random.NormalTest&lt;br/&gt;
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.437 sec&lt;br/&gt;
Running org.apache.mahout.math.TestMatrixView&lt;br/&gt;
Tests run: 49, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.283 sec&lt;br/&gt;
Running org.apache.mahout.math.TestDenseVector&lt;br/&gt;
Tests run: 41, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.134 sec&lt;br/&gt;
Running org.apache.mahout.math.TestSparseMatrix&lt;br/&gt;
Tests run: 58, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.315 sec&lt;br/&gt;
Running org.apache.mahout.math.jet.stat.ProbabilityTest&lt;br/&gt;
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.162 sec&lt;br/&gt;
Running org.apache.mahout.math.VectorListTest&lt;br/&gt;
Tests run: 58, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.291 sec&lt;br/&gt;
Running org.apache.mahout.math.stats.LogLikelihoodTest&lt;br/&gt;
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.11 sec&lt;br/&gt;
Running org.apache.mahout.math.jet.random.engine.MersenneTwisterTest&lt;br/&gt;
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.049 sec&lt;br/&gt;
Running org.apache.mahout.math.decomposer.hebbian.TestHebbianSolver&lt;br/&gt;
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.071 sec&lt;br/&gt;
Running org.apache.mahout.math.TestDenseMatrix&lt;br/&gt;
Tests run: 58, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.054 sec&lt;br/&gt;
Running org.apache.mahout.math.jet.random.ExponentialTest&lt;br/&gt;
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.707 sec&lt;br/&gt;
Running org.apache.mahout.math.jet.stat.GammaTest&lt;br/&gt;
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.193 sec&lt;br/&gt;
Running org.apache.mahout.math.stats.OnlineSummarizerTest&lt;br/&gt;
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.06 sec&lt;br/&gt;
Running org.apache.mahout.math.VectorTest&lt;br/&gt;
Tests run: 20, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.048 sec&lt;br/&gt;
Running org.apache.mahout.common.RandomUtilsTest&lt;br/&gt;
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.022 sec&lt;br/&gt;
Running org.apache.mahout.math.TestSparseColumnMatrix&lt;br/&gt;
Tests run: 58, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.104 sec&lt;br/&gt;
Running org.apache.mahout.math.TestSequentialAccessSparseVector&lt;br/&gt;
Tests run: 41, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.05 sec&lt;br/&gt;
Running org.apache.mahout.math.QRDecompositionTest&lt;br/&gt;
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.045 sec&lt;br/&gt;
Running org.apache.mahout.math.TestSparseRowMatrix&lt;br/&gt;
Tests run: 58, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.09 sec&lt;br/&gt;
Running org.apache.mahout.math.TestRandomAccessSparseVector&lt;br/&gt;
Tests run: 41, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.036 sec&lt;br/&gt;
Running org.apache.mahout.math.TestOrderedIntDoubleMapping&lt;br/&gt;
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.003 sec&lt;br/&gt;
Running org.apache.mahout.math.als.AlternateLeastSquaresSolverTest&lt;br/&gt;
Tests run: 4, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.009 sec &amp;lt;&amp;lt;&amp;lt; FAILURE!&lt;br/&gt;
Running org.apache.mahout.math.jet.random.NegativeBinomialTest&lt;br/&gt;
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.008 sec&lt;br/&gt;
Running org.apache.mahout.math.TestVectorView&lt;br/&gt;
Tests run: 36, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.054 sec&lt;br/&gt;
Running org.apache.mahout.math.jet.random.GammaTest&lt;br/&gt;
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.72 sec&lt;br/&gt;
Running org.apache.mahout.math.TestSingularValueDecomposition&lt;br/&gt;
Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.016 sec&lt;br/&gt;
Running org.apache.mahout.math.decomposer.lanczos.TestLanczosSolver&lt;br/&gt;
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.875 sec&lt;/p&gt;

&lt;p&gt;Results :&lt;/p&gt;

&lt;p&gt;Failed tests:&lt;/p&gt;

&lt;p&gt;createRiIiMaybeTransposedExceptionOnNonSequentialVector(org.apache.mahout.math.als.AlternateLeastSquaresSolverTest)&lt;/p&gt;

&lt;p&gt;ubuntu@ip-10-195-226-63:/usr/local/mahout-0.4$ cat math/target/surefire-reports/org.apache.mahout.math.als.AlternateLeastSquaresSolverTest.txt&lt;br/&gt;
-------------------------------------------------------------------------------&lt;br/&gt;
Test set: org.apache.mahout.math.als.AlternateLeastSquaresSolverTest&lt;br/&gt;
-------------------------------------------------------------------------------&lt;br/&gt;
Tests run: 4, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.009 sec &amp;lt;&amp;lt;&amp;lt; FAILURE!&lt;br/&gt;
createRiIiMaybeTransposedExceptionOnNonSequentialVector(org.apache.mahout.math.als.AlternateLeastSquaresSolverTest) Time elapsed: 0.002 sec  &amp;lt;&amp;lt;&amp;lt; FAILURE!&lt;br/&gt;
java.lang.AssertionError:&lt;br/&gt;
       at org.junit.Assert.fail(Assert.java:91)&lt;br/&gt;
       at org.junit.Assert.fail(Assert.java:98)&lt;br/&gt;
       at org.apache.mahout.math.als.AlternateLeastSquaresSolverTest.createRiIiMaybeTransposedExceptionOnNonSequentialVector(AlternateLeastSquaresSolverTest.java:94)&lt;br/&gt;
       at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
       at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)&lt;br/&gt;
       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
       at java.lang.reflect.Method.invoke(Method.java:616)&lt;br/&gt;
       at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)&lt;br/&gt;
       at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)&lt;br/&gt;
       at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)&lt;br/&gt;
       at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)&lt;br/&gt;
       at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)&lt;br/&gt;
       at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)&lt;br/&gt;
       at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)&lt;br/&gt;
       at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)&lt;br/&gt;
       at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)&lt;br/&gt;
       at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)&lt;br/&gt;
       at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)&lt;br/&gt;
       at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)&lt;br/&gt;
       at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)&lt;br/&gt;
       at org.junit.runners.ParentRunner.run(ParentRunner.java:236)&lt;br/&gt;
       at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)&lt;br/&gt;
       at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:115)&lt;br/&gt;
       at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:102)&lt;br/&gt;
       at org.apache.maven.surefire.Surefire.run(Surefire.java:180)&lt;br/&gt;
       at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
       at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)&lt;br/&gt;
       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
       at java.lang.reflect.Method.invoke(Method.java:616)&lt;br/&gt;
       at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:350)&lt;br/&gt;
       at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1021)&lt;/p&gt;


&lt;p&gt;Can you please take a look and instruct me what to do?&lt;/p&gt;

&lt;p&gt;Thanks,&lt;/p&gt;

&lt;p&gt;Danny Bickson&lt;/p&gt;</comment>
                            <comment id="12986859" author="ssc" created="Wed, 26 Jan 2011 06:18:37 +0000"  >&lt;p&gt;Hi Danny,&lt;/p&gt;

&lt;p&gt;you&apos;re right the latest patch is out of sync with the current trunk as AlternateLeastSquaresSolver was already committed for &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-572&quot; title=&quot;Non-distributed implementation of ALS-WR matrix factorization&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-572&quot;&gt;&lt;del&gt;MAHOUT-572&lt;/del&gt;&lt;/a&gt;. I&apos;m a little busy currently but I&apos;ll try to look at your patch this weekend, hope that&apos;s ok for you.&lt;/p&gt;

&lt;p&gt;--sebastian&lt;/p&gt;</comment>
                            <comment id="12987175" author="danny.bickson" created="Wed, 26 Jan 2011 19:27:05 +0000"  >&lt;p&gt;Thanks Sebastian!&lt;/p&gt;

&lt;p&gt;I have access to some large clusters, once you are done I will be happily help you test the current patch on Netflix or other larger datasets.&lt;/p&gt;

&lt;p&gt;Best, &lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Danny&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12987359" author="danny.bickson" created="Thu, 27 Jan 2011 02:55:42 +0000"  >&lt;p&gt;Hi everyone,&lt;br/&gt;
Issue solved. I&apos;ve created a new patch and tested it to work on movielens 1M dataset. On amaon EC2 machine it takes 1998 seconds (large instance).&lt;br/&gt;
With an accuracy of: RMSE: 0.8546120366924382, MAE: 0.6798083002225481&lt;/p&gt;

&lt;p&gt;It seems that 3 files where already included in the patch while being in SVN.&lt;br/&gt;
Another issue was line 94 of math.als.AlternateLeastSquaresSolverTest.java &lt;br/&gt;
which calls fail(). This killed the unit testing. Commenting it and running the rest of the command did not seem to affect the running results.&lt;/p&gt;

&lt;p&gt;Please take a look into this.&lt;/p&gt;

&lt;p&gt;Best, &lt;/p&gt;


&lt;p&gt;Danny Bickson&lt;/p&gt;</comment>
                            <comment id="12988065" author="ssc" created="Fri, 28 Jan 2011 12:54:25 +0000"  >&lt;p&gt;updated the patch to work with the current trunk. I only had to remove AlternateLeastSquaresSolver as this class was already committed.&lt;/p&gt;

&lt;p&gt;btw: the call to fail() in the unit test is valid, it&apos;s used to make a test fail when a exception is not thrown that you expect to be thrown&lt;/p&gt;


&lt;p&gt;It would be great if you use the current version of this patch to run the factorization on the netflix dataset with the parameters described in the paper. I&apos;d love to see if we get roughly the same results.&lt;/p&gt;</comment>
                            <comment id="12988069" author="danny.bickson" created="Fri, 28 Jan 2011 13:12:23 +0000"  >&lt;p&gt;Hi again,&lt;br/&gt;
When I run the testing previous time (after removing AlternateLeastSquareSolver) I got the test error as described in my previous post (line 94 of the test org.apache.mahout.math.als.AlternateLeastSquaresSolverTest), fail() was called since exception was not thrown.&lt;/p&gt;

&lt;p&gt;Did you make changes in the patch to address this problem?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;

&lt;p&gt;DB&lt;/p&gt;</comment>
                            <comment id="12988091" author="ssc" created="Fri, 28 Jan 2011 14:40:10 +0000"  >&lt;p&gt;Did you apply the patch on the current trunk? Without any other of the patches attached here applied? I did this on my local machine, ran the tests and everything worked fine.&lt;/p&gt;</comment>
                            <comment id="12991718" author="danny.bickson" created="Mon, 7 Feb 2011 23:48:43 +0000"  >&lt;p&gt;Hi,&lt;br/&gt;
Everything works now with the new patch (542-5). With the MovieLens 1M data everything works fine, I have tested with one, two and four slaves.&lt;br/&gt;
With Netflix data, I get the following exception:&lt;/p&gt;

&lt;p&gt;2011-02-04 19:42:45,613 INFO org.apache.hadoop.mapred.TaskInProgress: Error from attempt_201102041322_0007_r_000000_0: Error: GC overhead limit exceeded&lt;br/&gt;
2011-02-04 19:42:45,614 INFO org.apache.hadoop.mapred.JobTracker: Adding task (cleanup)&apos;attempt_201102041322_0007_r_000000_0&apos; to tip task_201102041322_0007_r_000000, for tracker &apos;tracker_ip-10-202-161-172.ec2.internal:localhost/127.0.0.1:49339&apos;&lt;/p&gt;

&lt;p&gt;2011-02-04 19:42:48,617 INFO org.apache.hadoop.mapred.JobTracker: Adding task &apos;attempt_201102041322_0007_r_000000_1&apos; to tip task_201102041322_0007_r_000000, for tracker &apos;tracker_ip-10-202-161-172.ec2.internal:localhost/127.0.0.1:49339&apos;&lt;/p&gt;

&lt;p&gt;2011-02-04 19:42:48,618 INFO org.apache.hadoop.mapred.JobTracker: Removed completed task &apos;attempt_201102041322_0007_r_000000_0&apos; from &apos;tracker_ip-10-202-161-172.ec2.internal:localhost/127.0.0.1:49339&apos;&lt;/p&gt;

&lt;p&gt;2011-02-04 21:10:48,014 INFO org.apache.hadoop.mapred.TaskInProgress: Error from attempt_201102041322_0007_r_000000_1: Error: GC overhead limit exceeded&lt;br/&gt;
2011-02-04 21:10:48,030 INFO org.apache.hadoop.mapred.JobTracker: Adding task (cleanup)&apos;attempt_201102041322_0007_r_000000_1&apos; to tip task_201102041322_0007_r_000000, for tracker &apos;tracker_ip-10-202-161-172.ec2.internal:localhost/127.0.0.1:49339&apos;&lt;/p&gt;

&lt;p&gt;2011-02-04 21:10:54,036 INFO org.apache.hadoop.mapred.JobTracker: Adding task &apos;attempt_201102041322_0007_r_000000_2&apos; to tip task_201102041322_0007_r_000000, for tracker &apos;tracker_ip-10-202-161-172.ec2.internal:localhost/127.0.0.1:49339&apos;&lt;/p&gt;

&lt;p&gt;2011-02-04 21:10:54,036 INFO org.apache.hadoop.mapred.JobTracker: Removed completed task &apos;attempt_201102041322_0007_r_000000_1&apos; from &apos;tracker_ip-10-202-161-172.ec2.internal:localhost/127.0.0.1:49339&apos;&lt;/p&gt;

&lt;p&gt;2011-02-04 22:36:46,339 INFO org.apache.hadoop.mapred.TaskInProgress: Error from attempt_201102041322_0007_r_000000_2: Error: GC overhead limit exceeded&lt;br/&gt;
2011-02-04 22:36:46,339 INFO org.apache.hadoop.mapred.JobTracker: Adding task (cleanup)&apos;attempt_201102041322_0007_r_000000_2&apos; to tip task_201102041322_0007_r_000000, for tracker &apos;tracker_ip-10-202-161-172.ec2.internal:localhost/127.0.0.1:49339&apos;&lt;/p&gt;

&lt;p&gt;2011-02-04 22:36:49,342 INFO org.apache.hadoop.mapred.JobTracker: Adding task &apos;attempt_201102041322_0007_r_000000_3&apos; to tip task_201102041322_0007_r_000000, for tracker &apos;tracker_ip-10-202-161-172.ec2.internal:localhost/127.0.0.1:49339&apos;&lt;/p&gt;

&lt;p&gt;2011-02-04 22:36:49,355 INFO org.apache.hadoop.mapred.JobTracker: Removed completed task &apos;attempt_201102041322_0007_r_000000_2&apos; from &apos;tracker_ip-10-202-161-172.ec2.internal:localhost/127.0.0.1:49339&apos;&lt;/p&gt;


&lt;p&gt;Any ideas about how to fix this?&lt;/p&gt;

&lt;p&gt;Thanks!!&lt;/p&gt;

&lt;p&gt;Danny Bickson&lt;/p&gt;</comment>
                            <comment id="12991722" author="ssc" created="Mon, 7 Feb 2011 23:54:43 +0000"  >&lt;p&gt;Can you share some more details about the machines you ran this on?&lt;/p&gt;</comment>
                            <comment id="12991811" author="lancenorskog" created="Tue, 8 Feb 2011 06:32:12 +0000"  >&lt;blockquote&gt;&lt;p&gt;GC overhead limit exceeded&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;This means that the JVM spends 98% of its time doing garbage collection. It is usually a sign of &quot;not quite enough memory&quot;. There are other JVM parameters around garbage collection that you can tune: change the algorithm, change specific parameters. &apos;jvisualvm&apos; or &apos;visualgc&apos; is a free program in Java 1.6. You can grab onto the running Map/Reduce app and watch the garbage collection and memory allocation levels. &lt;/p&gt;</comment>
                            <comment id="12991835" author="ssc" created="Tue, 8 Feb 2011 08:21:23 +0000"  >&lt;p&gt;I also think it&apos;s a memory problem. When recomputing user features the algorithm needs to look at the feature vectors of all movies the user has rated and when recomputing movie features the algorithm has to look at the feature vectors of all users which have rated this movie. I&apos;m not very familiar with the netflix dataset but I think there might be some very popular movies that have been rated by lots of users and there might also be some &quot;power&quot; users that have rated lots of movies. So the memory consumption might get very high in some steps.&lt;/p&gt;

&lt;p&gt;What kind of ec2 instances did you run this on? Did you use small instances? From my experience those are not very helpful, maybe you could retry this with large or c1.medium instances.&lt;/p&gt;</comment>
                            <comment id="12992108" author="danny.bickson" created="Tue, 8 Feb 2011 19:01:52 +0000"  >&lt;p&gt;Hi!&lt;br/&gt;
I used two m2.xlarge EC2 machines.&lt;br/&gt;
I attach the program output (see attachment file logs.zip above), including all the Hadoop configuration files from the two machines and all the Hadoop logs. It seems that I got the GC exception again. Let me know if you identify a reason why I am getting out of memory.&lt;/p&gt;

&lt;p&gt;Thanks, &lt;/p&gt;

&lt;p&gt;Danny Bickson&lt;/p&gt;</comment>
                            <comment id="12994872" author="danny.bickson" created="Tue, 15 Feb 2011 16:38:41 +0000"  >&lt;p&gt;Problem solved - &lt;br/&gt;
1)I have increased heap size to 4GB&lt;br/&gt;
2)Moved to a larger instance: m2.2xlarge&lt;br/&gt;
3)Increased children mappers memory to 2GB&lt;/p&gt;

&lt;p&gt;One or more of those changes fixed the memory error.&lt;/p&gt;

&lt;p&gt;One iteration using the full netflix data takes around 75 minutes.&lt;br/&gt;
RMSE looks good: 1.04 after one iteration.&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Danny&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12994874" author="ssc" created="Tue, 15 Feb 2011 16:42:37 +0000"  >&lt;p&gt;Wow, that&apos;s really awesome news. Do you plan on doing more experiments?&lt;/p&gt;</comment>
                            <comment id="12994875" author="danny.bickson" created="Tue, 15 Feb 2011 16:48:21 +0000"  >&lt;p&gt;Yes, I am doing some more experiments, because we are comparing our system, GraphLab&lt;br/&gt;
&lt;a href=&quot;http://www.graphlab.ml.cmu.edu/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.graphlab.ml.cmu.edu/&lt;/a&gt; to Mahout.&lt;/p&gt;

&lt;p&gt;I am not sure if this is the right forum to update about performance, but I will be glad to&lt;br/&gt;
update anyone interested - email me...&lt;/p&gt;</comment>
                            <comment id="13006033" author="ssc" created="Sat, 12 Mar 2011 11:36:58 +0000"  >&lt;p&gt;Attached a new version of the patch. I&apos;d like to commit this one in the next days, if there are no objections (and no errors found). This patches removes some parts of the code that were highly memory intensive and hopefully enables tests with a higher number of features. It introduces a set of tools that might enable a first realworld usage of this algorithm:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;DatasetSplitter: split a rating dataset into training and probe parts&lt;/li&gt;
	&lt;li&gt;ParallelALSFactorizationJob: parallel ALS-WR factorization of a rating matrix&lt;/li&gt;
	&lt;li&gt;PredictionJob: predict preferences using the factorization of a rating matrix&lt;/li&gt;
	&lt;li&gt;InMemoryFactorizationEvaluator: compute RMSE of a rating matrix factorization against probes in memory&lt;/li&gt;
	&lt;li&gt;ParallelFactorizationEvaluator: compute RMSE of a rating matrix factorization against probes&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;There are still open points, in particular how to find a good regularization parameter automatically and efficiently and how to create an automated recommender pipeline similar to that of RecommenderJob using these tools. But I think these issues can be tackled in the future.&lt;/p&gt;

&lt;p&gt;Here&apos;s how to play with the code:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;# convert the movielens 1M dataset to mahout&apos;s common format for ratings
cat /path/to/ratings.dat |sed -e s/::/,/g| cut -d, -f1,2,3 &amp;gt; /path/to/ratings.csv

# create a 90% percent training set and a 10% probe set
bin/mahout splitDataset --input /path/to/ratings.csv --output /tmp/dataset --trainingPercentage 0.9 --probePercentage 0.1

# run distributed ALS-WR to factorize the rating matrix based on the training set
bin/mahout parallelALS --input /tmp/dataset/trainingSet/ --output /tmp/als/out --tempDir /tmp/als/tmp --numFeatures 20 --numIterations 10 --lambda 0.065

# compute predictions against the probe set, measure the error
bin/mahout evaluateFactorizationParallel --output /tmp/als/rmse --pairs /tmp/dataset/probeSet/ --userFeatures /tmp/als/out/U/ --itemFeatures /tmp/als/out/M/

# print the error
cat /tmp/als/rmse/rmse.txt 
0.8531723318490103

# alternatively you can use the factorization to predict unknown ratings
bin/mahout predictFromFactorization --output /tmp/als/predict --pairs /tmp/dataset/probeSet/ --userFeatures /tmp/als/out/U/ --itemFeatures /tmp/als/out/M/ --tempDir /tmp/als/predictTmp

# look at the predictions
cat /tmp/als/predict/part-r-*
1,150,4.0842405867880975
1,1029,4.163510579205656
1,745,3.7759166479388777
1,2294,3.495085673991081
1,938,3.6820865362790594
2,2067,3.8303249557251644
2,1090,3.954322089979675
2,1196,3.912089186677311
2,498,2.820740198815573
2,593,4.090550572202017
...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13006109" author="tdunning" created="Sat, 12 Mar 2011 23:33:10 +0000"  >&lt;p&gt;Sebastian,&lt;/p&gt;

&lt;p&gt;I don&apos;t want to derail your commit, but your question about regularization suggested a thought to me.&lt;/p&gt;

&lt;p&gt;One of the great advantages of the random projection methods over power law methods is due to the fact that iteration is so evil in Hadoop-base map-reduce, especially when you are simply reading the same input over and over.&lt;/p&gt;

&lt;p&gt;With ALS-WR, you can run the program again for each value of regularization parameter, but there is really nothing except possibly memory size from running all of these optimizations at the same time.&lt;/p&gt;

&lt;p&gt;How hard would that be, do you think, to interleave the computations for multiple values of regularization parameter into a single run of ALS-WR?&lt;/p&gt;</comment>
                            <comment id="13006350" author="ssc" created="Mon, 14 Mar 2011 08:38:16 +0000"  >&lt;p&gt;Hi Ted,&lt;/p&gt;

&lt;p&gt;I already spent some thinking on that, unfortunately no easy way to do that came on my mind. I&apos;ll share my line of thoughts, maybe there&apos;s some error in how I understand the algorithm:&lt;/p&gt;

&lt;p&gt;The goal of ALS-WR is to factorize the rating matrix R into the user feature matrix U and the item feature matrix M. I tries to minimize the regularized squared error between R und U(T)M via an iterative algorithm. We start with a randomly initialized M, use this to compute an optimized U, fix U after that to compute an optimized M. We rotate between a fixed U and a fixed M until the error converges (or a maximum number of iterations is reached).&lt;/p&gt;

&lt;p&gt;The question is now whether we can modify this process to use multiple lambda values (the regularization parameter used for the internal computations). You stated that we are &quot;reading the same input over and over&quot;, which I don&apos;t see as all versions of U and M (with the exception of the first randomly initialized version of M) are dependent on the lambda that was used to produce them. So as I understand the algorithm, n values for lambda would not only mean n times the memory for computations is needed but also n versions of each U and M would need to be moved around. I think I could find a way to make the code do that (by maybe using something like a multi-version-vector that carries different results for different lambdas), but I&apos;m not sure whether that&apos;s what you had in mind.&lt;/p&gt;</comment>
                            <comment id="13006746" author="tdunning" created="Tue, 15 Mar 2011 00:39:39 +0000"  >&lt;p&gt;Yes.&lt;/p&gt;

&lt;p&gt;Keeping multiple versions in the computation is exactly what I had in mind.  Whether this is useful or not depends on whether each version of U and M are smaller than the original.  They certainly will be more efficient to read.  As such, reading them multiple times might be a win.&lt;/p&gt;

&lt;p&gt;Or not.  I haven&apos;t worked out the details and that is what really, really matters here.&lt;/p&gt;</comment>
                            <comment id="13008464" author="ssc" created="Fri, 18 Mar 2011 15:01:05 +0000"  >&lt;p&gt;I have found a way to make the code do the factorization for multiple lambdas at once by making the intermediate writables carry an extra feature vector for each extra lambda. How do we proceed now, do we try to automatically find a near-optimal lambda or do we leave it up to the user to experiment a little?&lt;/p&gt;</comment>
                            <comment id="13009633" author="srowen" created="Tue, 22 Mar 2011 13:10:58 +0000"  >&lt;p&gt;In terms of this issue, it sounds like you&apos;ve made enough progress that it&apos;s worth committing so people can start to use it. Sounds like it works and you&apos;re just thinking about future optimizations.&lt;/p&gt;</comment>
                            <comment id="13010033" author="ssc" created="Wed, 23 Mar 2011 09:09:39 +0000"  >&lt;p&gt;I think so too, we should commit this now and reiterate for automatic lambda detection and optimization, are you ok with this Ted?&lt;/p&gt;</comment>
                            <comment id="13010212" author="tdunning" created="Wed, 23 Mar 2011 16:47:37 +0000"  >&lt;p&gt;Progress always sounds good to me.  It is even better when there is a plan for further progress.&lt;/p&gt;

&lt;p&gt;I say ship it!&lt;/p&gt;</comment>
                            <comment id="13010530" author="hudson" created="Thu, 24 Mar 2011 00:41:09 +0000"  >&lt;p&gt;Integrated in Mahout-Quality #690 (See &lt;a href=&quot;https://hudson.apache.org/hudson/job/Mahout-Quality/690/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://hudson.apache.org/hudson/job/Mahout-Quality/690/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-542&quot; title=&quot;MapReduce implementation of ALS-WR&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-542&quot;&gt;&lt;del&gt;MAHOUT-542&lt;/del&gt;&lt;/a&gt; MapReduce implementation of ALS-WR&lt;/p&gt;</comment>
                            <comment id="13018803" author="hudson" created="Tue, 12 Apr 2011 13:42:56 +0100"  >&lt;p&gt;Integrated in Mahout-Quality #742 (See &lt;a href=&quot;https://hudson.apache.org/hudson/job/Mahout-Quality/742/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://hudson.apache.org/hudson/job/Mahout-Quality/742/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-542&quot; title=&quot;MapReduce implementation of ALS-WR&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-542&quot;&gt;&lt;del&gt;MAHOUT-542&lt;/del&gt;&lt;/a&gt; missing evaluation classes&lt;/p&gt;</comment>
                            <comment id="13066668" author="camclive" created="Sun, 17 Jul 2011 16:10:23 +0100"  >&lt;p&gt;An earlier comment said:&lt;br/&gt;
&quot;...if this job here should produce recommendations for all users, we cannot naively multiply the transpose of the user features matrix with the item features matrix to estimate all possible preferences as these are dense matrices.&quot;&lt;/p&gt;

&lt;p&gt;Can people explain further the problems with the naive approach?&lt;/p&gt;

&lt;p&gt;How are people generally deriving recommendations from Matrix Factorization techniques? Get a neighnourhood from some other CF algorithm and then score only that neighnbourhood using the derived matrices?&lt;/p&gt;</comment>
                            <comment id="13066825" author="ssc" created="Mon, 18 Jul 2011 08:16:31 +0100"  >&lt;p&gt;The problem with this naive approach is that the resulting matrix is going to be huge (millions of users times hundred thousands of items) and dense, which makes it uncomputable.&lt;/p&gt;

&lt;p&gt;I&apos;m not aware of a general approach of computing recommendations from matrix decompositions, in the scientific literature these are only used for measuring the prediction error on held out data (as far as I know)&lt;/p&gt;</comment>
                            <comment id="13070018" author="pragatimeena" created="Sat, 23 Jul 2011 19:31:06 +0100"  >&lt;p&gt;hi sebastian, &lt;/p&gt;

&lt;p&gt;i am trying to run the example in windows using hadoop on cygwin , but i keep getting the following error ,even  though history file exists at &lt;br/&gt;
same directory location &lt;/p&gt;

&lt;p&gt;Exception in thread &quot;main&quot; java.lang.IllegalStateException: java.io.FileNotFoundException: File does not exist: /user/hadoop/temp/errors/_logs&lt;br/&gt;
at org.apache.mahout.common.iterator.sequencefile.SequenceFileDirIterator$1.apply(SequenceFileDirIterator.java:73)&lt;br/&gt;
at org.apache.mahout.common.iterator.sequencefile.SequenceFileDirIterator$1.apply(SequenceFileDirIterator.java:67)&lt;br/&gt;
at com.google.common.collect.Iterators$8.next(Iterators.java:730)&lt;br/&gt;
at com.google.common.collect.Iterators$5.hasNext(Iterators.java:508)&lt;br/&gt;
at com.google.common.collect.ForwardingIterator.hasNext(ForwardingIterator.java:40)&lt;br/&gt;
at org.apache.mahout.utils.eval.ParallelFactorizationEvaluator.computeRmse(ParallelFactorizationEvaluator.java:111)&lt;/p&gt;

&lt;p&gt;Any ideas on how to fix this &lt;/p&gt;

&lt;p&gt;regards &lt;/p&gt;

&lt;p&gt;Pragati Meena &lt;/p&gt;</comment>
                            <comment id="13107453" author="faal" created="Sun, 18 Sep 2011 15:56:17 +0100"  >&lt;p&gt;Hi, I was thinking of rewriting the itemRatings and userRatings job into one job using MultipleOutputs. Based on my understanding release 0.20.2* supports MultipleOutputs, although using deprecated APIS. Would such a patch be accepted or are there issues prohibiting such a change? &lt;/p&gt;

&lt;p&gt;What is the current target version of Hadoop?&lt;/p&gt;</comment>
                            <comment id="13107454" author="ssc" created="Sun, 18 Sep 2011 16:03:06 +0100"  >&lt;p&gt;Current hadoop version is 0.20.204.0&lt;/p&gt;

&lt;p&gt;I don&apos;t think that the ALS-WR is really used at the moment and I don&apos;t think it fits the MapReduce paradigm very well, so you&apos;d have my go in playing with the code. We still use the deprecated APIs for a lot of our distributed linear algebra stuff, so I&apos;d be okay with it. But I&apos;d like to hear other opinions first.&lt;/p&gt;</comment>
                            <comment id="13107458" author="faal" created="Sun, 18 Sep 2011 16:19:00 +0100"  >&lt;p&gt;Okay. I&apos;ll wait a bit to see if anyone objects.&lt;/p&gt;

&lt;p&gt;I&apos;m going to implement &lt;a href=&quot;http://research.yahoo.com/pub/2433&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;Collaborative Filtering for Implicit Feedback Datasets&lt;/a&gt; and my plan was to try and reuse as much of the ParallelAlsFactorization code as possible. Do you see any issues with this?&lt;/p&gt;</comment>
                            <comment id="13107508" author="tdunning" created="Sun, 18 Sep 2011 20:13:11 +0100"  >&lt;p&gt;Go for it.  The Hadoop API is very confused at the moment in any case.&lt;/p&gt;

&lt;p&gt;Use whatever you like from 0.20.204&lt;/p&gt;</comment>
                            <comment id="13118267" author="alvina" created="Fri, 30 Sep 2011 19:38:58 +0100"  >&lt;p&gt;Hi Sebastian,&lt;/p&gt;

&lt;p&gt;First of all, many thanks for contributing this ALS implementation. It&apos;s very useful. Like others on this list, I&apos;m trying to run some experiments on it using the Netflix data, but I&apos;m seeing an error I am having trouble diagnosing. After completing the first 4 jobs, reduce copiers are failing for the 5th job (Mapper-SolvingReducer). I&apos;m running on hadoop-0.20.2 and checked out mahout from the trunk, so I believe any patches you&apos;ve mentioned should be incorporated. &lt;/p&gt;

&lt;p&gt;Here is a description of the job I&apos;m running: MAHOUT-JOB: /home/auyoung/mahout/examples/target/mahout-examples-0.6-SNAPSHOT-job.jar&lt;br/&gt;
11/09/30 01:20:56 INFO common.AbstractJob: Command line arguments: {--endPhase=2147483647, --input=training_all_triplets_norm, --lambda=0.065, --numFeatures=25, --numIterations=5, --output=als.out, --startPhase=0, --tempDir=temp}&lt;/p&gt;

&lt;p&gt;Do you have any ideas what might be wrong? I&apos;m running it on a physical cluster of 20 slaves, each with 2 mappers and reducers, and there is &amp;gt; 8 GB memory (per jvm), &amp;gt; 2 GB HADOOP_HEAPSIZE, and the maximum allowable io.sort.mb of 2047. Also, there is plenty of disk space remaining. Here is a transcript of one of the several failures on the ParallelALSFactorizationJob-Mapper-SolvingReducer:&lt;/p&gt;

&lt;p&gt;2011-09-30 02:05:37,115 INFO org.apache.hadoop.mapred.Merger: Merging 16 sorted segments&lt;br/&gt;
2011-09-30 02:05:37,115 INFO org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 16 segments left of total size: 1039493457 bytes&lt;br/&gt;
2011-09-30 02:05:37,116 WARN org.apache.hadoop.mapred.ReduceTask: attempt_201109300120_0005_r_000000_0 Merge of the inmemory files threw an exception: java.io.IOException: Intermediate merge failed&lt;br/&gt;
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$InMemFSMergeThread.doInMemMerge(ReduceTask.java:2576)&lt;br/&gt;
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$InMemFSMergeThread.run(ReduceTask.java:2501)&lt;br/&gt;
Caused by: java.lang.RuntimeException: java.io.EOFException&lt;br/&gt;
	at org.apache.hadoop.io.WritableComparator.compare(WritableComparator.java:103)&lt;br/&gt;
	at org.apache.hadoop.mapred.Merger$MergeQueue.lessThan(Merger.java:373)&lt;br/&gt;
	at org.apache.hadoop.util.PriorityQueue.downHeap(PriorityQueue.java:136)&lt;br/&gt;
	at org.apache.hadoop.util.PriorityQueue.adjustTop(PriorityQueue.java:103)&lt;br/&gt;
	at org.apache.hadoop.mapred.Merger$MergeQueue.adjustPriorityQueue(Merger.java:335)&lt;br/&gt;
	at org.apache.hadoop.mapred.Merger$MergeQueue.next(Merger.java:350)&lt;br/&gt;
	at org.apache.hadoop.mapred.Merger.writeFile(Merger.java:156)&lt;br/&gt;
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$InMemFSMergeThread.doInMemMerge(ReduceTask.java:2560)&lt;br/&gt;
	... 1 more&lt;br/&gt;
Caused by: java.io.EOFException&lt;br/&gt;
	at java.io.DataInputStream.readByte(DataInputStream.java:250)&lt;br/&gt;
	at org.apache.mahout.math.Varint.readUnsignedVarInt(Varint.java:159)&lt;br/&gt;
	at org.apache.mahout.math.Varint.readSignedVarInt(Varint.java:140)&lt;br/&gt;
	at org.apache.mahout.cf.taste.hadoop.als.IndexedVarIntWritable.readFields(IndexedVarIntWritable.java:64)&lt;br/&gt;
	at org.apache.hadoop.io.WritableComparator.compare(WritableComparator.java:97)&lt;br/&gt;
	... 8 more&lt;/p&gt;

&lt;p&gt;2011-09-30 02:05:37,116 WARN org.apache.hadoop.mapred.ReduceTask: attempt_201109300120_0005_r_000000_0 Merging of the local FS files threw an exception: java.io.IOException: java.lang.RuntimeException: java.io.EOFException&lt;br/&gt;
	at org.apache.hadoop.io.WritableComparator.compare(WritableComparator.java:103)&lt;br/&gt;
	at org.apache.hadoop.mapred.Merger$MergeQueue.lessThan(Merger.java:373)&lt;br/&gt;
	at org.apache.hadoop.util.PriorityQueue.downHeap(PriorityQueue.java:139)&lt;br/&gt;
	at org.apache.hadoop.util.PriorityQueue.adjustTop(PriorityQueue.java:103)&lt;br/&gt;
	at org.apache.hadoop.mapred.Merger$MergeQueue.adjustPriorityQueue(Merger.java:335)&lt;br/&gt;
	at org.apache.hadoop.mapred.Merger$MergeQueue.next(Merger.java:350)&lt;br/&gt;
	at org.apache.hadoop.mapred.Merger.writeFile(Merger.java:156)&lt;br/&gt;
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$LocalFSMerger.run(ReduceTask.java:2454)&lt;br/&gt;
Caused by: java.io.EOFException&lt;br/&gt;
	at java.io.DataInputStream.readByte(DataInputStream.java:250)&lt;br/&gt;
	at org.apache.mahout.math.Varint.readUnsignedVarInt(Varint.java:159)&lt;br/&gt;
	at org.apache.mahout.math.Varint.readSignedVarInt(Varint.java:140)&lt;br/&gt;
	at org.apache.mahout.cf.taste.hadoop.als.IndexedVarIntWritable.readFields(IndexedVarIntWritable.java:64)&lt;br/&gt;
	at org.apache.hadoop.io.WritableComparator.compare(WritableComparator.java:100)&lt;br/&gt;
	... 7 more&lt;/p&gt;

&lt;p&gt;	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$LocalFSMerger.run(ReduceTask.java:2458)&lt;/p&gt;

&lt;p&gt;Thanks,&lt;/p&gt;

&lt;p&gt;Alvin&lt;/p&gt;</comment>
                            <comment id="13150419" author="cendrillon" created="Tue, 15 Nov 2011 12:07:39 +0000"  >&lt;p&gt;I&apos;d like to get more involved in contributing to Mahout. In particular if there&apos;s any area you need support regarding ALS-WR or other topics as well I&apos;d be very happy to lend a hand. &lt;/p&gt;

&lt;p&gt;In particular I was quite interested in your comments on automatically finding a good setting for lambda. I&apos;m wondering whether something more sophisticated could be done than exhaustive search, for example if the loss function evaluated on the hold-out dataset is a convex function of lambda then gradient descent (or quasi-Newton methods) could be used.&lt;/p&gt;</comment>
                            <comment id="13150471" author="ssc" created="Tue, 15 Nov 2011 13:18:23 +0000"  >&lt;p&gt;Its an interesting idea. Please open a new jira issue for it, as this is already closed and was thought to only represent the initial implementation.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12459536" name="MAHOUT-452.patch" size="46033" author="ssc" created="Sat, 13 Nov 2010 22:18:23 +0000"/>
                            <attachment id="12466667" name="MAHOUT-542-2.patch" size="39371" author="ssc" created="Mon, 20 Dec 2010 23:27:02 +0000"/>
                            <attachment id="12466838" name="MAHOUT-542-3.patch" size="53452" author="ssc" created="Wed, 22 Dec 2010 21:41:54 +0000"/>
                            <attachment id="12469509" name="MAHOUT-542-4.patch" size="7708" author="danny.bickson" created="Thu, 27 Jan 2011 02:55:42 +0000"/>
                            <attachment id="12469671" name="MAHOUT-542-5.patch" size="48422" author="ssc" created="Fri, 28 Jan 2011 12:54:25 +0000"/>
                            <attachment id="12473470" name="MAHOUT-542-6.patch" size="88899" author="ssc" created="Sat, 12 Mar 2011 11:24:42 +0000"/>
                            <attachment id="12470614" name="logs.zip" size="2725847" author="danny.bickson" created="Tue, 8 Feb 2011 19:01:52 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>7.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Sun, 14 Nov 2010 04:48:10 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9520</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hxy4h3:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>22876</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                </customfields>
    </item>
</channel>
</rss>