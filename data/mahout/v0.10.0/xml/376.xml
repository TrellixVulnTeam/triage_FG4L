<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 04:28:46 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/MAHOUT-376/MAHOUT-376.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[MAHOUT-376] Implement Map-reduce version of stochastic SVD</title>
                <link>https://issues.apache.org/jira/browse/MAHOUT-376</link>
                <project id="12310751" key="MAHOUT">Mahout</project>
                    <description>&lt;p&gt;See attached pdf for outline of proposed method.&lt;/p&gt;

&lt;p&gt;All comments are welcome.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12461748">MAHOUT-376</key>
            <summary>Implement Map-reduce version of stochastic SVD</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="3">Duplicate</resolution>
                                        <assignee username="tdunning">Ted Dunning</assignee>
                                    <reporter username="tdunning">Ted Dunning</reporter>
                        <labels>
                    </labels>
                <created>Sun, 11 Apr 2010 14:41:12 +0100</created>
                <updated>Sat, 21 May 2011 04:18:52 +0100</updated>
                            <resolved>Mon, 7 Mar 2011 10:53:09 +0000</resolved>
                                                    <fixVersion>0.5</fixVersion>
                                    <component>Math</component>
                        <due></due>
                            <votes>2</votes>
                                    <watches>4</watches>
                                                                <comments>
                            <comment id="12855724" author="tdunning" created="Sun, 11 Apr 2010 14:42:52 +0100"  >&lt;p&gt;Algorithm details.&lt;/p&gt;</comment>
                            <comment id="12856108" author="dlyubimov" created="Mon, 12 Apr 2010 19:32:22 +0100"  >&lt;p&gt;Per Ted&apos;s request, i am attaching a conspectus of our previous discussion of Ted&apos;s suggested mods to Tropp&apos;s stochastic svd. It doesn&apos;t include Q orthonormalization.&lt;/p&gt;</comment>
                            <comment id="12865416" author="tdunning" created="Sat, 8 May 2010 07:21:35 +0100"  >&lt;p&gt;Here is a work-in-progress patch that illustrates how I plan to do the stochastic multiplication.&lt;/p&gt;

&lt;p&gt;For moderate sized problems, this will be the major step required since all of the dense intermediate products will fit in memory.  For larger problems, additional tricks will be necessary.&lt;/p&gt;</comment>
                            <comment id="12865428" author="srowen" created="Sat, 8 May 2010 08:39:26 +0100"  >&lt;p&gt;More minor comments:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Don&apos;t forget a copyright header for new files&lt;/li&gt;
	&lt;li&gt;murmurInt should inline those constants or declare them constants&lt;/li&gt;
	&lt;li&gt;More VirtualRandomMatrix fields ought be final, conceptually&lt;/li&gt;
	&lt;li&gt;VirtualRandomVector doesn&apos;t need &quot;size&quot;&lt;/li&gt;
	&lt;li&gt;&quot;Utils&quot; classes ought to have a private constructor IMHO&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12913110" author="tdunning" created="Tue, 21 Sep 2010 18:49:41 +0100"  >&lt;p&gt;Postponing since it is still too raw.&lt;/p&gt;</comment>
                            <comment id="12914284" author="dlyubimov2" created="Fri, 24 Sep 2010 00:57:34 +0100"  >&lt;p&gt;i am chipping out on mapper implementation on weekends. so far i got to Q orthogonalization, and should be able to produce BB^t at the next one. &lt;br/&gt;
I&apos;ll post a patch when there&apos;s something working, but it is a slow progress due to my load. My employer at the moment is not eager to advance tasks that may depend on this, so i have to do it on my own time. &lt;/p&gt;</comment>
                            <comment id="12917010" author="dlyubimov2" created="Fri, 1 Oct 2010 20:22:41 +0100"  >&lt;p&gt;I have couple of doubts. I do amended Gram-Schmidt for the blocks of Y to produce blocks of Q, but while Q would end up orthonormal, i am not sure that Q and Y would end up spanning the same space. Although the fact that Y is random product means Q may also be more or less random basis so maybe it doesn&apos;t matter so much that span(Q)=exactly span(Y).&lt;/p&gt;

&lt;p&gt;Second concern is still the situation when last split producted by MR doesn&apos;t have minimally sufficient k+p records of A for producing orthogonal Q. The ideal outcome is then just to add it to another split, but i can&apos;t figure an easy enough way to do that within MR framework (esp. if the input is serialized using compressed sequence file). one way is to do custom split indexing based on # of records encountered (similar to what that lzo MR project does). but it sounds too complicated to me. Another way is just to do a pre-pass over A and prepartition it the way that this condition is satisfied. Then have a custom split so that there&apos;s 1 mapper per partition. But that&apos;s still one additional preprocessing step which we&apos;d make just for the sake of just a fraction of A. Ideas are welcome here.&lt;/p&gt;</comment>
                            <comment id="12917024" author="tdunning" created="Fri, 1 Oct 2010 21:22:44 +0100"  >&lt;blockquote&gt;
&lt;p&gt;I have couple of doubts. I do amended Gram-Schmidt for the blocks of Y to produce blocks of Q, but while Q would end up orthonormal, i am not sure that Q and Y would end up spanning the same space. Although the fact that Y is random product means Q may also be more or less random basis so maybe it doesn&apos;t matter so much that span(Q)=exactly span(Y).&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Since Q is orthonormal and since QR = Y, Q is exactly a basis of Y.  The only issue is that R isn&apos;t really right triangular.  That doesn&apos;t matter here.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Second concern is still the situation when last split producted by MR doesn&apos;t have minimally sufficient k+p records of A for producing orthogonal Q. The ideal outcome is then just to add it to another split, but i can&apos;t figure an easy enough way to do that within MR framework (esp. if the input is serialized using compressed sequence file). &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;If necessary, the input format can just note when the last block is small compared to previous blocks and round up on the next to last block.&lt;/p&gt;</comment>
                            <comment id="12917375" author="dlyubimov2" created="Sun, 3 Oct 2010 19:27:41 +0100"  >&lt;p&gt;Actually, i reviewed getSplits() code for sequence file. it seems to honor minSplitSize property of the FileInputFormat. What&apos;s more, it makes sure that the last block is no less than 1.1 times min split size. So that should work nicely. if we get insufficient # of rows in mappers, i guess we can just increase the minSplitSize. Unless input is partitioned so that min split size &amp;gt; min file size  in the input.&lt;/p&gt;</comment>
                            <comment id="12917392" author="dlyubimov2" created="Sun, 3 Oct 2010 21:17:09 +0100"  >&lt;p&gt;Another detail in Q computation: &lt;/p&gt;

&lt;p&gt;For now we assume that Q=(1/L)Q&apos;, where Q&apos; is the output of block orthonormalization, and L is the number of blocks we ended up with.&lt;/p&gt;

&lt;p&gt;However, in case when A is rather sparse, some Y blocks may not end up spanning R^(k+p) in which case orthogonalization would not produce normal columns. It doesn&apos;t mean that we can&apos;t orthonormalize the entire Y though. It just means that L is not for the entire Q but rather is individual for every column vector of Q and our initial assumption Q=(1/L)Q&apos; doesn&apos;t always hold.&lt;/p&gt;

&lt;p&gt;Which is fine per se, but then it means that there are perhaps quite frequent exceptions to assumption B =(1/L) Q&apos;^transpose * A. It would be easy to compute B^transpose in the same map-reduce pass as Q using multiple outputs , one for Q and one for B-transpose (which is what i am doing right now). But unless there&apos;s any workaround for the problem above, this B^transpose is incorrect for some sparse cases.&lt;/p&gt;

&lt;p&gt;Ted, any thoughts? Thank you.&lt;br/&gt;
-Dima&lt;/p&gt;</comment>
                            <comment id="12917394" author="tdunning" created="Sun, 3 Oct 2010 21:31:45 +0100"  >&lt;p&gt;I think that what you are saying is that some of the A_i blocks that make up A may be rank deficient.  &lt;/p&gt;

&lt;p&gt;That is definitely a risk if the number of rows in A_i is small compared to the size of Q_i.   If the number of rows of A_i is much larger than the size of Q_i, then that is very, very unlikely.&lt;/p&gt;

&lt;p&gt;Regardless, we will still have Q*Q = I and Q_i will still span the projection of A_i even if A_i and thus A_i \Omega is rank deficient.  Thus Q will still span A \Omega.  This gives us Q Q* A \approx A as desired.&lt;/p&gt;

&lt;p&gt;So, my feeling is that what you say is correct, but it isn&apos;t a problem. &lt;/p&gt;</comment>
                            <comment id="12917402" author="dlyubimov2" created="Sun, 3 Oct 2010 22:12:31 +0100"  >&lt;p&gt;yes,  I mean rank (Y-block) &amp;lt; (k+p) sometimes. &lt;/p&gt;

&lt;p&gt;Ok. I don&apos;t know how often matrix A may be too sparse. &lt;/p&gt;

&lt;p&gt;Just in case, i gave it a thought and here&apos;s what i think may help to account for this. &lt;/p&gt;

&lt;p&gt;It would seem that we can address that by keeping vector L of dimension k+p where L&lt;span class=&quot;error&quot;&gt;&amp;#91;i&amp;#93;&lt;/span&gt;=# of blocks of Q where rank(Q-block)&amp;gt;i. &lt;/p&gt;

&lt;p&gt;if B&apos; is compiled in the same pass as B&apos;=sum[ Q^t_(i*)A_(i*&lt;em&gt;)] then it just means that for actual B we need to correct rows of B as B&lt;/em&gt;(i*)=(1/L&lt;span class=&quot;error&quot;&gt;&amp;#91;i&amp;#93;&lt;/span&gt;) * B&apos;_(i*). Of course we don&apos;t actually have to correct them but just rather keep in mind that B is defined not just by the data but also by this scaling vector L. So subsequent steps may just account for it . &lt;/p&gt;

&lt;p&gt;Of course, as an intermediate validation step, we check if any of L&lt;span class=&quot;error&quot;&gt;&amp;#91;i&amp;#93;&lt;/span&gt; is 0, and if it is it pretty much means that rank(A)&amp;lt;k+p and we can&apos;t have a good svd anyway so we will probably raise and exception in this case and ask to consider to reduce oversampling or k. Or perhaps it is a bad case for distributed computation anyway.&lt;/p&gt;

&lt;p&gt;Right now i am just sending partial L vectors as q row with index -1 and sum it up in combiner and reducer.&lt;/p&gt;</comment>
                            <comment id="12919567" author="dlyubimov2" created="Sun, 10 Oct 2010 05:37:16 +0100"  >&lt;p&gt;&lt;br/&gt;
&lt;br/&gt;
so i got to singular values now. I run a unit test so that k+p=n.  When i parameterize algorithm so that only one Q-block is produced , the eigenvalues match the stock result at least as good as 10E-5.  Which is expected under the circumstances. however as soon as i increase number of Q-blocks &amp;gt;1, the singular values go astray as much as 10%. Not good. In both cases, the entire Q passes the orthonormality test. I guess it means that as i thought before, doing block orthonormalization this way does result in a subspace different from original span by Y.I need to research on doing orthonormalization with blocks.  I think that&apos;s the only showstopper here that is still left. It may result in a rewrite that splits one job producing both Q and Bt, into several though.&lt;/p&gt;</comment>
                            <comment id="12919723" author="dlyubimov2" created="Mon, 11 Oct 2010 06:41:16 +0100"  >&lt;p&gt;attaching my first iteration implementation doc&lt;/p&gt;</comment>
                            <comment id="12920267" author="dlyubimov2" created="Tue, 12 Oct 2010 18:34:10 +0100"  >&lt;p&gt;My quick scan of the literature seems to indicate that best promise for full scale QR over map/reduce is row-wise givens with top k+p rows of the blocks being processed in combiners and reducers for the reminder of the first step of givens QR step. this can be done in the first step of the job, but i am yet to figure how to combine and reduce &apos;rho&apos; intermediate results into final Q. &lt;span class=&quot;error&quot;&gt;&amp;#91;Golub, Van Loan  3rd ed&amp;#93;&lt;/span&gt; p. $5.2.3&lt;/p&gt;</comment>
                            <comment id="12920367" author="tdunning" created="Tue, 12 Oct 2010 22:49:56 +0100"  >&lt;p&gt;Updated outline document to show corrected form of block-wise extraction of Q&lt;/p&gt;</comment>
                            <comment id="12920369" author="tdunning" created="Tue, 12 Oct 2010 22:55:31 +0100"  >&lt;p&gt;Dima,&lt;/p&gt;

&lt;p&gt;I just attached an update to the original outline document I posted.  The gist of it is that the Q_i need to be arranged in block diagonal form in order to form a bases of A\Omega.  When that is done, my experiments show complete agreement with the original algorithm.  &lt;/p&gt;

&lt;p&gt;Here is R code that demonstrates decomposition without blocking and a 2 way block decomposition:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
# SVD decompose a matrix, extracting the first k singular values/vectors
# using k+p random projection
svd.rp = function(A, k=10, p=5) {
  n = nrow(A)
  y = A %*% matrix(rnorm(n * (k+p)), nrow=n)
  q = qr.Q(qr(y))
  b = t(q) %*% A
  svd = svd(b)
  list(u=q%*%svd$u, d=svd$d, v=svd$v)
}

# block-wise SVD decompose a matrix, extracting the first k singular values/vectors
# using k+p random projection
svd.rpx = function(A, k=10, p=5) {
  n = nrow(A)
  # block sizes
  n1 = floor(n/2)
  n2 = n-n1

  r = matrix(rnorm(n * (k+p)), nrow=n)
  A1 = A[1:n1,]
  A2 = A[(n1+1):n,]

  # block-wise multiplication and basis
  y1 = A1 %*% r
  q1 = qr.Q(qr(y1))

  y2 = A2 %*% r
  q2 = qr.Q(qr(y2))

  # construction of full q (not really necessary)
  z1 = diag(0, nrow=nrow(q1), ncol=(k+p))
  z2 = diag(0, nrow=nrow(q2), ncol=(k+p))
  q = rbind(cbind(q1, z1), cbind(z2, q2))
  b = t(q) %*% A

  # we can compute b without forming the block diagonal Q
  bx = rbind(t(q1)%*%A1, t(q2)%*%A2)

  # now the decomposition continues
  svd = svd(bx)

  # &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; all the pieces &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; checking
  list(u=q%*%svd$u, d=svd$d, v=svd$v, q1=q1, q2=q2, q=q, b=b, bx=bx)
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Note that this code has a fair bit of fat in it for debugging or illustrative purposes.&lt;/p&gt;</comment>
                            <comment id="12921444" author="tdunning" created="Fri, 15 Oct 2010 18:15:30 +0100"  >&lt;p&gt;Updates.&lt;/p&gt;

&lt;p&gt;I think that the outline of the algorithm is now in place.  As it currently stands, it should scale to &lt;br/&gt;
about a thousand cores (maybe more) and should allow very small memory footprint machines&lt;br/&gt;
to participate.&lt;/p&gt;

&lt;p&gt;That level should allow us to decompose data in the scale of 1-10 billion non-zeros, at a guess.&lt;/p&gt;</comment>
                            <comment id="12924293" author="tdunning" created="Sun, 24 Oct 2010 08:10:14 +0100"  >&lt;p&gt;Updated version.&lt;/p&gt;

&lt;p&gt;I think that this is actually a feasible algorithm.&lt;/p&gt;</comment>
                            <comment id="12931756" author="tdunning" created="Sun, 14 Nov 2010 04:42:36 +0000"  >&lt;p&gt;These are really the same.&lt;/p&gt;</comment>
                            <comment id="12931786" author="dlyubimov2" created="Sun, 14 Nov 2010 07:55:46 +0000"  >&lt;p&gt;I am currently working to drive the working prototype the version with standalone thin QR step which would be memory independent of num of rows in A and would result into BBt eigensolution of (k+p)x(k+p) dimensionality only, the rest being driven by Map Reduce.  I&apos;ve got single stream version working seemingly well, here&apos;s the update on this WIP. Although in the end i am not sure it would offer any real-life improvement, as it would seem to require a second pass over A as it is not possible to finish Q^t x B computation in single step with this approach. &lt;/p&gt;

&lt;p&gt;Still, after 2 passes, we should be done with eigen values (and perhaps (k+p)x(k+p) dimensionality for eigensolver input would allow us to increase oversampling p somewhat, hence precision). Hard to see from here yet though. Additional (optional) MR steps would only be needed if U or V is or both are desired.&lt;/p&gt;</comment>
                            <comment id="12932341" author="dlyubimov2" created="Tue, 16 Nov 2010 05:26:37 +0000"  >&lt;p&gt;QR step doc is ready for review. &lt;/p&gt;

&lt;p&gt;Especially sections 3.2 and on which i still haven&apos;t worked up in the actual code. These reflect my original ideas to deal with blockwise QR in a MapReduce settings. I am probably retracing some block computations already found elsewhere (such as in LAPack) but i think it may actually work out ok. &lt;/p&gt;</comment>
                            <comment id="12933542" author="dlyubimov2" created="Thu, 18 Nov 2010 19:45:10 +0000"  >&lt;p&gt;QR step is now fully verified with MapReduce emulation. Updated QR step document with fixes that resulted from verification. I will be putting all that into complete map reduce, hopefully within a couple of weeks now. Initial patch will be dependent on CDH3b3 hadoop, if we need a backport to legacy api, that&apos;ll take some more time on best effort basis on my part.&lt;/p&gt;</comment>
                            <comment id="12936080" author="dlyubimov2" created="Fri, 26 Nov 2010 19:36:19 +0000"  >&lt;p&gt;git patch m1 : WIP but important milestone: prototype &amp;amp; MR implementation at the level of computing full Q and singlular values. as i mentioned, needs CDH3b2 (or b3). &lt;br/&gt;
Local MR test runs MR solver in local mode for a moderately low rank random matrix 80,000x100 (r=251, k+p=100). Test output i get on my laptop (first are singulars for 100x100 BBt matrix using commons-math Eigensolver; 2 &amp;#8211; output of SVs produced by Colt SVD of 80,000x100 same source matrix&lt;/p&gt;

&lt;p&gt;--SSVD solver singular values:&lt;br/&gt;
svs: 4220.258342  4215.924299  4213.352353  4210.786495  4203.422385  4201.047189  4194.987920  4193.434856  4187.610381  4185.546818  4179.867986  4176.056232  4172.784145  4169.039073  4168.384457  4164.293827  4162.647531  4160.483398  4157.878385  4154.713189  4152.172788  4149.823917  4146.500139  4144.565227  4142.625983  4141.291209  4138.105799  4135.564939  4134.772833  4129.223450  4129.101594  4126.679080  4124.385614  4121.791730  4119.645948  4115.975993  4112.947092  4109.586452  4107.985419  4104.871381  4102.438854  4099.762117  4098.968505  4095.720204  4091.114871  4090.190141  (...omited) 3950.897035  &lt;br/&gt;
--Colt SVD solver singular values:&lt;br/&gt;
svs: 4220.258342  4215.924299  4213.352353  4210.786495  4203.422385  4201.047189  4194.987920  4193.434856  4187.610381  4185.546818  4179.867986  4176.056232  4172.784145  4169.039073  4168.384457  4164.293827  4162.647531  4160.483398  4157.878385  4154.713189  4152.172788  4149.823917  4146.500139  4144.565227  4142.625983  4141.291209  4138.105799  4135.564939  4134.772833  4129.223450  4129.101594  4126.679080  4124.385614  4121.791730  4119.645948  4115.975993  4112.947092  4109.586452  4107.985419  4104.871381  4102.438854  4099.762117  4098.968505  4095.720204  4091.114871  4090.190141  (....omited)  3950.897035  &lt;/p&gt;

&lt;p&gt;I will be updating my notes with a couple of optimizations i applied in this code not yet mentioned.&lt;/p&gt;

&lt;p&gt;-Dima&lt;/p&gt;
</comment>
                            <comment id="12964413" author="dlyubimov2" created="Sat, 27 Nov 2010 17:34:49 +0000"  >&lt;p&gt;Working notes for the process i used. i think these are pretty much final. I guess it turned out to be a little voluminous, but it mentions pretty much all essential details i may want to put down for my future reference. If one reads it, he/she may start directly with p. 5 of MapReduce implementation and then refer to algorithms in previous sections and dicussion that lead to their formation as needed.&lt;/p&gt;</comment>
                            <comment id="12964475" author="dlyubimov2" created="Sun, 28 Nov 2010 00:25:25 +0000"  >&lt;p&gt;Added U and V computations. Labels of A rows are propagated to U rows as keys. bumped up the test to A of dimensions of 100,000x100 (so it produces 3 A splts now). Actually i just realize that technically U and V should be k columns wide, and i am producing k+p. ok, who cares. Distinction between k and p becomes really nominal now.&lt;/p&gt;

&lt;p&gt;I&apos;d appreciate if somebody reviews V computation (p.5.5.2 in working notes), just in case, i already forget the derivation of V computation.&lt;/p&gt;

&lt;p&gt;Another uncertainty i have is that i am not sure how to best construct tests for U and V outputs, but i am not sure if i care that much since the computation logic is really trivial (compared to everything else). Existing tests verify singlular values against independent memory-only svd and assert orthogonality of Q output only.&lt;/p&gt;

&lt;p&gt;Stuff that is left is really minor and mahout-specific or engineering: &lt;br/&gt;
&amp;#8211; figure out how to integrate with mahout CLI &lt;br/&gt;
&amp;#8211; add minSplitSize parameter to CLI, and others (k,p, computeU, computeV, .. )&lt;br/&gt;
&amp;#8211; do we want to backport it to apache 0.20? multiple outputs are crippled in 0.20 and i don&apos;t know how one can live without multiple outputs in a job like this.&lt;/p&gt;</comment>
                            <comment id="12964655" author="dlyubimov2" created="Mon, 29 Nov 2010 08:43:53 +0000"  >&lt;p&gt;patch m3: &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;updated U, V jobs to produce m x k and n x k geometries, respectively. Notes updated to reflect that.&lt;/li&gt;
	&lt;li&gt;added minSplitSize setting to enable larger n and (k+p).&lt;/li&gt;
	&lt;li&gt;added apache license statements and minor code cleanup.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12965597" author="dlyubimov2" created="Wed, 1 Dec 2010 07:43:18 +0000"  >&lt;p&gt;Final trunk patch for CDH3 or 0.21 api. &lt;br/&gt;
This includes code cleanup, javadoc updates, and mahout CLI class (not tested though). &lt;/p&gt;

&lt;p&gt;all existing tests and this test are passing. I tested 100Kx100 matrix in local mode only, S values coincide with 1e-10 or better.&lt;/p&gt;

&lt;p&gt;changes to dependencies i had to make &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;hadoop 0.21 or cdh3 to support multiple outputs&lt;/li&gt;
	&lt;li&gt;local MR mode has dependency on commons-http client, so i included it for test scope only in order for test to work&lt;/li&gt;
	&lt;li&gt;changed apache-math dependency from 1.2 &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/help_16.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; to 2.1. Actually mahout math module seems to depend on 2.1 too, not clear why it was not transitive for this one.&lt;/li&gt;
	&lt;li&gt;commons-math 1.2 seemed to have depended on commons-cli and 2.1 doesn&apos;t have it transitively anymore, but one of  the classes in core required it. so i added commons-cli in order to fix the build.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;b&gt;Ted&lt;/b&gt;, sorry i kind of polluted your issue here. Thank you for your encouragement and help. i probably should&apos;ve opened another issue once it was clear it diverged far enough, instead of keep putting stuff here. &lt;/p&gt;

&lt;p&gt;This should be compatible with DistributedRowMatrix. I did not have real distributed test yet as i don&apos;t have a suitable data set yet, but perhaps somebody in the user community with the interest in the method could do it faster than i get to it. I will do tests with moderate scale at some point but i don&apos;t want to do it on my company&apos;s machine cluster yet and i don&apos;t exactly own a good one myself.&lt;/p&gt;

&lt;p&gt;I did have a rather mixed use of mahout vector math and just dense arrays. Partly becuase i did not quite have enough time to study all capabilities in math module, and partly becuase i wanted explicit access to memory for control over its more efficient re-use in mass iterations.  This may or may not need be rectified over time. But it seems to work pretty well as is.&lt;/p&gt;

&lt;p&gt;The patch is git patch (so one needs to use patch -p1 instead of -p0). I know the standard is set to use svn patches... but i already used git for pulling the trunk  (so happens i prefer git in general too so i can have my own commit tree and branching for this work). &lt;/p&gt;

&lt;p&gt;If there&apos;s enough interest from the project to this contribution, i will support it, and if requested, i can port it to 0.20 if that&apos;s the target platform for 0.5, as well as doing other specific mahout architectural tweaks.  Please kindly let me know. &lt;/p&gt;


&lt;p&gt;Thank you.&lt;/p&gt;</comment>
                            <comment id="12965598" author="tdunning" created="Wed, 1 Dec 2010 07:50:28 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Ted, sorry i kind of polluted your issue here. Thank you for your encouragement and help. i probably should&apos;ve opened another issue once it was clear it diverged far enough, instead of keep putting stuff here.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;You didn&apos;t pollute anything at all.  You contributed a bunch of code here.&lt;/p&gt;

&lt;p&gt;So far, the only thing that I worry about is the mixture of array and matrices, but even that might be fine.&lt;/p&gt;

&lt;p&gt;I was dubious of the scalability of the streaming QR, but if that works then we should be good to go with very little more work.&lt;/p&gt;

&lt;p&gt;Do you have any idea if this will work for large sparse matrices as opposed to the 100K x 100 matrix you are using?&lt;/p&gt;</comment>
                            <comment id="12965600" author="tdunning" created="Wed, 1 Dec 2010 07:57:51 +0000"  >
&lt;p&gt;Doesn&apos;t the streaming QR decomposition require that we look at each row of Y one at a time in a streaming fashion?  That is, isn&apos;t that a completely sequential algorithm?&lt;/p&gt;
</comment>
                            <comment id="12965608" author="dlyubimov2" created="Wed, 1 Dec 2010 08:28:16 +0000"  >&lt;p&gt;yes, it is 100% streaming in terms of A and Y rows. Assumption is that we are ok to load one A row into memory at a time and we optimize for tall matrices (such as billion by million) So we are bound at n but not at m from memory point of view.  Even if it is dense, one such vector would take 8MB memory at a time. but sparse sequential vectors should be ok too (it will probably require a little tweak during Y computations to scan it one time sequentially instead of k+p times as i think it is done now with assumption it can be random).&lt;/p&gt;

&lt;p&gt;For memory, the concern is random access q blocks which can be no less than k+p by k+p (that is, for the case of k+p=500, it gets to be 2 Mb). But this is all as far as memory is concerned. (well actually 2 times that, plus there&apos;s a Y lookahead buffer in order to make sure we can safely form next block. Plus there&apos;s a packed R. so for k+p=500 it looks like minimum memory requirement is rougly in the area of 7-8Mb. which is well below anything). &lt;/p&gt;

&lt;p&gt;Y lookahead buffer is not a requirement of algorithm itself, it&apos;s MR specific, to make sure we have at least k+p rows to process in the split before we start next block. i thought about it but not much; at 2mb minimum dense memory requirement it did not strike me as a big issue.&lt;/p&gt;

&lt;p&gt;CPU may be more of a problem, it is quadratic, as in any QR algorithm known to me. but i am actually not sure if Givens series would produce more crunching than e.g. Householder&apos;s . Givens certainly is as numerically stable as householder&apos;s and better than Gramm-Schmidt. In my tests for 100k tall matrix the orthonormality residuals seem to hold at about  no less than 10e-13 and surprisingly i did not notice any degradataion at all compared to smaller sizes.  Actually I happened to read aobut  LAPack methods ithat  prefer Givens for possiblity of re-ordering and thus easier parallelization). &lt;/p&gt;

&lt;p&gt;Anyway, speaking of numerical stability, whatever degradation occurs, i think it would be dwarfed by stochastic inaccuracy which grows quite significantly in my low rank tests. Perhaps for kp=500 it should degrade much less than for 20-30.&lt;/p&gt;

&lt;p&gt;But i guess we need to test at scale to see the limitations.&lt;/p&gt;</comment>
                            <comment id="12965742" author="dlyubimov2" created="Wed, 1 Dec 2010 16:52:55 +0000"  >&lt;p&gt;There is a catch though. With this implementation, m is memory bound. It is not in mapper though but in combiners and mapper of the next step. &lt;/p&gt;

&lt;p&gt;But my reasoning was, with 1G memory and k+p =500 it seems to exist rather wide spectrum of admissible combinations of r (block height) and minSplitSize (governing essentually # of mappers needed) that would cover billion rows, and the sweet spot of this combination seem to exceed 1 bln rows. &lt;/p&gt;

&lt;p&gt;In addition, there are 2 remedies to consider. First is rather straightforward application of compression on R sequence. &lt;/p&gt;

&lt;p&gt;Second remedy results from the fact that our QR merge process is hierarchical. Right now it&apos;s two-level hierarchy. I.e if processes at each stage merge 1000 q blocks, then at the next level we can merge another 1000 q blocks, so total number of q blocks is thus approx. 1 mln. (for 1G memory the number of some 600-800k blocks is more plausible). Assuming Q blocks are k+p rows high, that gives us aprroximately 300 mln rows for m. But the trick is that if we have 1G memory in the mapper, then Q blocks don&apos;t have to be 500 rows high, then can easily be 200k rows high. Which immediately puts us in the range, conservatively, of 10 bln rows or so without any additional remedies, which i think is about the scale of Google&apos;s document index in 2005, if we wanted to do an LSI on it, assuming there&apos;s 1 mln lemmas in English, which there&apos;s not. &lt;/p&gt;

&lt;p&gt;But if we add another map-only pass over blocked Q data, then we can have 1 bln blocks with all the considerations above and that should put us in the range of 30 trillion rows of A. This number grows exponentially with every added MR pass which is why i am saying m is virtually unbound. &lt;/p&gt;

&lt;p&gt;Adding these remedies seems to be pretty straighforward, but for a first stab at the problem my estimates for m bound seem to be adequate. With these kind of numbers, this may easily become a technology in a search of a problem. We may add some analysis on optimality of combination of block size and minSplitSize. My initial thought is that finding maximum here is pretty straigtoward, seems to be a task of finding maximum on a second degree polynomial function.&lt;/p&gt;

&lt;p&gt;It&apos;s more likely that much more memory would go into precision effort rather than maximizing m bound though. E.g. instead of covering the scale, the resources may go into increasing precision and oversampling. In which case additional map-only passes over Q will be tremendously useful. (imagine this could do k+p=5000 with just one additional map-only pass over Q data) If this is the case, then the next low hanging fruit step is to add map-only hierarchical merge of Rs on Q blocks.&lt;/p&gt;

&lt;p&gt;However, it&apos;s a stochastic algorithm and as such it is probably not good enough for processes that would require such precision (e.g certainly not to do math work on rocket boosters, i think). That said, k+p=5000 probably doesn&apos;t make sense. I think applications sharply divide into 2 categories, where precision requirements are either much higher than that, or much lower. I can&apos;t think of much in between. &lt;/p&gt;

&lt;p&gt;Apologies  for multiple editions, mostly typos and correction to numbers. These numbers are still pretty offhand and exercise in mental arithmetics, actual mileage probably will vary as much as +- 40% because of unaccounted overhead. I tried to be conservative though.&lt;/p&gt;</comment>
                            <comment id="12965961" author="dlyubimov2" created="Thu, 2 Dec 2010 01:51:04 +0000"  >&lt;blockquote&gt;&lt;p&gt;Doesn&apos;t the streaming QR decomposition require that we look at each row of Y one at a time in a streaming fashion? That is, isn&apos;t that a completely sequential algorithm?&lt;/p&gt;&lt;/blockquote&gt;

&lt;blockquote&gt;&lt;p&gt; Even if it is dense, one such vector would take 8MB memory at a time. but sparse sequential vectors should be ok too (it will probably require a little tweak during Y computations to scan it one time sequentially instead of k+p times as i think it is done now with assumption it can be random). &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Oh. I guess you hinted at possibilty that if we use sparse sequential vector for A rows, then we memory-unbound for n! so who cares about m then! And then we can have billion by billion even with this implemetnation. Wow. That&apos;s an extremely powerful suggestion.  But that&apos;s definitely requires code review  and performance tests. And we go over A only one time so there&apos;s no need to revisit the sparse vectors. I&apos;ll take a look at it to see if i can engineer a solution. If it is possible at all, it should be extremely simple.&lt;/p&gt;</comment>
                            <comment id="12965963" author="dlyubimov2" created="Thu, 2 Dec 2010 01:59:57 +0000"  >&lt;p&gt;Although can one really implement a streaming read of a value for a mapper value? I am not sure. i guess i need to look at implementation of sparse sequential vector. It would seem to me though that sequenceFile.next() requires deserialization of the whole record, doesn&apos;t it? so reading mapper value in a streaming fashion should not be possible, not without some sort of a hack, right? &lt;/p&gt;</comment>
                            <comment id="12965968" author="dlyubimov2" created="Thu, 2 Dec 2010 02:27:24 +0000"  >&lt;p&gt;Also, if we consider wide matrices instead of tall matrices, then maximum number of mappers might be reduced which would affect parallelism on big clusters. &lt;/p&gt;

&lt;p&gt;Another consideration for extremely wide matrices i guess is that the A block in this case will certainly overshoot several standard DFS blocks so it may only be efficient if we force data collocation for a big number of blocks (or just increase number of blocks).  I am not sure if hadoop quite there yet to tweak that on individual file basis.&lt;/p&gt;

&lt;p&gt;-d&lt;/p&gt;</comment>
                            <comment id="12965990" author="dlyubimov2" created="Thu, 2 Dec 2010 04:42:55 +0000"  >&lt;blockquote&gt;&lt;p&gt;Although can one really implement a streaming read of a value for a mapper value?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Of course we can split a vector into several slices shoved into sequential records. That would require some work to tweak SequenceFile&apos;s record reader logic so it doesn&apos;t stop in the middle of a vector (and respectively skips records to the next vector at the beginning of a  split) but such possibility definitely exists.  &lt;/p&gt;

&lt;p&gt;Not sure if Mahout has already something like that, need to look closer. But it should be possible to develop something like DistributedSlicedRowMatrix.&lt;/p&gt;</comment>
                            <comment id="12966010" author="dlyubimov2" created="Thu, 2 Dec 2010 07:59:41 +0000"  >&lt;p&gt;small update. &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;added special treatment for SequentialAccessSparseVector computations during dot product computation like it is done everywhere else.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I guess it is about all i can do at this point for n scaling efficiently. &lt;/p&gt;

&lt;p&gt;We could scale n even further by splitting the vector into slices as said before, but not before we solve the problem of code-data collocation in &apos;supersplits&apos; for wide matrices. If we don&apos;t do that, it will cause a lot of IO in mappers and kind of defeats the purpose of MR imo.&lt;/p&gt;</comment>
                            <comment id="12966019" author="tdunning" created="Thu, 2 Dec 2010 08:35:24 +0000"  >&lt;blockquote&gt;
&lt;p&gt;We could scale n even further by splitting the vector into slices as said before, but not before we solve the problem of code-data collocation in &apos;supersplits&apos; for wide matrices. If we don&apos;t do that, it will cause a lot of IO in mappers and kind of defeats the purpose of MR imo.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think that my suggested approach handles this already.&lt;/p&gt;

&lt;p&gt;The block decomposition of Q via the blockwise QR decomposition implies a breakdown of B into column-wise blocks which can each be handled separately.  The results are then combined using concatenation.&lt;/p&gt;
</comment>
                            <comment id="12966223" author="dlyubimov2" created="Thu, 2 Dec 2010 18:32:36 +0000"  >&lt;blockquote&gt;&lt;p&gt;I think that my suggested approach handles this already.&lt;/p&gt;

&lt;p&gt;The block decomposition of Q via the blockwise QR decomposition implies a breakdown of B into column-wise blocks which can each be handled separately. The results are then combined using concatenation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Ted, yes, i understand that part, but i think we are talking about different things. What I am talking about is formation of Y rows well before orthonotmalization is even concerned. &lt;/p&gt;

&lt;p&gt;What i mean is that right now VectorWritable loads the entire thing into memory. Hence, the bound for width of A. (i.e. we can&apos;t load A row that is longer than some memory chunk we can afford for it). &lt;/p&gt;

&lt;p&gt;However, what A row is participating in in each case is a bunch of (namely, k+p) dot-products. In order to produce those, it is sufficient to examine A row sequentialy  (i.e. streamingly) in one pass while keeping only k+p values in memory as dot-accumulators.&lt;/p&gt;

&lt;p&gt;Hence,  say if we equipped VectorWritable with a push-parser like element handler (notorious DocumentHanlder immediately pops in memory form SAXParser) then we will never have to examine more than one element of A row at a time. And hence we are not bound by memory for n (A width) anymore. That handler would form Y rows directly during the sequential examination of A rows. Identical considerations are in effect when forming Qt*A partial products (i already checked for this).&lt;/p&gt;

&lt;p&gt;I already thought about this approach a little (and i believe Jake Mannix also posted something very similar to that recently to effect of sequential vector examination backed by a streaming read). &lt;/p&gt;

&lt;p&gt;Since it is touching VectorWritable internals, i think i would need to make a change proposal for it and if seems reasonable handle it in another issue. I will do so but i need to check couple of things first in Hadoop see if it is feasible within current MR framework and doesn&apos;t blow off all benefits code-data collocation.&lt;/p&gt;

&lt;p&gt;If that proposal is implemented, and MR considerations are tackled, we will have SSVD that scales to about a billion rows for 500 singular vlaues and 1G memory in mapper vertically (m) and a gazillion in n (width). &lt;br/&gt;
theoretically. How about that.&lt;/p&gt;
</comment>
                            <comment id="12966229" author="tdunning" created="Thu, 2 Dec 2010 18:48:59 +0000"  >&lt;blockquote&gt;
&lt;p&gt;&amp;gt; I think that my suggested approach handles this already.&lt;/p&gt;

&lt;p&gt;&amp;gt; The block decomposition of Q via the blockwise QR decomposition implies a breakdown of B into column-wise blocks which &lt;br/&gt;
&amp;gt; can each be handled separately. The results are then combined using concatenation.&lt;/p&gt;

&lt;p&gt;Ted, yes, i understand that part, but i think we are talking about different things. What I am talking about is formation of Y rows well before orthonotmalization is even concerned.&lt;/p&gt;

&lt;p&gt;What i mean is that right now VectorWritable loads the entire thing into memory. Hence, the bound for width of A. (i.e. we can&apos;t load A row that is longer than some memory chunk we can afford for it).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I understand that now.&lt;/p&gt;

&lt;p&gt;The current limitation is that the sparse representation of a row has to fit into memory.  That means that we are limited to cases with a few hundred million non-zero elements and are effectively unlimited on the number of potential columns of A.&lt;/p&gt;

&lt;p&gt;The only other place that the total  number of elements in a row comes into play is in B.  Using the block form of Q, however, we never&lt;br/&gt;
have to store an entire row of B, just manageable chunks.&lt;/p&gt;

&lt;p&gt;My real worry with your approach is that the average number of elements per row of A is likely to be comparable to p+k.  This means that Y = A \Omega will be about as large as A.  Processing that sequentially is a non-starter and the computation of Q without block QR means that Y is processed sequentially.  On the other hand, if we block decompose Y, we want blocks that fit into memory because that block size lives on in B and all subsequent steps.  Thus, streaming QR is a non-issue in a blocked implementation.  The blocked implementation gives a natural parallel implementation.&lt;/p&gt;
</comment>
                            <comment id="12966265" author="dlyubimov2" created="Thu, 2 Dec 2010 20:22:18 +0000"  >&lt;blockquote&gt;
&lt;p&gt;My real worry with your approach is that the average number of elements per row of A is likely to be comparable to p+k. This means that Y = A \Omega will be about as large as A. Processing that sequentially is a non-starter and the computation of Q without block QR means that Y is processed sequentially. On the other hand, if we block decompose Y, we want blocks that fit into memory because that block size lives on in B and all subsequent steps. Thus, streaming QR is a non-issue in a blocked implementation. The blocked implementation gives a natural parallel implementation.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I think you misunderstanding it a little. the actual implementation is not that naive. let me clarify. &lt;/p&gt;

&lt;p&gt;First, there &lt;b&gt;is&lt;/b&gt; blocking. More over, it&apos;s a hierarchical blocking. &lt;/p&gt;

&lt;p&gt;the way it works, you specify block height, which is more k+p but ideally less than a MR split would host (you can specify more but you may be producing some network traffic then to move non-collocated parts of the split). Blocks are considered completely in parallel.  Hence, initial parallelizm degree is m/r where r is average block height. They can (and are) considered independently, among the splits. &quot;thin streaming QR&quot; runs &lt;b&gt;inside&lt;/b&gt; the blocks, not on the whole Y. &lt;/p&gt;

&lt;p&gt;Secondly, Y matrix, or even its blocks, are never formed. What is formed is shifting intermediate Q buffer of the size (k+p)xr and intermediate upper triangular R of size (k+p)x(k+p). Since they are triangular, there&apos;s a rudimental implementation of Matrix itnerface called UpperTriangular not to waste space on lower triangle but still allow random access.&lt;/p&gt;

&lt;p&gt;Thirdly, the hierarchy. when we form Q blocks, we will have to update them with Givens operations resulting from merging R matrices. This is done in combiner and this  comes very natural to it. If there&apos;s say z blocks in a mapper then Q1 goes thru updates resulting from z merges of R, Q2 goes thru udpates resulting from z-1 merges and so on. Nothing being concatenated (or unblocked) there except for the R sequence (but it is still sequence, that is sequentially accessed thing) which i already provided memory estimates for. Most importantly, it does not depend on the block height, so you can shrink R sequence length if you have higher Q blocks, but higher Q blocks also take more memory to process at a time. there&apos;s a sweet spot to be hit here with parameters defining block height and split size, so it maximizes the thruput. for k+p=500 i don&apos;t see any memory concerns there in a single combiner run. &lt;/p&gt;

&lt;p&gt;And there&apos;s no reducer (i.e. any sizable shuffle and sort) here. At the end of this operation we have a bunch of Rs which corresponds to the number of splits, and a bunch of interbediate Q blocks still same size which correspond to number of Q-blocks. &lt;/p&gt;

&lt;p&gt;Now we can repeat this process  hierarchically with additional map-only passes over Q blocks until only one R block is left. with 1G memory, as i said, my estimate is we can merge up to 1000 Rs &lt;b&gt;per combiner&lt;/b&gt; with one MR pass (without extra overhead for single Q block and other java things). (in reality in this implementation there are 2 levels in this hierarchy which seems to point to over 1 bln rows, or about 1 mln Q blocks of some relatively moderate height r&amp;gt;&amp;gt;k+p, but like i said with just one  map-only pass one can increase scale of m to single trillions ). This hierarchical merging is exactly what i meant by &apos;making MR work harder&apos; for us. &lt;/p&gt;

&lt;p&gt;There is a poor illustration of this hierarchical process in the doc that makes it perhaps more clear than words. &lt;/p&gt;

&lt;p&gt;Also let me point out that the fact that the processes involved in R merging are &lt;b&gt;map-only&lt;/b&gt;,  which means that if we play the splitting game right in MR, there would  &lt;b&gt;practically be no networking IO&lt;/b&gt; per MR theory. This is very important imo for such scale. The only IO that occurs is to &apos;slurp&apos; r sequences from HDFS before next stage of hierarchical R-merge. For a sequence of 1000 R, k+p 500, the size of R, dense and uncompressed, is approximately 1 mb each, so for a sequence of thousand Rs, the size of such slurp IO, dense and uncompressed,  would be 1G, which is less than what i am having today in a single step with Pig for a 200k of proto-packed log records today in production and that finishes in a minute.&lt;/p&gt;

&lt;p&gt;Bottom line, let&apos;s benchmark it. So we don&apos;t have to guess. Especially if we can do  A vector streaming. I am personally having trouble with logistics of this so far, as i mentioned before. I will get to benchmarking it sooner or later. Important thing for me at this point was to make sure it does correct computation (which it does) and make educated guess about the scale (which is billion by million without vector streaming support or billion to gazillion with vector streaming support, with potential to extend m scale thousand times with each additional map-only pass over Q data (which is (k+p)xm which is again unbounded for n).&lt;/p&gt;


&lt;p&gt;thanks. &lt;/p&gt;
</comment>
                            <comment id="12966297" author="tdunning" created="Thu, 2 Dec 2010 21:56:16 +0000"  >&lt;blockquote&gt;
&lt;p&gt;I think you misunderstanding it a little. the actual implementation is not that naive. let me clarify.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I was hoping I misunderstood it.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;And there&apos;s no reducer (i.e. any sizable shuffle and sort) here. At the end of this operation we have a bunch of Rs which corresponds to the number of splits, and a bunch of interbediate Q blocks still same size which correspond to number of Q-blocks.&lt;/p&gt;

&lt;p&gt;Now we can repeat this process hierarchically with additional map-only passes over Q blocks until only one R block is left. with 1G memory, as i said, my estimate is we can merge up to 1000 Rs per combiner with one MR pass (without extra overhead for single Q block and other java things). (in reality in this implementation there are 2 levels in this hierarchy which seems to point to over 1 bln rows, or about 1 mln Q blocks of some relatively moderate height r&amp;gt;&amp;gt;k+p, but like i said with just one map-only pass one can increase scale of m to single trillions ). This hierarchical merging is exactly what i meant by &apos;making MR work harder&apos; for us.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Sounds to me like the reducer could replicate the combiner and thus implement the second step of your hierarchy which would avoid the second MR pass.  You could have a single reducer which receives all combiner outputs and thus merge everything.  Since you can&apos;t guarantee that the combiner does any work, this is best practice anyway.  The specification is that the combiner will run zero or more times.  &lt;/p&gt;

&lt;p&gt;This also raises the question of whether your combiner can be applied multiple times.  I suspect yes.  You will know better than I.&lt;/p&gt;</comment>
                            <comment id="12966333" author="dlyubimov2" created="Thu, 2 Dec 2010 23:01:33 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Sounds to me like the reducer could replicate the combiner and thus implement the second step of your hierarchy which would avoid the second MR pass. You could have a single reducer which receives all combiner outputs and thus merge everything.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;First of all, second level hierarchical merge  is done in Bt job mappers &amp;#8211; so moving this code into reducer wouldn&apos;t actually really win anything in terms of actual IO or # of MR passes.&lt;/p&gt;

&lt;p&gt;Secondly,  I don&apos;t beleive single reducer pass is efficient. All parallelization is gone, isn&apos;t it? But the problem is not even that but limitation on how many Rs you can preload for the merges into same process.  Rs are preloaded as a side info (and we relinquish one R with every Q we process in combiner, so initially it loads all but then throws them away one by one. So we can&apos;t merge them all in one pass, but we can divide them into subsequences. If they merged by subsequences, subsequences must be completed and order is important and should be exactly the same for all Q blocks. The catch is that each Q update has to complete the merges of the reminder of the R sequence. Such subsequence merge is described in computeQHatSequence algorithm in my notes.&lt;/p&gt;

&lt;p&gt;Finally, there&apos;s no need to sort. When we do R merge, as it is shown in the notes, we have to revisit  Q blocks (and Rs) exactly in the same order we just produced them in. Problem is that when we revisit a Q block, we have to have the tail of all subsequent R blocks handy.  So even if we sent them to reducers (which was my initial plan), we&apos;d have to sort them by task id they came from and then by their order inside the task that produced them. And then we might need to duplicate R traffic to every reducer process. Huge waste, again.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt; Since you can&apos;t guarantee that the combiner does any work, this is best practice anyway. The specification is that the combiner will run zero or more times.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That  actually is absolutely valid and i was very concerned about it in the beginning. I expected it but it did not turn up in tests so far, so that issue was  slowly glimmering in my subconsciousness ever since.  &lt;/p&gt;

&lt;p&gt;If that actually is a problem then yes I might be forced to extend Q pass to include reducer. Which IMO would be a humongous exercise in IO bandwidth waste. there&apos;s no need for that. there&apos;s only need a for local merge of R sequences and doing second pass over q block sequence  you just generated, in the same order you generated it. Even a combiner is an overshoot for that, if i were implementing it in a regular parallel batching way, since there&apos;s no need to reorder q blocks. &lt;/p&gt;

&lt;p&gt;I also considered that If i am forced to address that i can still do it in the mapper only without even a combiner by pushing Q blocks to local FS side files and doing a second pass over them and that&apos;d be much more efficient than pushing them to another networked process. Not mentioning all the unneeded sorting involved in between and all the armtwisting i was struggling with involved in sending them in same task  chunks, same order. There&apos;s still rudimentary remainder of that attempt in the code (the key that carries task id and order id for Q block that are never subsequently used anywhere). But saving local side file in mapper and do second pass over it is still way more attractive than reducer approach if i need to fix this.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;This also raises the question of whether your combiner can be applied multiple times. I suspect yes. You will know better than I.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, that&apos;s the optional hierarchy increment that  i was talking about, that we can additionally implement if billion for m  and  unbound n is not good enough.  &lt;/p&gt;
</comment>
                            <comment id="12966342" author="dlyubimov2" created="Thu, 2 Dec 2010 23:23:26 +0000"  >&lt;p&gt;Actually i am seriously considering reworking combiner approach into two-pass-in-the-mapper approach.&lt;/p&gt;

&lt;p&gt;This should be ok because side file is going to be n/(k+p) times smaller than original split.  Most likely IO cache will not even let us wait on io in some cases. but if there is disk io, it would be 100% sequential speed. &lt;/p&gt;

&lt;p&gt;And that should be even more efficient than asking combiner to sort what is already sorted (which i mostly did out of aesthetics as combiner looks much more fanciful than some explicit sequence file read/writes)&lt;/p&gt;</comment>
                            <comment id="12966352" author="dlyubimov2" created="Thu, 2 Dec 2010 23:50:24 +0000"  >&lt;blockquote&gt;&lt;p&gt;That means that we are limited to cases with a few hundred million non-zero elements and are effectively unlimited on the number of potential columns of A.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I beleive in case of SSVD this statement only partially valid. &lt;/p&gt;

&lt;p&gt;It all depends on what you are spec&apos;d to. say we are spec&apos;d to 1G + java/mr overhead, and few hundred million non-zero elements will take few hundred megabytes multiplied by 8. Which is already all or more than all i have.  In my spec, (million non-zeros) it&apos;s only 8 mb that&apos;s seems ok. SSVD assumption doesn&apos;t include any significant memory allocation for rows of A, and most importantly, it doesn&apos;t have to, i think.  Philosophy here is that A is a file, and read it with a buffer to optimize I/O, but my stream buffer doesn&apos;t have to be forced to be 1G on me. &lt;/p&gt;</comment>
                            <comment id="12966368" author="tdunning" created="Fri, 3 Dec 2010 00:56:19 +0000"  >&lt;blockquote&gt;
&lt;p&gt;&amp;gt; This also raises the question of whether your combiner can be applied multiple times. I suspect yes. You will know better than I.&lt;/p&gt;

&lt;p&gt;Yes, that&apos;s the optional hierarchy increment that  i was talking about, that we can additionally implement if billion for m  and  unbound n is not good enough.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think I wrote poorly here.&lt;/p&gt;

&lt;p&gt;You don&apos;t get a choice.  The frame will run the combiner as many times as it feels it wants to.  It will run in the mapper or reducer or both or neither.&lt;/p&gt;

&lt;p&gt;Your combiner has to be ready to run any number of times on the output of the mapper or the output of other combiners or a combination of the same.  This isn&apos;t optional in Hadoop.  It may not have happened in the test runs you have done, but that doesn&apos;t imply anything about whether it will happen at another time.&lt;/p&gt;</comment>
                            <comment id="12966394" author="dlyubimov2" created="Fri, 3 Dec 2010 02:09:36 +0000"  >&lt;blockquote&gt;&lt;p&gt;You don&apos;t get a choice. The frame will run the combiner as many times as it feels it wants to. It will run in the mapper or reducer or both or neither.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ted, thank you for pointing it. As i mentioned couple of comments above, this is very well understood, was a concern from the beginning, never came up as a problem in tests, but I will do a small patch per my comment above that will make processing complexity even faster albeit code might become a little uglier. I am very well aware mapper can spill some records past combiner and send them to sort in the reducer per spec. I&lt;/p&gt;

&lt;p&gt;In fact, i had version that does just that on another branch. i just need to yank it and bring in sync with this branch. &lt;/p&gt;

&lt;p&gt;Thank you.&lt;/p&gt;</comment>
                            <comment id="12966576" author="dlyubimov2" created="Fri, 3 Dec 2010 16:36:48 +0000"  >&lt;p&gt;Patch update&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;eliminates use of combiners. Combiner&apos;s code moved back to mapper of the same task. This makes first stage of hierarchical R merges more efficient and consistent with MR spec.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Retested with 100k rows matrix.I will be updating working notes to reflect the change shortly.&lt;/p&gt;</comment>
                            <comment id="12966699" author="dlyubimov2" created="Fri, 3 Dec 2010 22:20:32 +0000"  >&lt;p&gt;Actually i think the biggest issue here is not scale for memory but what i call &apos;supersplits&apos;. &lt;/p&gt;

&lt;p&gt;if we have a row-wise matrix format, and by virtue of SSVD algorithm we have to consider no less than 500x500 blocks, then even with terasort 40tb 2000 node cluster block size parameter setting (128Mb) we are constrained to approx. ~30-50k densely wide matrices (even then the expectation is that half of the mapper&apos;s data would have to be downloaded from some other node). Which kind of defeats one of the main pitches of MR, code-data collocation. so in case of 1 mln densely wide matrices, and big cluster, we&apos;d be downloading like 99% of data from somewhere else. But we already paid in IO bandwidth when we created input matrix file in the first place, so why should it be a giant inefficient model of a supercomputer in a cloud? Custom batching approach would be way more efficient.&lt;/p&gt;

&lt;p&gt;I kind of dubbed the problem above as a &apos;supersplits problem&apos; in my head. &lt;/p&gt;

&lt;p&gt;-------&lt;br/&gt;
I beleive i am largely done with this mahout issue as far as method and code are concerned. We, of course, need to test it on something sizable. Benchmarks thus are a pending matter, and i expect they will be net io-bound but they  would be reasonably scaled for memory per discussion (less the issue of deficient prebuffering in VectorWritable on wide matrices) but  additional remedies are clear if needed. There might be some minor tweaks for outputting U,V required. Maybe add one or two more map-only passes over Q data to get additional scale for m. Maybe backport for hadoop 0.20 if mahout decides to release this code.&lt;/p&gt;

&lt;p&gt;Next problem i am going to ponder as a side project is devising an SSVD MR method on a block-wise serialized matrices. I think i can devise an SSVD method  that can efficiently address &quot;supersplits&quot; problem (with more shuffle and sort I/O though but it would be much more mr-like). Since I think Mahout supports neither block wise formatted matrices, nor, respectively, any BLAS ops for such inputs, an alternative approach to matrix (de-)serialization would have to be created. Conceivable scenario would be to reprocess mahout&apos;s row-wise matrices into such SSVD block-wise input at additional expense, but single-purposed data perhaps may well just vectorise block-wise directly.&lt;/p&gt;</comment>
                            <comment id="12966869" author="dlyubimov2" created="Sat, 4 Dec 2010 22:44:25 +0000"  >&lt;p&gt;update to working notes&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;changes in latest patch brought to sync with the doc (Q-Job mapper, no combiner)&lt;/li&gt;
	&lt;li&gt;added issues section including my thoughts on limitations of this approach and possible attack angles to alleviate them.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12967082" author="dlyubimov2" created="Mon, 6 Dec 2010 04:39:12 +0000"  >&lt;p&gt;Patch update. &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;finalized mahout CLI integration &amp;amp; tested.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Tested on 10k x 10k dense matrix in distributed hadoop mode (compressed source sequence file 743mb)  on my 3 yo dual core. It is indeed, as i expected, quite cpu-bound but good news is that it is so well parallelizable with most load on map-only jobs that it should be no problem to redistribute and front end doesn&apos;t require any or cpu capacity at all. Square symmetric matrix of 200x200 sizes computes instantaneously.&lt;/p&gt;

&lt;p&gt;The command line i used was: &lt;/p&gt;

&lt;p&gt;bin/mahout ssvd -i /mahout/ssvdtest/A -o /mahout/ssvd-out/1 -k 100 -p 100 -r 200 -t 2&lt;/p&gt;

&lt;p&gt;I also was testing this with CDH3b3 setup.&lt;/p&gt;</comment>
                            <comment id="12967089" author="dlyubimov2" created="Mon, 6 Dec 2010 05:28:39 +0000"  >&lt;p&gt;Sorry for iterating too often, but this was small but important fix for a showstopper.&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;added orthonormality assertions in local tests for V, U (they pass with epsilon 1e-10 or better).&lt;/li&gt;
	&lt;li&gt;small fixes to U, V jobs.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Now should be suitable for LSI type of work with documents having 10k-30k lemmas average. &lt;/p&gt;

&lt;p&gt;BTW when inserting dependencies for CDH3b3, additional jackson jar is required in order for local hadoop  test to work. &lt;br/&gt;
in CDH3b2 no such change is required. &lt;br/&gt;
Not sure about 0.21, but proper care should be taken as usual to integrate hadoop client&apos;s transitive dependencies into mahout dependencies.&lt;/p&gt;</comment>
                            <comment id="12974072" author="dlyubimov2" created="Wed, 22 Dec 2010 06:31:15 +0000"  >&lt;p&gt;Working notes update: &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;added more or less formal description of hierarchical QR technique.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12976442" author="dlyubimov2" created="Sat, 1 Jan 2011 19:56:47 +0000"  >&lt;p&gt;a code drop from the head of my stable branch. Various number of computational improvements. Cloudera repo integration for CDH dependency. Automatic detection of the label writable type in the input and propagating that to the U output. This is not the most efficient code i have but that&apos;s the only one that contains 0 Mahout code hacks.&lt;/p&gt;</comment>
                            <comment id="12976593" author="dlyubimov2" created="Mon, 3 Jan 2011 01:49:05 +0000"  >&lt;ul&gt;
	&lt;li&gt;couple commits for U, V jobs went missing. restored.&lt;/li&gt;
	&lt;li&gt;added couple options for computing U*pow(Sigma,0.5), V*pow(Sigma,0.5) outputs instead of U,V (makes &quot;user&quot;-&quot;item&quot; similarity computations easier)&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12977099" author="dlyubimov2" created="Tue, 4 Jan 2011 03:54:42 +0000"  >&lt;p&gt;Apparently, i lost several commits when setting code with github. Some CLI patches were lost. So, repared. &lt;/p&gt;

&lt;p&gt;just ran thru Reuters dataset example with no problems in hadoop mode on a single noder(with another performance branch, sibling of this one). The longest job seems to be B*Bt job at the moment, will have to look at it to see if there&apos;s a space for efficiency improvement. &lt;/p&gt;

&lt;p&gt;But the branch with VectorWritable preprocessing is especially usable as it saved on GC iterations a lot and runs much faster&lt;/p&gt;</comment>
                            <comment id="12977617" author="dlyubimov2" created="Wed, 5 Jan 2011 04:20:55 +0000"  >&lt;ul&gt;
	&lt;li&gt;Optimized B x B&apos; job . Now BBt step runs in 38 seconds in distributed mode on a single node (including the job setup which is about 20 seconds) (Reuters dataset with bigrams).&lt;/li&gt;
	&lt;li&gt;fixes R computation bug introduced in recent optimizations&lt;/li&gt;
	&lt;li&gt;Verified on Reuters dataset with bigrams. Ran in almost exactly 10 minutes on a single node per command line report (300 singular values) with both U and V jobs. V computation is now the longest (which is expected as likely # of stems &amp;gt;&amp;gt; # of documents). Not sure if there&apos;s much space for improvement there, but i will take a look.&lt;/li&gt;
	&lt;li&gt;Verified with multiple mappers, multiple q blocks again with full rank decomposition to confirm singular value matches to Colt stock solver&apos;s and orthogonality of U and V.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12986153" author="dlyubimov2" created="Tue, 25 Jan 2011 00:53:56 +0000"  >&lt;p&gt;This is essentially &apos;patch available&apos; except it requires 0.21 or CDH3 to compile (new API which is incomplete in 0.20). Hence i will not be changing the status here &amp;#8211; i guess we can use this when Mahout switches to new api. &lt;/p&gt;

&lt;p&gt;I will open another issue for backport of this to 0.20.&lt;/p&gt;

&lt;p&gt;PS. I guess i did click on &apos;patch available&apos;, didn&apos;t I? I did not mean to &amp;#8211; this patch changes the whole dependency thing for mahout. please roll back the status. My apologies.&lt;/p&gt;</comment>
                            <comment id="12986168" author="tdunning" created="Tue, 25 Jan 2011 01:31:58 +0000"  >&lt;blockquote&gt;
&lt;p&gt;PS. I guess i did click on &apos;patch available&apos;, didn&apos;t I? I did not mean to &amp;#8211; this patch changes the whole dependency thing for mahout. please roll back the status. My apologies.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Don&apos;t worry about it.  Until it is committed, it isn&apos;t real.  And as soon as somebody applies the patch and compiles, they will see failures and thus shouldn&apos;t commit it.&lt;/p&gt;</comment>
                            <comment id="12986176" author="dlyubimov2" created="Tue, 25 Jan 2011 02:01:46 +0000"  >&lt;p&gt;Actually, the last time i checked, and unless it got out of sync since then, it would compile and although there are couple of tests that would not pass with CDH3b3 (but all would pass with CDH3b2). It&apos;s just it grabs CDH3 from cloudera&apos;s repo. So integration attempt might be dangerous.&lt;/p&gt;</comment>
                            <comment id="12986179" author="tdunning" created="Tue, 25 Jan 2011 02:08:44 +0000"  >&lt;blockquote&gt;
&lt;p&gt;It&apos;s just it grabs CDH3 from cloudera&apos;s repo. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Meaning the pom references CDH?&lt;/p&gt;</comment>
                            <comment id="12986183" author="dlyubimov2" created="Tue, 25 Jan 2011 02:13:38 +0000"  >&lt;p&gt;yes. current patch references CDH. It&apos;s either 0.21 or CDH but since we use CDH in production, i retained CDH reference. We will have to change it to a concrete 0.21 reference. My best hope is that there&apos;s going to be another release of 0.21 or later which Hadoop group would more or less assert as a stable, at which point Mahout could switch dependencies. It looks like it will indeed require some revamping as there&apos;s a number of tests that is currently failing with that switch. Unless it is CDH2b2.&lt;/p&gt;</comment>
                            <comment id="13003285" author="hudson" created="Mon, 7 Mar 2011 07:50:58 +0000"  >&lt;p&gt;Integrated in Mahout-Quality #658 (See &lt;a href=&quot;https://hudson.apache.org/hudson/job/Mahout-Quality/658/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://hudson.apache.org/hudson/job/Mahout-Quality/658/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-593&quot; title=&quot;Backport of Stochastic SVD patch (Mahout-376) to hadoop 0.20 to ensure compatibility with current Mahout dependencies.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-593&quot;&gt;&lt;del&gt;MAHOUT-593&lt;/del&gt;&lt;/a&gt;: initial addition of Stochastic SVD method (related issue is &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-376&quot; title=&quot;Implement Map-reduce version of stochastic SVD&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-376&quot;&gt;&lt;del&gt;MAHOUT-376&lt;/del&gt;&lt;/a&gt;)&lt;/p&gt;</comment>
                            <comment id="13003322" author="srowen" created="Mon, 7 Mar 2011 10:53:09 +0000"  >&lt;p&gt;Am I correct that this is, for our purposes, subsumed into &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-593&quot; title=&quot;Backport of Stochastic SVD patch (Mahout-376) to hadoop 0.20 to ensure compatibility with current Mahout dependencies.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-593&quot;&gt;&lt;del&gt;MAHOUT-593&lt;/del&gt;&lt;/a&gt;? It&apos;s the same patch, just ported to work with Mahout now. I presume that going forward we&apos;ll want to iterate on that version rather than entertain this version any more.&lt;/p&gt;

&lt;p&gt;Reopen if I&apos;m wrong.&lt;/p&gt;</comment>
                            <comment id="13003493" author="dlyubimov2" created="Mon, 7 Mar 2011 18:24:53 +0000"  >&lt;p&gt;That&apos;s fine. We&apos;ll have to go back to a slight permutation of this patch with 0.21 api, but it&apos;s fine. I&apos;ll re-create it when it&apos;s time. &lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                            <outwardlinks description="duplicates">
                                        <issuelink>
            <issuekey id="12457366">MAHOUT-309</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12496624">MAHOUT-593</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12501085">MAHOUT-623</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12441529" name="ASF.LICENSE.NOT.GRANTED--Stochastic SVD using eigensolver trick.pdf" size="250394" author="dlyubimov" created="Mon, 12 Apr 2010 19:32:22 +0100"/>
                            <attachment id="12441386" name="ASF.LICENSE.NOT.GRANTED--sd-bib.bib" size="302" author="tdunning" created="Sun, 11 Apr 2010 14:42:52 +0100"/>
                            <attachment id="12441387" name="ASF.LICENSE.NOT.GRANTED--sd.pdf" size="139324" author="tdunning" created="Sun, 11 Apr 2010 14:42:52 +0100"/>
                            <attachment id="12441385" name="ASF.LICENSE.NOT.GRANTED--sd.tex" size="5091" author="tdunning" created="Sun, 11 Apr 2010 14:42:52 +0100"/>
                            <attachment id="12444034" name="MAHOUT-376.patch" size="22589" author="tdunning" created="Sat, 8 May 2010 07:21:35 +0100"/>
                            <attachment id="12456827" name="Modified stochastic svd algorithm for mapreduce.pdf" size="349161" author="dlyubimov2" created="Mon, 11 Oct 2010 06:41:16 +0100"/>
                            <attachment id="12459940" name="QR decomposition for Map.pdf" size="367824" author="dlyubimov2" created="Thu, 18 Nov 2010 19:45:10 +0000"/>
                            <attachment id="12459679" name="QR decomposition for Map.pdf" size="358201" author="dlyubimov2" created="Tue, 16 Nov 2010 05:26:37 +0000"/>
                            <attachment id="12459549" name="QR decomposition for Map.pdf" size="317376" author="dlyubimov2" created="Sun, 14 Nov 2010 07:55:46 +0000"/>
                            <attachment id="12466791" name="SSVD working notes.pdf" size="1140103" author="dlyubimov2" created="Wed, 22 Dec 2010 06:31:15 +0000"/>
                            <attachment id="12465315" name="SSVD working notes.pdf" size="1079328" author="dlyubimov2" created="Sat, 4 Dec 2010 22:44:24 +0000"/>
                            <attachment id="12464833" name="SSVD working notes.pdf" size="1046386" author="dlyubimov2" created="Mon, 29 Nov 2010 08:43:53 +0000"/>
                            <attachment id="12464793" name="SSVD working notes.pdf" size="1044698" author="dlyubimov2" created="Sun, 28 Nov 2010 00:25:25 +0000"/>
                            <attachment id="12464782" name="SSVD working notes.pdf" size="1039715" author="dlyubimov2" created="Sat, 27 Nov 2010 17:34:48 +0000"/>
                            <attachment id="12457920" name="sd.pdf" size="154173" author="tdunning" created="Sun, 24 Oct 2010 08:10:14 +0100"/>
                            <attachment id="12457275" name="sd.pdf" size="156665" author="tdunning" created="Fri, 15 Oct 2010 18:15:29 +0100"/>
                            <attachment id="12457018" name="sd.pdf" size="149006" author="tdunning" created="Tue, 12 Oct 2010 22:49:56 +0100"/>
                            <attachment id="12457921" name="sd.tex" size="11076" author="tdunning" created="Sun, 24 Oct 2010 08:10:14 +0100"/>
                            <attachment id="12457276" name="sd.tex" size="10696" author="tdunning" created="Fri, 15 Oct 2010 18:15:29 +0100"/>
                            <attachment id="12457017" name="sd.tex" size="8632" author="tdunning" created="Tue, 12 Oct 2010 22:49:56 +0100"/>
                            <attachment id="12467514" name="ssvd-CDH3-or-0.21.patch.gz" size="70227" author="dlyubimov2" created="Wed, 5 Jan 2011 04:20:55 +0000"/>
                            <attachment id="12467397" name="ssvd-CDH3-or-0.21.patch.gz" size="69435" author="dlyubimov2" created="Tue, 4 Jan 2011 03:54:42 +0000"/>
                            <attachment id="12467301" name="ssvd-CDH3-or-0.21.patch.gz" size="25513" author="dlyubimov2" created="Mon, 3 Jan 2011 01:49:05 +0000"/>
                            <attachment id="12467256" name="ssvd-CDH3-or-0.21.patch.gz" size="25011" author="dlyubimov2" created="Sat, 1 Jan 2011 19:56:47 +0000"/>
                            <attachment id="12465364" name="ssvd-CDH3-or-0.21.patch.gz" size="24282" author="dlyubimov2" created="Mon, 6 Dec 2010 05:28:39 +0000"/>
                            <attachment id="12465361" name="ssvd-CDH3-or-0.21.patch.gz" size="24251" author="dlyubimov2" created="Mon, 6 Dec 2010 04:39:12 +0000"/>
                            <attachment id="12465244" name="ssvd-CDH3-or-0.21.patch.gz" size="23802" author="dlyubimov2" created="Fri, 3 Dec 2010 16:36:48 +0000"/>
                            <attachment id="12465113" name="ssvd-CDH3-or-0.21.patch.gz" size="23786" author="dlyubimov2" created="Thu, 2 Dec 2010 07:59:41 +0000"/>
                            <attachment id="12465032" name="ssvd-CDH3-or-0.21.patch.gz" size="23713" author="dlyubimov2" created="Wed, 1 Dec 2010 07:43:18 +0000"/>
                            <attachment id="12460519" name="ssvd-m1.patch.gz" size="18870" author="dlyubimov2" created="Fri, 26 Nov 2010 19:36:19 +0000"/>
                            <attachment id="12464792" name="ssvd-m2.patch.gz" size="20957" author="dlyubimov2" created="Sun, 28 Nov 2010 00:25:25 +0000"/>
                            <attachment id="12464832" name="ssvd-m3.patch.gz" size="21871" author="dlyubimov2" created="Mon, 29 Nov 2010 08:43:53 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>32.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Mon, 12 Apr 2010 18:32:22 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9688</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hxy5hz:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>23042</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                </customfields>
    </item>
</channel>
</rss>