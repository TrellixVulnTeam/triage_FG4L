<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 04:22:16 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/MAHOUT-124/MAHOUT-124.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[MAHOUT-124] Online Classification using HBase</title>
                <link>https://issues.apache.org/jira/browse/MAHOUT-124</link>
                <project id="12310751" key="MAHOUT">Mahout</project>
                    <description>&lt;ol&gt;
	&lt;li&gt;Batch classification of flat file documents and flat file model:&lt;/li&gt;
	&lt;li&gt;Storing the model in HBase and the end of Model Building Map/Reduce stages&lt;/li&gt;
	&lt;li&gt;Using the model stored in HBase create an interface (both command line and web service) to classify a give document&lt;/li&gt;
	&lt;li&gt;Using the model stored in HBase, batch classify documents stored on the HDFS&lt;/li&gt;
&lt;/ol&gt;
</description>
                <environment></environment>
        <key id="12426479">MAHOUT-124</key>
            <summary>Online Classification using HBase</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="isabel">Isabel Drost-Fromm</assignee>
                                    <reporter username="robinanil">Robin Anil</reporter>
                        <labels>
                    </labels>
                <created>Wed, 27 May 2009 16:18:50 +0100</created>
                <updated>Wed, 18 Nov 2009 14:05:54 +0000</updated>
                            <resolved>Fri, 28 Aug 2009 10:41:08 +0100</resolved>
                                    <version>0.2</version>
                                    <fixVersion>0.2</fixVersion>
                                    <component>Classification</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                <comments>
                            <comment id="12722991" author="robinanil" created="Tue, 23 Jun 2009 08:22:24 +0100"  >&lt;p&gt;Added HBase Support for CBayes Classifier&lt;/p&gt;

&lt;p&gt;The HBase data model is&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt; 
{
   &quot;feature1&quot;:
    {
              &quot;info:label1&quot;:  &quot;score1&quot;,
              &quot;info:label2&quot;:  &quot;score2&quot;,
              &quot;info:Sigma_j&quot;:  &quot;sum&quot; //sum of weights
    }
    &quot;feature2&quot;:
    {
              &quot;info:label1&quot;:  &quot;score1&quot;,
              &quot;info:Sigma_j&quot;:  &quot;sum&quot; //sum of weights
    }
    &quot;feature2&quot;:
    {
              &quot;info:label2&quot;:  &quot;score2&quot;,
              &quot;info:Sigma_j&quot;:  &quot;sum&quot; //sum of weights
    }

}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt; 

&lt;p&gt;Here are some links to get you started on Hbase&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt; 
  http://wiki.apache.org/hadoop/Hbase/MapReduce
  http://jimbojw.com/wiki/index.php?title=Understanding_Hbase_and_BigTable
  http://jimbojw.com/wiki/index.php?title=Understanding_HBase_column-family_performance_options
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt; 

&lt;ul&gt;
	&lt;li&gt;I have disabled get-enwiki maven task which was done by default while compiling examples. It should be kept as an option not as default.  I dont like downloading 2.4 gigs just to run Mahout.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Put hbase-0.19.3 JAR file in the core/lib directory. the maven build files take it up from there (Thanks Isabel for pointing it to me)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;I have also commented out a like in the ant jar which was causing the mahout-example job file to take twice the size due to multiple copies of dependent jar files getting zipped up&lt;br/&gt;
This was causing the map reduce jobs to take a couple of seconds extra to start&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;This patch breaks the Bayes code, Hbase is Server is necessary to run this Bayes examples if you apply this patch, it uses HBase to get the weight sparse matrix while loading the label sums from hdfs as it was doing previously.&lt;br/&gt;
        More work is needed to refactor Bayes code this so that users can independantly use eihter hdfs / hbase to store the classification model&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Added meaningful jobnames and reporter status to monitor the job.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;removed the map task number setting from the code. It now uses the default map  task as specifed in your hadoop conf&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Hbase inserts takes place at around 4000/s on a 2.4GHz core2duo 1GB VMware running ubuntu karmic koala.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;The Hbase reads are very slow at the moment. at around 150/s. I had enabled inMemory and BloomFilters on both the HBase table and column.  More investigation is needed to improve the speed. It seems more time is spent in searching non-existant columns. When you classify a document, it tries to go through all the given features in the document for all the labels. Suppose a document has 1000 words. then it takes 1000x20 lookups (in the 20 news groups example). A majority of these are empty cells, HBase talks about enabling bloom filters to improve efficiency. But as of 0.19.3 i believe its not part of the code. atleast i cant perceive any benefits&lt;/li&gt;
&lt;/ul&gt;
















</comment>
                            <comment id="12724159" author="otis" created="Thu, 25 Jun 2009 18:50:04 +0100"  >&lt;p&gt;Just read about HBase 0.20 the other day - over and order of magnitude performance improvements and random reads approaching the speed of RDBMS is what the presentation I saw claimed.&lt;/p&gt;</comment>
                            <comment id="12725258" author="robinanil" created="Mon, 29 Jun 2009 18:22:38 +0100"  >&lt;p&gt;I played around with Hbase-0.20.0. It was fast but not fast enough&lt;/p&gt;

&lt;p&gt;I had trouble running HBase-0.20.0-alpha. The Reducers were crashing because zookeeper connections kept crossing the limit(10 default).  I had to manually kill some net connections to zookeeper to finish the program. I tried overriding Closable.close() method in Reducer to close the table object. But things were still problematic. I have not received a reply to it on IRC. I will post this to Hbase soon. With some luck I finished the map/reduces and loaded the model into Hbase.&lt;/p&gt;

&lt;p&gt;Listen to the following Highlights&lt;/p&gt;

&lt;p&gt;My model had 196K rows with 20+1 columns of 8 bytes each for the 20-newsgroups data that means 180 byte records with LZO compression enabled. I am creating a blog entry soon which details how to enable lzo for version 0.20.0&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Inserts were around the same range. 4K Cell/s  (feature, label =&amp;gt; value)&lt;/li&gt;
	&lt;li&gt;Read speed had definitely improved. I got something like 800 Cells/s up from 150/s i had earlier (translating to 40 rows/s ==  25 ms/row  but way less than 1.42 ms which I believe(Correct me) is for a single 1000 byte record, in the following slides &lt;a href=&quot;http://wiki.apache.org/hadoop-data/attachments/HBase(2f)HBasePresentations/attachments/HBase_Goes_Realtime.pdf&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://wiki.apache.org/hadoop-data/attachments/HBase(2f)HBasePresentations/attachments/HBase_Goes_Realtime.pdf&lt;/a&gt;)&lt;/li&gt;
	&lt;li&gt;On suggestion by isabel used a LRU cache using LinkedHashMap. I got nearly 98%+ cache Hit Rate. But ran into trouble when it tried to classify a spam document which had aroudn 30K junk words.&lt;/li&gt;
	&lt;li&gt;So i increased cache to 100K rows.  Entire 20-newsgroups(24 MB) was classified in just above 5 mins as compared to 1 min load time + 1 min classification time of the non Hbase version(In memory hashMap) with a 99.2% hitrate and total Hbase lookup == 200859 which is around 5K above the total lookup actually necessary 196K. Maybe a better caching mechanism can take care of this.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Looking ahead&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Try to fix the zookeeper bug or close Htable Properly.&lt;/li&gt;
	&lt;li&gt;Implement a good caching mechanism.  Currently I have zeroed in on EHCache based on this performance test
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;                      http://javalandscape.blogspot.com/2009/01/cachingcaching-algorithms-and-caching.html 
                      http://javalandscape.blogspot.com/2009/05/intro-to-cachingcaching-algorithms-and.html
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Currently classifying a single document  with 1000 features from cold start will take around  10-20 seconds. Once the cache is full the lookups would be rather fast&lt;br/&gt;
Maybe i could try with LFU instead of LRU, I will post the results once i get a go ahead from the mahout community with EHCache or something equivalent. EHCache is also on maven. &lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;I have read that ARC(&lt;a href=&quot;http://en.wikipedia.org/wiki/Adaptive_Replacement_Cache&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Adaptive_Replacement_Cache&lt;/a&gt;) improves over LFU and LRU, And if time permits, I may be able to code it up (EH cache currently doesn&apos;t support ARC) subject to what Mahout developers suggest&lt;/li&gt;
	&lt;li&gt;I need a way to enable the client to use the specified or maximum heap possible for caching. This method should be better than keeping a fixed row entries, Since we have no control over the number of columns(varies according to data)&lt;/li&gt;
	&lt;li&gt;Next Step Refactor code and submit patch.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Any thoughts&lt;/p&gt;



</comment>
                            <comment id="12726203" author="robinanil" created="Wed, 1 Jul 2009 20:57:53 +0100"  >&lt;p&gt;Implemented Hybrid Caching with LFU and LRU.&lt;br/&gt;
Found EHCache is slower than HashMap. Get usually take around 0.0018 s avg for a get(inMemory). which i thought was very poor.&lt;/p&gt;

&lt;p&gt;I did some testing. where i check the EHCache second if my LRU returns nothing and inserts into EHCache only till it evicts an amount == Capacity as given below&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;
EHcache LFU Capacity = 20000 
LinkedHashMap LRU Capacity  = 100000
nCalls = 18828;
sumTime = 1213.468s;
minTime = 2.67E-4ms;
maxTime = 67123.484ms;
meanTime = 64.45018ms;
stdDevTime = 650.7636ms;

EHcache LFU Capacity = 5000 
LinkedHashMap LRU Capacity  = 100000
nCalls = 18828;
sumTime = 622.92816s;
minTime = 1.6E-4ms;
maxTime = 20972.49ms;
meanTime = 33.0852ms;
stdDevTime = 206.95724ms;

EHcache LFU Capacity = 0
LinkedHashMap LRU Capacity  = 100000
nCalls = 18828;
sumTime = 353.0143s;
minTime = 0.0ms;
maxTime = 9331.663ms;
meanTime = 18.749432ms;
stdDevTime = 99.65778ms;

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;p&gt;The numbers speak for themselves. I think i will stick to LinkedHashMap  for now&lt;/p&gt;</comment>
                            <comment id="12726206" author="robinanil" created="Wed, 1 Jul 2009 21:00:02 +0100"  >&lt;p&gt;The numbers above are for per document classification time. or the time spend in Classifier.classify()&lt;/p&gt;</comment>
                            <comment id="12726211" author="tdunning" created="Wed, 1 Jul 2009 21:20:19 +0100"  >&lt;blockquote&gt;&lt;p&gt;The numbers speak for themselves. I think i will stick to LinkedHashMap for now&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;They do indeed.  Simple wins again.&lt;/p&gt;

&lt;p&gt;Nice work.  Finding out the simpler solution is better is always pleasant, but many avoid doing the tests to check it.&lt;/p&gt;</comment>
                            <comment id="12726923" author="robinanil" created="Fri, 3 Jul 2009 12:52:15 +0100"  >&lt;p&gt;Added&lt;/p&gt;

&lt;p&gt;LRUCache&amp;lt;K,V&amp;gt; &lt;br/&gt;
LFUCache&amp;lt;K,V&amp;gt; &lt;br/&gt;
HybridCache&amp;lt;K,V&amp;gt;&lt;/p&gt;

&lt;p&gt;The LFU was implemented using two Maps a TreeMap for eviction of keys and a HashMap for the data&lt;br/&gt;
LRU uses LinkedHashMap directly.&lt;/p&gt;

&lt;p&gt;PS: the numbers given below differs drastically from those above due to two reasons&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;(I am playing an mp3 + a big download is going on)&lt;/li&gt;
	&lt;li&gt;I have encapsulated LinkedHashMap LRU inside LRUCache&amp;lt;K,V&amp;gt; template class. additional over head is due to the extra call&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;So I ran the tests twice one with only LRU and another with both LFU and LRU, so its safe to say the numbers are stable given two steady background process are going on.&lt;/p&gt;

&lt;p&gt;Given below are the numbers. &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;The number of HBase row lookups have gone down from 200859 to 194228 when adding an LFUCache without inflicting additional overhead.&lt;/li&gt;
	&lt;li&gt;The Max Classification time and std Deviation have gone down with the inclusion of an LFU Cache.&lt;/li&gt;
&lt;/ul&gt;


&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;LFU Capacity = 0
LRU Capacity  = 100000
09/07/03 16:49:21 INFO cbayes.CBayesModel: 48000000 47799141 200859 99.58154 100000
nCalls = 18828;
sumTime = 431.78778s;
minTime = 0.0ms;
maxTime = 13197.114ms;
meanTime = 22.933277ms;
stdDevTime = 128.23552ms;

LFU Capacity = 20000
LRU Capacity  = 100000
09/07/03 17:01:17 INFO cbayes.CBayesModel: 48000000 47805772 194228 99.59536 120000
nCalls = 18828;
sumTime = 428.96442s;
minTime = 0.0ms;
maxTime = 10064.428ms;
meanTime = 22.783323ms;
stdDevTime = 110.68307ms;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12727364" author="robinanil" created="Sun, 5 Jul 2009 22:20:20 +0100"  >&lt;ul&gt;
	&lt;li&gt;Added command line option dataSource to choose between hdfs or hbase+hdfs model storage&lt;/li&gt;
	&lt;li&gt;replaced command line option -p (path) with -m (model location) which either takes sequence file or Hbase table depending on above&lt;/li&gt;
	&lt;li&gt;First level refactor. Build the just hbase model or just the sequence file model. Further revisions will streamline the code to remove redundancies.&lt;/li&gt;
	&lt;li&gt;Hbase get encapsulated in Hybrid Cache(LFU + LRU)&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12728311" author="isabel" created="Tue, 7 Jul 2009 20:25:16 +0100"  >&lt;p&gt;Some initial comments on the patch:&lt;/p&gt;

&lt;p&gt;org/apache/mahout/utils/Cache.java - I am missing some documentation for the methods. For interfaces, you can omit the public with methods. For classes implementing this interface, you might want to at least use @inheritDoc to link back to the original documentation. Please also note in the class comment whether your implementation is safe to use in a multi-threaded context or not.&lt;/p&gt;

&lt;p&gt;org.apache.mahout.common.Model - To me it looks a bit weird to add a dependency to HBase directly to the model. I would prefer the HBase implementation to be less tightly coupled with the core code. Currently it looks like the model is really doing two tasks at once: Implementing an in-memory-model as well as an HBase model. I think it should be possible to refactor the code such that the two can be separated into distinct classes that can then be used interchangeably. My first guess would be that the strategy pattern should be helpful with this task. &lt;/p&gt;

&lt;p&gt;You probably will have to refactor CBayesModel and BayesModel as well. The same applies to org/apache/mahout/classifier/Classify.java and CBayesModel, Model, BayesTfIdfDriver, BayesTfIDFReducer, BayesWeightSummerReducer.&lt;/p&gt;

&lt;p&gt;org.apache.mahout.classifier.cbase - I really like your additions for reporting progress back to Hadoop. I would suggest to split these from the patch, open a separate Issue and attach the changes there. This would keep this patch more focussed on the original task of adding HBase support.&lt;/p&gt;

&lt;p&gt;org.apache.mahout.classifier.cbase.CBayesModel - Please remove the code you commented out if you do not need it anymore. In case of catching an IOException you should at least write some warning log message (e.g. line 60). &lt;/p&gt;</comment>
                            <comment id="12728325" author="stack" created="Tue, 7 Jul 2009 20:52:59 +0100"  >&lt;p&gt;Here&apos;s a few small comments Robin:&lt;/p&gt;

&lt;p&gt;In HybridCache, you import&lt;/p&gt;

&lt;p&gt;+import org.apache.hadoop.hbase.client.Result;&lt;/p&gt;

&lt;p&gt;Is this intentional?&lt;/p&gt;

&lt;p&gt;Similarily in BayesModel and HbaseModelReader, there are hbase imports that do not look as though they are being used.&lt;/p&gt;

&lt;p&gt;Do you have to write your own LRUCache?  Can you not rob one from elsewhere?&lt;/p&gt;

&lt;p&gt;In BayesTfIdfReducer, if you write hbase, you also write the collector?  You might consider using TableOutputFormat if using hbase?&lt;/p&gt;

&lt;p&gt;In the same class, in configure, you make an HTable whether its being used or not.&lt;/p&gt;

&lt;p&gt;Regards:&lt;/p&gt;

&lt;p&gt;+      hcd.setBloomfilter(true);&lt;br/&gt;
+      hcd.setInMemory(true);&lt;br/&gt;
+      hcd.setBlockCacheEnabled(true);&lt;br/&gt;
+      hcd.setCompressionType(Algorithm.LZO);&lt;/p&gt;

&lt;p&gt;FYI, bloom filter has no effect.  You&apos;ve done the work to put in the LZO support?&lt;/p&gt;

&lt;p&gt;Would suggest you not suppress these exceptions:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+      &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
+        hba.disableTable(output);
+        hba.deleteTable(output);
+      } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (TableNotFoundException ex) {
+      } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (TableNotDisabledException ex) {
+      }
+      hba.createTable(ht);

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If table not fully disabled, then its going to give you bother when you try to recreate.  Be good to know why.  Should also add in a compaction on .META. between delete and create as the shell command does to avoid known big table remove issues.&lt;/p&gt;






</comment>
                            <comment id="12728971" author="robinanil" created="Thu, 9 Jul 2009 01:14:29 +0100"  >&lt;p&gt;Refactored the code. Removed Model BayesModel CBayesModel Classifier BayesClassifier CBayes Classifier&lt;/p&gt;

&lt;p&gt;Now there are 4 classes BayesAlgorithm CBayesAlgorithm InMemoryBayesDatastore HBaseBayesDataStore&lt;/p&gt;

&lt;p&gt;Initialize a Classifier as&lt;/p&gt;

&lt;p&gt;new ClassifierContext(new BayesAlgorithm, new HBaseBayesDataStore)&lt;/p&gt;

&lt;p&gt;Interface Algorithm assumes Datastore is a collection of Matrices and Vectors which can be accessed by the Matrix/Vector name(String), and row/column or index of the cell &lt;/p&gt;

&lt;p&gt;Tests therefore have become useless. So I am writing new Tests now&lt;/p&gt;

&lt;p&gt;Have tried to cleanup code whereever possible. &lt;/p&gt;

&lt;p&gt;In Reply to stacks comments. (In order)&lt;/p&gt;

&lt;p&gt;removed all unused imports. &lt;/p&gt;

&lt;p&gt;HbayesModelReader is removed. Made useless after the above refactor&lt;/p&gt;

&lt;p&gt;LRUCache the class just uses LinkedHashMap in the backend, I had tried EHCache(LFU as well as LRU) earlier(See above) . It was slower than HashMaps &lt;/p&gt;

&lt;p&gt;In the BayesTfIdfReducer, I have to write both to the filesystem or the Hbase depending on the configuration. Will TableOutputFormat be faster interms of Hbase Inserts ? If then I might  create another set of Map/Reduces specifically for Hbase instead of using the same class&lt;br/&gt;
Htable is used only if the output is set as hbase not hdfs&lt;/p&gt;

&lt;p&gt;About setInMemory and LZO support: specifically done for 20news groups and for my test setup. I would remove it in the final patch. &lt;br/&gt;
About bloomFilers?  Are they still not implemented yet? &lt;/p&gt;

&lt;p&gt;Yeah, sure thing will replace those with  a hba.TableExists(output) check. &lt;br/&gt;
about compaction just this right hba.compact(&quot;.META.&quot;);?&lt;/p&gt;




</comment>
                            <comment id="12729039" author="stack" created="Thu, 9 Jul 2009 05:02:33 +0100"  >&lt;p&gt;@Robin&lt;/p&gt;

&lt;p&gt;On &quot;tried EHCache&quot;, pardon me.  I just read up in issue and see you tried it.  Pardon me.&lt;/p&gt;

&lt;p&gt;.bq Will TableOutputFormat be faster interms of Hbase Inserts&lt;/p&gt;

&lt;p&gt;The new mapreduce package hbase classes in TRUNK have been all rejiggered.  Should be better but IIUC, you are running all on your laptop, virtual machines too?  If so, you probably won&apos;t notice any difference.&lt;/p&gt;

&lt;p&gt;.bq ....bloomfilters...&lt;/p&gt;

&lt;p&gt;Not implemented.  Currently thought is too little benefit for amount of RAM consumed.  Related, in-memory was just implemented so update and you should catch benefits.&lt;/p&gt;

&lt;p&gt;.bq ....about compaction....&lt;/p&gt;

&lt;p&gt;You want majorCompact, not compact.  Here is API from 0.19: &lt;a href=&quot;http://hadoop.apache.org/hbase/docs/r0.19.3/api/org/apache/hadoop/hbase/client/HBaseAdmin.html#majorCompact(byte[&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hadoop.apache.org/hbase/docs/r0.19.3/api/org/apache/hadoop/hbase/client/HBaseAdmin.html#majorCompact(byte[&lt;/a&gt;])&lt;/p&gt;

&lt;p&gt;Good stuff Robin&lt;/p&gt;


</comment>
                            <comment id="12730342" author="robinanil" created="Mon, 13 Jul 2009 14:33:11 +0100"  >&lt;ul&gt;
	&lt;li&gt;Changes in CLI switches for  TestClassifier
	&lt;ul&gt;
		&lt;li&gt;-m or --model for ModelName of BasePath&lt;/li&gt;
		&lt;li&gt;-source or --dataSource = (hdfs|hbase) for specifying dataSource&lt;/li&gt;
		&lt;li&gt;-d  or --testDir (base directory of test data)&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Deleted BayesModel and CBayesModel(kept the Model abstract class since it has reference implementation codes. Can remove it later)&lt;/li&gt;
	&lt;li&gt;Usage :  ClassifierContext classifier = new ClassiferContext(new Algorithm, new Datastore)&lt;/li&gt;
	&lt;li&gt;Choice of Bayes/CBayes Algorithm  and InMemoryBayes/HBaseBayes Datastore&lt;/li&gt;
	&lt;li&gt;Added documentation to the public interfaces and classes&lt;/li&gt;
	&lt;li&gt;Earlier tests modifed to work on the new API. All Inmemory Tests pass. Will have to figure out a way to mimick Hbase Server to test HbaseDatastore class.&lt;/li&gt;
	&lt;li&gt;Verified: Same classification results for Bayes Cbayes over Imemory or Hbase Datastore&lt;/li&gt;
	&lt;li&gt;Removed HBase LZO Compression/InMemory Option in this Patch. Will have to provide external configuration Methods for Users to choose these Optimizations if their Cluster supports it&lt;/li&gt;
&lt;/ul&gt;

</comment>
                            <comment id="12733069" author="isabel" created="Sun, 19 Jul 2009 22:26:15 +0100"  >&lt;p&gt;*ThetaNormalizerReducer, *BayesTFIDFReducer and *BayesSummerReducer still have dependencies to HBase - I think one can factor them out.&lt;/p&gt;

&lt;p&gt;Interface &quot;Algorithm&quot; - I think it might sense to initialise the the Algorithm with a reference to the datastore instead of injecting that reference with every method call. Other than that: Looks good. Bayes and CBayes look a lot cleaner now.&lt;/p&gt;

&lt;p&gt;Interface Datastore looks good. I like the separation of data handling and actual algorithm implementation.&lt;/p&gt;

&lt;p&gt;I would move Pair over to the utils package.&lt;/p&gt;

&lt;p&gt;Good work Robin.&lt;/p&gt;</comment>
                            <comment id="12733075" author="isabel" created="Sun, 19 Jul 2009 22:32:45 +0100"  >&lt;p&gt;Just forgot two final notes:&lt;/p&gt;

&lt;p&gt;You should update your svn-checkout. The patch was done against an old revision of trunk and does no longer apply cleanly.&lt;/p&gt;

&lt;p&gt;The patch was broken - line 988 in the patch file has a broken directive: &lt;/p&gt;

&lt;p&gt;@@ -48,67 +54,107 @@&lt;/p&gt;

&lt;p&gt;should really be&lt;/p&gt;

&lt;p&gt;@@ -48,67 +54,105 @@&lt;/p&gt;

&lt;p&gt;the effect being that &quot;patch&quot; assumes a hunk length of 107 lines which makes it fail. Your hunk is only 105 lines, so better not lie to &quot;patch&quot; &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; However, that one was trivial to fix.&lt;/p&gt;

&lt;p&gt;(Thanks to Thilo Fromm for helping me fix and explain that.)&lt;/p&gt;</comment>
                            <comment id="12734219" author="gsingers" created="Wed, 22 Jul 2009 18:52:11 +0100"  >&lt;p&gt;FYI, patch no longer applies.&lt;/p&gt;</comment>
                            <comment id="12734268" author="robinanil" created="Wed, 22 Jul 2009 20:34:31 +0100"  >&lt;p&gt;Fixed the broken code after Checkstyle update was done. Tests Pass. Checkstyle will throw warnings.  &lt;/p&gt;</comment>
                            <comment id="12738034" author="robinanil" created="Sun, 2 Aug 2009 11:09:42 +0100"  >&lt;p&gt;Added parallel Classification from both hdfs and hbase. &lt;/p&gt;

&lt;p&gt;the usage is given below. &lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;hadoop jar examples/target/mahout-examples-0.2-SNAPSHOT.job org.apache.mahout.classifier.bayes.TestClassifier -m model -d &amp;lt;INPUT&amp;gt; -ng 1 -type cbayes -source &amp;lt;hdfs|hbase&amp;gt; -method &amp;lt;sequential|mapreduce&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The patch is using the latest trunk. I hope this get committed soon. Its has become too difficult to manage files across &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-124&quot; title=&quot;Online Classification using HBase&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-124&quot;&gt;&lt;del&gt;MAHOUT-124&lt;/del&gt;&lt;/a&gt; and FPGrowth algo. &lt;/p&gt;

&lt;p&gt;I am waiting for this commit before attempting to change all Text Writables to Vector.&lt;/p&gt;

&lt;p&gt;Package summary will be added as a part of another issue&lt;/p&gt;</comment>
                            <comment id="12742031" author="isabel" created="Tue, 11 Aug 2009 20:38:57 +0100"  >
&lt;p&gt;Alltogether really nice changes. The patch now applies to trunk without problems and builds (except for the missing hbase dependency). As this will be one of the last reviews, I tried to be a little more picky also with minor changes like added System.out.println and missing documentation...&lt;/p&gt;

&lt;p&gt;The ant config file (build.xml) contains changes that I see nowhere explained. Are they supposed to remain for the final patch?&lt;/p&gt;

&lt;p&gt;In the examples concerning the TestClassifier&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;it has imports for java.io.* and java.util.* - for the final patch could you please revert those to the specific imports?&lt;/li&gt;
	&lt;li&gt;could you please try to avoid reformatting the code as much as possible? It makes reading patches a whole lot easier.&lt;/li&gt;
	&lt;li&gt;in line 129 there is quite a bit of code commented out - better through it out entirely? If needed later the snippet is still in jira.&lt;/li&gt;
	&lt;li&gt;line 224 - have the timing statistics been left in intentionally?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;utils/nlp/NGrams&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;The class is missing documentation. I guess your intention was to generate nGrams from a line of text, not the whole document? Otherwise holding document and nGrams both in memory seems a little bit much. There also seems to be no unit test for it?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The classes implementing the caching algorithms are missing documentation. At least some /** &lt;/p&gt;
{@inheritDoc}
&lt;p&gt; */ and a short comment on top that explains the purpose of the implemention would be nice. (Same applies for Pair and Parameters).&lt;/p&gt;

&lt;p&gt;CBayesNormalizerReducer still has HBase Dependencies - is it possible to factor them out?&lt;/p&gt;

&lt;p&gt;BayesThetaNormalizerDriver - setting the number of map tasks was commented away compared with trunk. Intentional?&lt;/p&gt;

&lt;p&gt;BayesClassifierMapper - lines 106, 110 and following: Shouldn&apos;t the log message be something like &quot;Using ...&quot; instead of &quot;Testing ...&quot;?&lt;/p&gt;

&lt;p&gt;classifier/bayes/interfaces/algorithm/Algorithm - you still give a pointer to the datastore with every method call to the Algorithm. Wouldn&apos;t the interface look cleaner if the Algorithm would hold a reference to an initialized datastore and use that for further requests? I don&apos;t think it is very likely that users will go to HBase for the first document to classify and to an InMemoryStore for the next document.&lt;/p&gt;

&lt;p&gt;bayes/algorithm/CBayesAlgorithm, BayesAlgorithm, bayes/common/ClassifierPriorityQueue - is missing some basic javaDoc.&lt;/p&gt;

&lt;p&gt;BayesTfIdfDriver, BayesTfIdfReducer, BayesWeightSummerReducer - I assume the dependency to HBase cannot be factored out?&lt;/p&gt;

&lt;p&gt;BayesFeatureMapper - there is a System.out.println in there...&lt;/p&gt;

&lt;p&gt;One last question: You reference hbase-0.20.0 which is not released yet. I guess we should include a prebuilt version in our lib directory and ship that until hbase has an official release to use?&lt;/p&gt;</comment>
                            <comment id="12742210" author="robinanil" created="Wed, 12 Aug 2009 05:22:09 +0100"  >&lt;ul&gt;
	&lt;li&gt;Throw out commented code... Done&lt;/li&gt;
	&lt;li&gt;Ant config was done to decrease the job jar file size. See first comment in this issue point No:3&lt;/li&gt;
	&lt;li&gt;I need the new Eclipse Code formatter for that purpose. I am still using the lucene code formatter, which is causing this break.&lt;/li&gt;
	&lt;li&gt;Docs... already on it!&lt;/li&gt;
	&lt;li&gt;Map/Reduce jobs doesnt do much leg work that it confuses reading the code, I could factor them out as well if needed.&lt;/li&gt;
	&lt;li&gt;Removed all hard coded map/Reduce task number limit from code. Will conform to the cluster its being run on.&lt;/li&gt;
	&lt;li&gt;TODO: Algorithm will keep datastore internally.&lt;/li&gt;
	&lt;li&gt;TODO: add jar from latest trunk of HBase&lt;/li&gt;
&lt;/ul&gt;

</comment>
                            <comment id="12742227" author="isabel" created="Wed, 12 Aug 2009 06:56:08 +0100"  >&lt;p&gt;&amp;gt; Ant config was done to decrease the job jar file size. See first comment in this issue point No:3&lt;/p&gt;

&lt;p&gt;Ah, thanks for the reminder...&lt;/p&gt;

&lt;p&gt;&amp;gt; I need the new Eclipse Code formatter for that purpose. I am still using the lucene code formatter, which is causing this break.&lt;/p&gt;

&lt;p&gt;Ok, I see. I guess that should be no show-stopper for the code to get in.&lt;/p&gt;

&lt;p&gt;&amp;gt; Docs... already on it!&lt;br/&gt;
&amp;gt; Removed all hard coded map/Reduce task number limit from code. Will conform to the cluster its being run on.&lt;/p&gt;

&lt;p&gt;Great!&lt;/p&gt;

&lt;p&gt;&amp;gt; Map/Reduce jobs doesnt do much leg work that it confuses reading the code, I could factor them out as well if needed.&lt;/p&gt;

&lt;p&gt;I think we could leave that open for a later patch.&lt;/p&gt;

&lt;p&gt;&amp;gt; TODO: Algorithm will keep datastore internally.&lt;br/&gt;
&amp;gt; TODO: add jar from latest trunk of HBase&lt;/p&gt;

&lt;p&gt;You could probably add a JIRA task to upgrade HBase to the official release as soon as that is out. Just so we do not forget that task. Other than that, to me it looks like this code code go in by the end of this week. If anyone else would like to have a look over the code before and needs more time, please do tell.&lt;/p&gt;</comment>
                            <comment id="12744078" author="gsingers" created="Mon, 17 Aug 2009 15:24:28 +0100"  >&lt;p&gt;A few comments after a quick scan:&lt;/p&gt;

&lt;p&gt;1. DataStore should likely be an abstract class.  Probably true for Algorithm, too.  It&apos;s generally easier to support back compatibility that way, although maybe we don&apos;t need to worry about it so much yet.&lt;/p&gt;

&lt;p&gt;2. I&apos;m confused as to why HDFS/Filesystem isn&apos;t modeled as a DataStore&lt;/p&gt;

&lt;p&gt;3. I&apos;m getting build errors trying to find hbase.&lt;/p&gt;</comment>
                            <comment id="12744155" author="robinanil" created="Mon, 17 Aug 2009 19:21:51 +0100"  >&lt;p&gt;Ran FindBugs through the code.  Everything looks fine.&lt;/p&gt;

&lt;p&gt;Inmemory Datastore reads the whole model from HDFS into memory. Had the model been read directly from HDFS we could have called it a Datastore. Maybe a 2 level (memory + HDFS) storage could be called a HDFS datastore in the future. Does that sound sane?&lt;/p&gt;

&lt;p&gt;Could you try this new patch.  Also try with 0.20 RC1 of Hbase &lt;a href=&quot;http://people.apache.org/~stack/hbase-0.20.0-candidate-1/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://people.apache.org/~stack/hbase-0.20.0-candidate-1/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Scaling tests need to be done on Amazon EC2. &lt;/p&gt;

&lt;p&gt;Well GSOC ends today, but mahout-ing continues.&lt;/p&gt;</comment>
                            <comment id="12747896" author="robinanil" created="Wed, 26 Aug 2009 12:35:01 +0100"  >&lt;p&gt;Refactored out Jobs in bayes.mapreduce.*&lt;br/&gt;
cleaned up Htable objects at the end of map or reduce &lt;/p&gt;</comment>
                            <comment id="12748768" author="robinanil" created="Fri, 28 Aug 2009 10:41:08 +0100"  >&lt;p&gt;Committed&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12430881">MAHOUT-148</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12415275" name="MAHOUT-124-August-2.patch" size="208765" author="robinanil" created="Sun, 2 Aug 2009 11:09:42 +0100"/>
                            <attachment id="12417728" name="MAHOUT-124-August-26.patch" size="331812" author="robinanil" created="Wed, 26 Aug 2009 12:35:01 +0100"/>
                            <attachment id="12416784" name="MAHOUT-124-August17.patch" size="207242" author="robinanil" created="Mon, 17 Aug 2009 18:41:18 +0100"/>
                            <attachment id="12413296" name="MAHOUT-124-July-13.patch" size="190697" author="robinanil" created="Mon, 13 Jul 2009 14:33:11 +0100"/>
                            <attachment id="12414240" name="MAHOUT-124-July-23.patch" size="190970" author="robinanil" created="Wed, 22 Jul 2009 20:34:31 +0100"/>
                            <attachment id="12412568" name="MAHOUT-124-July-6.patch" size="132900" author="robinanil" created="Sun, 5 Jul 2009 22:20:20 +0100"/>
                            <attachment id="12411503" name="MAHOUT-124-June-23.patch" size="49670" author="robinanil" created="Tue, 23 Jun 2009 08:22:24 +0100"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>7.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 25 Jun 2009 17:50:04 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9941</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hxy71z:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>23294</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                </customfields>
    </item>
</channel>
</rss>