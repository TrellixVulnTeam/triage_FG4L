<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 04:19:16 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/MAHOUT-1489/MAHOUT-1489.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[MAHOUT-1489] Interactive Scala &amp; Spark Bindings Shell &amp; Script processor</title>
                <link>https://issues.apache.org/jira/browse/MAHOUT-1489</link>
                <project id="12310751" key="MAHOUT">Mahout</project>
                    <description>&lt;p&gt;Build an interactive shell /scripting (just like spark shell). Something very similar in R interactive/script runner mode.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12703766">MAHOUT-1489</key>
            <summary>Interactive Scala &amp; Spark Bindings Shell &amp; Script processor</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="dlyubimov">Dmitriy Lyubimov</assignee>
                                    <reporter username="kanjilal">Saikat Kanjilal</reporter>
                        <labels>
                    </labels>
                <created>Wed, 26 Mar 2014 17:49:33 +0000</created>
                <updated>Mon, 13 Apr 2015 11:21:48 +0100</updated>
                            <resolved>Tue, 6 May 2014 20:19:12 +0100</resolved>
                                    <version>1.0.0</version>
                                    <fixVersion>0.10.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>7</watches>
                                                                <comments>
                            <comment id="13948276" author="dlyubimov" created="Wed, 26 Mar 2014 18:34:13 +0000"  >&lt;p&gt;I cannot assign to a non-committer, so i will be watching it with assumption the patch is coming from Saikat. (that was condition of creating a new Jira).&lt;/p&gt;</comment>
                            <comment id="13948332" author="kanjilal" created="Wed, 26 Mar 2014 19:07:23 +0000"  >&lt;p&gt;Yes correct, however for my sake please feel free to add as much description as possible&lt;/p&gt;</comment>
                            <comment id="13948882" author="kanjilal" created="Thu, 27 Mar 2014 04:43:05 +0000"  >&lt;p&gt;Here is an initial list of functionality that I think can exist in the shell:&lt;/p&gt;

&lt;p&gt;1) Ability to execute against a local or remote spark cluster&lt;br/&gt;
2) Create parallelized collections based on an existing scala collection&lt;br/&gt;
3) Create a distributed dataset from a remote or local hadoop data set&lt;br/&gt;
4) A subset of transformations and actions as listed in the following link (&lt;a href=&quot;http://spark.incubator.apache.org/docs/0.8.1/scala-programming-guide.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://spark.incubator.apache.org/docs/0.8.1/scala-programming-guide.html&lt;/a&gt;)&lt;/p&gt;


&lt;p&gt;Love to hear your feedback.&lt;/p&gt;</comment>
                            <comment id="13948891" author="tdunning" created="Thu, 27 Mar 2014 04:57:07 +0000"  >&lt;p&gt;I think that creating a distributed object from an in-memory local matrix would be good as well.&lt;/p&gt;</comment>
                            <comment id="13948897" author="kanjilal" created="Thu, 27 Mar 2014 05:04:50 +0000"  >&lt;p&gt;Were you thinking of an in memory 2d array or something more elegant ?&lt;/p&gt;</comment>
                            <comment id="13948948" author="dlieu.7@gmail.com" created="Thu, 27 Mar 2014 06:41:16 +0000"  >
&lt;p&gt;This issues not about it and this is already supported.&lt;/p&gt;
</comment>
                            <comment id="13948952" author="dlieu.7@gmail.com" created="Thu, 27 Mar 2014 06:45:15 +0000"  >
&lt;p&gt;yes&lt;/p&gt;

&lt;p&gt;no this is not the scope&lt;/p&gt;

&lt;p&gt;no this is not the scope&lt;/p&gt;

&lt;p&gt;no this is not the scope&lt;/p&gt;

&lt;p&gt;This is purely engineering task, nothing fancy, no new functionality, just&lt;br/&gt;
the shell with proper packages pre-imported. I&apos;ve already done this several&lt;br/&gt;
times without Spark specifics though&lt;/p&gt;

&lt;p&gt;Implementor needs to famialize himself with the topics:&lt;/p&gt;

&lt;p&gt;(1) scala tools, in particular, scala shell API&lt;br/&gt;
(2) Spark modifications and additions to the original shell &amp;#8211; will just&lt;br/&gt;
need to take a rip of spark shell and add proper imports in the scope per&lt;br/&gt;
document of Scala Bindings.&lt;/p&gt;
</comment>
                            <comment id="13948959" author="dlyubimov" created="Thu, 27 Mar 2014 06:54:24 +0000"  >&lt;p&gt;hm email quoting did not work . i guess i should have converted to simple text to retain quoting.&lt;/p&gt;</comment>
                            <comment id="13948973" author="dlieu.7@gmail.com" created="Thu, 27 Mar 2014 06:59:15 +0000"  >
&lt;p&gt;yes&lt;/p&gt;

&lt;p&gt;no this is not the scope&lt;/p&gt;

&lt;p&gt;no this is not the scope&lt;/p&gt;

&lt;p&gt;no this is not the scope&lt;/p&gt;

&lt;p&gt;This is purely engineering task, nothing fancy, no new functionality,&lt;br/&gt;
just the shell with proper packages pre-imported. I&apos;ve already done&lt;br/&gt;
this several times without Spark specifics though&lt;/p&gt;

&lt;p&gt;Implementor needs to famialize himself with the topics:&lt;/p&gt;

&lt;p&gt;(1) scala tools, in particular, scala shell API&lt;br/&gt;
(2) Spark modifications and additions to the original shell &amp;#8211; will&lt;br/&gt;
just need to take a rip of spark shell and add proper imports in the&lt;br/&gt;
scope per document of Scala Bindings.&lt;/p&gt;</comment>
                            <comment id="13948978" author="dlyubimov" created="Thu, 27 Mar 2014 07:01:07 +0000"  >&lt;blockquote&gt;&lt;p&gt;1) Ability to execute against a local or remote spark cluster&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;yes &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;2) Create parallelized collections based on an existing scala collection&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;no this is not the scope&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;3) Create a distributed dataset from a remote or local hadoop data set&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;no this is not the scope&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;4) A subset of transformations and actions as listed in the following link (&lt;a href=&quot;http://spark.incubator.apache.org/docs/0.8.1/scala-programming-guide.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://spark.incubator.apache.org/docs/0.8.1/scala-programming-guide.html&lt;/a&gt;)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;no this is not the scope&lt;/p&gt;

&lt;p&gt;This is purely engineering task, nothing fancy, no new functionality,&lt;br/&gt;
just the shell with proper packages pre-imported. I&apos;ve already done&lt;br/&gt;
this several times without Spark specifics though&lt;/p&gt;

&lt;p&gt;Implementor needs to famialize himself with the topics:&lt;br/&gt;
(1) scala tools, in particular, scala shell API&lt;br/&gt;
(2) Spark modifications and additions to the original shell &#8211; will&lt;br/&gt;
just need to take a rip of spark shell and add proper imports in the&lt;br/&gt;
scope per document of Scala Bindings.&lt;/p&gt;</comment>
                            <comment id="13948985" author="dlyubimov" created="Thu, 27 Mar 2014 07:05:25 +0000"  >&lt;p&gt;See &lt;a href=&quot;https://github.com/apache/incubator-spark/blob/master/repl/src/main/scala/org/apache/spark/repl/Main.scala&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/incubator-spark/blob/master/repl/src/main/scala/org/apache/spark/repl/Main.scala&lt;/a&gt; (main class for Spark shell that starts loop over Scala shell).&lt;/p&gt;</comment>
                            <comment id="13948991" author="dlyubimov" created="Thu, 27 Mar 2014 07:13:07 +0000"  >&lt;p&gt;Hm. they actually copy-and-hack the original scala shell (rather than reusing it). It sounds like a lot of work that way. &lt;/p&gt;

&lt;p&gt;It is probably because they have to compile closures code and redistribute it to the back as the user creates them interactively. Bummer, it looks much more complicated than i hoped it would be.&lt;/p&gt;

&lt;p&gt;Still though, we probably could extend a few things here &amp;#8211; all we need to do, in reality, is to add a few imports. It would be Spark-specific that way, but oh well. We need to start somewhere.&lt;/p&gt;

&lt;p&gt;I probably need to think about it for a bit too.&lt;/p&gt;</comment>
                            <comment id="13949202" author="kanjilal" created="Thu, 27 Mar 2014 12:06:53 +0000"  >&lt;p&gt;I would vote to take the original scala shell code and create a derived extension/class that extends/inherits the scala shell capability and adds in the spark shell imports, kind of like a decorator in the design pattern world, what do you think ?&lt;/p&gt;</comment>
                            <comment id="13950004" author="kanjilal" created="Thu, 27 Mar 2014 22:01:29 +0000"  >&lt;p&gt;Initial github repo:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/skanjila/scala-spark-shell&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/skanjila/scala-spark-shell&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I&apos;ve merged the mahout spark code and the code inside the incubator-spark that deals with the shell into one project, I will now start adding implementations for the scala APIs for the shell to sit alongside the spark shell.  Dmitry please let me know if you have any more comments or feedback at this point on next steps for things to think about on top of adding the scala shell APIs.&lt;/p&gt;</comment>
                            <comment id="13950273" author="dlieu.7@gmail.com" created="Fri, 28 Mar 2014 02:02:16 +0000"  >
&lt;p&gt;I think it is a good start.&lt;br/&gt;
(1) We probably need it on a fork of Mahout, not as a standalone project.&lt;br/&gt;
(2) Are you sure we can&apos;t just inherit from spark shell, rather than&lt;br/&gt;
just copy-and-hack?&lt;br/&gt;
(3) we obviously need it to load with &quot;mahout spark-shell&quot; command&lt;br/&gt;
which would involve some hacking of mahout bash script to figure out&lt;br/&gt;
spark binaries inside SPARK_HOME etc.&lt;/p&gt;</comment>
                            <comment id="13950282" author="kanjilal" created="Fri, 28 Mar 2014 02:14:30 +0000"  >&lt;p&gt;Answers embedded:&lt;/p&gt;

&lt;p&gt;(1) We probably need it on a fork of Mahout, not as a standalone project.&lt;br/&gt;
I&apos;ll do that and send you the link&lt;/p&gt;

&lt;p&gt;(2) Are you sure we can&apos;t just inherit from spark shell, rather than&lt;br/&gt;
just copy-and-hack?&lt;br/&gt;
I&apos;ll look into either creating a child class that acts as a decorator which in turn is composed of the spark shell and the scala shell combined together or just inheriting directly from the spark shell&lt;/p&gt;


&lt;p&gt;(3) we obviously need it to load with &quot;mahout spark-shell&quot; command&lt;br/&gt;
which would involve some hacking of mahout bash script to figure out&lt;br/&gt;
spark binaries inside SPARK_HOME etc.&lt;/p&gt;

&lt;p&gt;Not too worried about this part, I wont get there for a bit but yes point taken, anything else, I was wondering whether there are pieces of the scala shell API that we should put on much lower priority than other features, I&apos;ll come up with a list of features that I think we should implement and add them to this JIRA.&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;</comment>
                            <comment id="13950290" author="dlieu.7@gmail.com" created="Fri, 28 Mar 2014 02:26:15 +0000"  >&lt;p&gt;yeah.&lt;br/&gt;
reallistically, functionality-wise it is not that much we need to add&lt;br/&gt;
here. It is basic Spark shell +&lt;/p&gt;

&lt;p&gt;(1) with Mahout classpath of mahout-spark and its transitives added in&lt;br/&gt;
addition to Spark stuff;&lt;br/&gt;
(2) importing our standard things automatically (i.e.&lt;br/&gt;
o.a.m.sparkbidings.&lt;em&gt;, o.a.m.sparkbindings.drm.&lt;/em&gt;, RLikeDrmOps._ etc per&lt;br/&gt;
manual &amp;#8211; make that default package imports easily to add to as we add&lt;br/&gt;
e.g. data frames dsl).&lt;/p&gt;

&lt;p&gt;This is not that much, no fundamental hacks are required. In fact, i&lt;br/&gt;
have done (2)-like things a lot with standard scala interpreter. In&lt;br/&gt;
our case we of course cannot use standard scala itnterpreter because&lt;br/&gt;
we need Spark to sync whatever new closures we put into script, with&lt;br/&gt;
the backend, for us. But we probably can just inherit from Spark&lt;br/&gt;
interpreter and then modify its automatic imports. The classpath&lt;br/&gt;
issues shuold be handled by mahout.sh script.&lt;/p&gt;

</comment>
                            <comment id="13951681" author="kanjilal" created="Sat, 29 Mar 2014 01:31:37 +0000"  >&lt;p&gt;Dmitry,&lt;br/&gt;
I&apos;ve gone ahead and forked mahout and added in the contents of the initial standalone github repo, result is here:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/skanjila/mahout-scala-spark-shell&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/skanjila/mahout-scala-spark-shell&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;I plan to start moving forward with implementing steps 1 and 2 above.&lt;/p&gt;</comment>
                            <comment id="13954848" author="kanjilal" created="Sun, 30 Mar 2014 23:04:28 +0100"  >&lt;p&gt;Ok more progress, so here&apos;s what I&apos;ve done so far and need a bit of brainstorming/guidanc, have checked in all changes to git hub repo specified above, although code still not building yet, regardless I&apos;ve:&lt;br/&gt;
1) merged code from spark onto the shell package under org.apache.mahout.sparkbindings &lt;br/&gt;
2) replaced all the classes under number 1 to have the correct package structure&lt;br/&gt;
3) added two new maven dependencies around the scala compiler and jlang both using 2.10.0 to resolve some errors associated with scala.tools and scala.***.jline etc&lt;/p&gt;

&lt;p&gt;Now I noticed that the spark code brings in an HTTPServer which is associated with running jetty locally, is this something we want inside our spark shell, if not I&apos;ll need to refactor a bunch of code to remove these dependencies and understand deeper what or how to replace this, the errors we are getting are associated with this&lt;/p&gt;

&lt;p&gt;Dmitry would love some guiodance/discussion on how to proceed around the HTTPServer&lt;/p&gt;

&lt;p&gt;Thanks in advance.&lt;/p&gt;</comment>
                            <comment id="13955429" author="dlyubimov" created="Mon, 31 Mar 2014 18:44:22 +0100"  >&lt;p&gt;i would say if it brings extra dependencies compared to transitive those of Spark-core, sure, it needs a separate mvn module. Note that in Spark the shell is also a standalone maven artifact. I would expect it to bring in at least scala-tools dependendency which normally is not needed by pure scala programs. So chances are high it needs a standalone maven module (say, &quot;shell&quot; module, &quot;mahout-shell&quot; for the artifact id). &lt;/p&gt;</comment>
                            <comment id="13955435" author="kanjilal" created="Mon, 31 Mar 2014 18:48:50 +0100"  >&lt;p&gt;Got i thanks, I&apos;ll create a stand-alone shell module and bring in all the needed dependencies, so I think that should live parallel to the spark directory in the source tree, we can call it mahout-shell , sound good?&lt;/p&gt;</comment>
                            <comment id="13958589" author="kanjilal" created="Thu, 3 Apr 2014 08:13:54 +0100"  >&lt;p&gt;Ok lots of changes here so far:&lt;/p&gt;

&lt;p&gt;1) added a new maven project called shell&lt;br/&gt;
2) created a new package under org/apache/mahout called shell which contains the spark shell code and all its dependencies (sub packages include server/storage/ui and util), we may not need these but for now my goal is to just get the project compiling&lt;br/&gt;
3) currently battling through a bunch of compilation errors with classes not being found which I&apos;ll be bringing in as needed (100+errors just related to this)&lt;/p&gt;

&lt;p&gt;github repo is here again for reference: &lt;a href=&quot;https://github.com/skanjila/mahout-scala-spark-shell&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/skanjila/mahout-scala-spark-shell&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Dmitry my goal is to get this compiling with all dependencies brought in from spark which we can remove later&lt;/p&gt;</comment>
                            <comment id="13961571" author="kanjilal" created="Mon, 7 Apr 2014 00:39:45 +0100"  >&lt;p&gt;Finally have something compiling:&lt;br/&gt;
1) Removed all the un-necessary packages around storage/scheduler and more&lt;br/&gt;
2) Trimmed down the code to copy the contents of the spark-shell for now since extending an object is not possible in scala, one alternative is to create an abstract trait that we could inherit from, need to talk this through as I want to reuse the spark code as much as possible&lt;br/&gt;
3) Moved the ReplSuite into a set of tests called MahoutShellSuite which we&apos;ll leverage&lt;br/&gt;
4) Added dependencies for various spark components into pom file&lt;/p&gt;

&lt;p&gt;Dmitry can you take a look at &lt;a href=&quot;https://github.com/skanjila/mahout-scala-spark-shell/shell&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/skanjila/mahout-scala-spark-shell/shell&lt;/a&gt; and let me know your thoughts on : 1) whether any obvious dependencies are missing in the pom file 2) whether the initial shell code is missing any pieces&lt;/p&gt;

&lt;p&gt;If not I&apos;ll be moving forward and fixing the tests to work with the code for the next big code drop&lt;/p&gt;

&lt;p&gt;Dmitry &lt;/p&gt;</comment>
                            <comment id="13962593" author="kanjilal" created="Tue, 8 Apr 2014 06:13:32 +0100"  >&lt;p&gt;Got a bit further, unit tests are at least compiling, they run but fail, will tackle this next&lt;/p&gt;</comment>
                            <comment id="13962600" author="dlyubimov" created="Tue, 8 Apr 2014 06:23:57 +0100"  >&lt;p&gt;So i take it you do extend original Spark shell. Nice!&lt;/p&gt;

&lt;p&gt;Any reason why you did not just include dependency on mahout-spark and use mahoutContext() method from there instead of copying over my code?&lt;/p&gt;</comment>
                            <comment id="13962604" author="dlyubimov" created="Tue, 8 Apr 2014 06:28:31 +0100"  >&lt;p&gt;This of course will need a bit of clean up, but this a good skeleton. nice.&lt;/p&gt;</comment>
                            <comment id="13963084" author="kanjilal" created="Tue, 8 Apr 2014 16:37:05 +0100"  >&lt;p&gt;1) Added MahoutLocalContext into MahoutShellSuite&lt;br/&gt;
2) Added the code and test sparkbindings targets inside pom.xml&lt;/p&gt;

&lt;p&gt;Currently tests are compiling and failing, will fix the tests next.&lt;/p&gt;</comment>
                            <comment id="13963796" author="kanjilal" created="Wed, 9 Apr 2014 05:32:50 +0100"  >&lt;p&gt;ok home stretch:&lt;/p&gt;

&lt;p&gt;1) added the scala.reflect dependencies that the MahoutShellSuite requires&lt;br/&gt;
2) brought over the computeClasspath.sh script from the spark incubator project&lt;br/&gt;
3) Tests are now at least running and are failing because of dependencies brought on by number 2&lt;/p&gt;


&lt;p&gt;Dmitry need to figure out what modifications need to be made to number 2, a question for you here do we in the mahout-spark project already have a dependency similar to this, if not which parts of this script should we remove and which parts should be kept, some insight on this would be very helpful&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;</comment>
                            <comment id="13963845" author="dlyubimov" created="Wed, 9 Apr 2014 06:56:36 +0100"  >&lt;p&gt;honestly, this sounds overdesign-y. Bringing in Spark scripts should not be necessary.&lt;/p&gt;

&lt;p&gt;I think i need to play with it but I am sure there&apos;s a way to figure spark classpath similar to how mahout context figures Mahout&apos;s jars. &lt;/p&gt;

&lt;p&gt;i also definitely do not understand all the troubles with transitive dependencies, maven should bring all that is needed automatically with minimum adjustments .&lt;/p&gt;</comment>
                            <comment id="13963851" author="kanjilal" created="Wed, 9 Apr 2014 07:06:37 +0100"  >&lt;p&gt;The requirement to bring in scala.reflect stemmed from a runtime error that indicated that this package was missing which I fixed by adding this dependency.  Let me know how you want to proceed with the shell script,  if MahoutContext is already loading all the jars correctly then I think the right approach would be to fix the MahoutShellSuite to load all the jars using the MahoutContext.&lt;/p&gt;
</comment>
                            <comment id="13965062" author="kanjilal" created="Thu, 10 Apr 2014 07:36:31 +0100"  >&lt;p&gt;Here&apos;s what I see when I run unitTests:&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; &amp;#8212; scalatest-maven-plugin:1.0-M2:test (test) @ mahout-shell &amp;#8212;&lt;br/&gt;
WARNING: -p has been deprecated and will be reused for a different (but still very cool) purpose in ScalaTest 2.0. Please change all uses of -p to -R.&lt;br/&gt;
�[36mDiscovery starting.�[0m&lt;br/&gt;
�[36mDiscovery completed in 632 milliseconds.�[0m&lt;br/&gt;
�[36mRun starting. Expected test count is: 9�[0m&lt;br/&gt;
�[32mDiscoverySuite:�[0m&lt;br/&gt;
�[32mMahoutShellSuite:�[0m&lt;br/&gt;
2014-04-09 23:35:51.462 java&lt;span class=&quot;error&quot;&gt;&amp;#91;583:b07&amp;#93;&lt;/span&gt; Unable to load realm info from SCDynamicStore&lt;/p&gt;

&lt;p&gt;ls: /Users/skanjila/code/java/mahout-scala-spark-shell/shell/assembly/target/scala-2.10.3/spark-assembly*hadoop*.jar: No such file or directory&lt;br/&gt;
ls: /Users/skanjila/code/java/mahout-scala-spark-shell/shell/assembly/target/scala-2.10.3/spark-assembly*hadoop*.jar: No such file or directory&lt;br/&gt;
ls: /Users/skanjila/code/java/mahout-scala-spark-shell/shell/assembly/target/scala-2.10.3/spark-assembly*hadoop*.jar: No such file or directory&lt;br/&gt;
ls: /Users/skanjila/code/java/mahout-scala-spark-shell/shell/assembly/target/scala-2.10.3/spark-assembly*hadoop*.jar: No such file or directory&lt;br/&gt;
ls: /Users/skanjila/code/java/mahout-scala-spark-shell/shell/assembly/target/scala-2.10.3/spark-assembly*hadoop*.jar: No such file or directory&lt;br/&gt;
ls: /Users/skanjila/code/java/mahout-scala-spark-shell/shell/assembly/target/scala-2.10.3/spark-assembly*hadoop*.jar: No such file or directory&lt;br/&gt;
ls: /Users/skanjila/code/java/mahout-scala-spark-shell/shell/assembly/target/scala-2.10.3/spark-assembly*hadoop*.jar: No such file or directory&lt;br/&gt;
ls: /Users/skanjila/code/java/mahout-scala-spark-shell/shell/assembly/target/scala-2.10.3/spark-assembly*hadoop*.jar: No such file or directory&lt;br/&gt;
ls: /Users/skanjila/code/java/mahout-scala-spark-shell/shell/assembly/target/scala-2.10.3/spark-assembly*hadoop*.jar: No such file or directory&lt;br/&gt;
ls: /Users/skanjila/code/java/mahout-scala-spark-shell/shell/assembly/target/scala-2.10.3/spark-assembly*hadoop*.jar: No such file or directory&lt;br/&gt;
0 &lt;span class=&quot;error&quot;&gt;&amp;#91;sparkMaster-akka.actor.default-dispatcher-4&amp;#93;&lt;/span&gt; ERROR org.apache.spark.deploy.master.Master  - Application Spark shell with ID app-20140409233702-0000 failed 10 times, removing it&lt;br/&gt;
21 &lt;span class=&quot;error&quot;&gt;&amp;#91;spark-akka.actor.default-dispatcher-2&amp;#93;&lt;/span&gt; ERROR org.apache.spark.deploy.client.AppClient$ClientActor  - Master removed our application: FAILED; stopping client&lt;/p&gt;


&lt;p&gt;My questions:&lt;br/&gt;
1) Given that I have repurposed the spark-repl unit tests, I am wondering whether we (as in the mahout spark shell) should have the same requirements for data store as in running or needing the spark assembly hadoop jar files which essentially means that a local install of spark is needed&lt;br/&gt;
2) When I run the unit tests now I see something like this: &lt;br/&gt;
�[32m- propagation of local properties�[0m&lt;br/&gt;
�[32m- simple foreach with accumulator�[0m&lt;br/&gt;
�[32m- external vars�[0m&lt;br/&gt;
�[32m- external classes�[0m&lt;br/&gt;
�[32m- external functions�[0m&lt;br/&gt;
�[32m- external functions that access vars�[0m&lt;br/&gt;
�[32m- broadcast vars�[0m&lt;br/&gt;
�[32m- interacting with files�[0m&lt;/p&gt;

&lt;p&gt;I&apos;m assuming this means its bypassing all the unit tests, I&apos;ll investigate this further&lt;/p&gt;

&lt;p&gt;3) Earlier you mentioned the mahout.sh script, should we merge the contents of this script with the one I have above and place that in the bin sub-directory, or more importantly I need to understand how mahout.sh is related to computeClasspath.sh&lt;/p&gt;

&lt;p&gt;Eager to hear your thoughts to proceed quickly with next steps &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13966097" author="dlyubimov" created="Fri, 11 Apr 2014 02:03:45 +0100"  >&lt;p&gt;Ok. this is way too complicated for me. I did a quick hack [1] which seems to work at least in local mode . Works like a charm. Here is the session dump (need just to start o.a.m.sparkbindings.shell.Main class from idea). I also filtered out most of spark debug messages that are enabled by default:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeHeader panelHeader&quot; style=&quot;border-bottom-width: 1px;&quot;&gt;&lt;b&gt;&quot;Mahout Spark Shell session&quot;&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
14/04/10 17:52:27 INFO spark.HttpServer: Starting HTTP Server
14/04/10 17:52:27 INFO server.Server: jetty-7.6.8.v20121106
14/04/10 17:52:27 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:60204
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  &apos;_/
   /___/ .__/\_,_/_/ /_/\_\   version 0.9.0
      /_/

Using Scala version 2.10.3 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_51)
Type in expressions to have them evaluated.
Type :help &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; more information.
Created spark context..
Spark context available as sc.

scala&amp;gt; val a = dense((1,2,3),(3,4,5))
a: org.apache.mahout.math.DenseMatrix = 
{
  0  =&amp;gt;	{0:1.0,1:2.0,2:3.0}
  1  =&amp;gt;	{0:3.0,1:4.0,2:5.0}
}

scala&amp;gt; val drmA = drmParallelize(a)
drmA: org.apache.mahout.sparkbindings.drm.CheckpointedDrm[Int] = org.apache.mahout.sparkbindings.drm.CheckpointedDrmBase@791b95a1

scala&amp;gt; val drmAtA = drmA.t %*% drmA
drmAtA: org.apache.mahout.sparkbindings.drm.DrmLike[Int] = OpAB(OpAt(org.apache.mahout.sparkbindings.drm.CheckpointedDrmBase@791b95a1),org.apache.mahout.sparkbindings.drm.CheckpointedDrmBase@791b95a1)

scala&amp;gt; drmAtA.collect
res0: org.apache.mahout.math.Matrix = 
{
  0  =&amp;gt;	{0:10.0,1:14.0,2:18.0}
  1  =&amp;gt;	{0:14.0,1:20.0,2:26.0}
  2  =&amp;gt;	{0:18.0,1:26.0,2:34.0}
}

scala&amp;gt; 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I suggest you to fork my branch, take it as a basis. It basically works, now the stuff that needs to happen is to verify it in distributed mode and modify mahout shell script to set up proper paths etc. to launch it w.r.t &quot;mahout shell &amp;lt;master&amp;gt;&quot; command. Should be simple an uneventful enough now. &lt;/p&gt;

&lt;p&gt;     [1]: &lt;a href=&quot;https://github.com/dlyubimov/mahout-commits/tree/shell&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/dlyubimov/mahout-commits/tree/shell&lt;/a&gt;&lt;/p&gt;
</comment>
                            <comment id="13966137" author="andrew.musselman" created="Fri, 11 Apr 2014 03:13:39 +0100"  >&lt;p&gt;I was thinking the other day that what I may want out of this is the kind of clear data flow I get when I write Pig.&lt;/p&gt;

&lt;p&gt;For example:&lt;/p&gt;

&lt;p&gt;a = load &apos;u&apos;;&lt;br/&gt;
b = load &apos;v&apos;;&lt;br/&gt;
c = a%.%b&lt;br/&gt;
store c into &apos;matrix-mult&apos;;&lt;/p&gt;

&lt;p&gt;Is this the right thread for that conversation?&lt;/p&gt;</comment>
                            <comment id="13966167" author="dlyubimov" created="Fri, 11 Apr 2014 04:17:47 +0100"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=andrew.musselman&quot; class=&quot;user-hover&quot; rel=&quot;andrew.musselman&quot;&gt;Andrew Musselman&lt;/a&gt; yes. it does that and more. you really perhaps should listen to one of Matei&apos;s Spark talks.&lt;/p&gt;

&lt;p&gt; It is basically a scala shell that aside from  line-by-line scala interpretation, turns all closures and classes you create on the fly into byte code and ships them to Spark backend automatically. No other distributed backend that i know is capable of anything comparable. usually they all require one to compile and ship tons of jars in this situation.&lt;/p&gt;</comment>
                            <comment id="13966168" author="dlyubimov" created="Fri, 11 Apr 2014 04:20:01 +0100"  >&lt;p&gt;PS All we do here is hack it to include mahout libraries, set up proper linalg type serialization, declare implicit spark context and pre-import relevant packages. so at the time you run it, you are already all set. &lt;/p&gt;

&lt;p&gt;This is also very coherent perception-wise with how RScript works.&lt;/p&gt;</comment>
                            <comment id="13966171" author="andrew.musselman" created="Fri, 11 Apr 2014 04:24:59 +0100"  >&lt;p&gt;Alright, I will read up.&lt;/p&gt;</comment>
                            <comment id="13966185" author="kanjilal" created="Fri, 11 Apr 2014 04:42:54 +0100"  >&lt;p&gt;Dmitry,&lt;br/&gt;
I tried to fork your repo but dont see the spark-shell showing up in my forked version of the repo for some reason, I also tried to clone from your repo and got a &quot;repo not found&quot; error.  I&apos;ll go ahead and copy the spark-shell directory contents over for now, regardless had a few questions:&lt;/p&gt;

&lt;p&gt;1) When you say mahout shell script which script are you referring to, is that the one that I brought over in my fork, I don&apos;t immediately see any shell scripts under the spark-shell or the master directory, let me know if you are referring to the MahoutSparkILoop.scala class that you created/extended&lt;/p&gt;

&lt;p&gt;2) Are there a specific set of commands that would be good to exercize in distributed mode or is it the same commands for example that you outlined above but now running in distributed mode, also by distributed mode should we also be able to point to some remote spark/hadoop cluster node(s)&lt;/p&gt;

&lt;p&gt;Thanks for your help&lt;/p&gt;</comment>
                            <comment id="13966192" author="dlyubimov" created="Fri, 11 Apr 2014 04:49:35 +0100"  >&lt;p&gt;are you forking correct branch? You need to fork shell. Keep in mind that you &lt;em&gt;do&lt;/em&gt; need to fork in github in order to issue a pull request (which is what i really would suggest you to do next in order to be able to put comment on the code).&lt;/p&gt;

&lt;p&gt;I assume everything will work exactly as it works today with spark interpreter (i.e. you supply master url to the shell invocation, optionally followed by the script file and execution options) &amp;#8211; the shell will take it from there. you need to check out spark shell help to figure how it works. &lt;/p&gt;

&lt;p&gt;the master url of local mode is simply &quot;local&quot;. Again see the spark manual, In the shell itself there&apos;s no distinction in command set regardless of the mode, everything is 100% identical. &lt;/p&gt;</comment>
                            <comment id="13966202" author="kanjilal" created="Fri, 11 Apr 2014 05:02:33 +0100"  >&lt;p&gt;This is what I&apos;m trying to fork:&lt;br/&gt;
&lt;a href=&quot;https://github.com/dlyubimov/mahout-commits/tree/shell&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/dlyubimov/mahout-commits/tree/shell&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I went to github and clicked on the fork button, when I did that here&apos;s my fork:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/skanjila/mahout-commits&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/skanjila/mahout-commits&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As you can see in my fork the spark-shell directory which I&apos;m most interested in is missing.&lt;/p&gt;

&lt;p&gt;Thanks for the info on the commands, I&apos;ll begin this effort as soon as I bring the code over correctly&lt;/p&gt;</comment>
                            <comment id="13966244" author="dlyubimov" created="Fri, 11 Apr 2014 06:15:36 +0100"  >&lt;p&gt;yes. go to branch tab and select &quot;shell&quot; branch. it is there.&lt;/p&gt;</comment>
                            <comment id="13971977" author="dlyubimov" created="Wed, 16 Apr 2014 22:24:14 +0100"  >&lt;p&gt;First working patch. Also tracked by &quot;shell&quot; branch in my github/mahout-commits. &lt;/p&gt;

&lt;p&gt;seems to be working with single node cluster in STANDALONE mode. Also tested on-the-fly closures with `mapBlock()`.&lt;/p&gt;

&lt;p&gt;to start &lt;/p&gt;

&lt;p&gt;(1) compile Mahout &lt;br/&gt;
(2) install and compile spark 0.9.1 &lt;br/&gt;
(3) make sure MAHOUT_HOME, SPARK_HOME pointing to mahout and spark 0.9.1 respectively&lt;/p&gt;

&lt;p&gt;Start Spark standalone cluster per instructions in Spark&lt;/p&gt;

&lt;p&gt;to start shell, use (for example)&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
 MASTER=spark:&lt;span class=&quot;code-comment&quot;&gt;//BigHP:7077 bin/mahout spark-shell&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Outstanding issues: &lt;/p&gt;

&lt;p&gt;(1) log level of course is not adjustable by Spark settings any more, since we use Mahout to start the jvm process. How do we set log4j.properties for Mahout??&lt;/p&gt;

&lt;p&gt;(2) After exiting from shell, the terminal driver on Linux ubuntu is screwed for some reason. (use &quot;stty sane&quot; to restore sanity to terminal control driver). Not sure why, this does not happen to either spark or scala shell. &lt;/p&gt;</comment>
                            <comment id="13971981" author="dlyubimov" created="Wed, 16 Apr 2014 22:31:47 +0100"  >&lt;p&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/12640541/mahout-spark-shell-running-standalone.png&quot; align=&quot;absmiddle&quot; border=&quot;0&quot; /&gt;&lt;/p&gt;</comment>
                            <comment id="13971990" author="kanjilal" created="Wed, 16 Apr 2014 22:43:43 +0100"  >&lt;p&gt;Dmitry,&lt;br/&gt;
I&apos;ve been out of town so will work on continuing testing this when I come back next week, does the distributed mode testing still need to be done?&lt;/p&gt;</comment>
                            <comment id="13971997" author="dlieu.7@gmail.com" created="Wed, 16 Apr 2014 22:52:21 +0100"  >&lt;p&gt;it seems to be working in distributed mode, i don&apos;t see any further&lt;br/&gt;
problems. There is a couple of cosmetic issues like i mentioned, but i am&lt;br/&gt;
not yet sure how to fix them. I need input from somebody who knows how&lt;br/&gt;
logger verbosity is managed in mahout, and i am not sure what it is&lt;br/&gt;
specific to mahout startup  might be screwing the terminal driver .&lt;/p&gt;


</comment>
                            <comment id="13976240" author="dlyubimov" created="Tue, 22 Apr 2014 01:36:56 +0100"  >&lt;p&gt;Ok i really want to commit it rather sooner than later because of the actively diverging trunk and because there are a few fixes (e.g. writeDRM must not require a view bound since implementation already carries it) so i &apos;d appreciate eyeballing the most recent patch.&lt;/p&gt;</comment>
                            <comment id="13976261" author="tdunning" created="Tue, 22 Apr 2014 02:01:15 +0100"  >&lt;p&gt;For new modules that really can&apos;t much break existing code, I think that&lt;br/&gt;
committing often is a good thing.  This is especially true since we are&lt;br/&gt;
using SVN.&lt;/p&gt;




</comment>
                            <comment id="13976272" author="dlyubimov" created="Tue, 22 Apr 2014 02:15:23 +0100"  >&lt;p&gt;well the dangerous code is the mahout shell modifications. &lt;/p&gt;

&lt;p&gt;Since there&apos;s a lot of code there devoted (in fairly inconsistent way) figuring out Hadoop setup and its classpath, and spark commands now need to make sure that doesn&apos;t happen, while adding spark jars... it becomes somewhat kludgy and may break MR things. I so far don&apos;t see evidence of that happening but i am not verifying mahout MR commands all the way either.&lt;/p&gt;</comment>
                            <comment id="13976273" author="dlyubimov" created="Tue, 22 Apr 2014 02:16:45 +0100"  >&lt;p&gt;also my questions are still relevant &amp;#8211; in particular, how do we set up logging levels in Mahout?&lt;/p&gt;</comment>
                            <comment id="13976338" author="andrew.musselman" created="Tue, 22 Apr 2014 04:13:57 +0100"  >&lt;p&gt;I think we need to add a log4j.properties file so we can control logging properly, but that&apos;s another ticket I think.&lt;/p&gt;</comment>
                            <comment id="13976341" author="andrew.musselman" created="Tue, 22 Apr 2014 04:20:30 +0100"  >&lt;p&gt;Filed &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1522&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/MAHOUT-1522&lt;/a&gt; for logging..&lt;/p&gt;</comment>
                            <comment id="13976342" author="dlyubimov" created="Tue, 22 Apr 2014 04:25:09 +0100"  >&lt;p&gt;Ok thanks for doing this. Thought there already was a way.&lt;/p&gt;</comment>
                            <comment id="13976354" author="tdunning" created="Tue, 22 Apr 2014 05:10:57 +0100"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dlieu.7%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;dlieu.7@gmail.com&quot;&gt;Dmitriy Lyubimov&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Why do we even need to integrate with Mahout shell command?&lt;/p&gt;
</comment>
                            <comment id="13977100" author="dlyubimov" created="Tue, 22 Apr 2014 18:41:26 +0100"  >&lt;p&gt;i had a thought to make it another command but i guess i got too lazy, there&apos;s too much common in code figuring classpath etc. I guess i&apos;d like to commit it as is, and if we don&apos;t like it, we&apos;d tweak it later. I am not big on extensive shell scripting.&lt;/p&gt;

&lt;p&gt;We&apos;ll also need to tweak all that to include something like &quot;config&quot; folder with log4j.properties in it. So it probably makes sense to start with a common script there.&lt;/p&gt;</comment>
                            <comment id="13977183" author="dlyubimov" created="Tue, 22 Apr 2014 19:43:02 +0100"  >&lt;p&gt;Ok, so two environment variables to pay attention to: MAHOUT_HOME and MASTER (Spark master). &lt;/p&gt;

&lt;p&gt;e.g. to run with &quot;standalone&quot; spark cluster: &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
MAHOUT_HOME=~/tools/mahout MASTER=&apos;spark:&lt;span class=&quot;code-comment&quot;&gt;//master-host:7077&apos; bin/mahout spark-shell&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13977185" author="dlyubimov" created="Tue, 22 Apr 2014 19:44:45 +0100"  >&lt;p&gt;PPS. SPARK_HOME is also expected to point to Spark 0.9.1 setup in non-local mode. It will try to run spark-classpath.sh to figure Spark&apos;s binaries so make sure this shell works (it doesn&apos;t work if you haven&apos;t run full spark assembly build).&lt;/p&gt;
</comment>
                            <comment id="13977422" author="hudson" created="Tue, 22 Apr 2014 22:05:48 +0100"  >&lt;p&gt;FAILURE: Integrated in Mahout-Quality #2589 (See &lt;a href=&quot;https://builds.apache.org/job/Mahout-Quality/2589/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/Mahout-Quality/2589/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1489&quot; title=&quot;Interactive Scala &amp;amp; Spark Bindings Shell &amp;amp; Script processor&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1489&quot;&gt;&lt;del&gt;MAHOUT-1489&lt;/del&gt;&lt;/a&gt; : initial Mahout spark shell commit&lt;/p&gt;

&lt;p&gt;Squashed commit of the following:&lt;/p&gt;

&lt;p&gt;commit 0124072b72fcdad9ccded43745c9b1d00e7ea089&lt;br/&gt;
Merge: c1a2c8a c9164c1&lt;br/&gt;
Author: Dmitriy Lyubimov &amp;lt;dlyubimov@apache.org&amp;gt;&lt;br/&gt;
Date:   Tue Apr 22 11:33:17 2014 -0700&lt;/p&gt;

&lt;p&gt;    Merge branch &apos;trunk&apos; into shell&lt;/p&gt;

&lt;p&gt;commit c1a2c8a414c015dcdce592b145498fc8b836addf&lt;br/&gt;
Merge: a3491f5 a8df05b&lt;br/&gt;
Author: Dmitriy Lyubimov &amp;lt;dlyubimov@apache.org&amp;gt;&lt;br/&gt;
Date:   Tue Apr 22 11:30:25 2014 -0700&lt;/p&gt;

&lt;p&gt;    Merge branch &apos;trunk&apos; into shell; misc fixes&lt;/p&gt;

&lt;p&gt;    Conflicts:&lt;br/&gt;
    	bin/mahout&lt;/p&gt;

&lt;p&gt;commit a3491f57e77b4b789051cf131bc7fdea73ad3e41&lt;br/&gt;
Author: Dmitriy Lyubimov &amp;lt;dlyubimov@apache.org&amp;gt;&lt;br/&gt;
Date:   Mon Apr 21 17:39:06 2014 -0700&lt;/p&gt;

&lt;p&gt;    -NonLocal&lt;/p&gt;

&lt;p&gt;commit bd0c83ebfa66e48f434f0ecc81bf81dd07d27f8c&lt;br/&gt;
Merge: ad01add 78c45c4&lt;br/&gt;
Author: Dmitriy Lyubimov &amp;lt;dlyubimov@apache.org&amp;gt;&lt;br/&gt;
Date:   Mon Apr 21 17:33:18 2014 -0700&lt;/p&gt;

&lt;p&gt;    Merge commit &apos;78c45c4c5d96f51e9&apos; into shell&lt;/p&gt;

&lt;p&gt;commit ad01add55c1c5a212dabcd727e31dd36162c1fd0&lt;br/&gt;
Author: Dmitriy Lyubimov &amp;lt;dlyubimov@apache.org&amp;gt;&lt;br/&gt;
Date:   Mon Apr 21 17:30:53 2014 -0700&lt;/p&gt;

&lt;p&gt;    Fixing writeDRM problems&lt;/p&gt;

&lt;p&gt;commit f9f20e364e4462b6ba0693d11ee939ef5afd43a4&lt;br/&gt;
Author: Dmitriy Lyubimov &amp;lt;dlyubimov@apache.org&amp;gt;&lt;br/&gt;
Date:   Mon Apr 21 14:58:28 2014 -0700&lt;/p&gt;

&lt;p&gt;    writeDRM broken, some unknown closure attributes yank &quot;this&quot;, not seeing where and which&lt;/p&gt;

&lt;p&gt;commit 7ca93f70ef7313cca251470c41032c08e9e612e2&lt;br/&gt;
Author: Dmitriy Lyubimov &amp;lt;dlyubimov@apache.org&amp;gt;&lt;br/&gt;
Date:   Mon Apr 21 12:57:40 2014 -0700&lt;/p&gt;

&lt;p&gt;    script errors&lt;/p&gt;

&lt;p&gt;commit ce5bdfaba160ecb6ee433b4682f62d9e6236e0b6&lt;br/&gt;
Author: Dmitriy Lyubimov &amp;lt;dlyubimov@apache.org&amp;gt;&lt;br/&gt;
Date:   Thu Apr 17 13:51:35 2014 -0700&lt;/p&gt;

&lt;p&gt;    drmFromHDFS() fixes &amp;#8211; load and initiailize classtag evidence correctly.&lt;br/&gt;
    TODO: use class evidence from key for saveDRM.&lt;/p&gt;

&lt;p&gt;commit aeba609a957b9ef5aac7aa4eba01edb46004116f&lt;br/&gt;
Author: Dmitriy Lyubimov &amp;lt;dlyubimov@apache.org&amp;gt;&lt;br/&gt;
Date:   Thu Apr 17 11:41:30 2014 -0700&lt;/p&gt;

&lt;p&gt;    renaming script extensions&lt;/p&gt;

&lt;p&gt;commit 9b02ac2b6aa4ff046a898876f9a8fb23936cd78b&lt;br/&gt;
Author: Dmitriy Lyubimov &amp;lt;dlyubimov@apache.org&amp;gt;&lt;br/&gt;
Date:   Thu Apr 17 11:17:04 2014 -0700&lt;/p&gt;

&lt;p&gt;    build fix&lt;/p&gt;

&lt;p&gt;commit 2fbd835c4c4f8fac3ff32d06b9578d436f4bbf09&lt;br/&gt;
Author: Dmitriy Lyubimov &amp;lt;dlyubimov@apache.org&amp;gt;&lt;br/&gt;
Date:   Wed Apr 16 18:04:47 2014 -0700&lt;/p&gt;

&lt;p&gt;    WIP &amp;#8211; unstable&lt;/p&gt;

&lt;p&gt;commit cc87347d393709ac0a6ab2adc66200caed07e911&lt;br/&gt;
Author: Dmitriy Lyubimov &amp;lt;dlyubimov@apache.org&amp;gt;&lt;br/&gt;
Date:   Wed Apr 16 15:27:00 2014 -0700&lt;/p&gt;

&lt;p&gt;    removing examples dependencies if running spark shell.&lt;/p&gt;

&lt;p&gt;commit 435992aa99090b9a75f3903ac5d10e43e6b49357&lt;br/&gt;
Author: Dmitriy Lyubimov &amp;lt;dlyubimov@apache.org&amp;gt;&lt;br/&gt;
Date:   Wed Apr 16 14:10:22 2014 -0700&lt;/p&gt;

&lt;p&gt;    WIP spark-shell, seems to be working&lt;/p&gt;

&lt;p&gt;commit 1f4fd51c2e5e18852d1b30d5a88897e14761d9e8&lt;br/&gt;
Author: Dmitriy Lyubimov &amp;lt;dlyubimov@apache.org&amp;gt;&lt;br/&gt;
Date:   Wed Apr 16 12:55:17 2014 -0700&lt;/p&gt;

&lt;p&gt;    WIP &amp;#8211; implementing &quot;mathout spark-shell&quot;&lt;/p&gt;

&lt;p&gt;commit 5922a111405cc66c1d598b123c7d28f456229559&lt;br/&gt;
Author: Dmitriy Lyubimov &amp;lt;dlyubimov@apache.org&amp;gt;&lt;br/&gt;
Date:   Thu Apr 10 17:53:23 2014 -0700&lt;/p&gt;

&lt;p&gt;    First shell  prototype works in local mode (dlyubimov: rev 1589246)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/mahout/trunk/bin/mahout&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/math-scala/pom.xml&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/pom.xml&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/spark-shell&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/spark-shell/pom.xml&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/spark-shell/src&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/spark-shell/src/main&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/spark-shell/src/main/scala&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/spark-shell/src/main/scala/org&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/spark-shell/src/main/scala/org/apache&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/spark-shell/src/main/scala/org/apache/mahout&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/spark-shell/src/main/scala/org/apache/mahout/sparkbindings&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/spark-shell/src/main/scala/org/apache/mahout/sparkbindings/shell&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/spark-shell/src/main/scala/org/apache/mahout/sparkbindings/shell/MahoutSparkILoop.scala&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/spark-shell/src/main/scala/org/apache/mahout/sparkbindings/shell/Main.scala&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/spark-shell/src/test&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/spark-shell/src/test/mahout&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/spark-shell/src/test/mahout/simple.mscala&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/spark/pom.xml&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/CheckpointedDrm.scala&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/CheckpointedDrmBase.scala&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/package.scala&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/spark/src/main/scala/org/apache/mahout/sparkbindings/package.scala&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/spark/src/test/scala/org/apache/mahout/sparkbindings/test/MahoutLocalContext.scala&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13982233" author="ssc" created="Sun, 27 Apr 2014 08:59:15 +0100"  >&lt;p&gt;What&apos;s the status here?&lt;/p&gt;</comment>
                            <comment id="13982482" author="dlyubimov" created="Sun, 27 Apr 2014 22:54:21 +0100"  >&lt;p&gt;status is patch available (and actually committed). &lt;/p&gt;

&lt;p&gt;One thing that would make this better is introducing explicit log level management in Mahout, but this has been filed as another issue. &lt;/p&gt;

&lt;p&gt;A minor  thing is as i mentioned, i can figure out why terminal driver is screwed after graceful shell exit. So i think we can close this issue and re-file this as a bug if so desired, along with other things as we find them.&lt;/p&gt;</comment>
                            <comment id="13983526" author="ssc" created="Mon, 28 Apr 2014 21:54:20 +0100"  >&lt;p&gt;I&apos;m having some trouble getting this work. Do I have to configure spark in a specific way or is it enough to build it and run ./sbin/start-master.sh ?&lt;/p&gt;</comment>
                            <comment id="13983533" author="dlyubimov" created="Mon, 28 Apr 2014 22:03:02 +0100"  >&lt;p&gt;let me recap what i said before. No, you don&apos;t have to configure spark in any special way.&lt;/p&gt;

&lt;p&gt;(1) install spark &lt;b&gt;0.9.1&lt;/b&gt;, make sure assembly is built, (&lt;b&gt;sb/sbt assembly in SPARK_HOME&lt;/b&gt;) set up SPARK_HOME, make sure $SPARK_HOME/bin/spark-classpath.sh (or whatever this script is) produces no errors&lt;br/&gt;
(2) compile mahout, set up MAHOUT_HOME. &lt;br/&gt;
(3) try with local mode &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  MASTER=&lt;span class=&quot;code-quote&quot;&gt;&quot;local&quot;&lt;/span&gt; bin/mahout spark-shell
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;that should be enough. LMK what troubles are happening after that.&lt;/p&gt;</comment>
                            <comment id="13983535" author="dlyubimov" created="Mon, 28 Apr 2014 22:04:38 +0100"  >&lt;p&gt;PS my environment is ubuntu LTS 12. YMMV on mac as i haven&apos;t tested it there.&lt;/p&gt;</comment>
                            <comment id="13983538" author="ssc" created="Mon, 28 Apr 2014 22:06:20 +0100"  >&lt;p&gt;I&apos;m also running Ubuntu 12 LTS. I&apos;m getting a NoClassDefFoundError:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
java.lang.NoClassDefFoundError: org/apache/mahout/common/IOUtils
	at org.apache.mahout.sparkbindings.&lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt;$.mahoutSparkContext(&lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt;.scala:131)
	at org.apache.mahout.sparkbindings.shell.MahoutSparkILoop.createSparkContext(MahoutSparkILoop.scala:44)
	at $iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:8)
	at $iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:14)
	at &amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:16)
	at .&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:20)
	at .&amp;lt;clinit&amp;gt;(&amp;lt;console&amp;gt;)
	at .&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:7)
	at .&amp;lt;clinit&amp;gt;(&amp;lt;console&amp;gt;)
	at $print(&amp;lt;console&amp;gt;)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:772)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1040)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:609)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:640)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:604)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:793)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:838)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:750)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:119)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:118)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:258)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:118)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:53)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:908)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:140)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:53)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:102)
	at org.apache.mahout.sparkbindings.shell.MahoutSparkILoop.postInitialization(MahoutSparkILoop.scala:20)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:925)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:881)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:881)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:881)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:973)
	at org.apache.mahout.sparkbindings.shell.Main$.main(Main.scala:14)
	at org.apache.mahout.sparkbindings.shell.Main.main(Main.scala)
Caused by: java.lang.ClassNotFoundException: org.apache.mahout.common.IOUtils
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.loadClass(&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.java:425)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.loadClass(&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.java:358)
	... 40 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13983562" author="dlyubimov" created="Mon, 28 Apr 2014 22:19:46 +0100"  >&lt;p&gt;can you please run &quot;bin/mahout -spark classpath&quot; here? thanks.&lt;/p&gt;

&lt;p&gt;my output is &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
bin/mahout -spark classpath | sed &lt;span class=&quot;code-quote&quot;&gt;&quot;s/:/\n/g&quot;&lt;/span&gt;
MAHOUT_LOCAL is not set; adding HADOOP_CONF_DIR to classpath.
Running on hadoop, using /home/dmitriy/tools/hadoop/bin/hadoop and HADOOP_CONF_DIR=/home/dmitriy/tools/hadoop/etc/hadoop

/home/dmitriy/projects/asf/mahout-commits/src/conf
/home/dmitriy/tools/hadoop/etc/hadoop
/home/dmitriy/tools/java/lib/tools.jar
/home/dmitriy/projects/asf/mahout-commits/mahout-*.jar
/home/dmitriy/projects/asf/mahout-commits/math-scala/target/mahout-math-scala-1.0-SNAPSHOT.jar
/home/dmitriy/projects/asf/mahout-commits/math-scala/target/mahout-math-scala-1.0-SNAPSHOT-sources.jar
/home/dmitriy/projects/asf/mahout-commits/math-scala/target/mahout-math-scala-1.0-SNAPSHOT-tests.jar
/home/dmitriy/projects/asf/mahout-commits/core/target/mahout-core-1.0-SNAPSHOT.jar
/home/dmitriy/projects/asf/mahout-commits/core/target/mahout-core-1.0-SNAPSHOT-job.jar
/home/dmitriy/projects/asf/mahout-commits/core/target/mahout-core-1.0-SNAPSHOT-sources.jar
/home/dmitriy/projects/asf/mahout-commits/core/target/mahout-core-1.0-SNAPSHOT-tests.jar
/home/dmitriy/projects/asf/mahout-commits/spark/target/mahout-spark-1.0-SNAPSHOT.jar
/home/dmitriy/projects/asf/mahout-commits/spark/target/mahout-spark-1.0-SNAPSHOT-sources.jar
/home/dmitriy/projects/asf/mahout-commits/spark/target/mahout-spark-1.0-SNAPSHOT-tests.jar
/home/dmitriy/projects/asf/mahout-commits/spark-shell/target/mahout-spark-shell-1.0-SNAPSHOT.jar
/home/dmitriy/projects/asf/mahout-commits/spark-shell/target/mahout-spark-shell-1.0-SNAPSHOT-sources.jar
/home/dmitriy/projects/asf/mahout-commits/spark-shell/target/mahout-spark-shell-1.0-SNAPSHOT-tests.jar

/home/dmitriy/tools/spark/conf
/home/dmitriy/tools/spark/assembly/target/scala-2.10/spark-assembly-0.9.1-hadoop2.0.0-cdh4.3.0.jar
/home/dmitriy/tools/hadoop/etc/hadoop
/home/dmitriy/tools/hadoop/etc/hadoop
/home/dmitriy/projects/asf/mahout-commits/lib/*.jar
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13983564" author="ssc" created="Mon, 28 Apr 2014 22:22:09 +0100"  >&lt;p&gt;core is  mrlegacy now, thats causing the the error.&lt;/p&gt;</comment>
                            <comment id="13983602" author="ssc" created="Mon, 28 Apr 2014 22:36:58 +0100"  >&lt;p&gt;I suggest to change the prompt from &quot;scala&amp;gt;&quot; to &quot;mahout&amp;gt;&quot;, like the idea?&lt;/p&gt;</comment>
                            <comment id="13983613" author="andrew.musselman" created="Mon, 28 Apr 2014 22:45:30 +0100"  >&lt;p&gt;+1 to the mahout&amp;gt; prompt&lt;/p&gt;</comment>
                            <comment id="13983680" author="dlyubimov" created="Mon, 28 Apr 2014 23:27:19 +0100"  >&lt;p&gt;sure.&lt;/p&gt;


&lt;p&gt;On Mon, Apr 28, 2014 at 2:37 PM, Sebastian Schelter (JIRA)&lt;/p&gt;
</comment>
                            <comment id="13983761" author="hudson" created="Tue, 29 Apr 2014 01:09:49 +0100"  >&lt;p&gt;FAILURE: Integrated in Mahout-Quality #2601 (See &lt;a href=&quot;https://builds.apache.org/job/Mahout-Quality/2601/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/Mahout-Quality/2601/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1489&quot; title=&quot;Interactive Scala &amp;amp; Spark Bindings Shell &amp;amp; Script processor&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1489&quot;&gt;&lt;del&gt;MAHOUT-1489&lt;/del&gt;&lt;/a&gt; Interactive Scala &amp;amp; Spark Bindings Shell &amp;amp; Script processor (ssc: rev 1590807)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/mahout/trunk/bin/mahout&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    </comments>
                    <attachments>
                            <attachment id="12640540" name="MAHOUT-1489.patch" size="17734" author="dlyubimov" created="Wed, 16 Apr 2014 22:24:14 +0100"/>
                            <attachment id="12641156" name="MAHOUT-1489.patch.1" size="30180" author="dlyubimov" created="Tue, 22 Apr 2014 01:36:56 +0100"/>
                            <attachment id="12640541" name="mahout-spark-shell-running-standalone.png" size="16341" author="dlyubimov" created="Wed, 16 Apr 2014 22:27:00 +0100"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 26 Mar 2014 18:34:13 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>382100</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hznoo7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>382375</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                </customfields>
    </item>
</channel>
</rss>