<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 04:16:42 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/MAHOUT-906/MAHOUT-906.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[MAHOUT-906] Allow collaborative filtering evaluators to use custom logic in splitting data set</title>
                <link>https://issues.apache.org/jira/browse/MAHOUT-906</link>
                <project id="12310751" key="MAHOUT">Mahout</project>
                    <description>&lt;p&gt;I want to start a discussion about factoring out the logic used in splitting the data set into training and testing.  Here is how things stand:  There are two independent evaluator based classes:  AbstractDifferenceRecommenderEvaluator, splits all the preferences randomly into a training and testing set.  GenericRecommenderIRStatsEvaluator takes one user at a time, removes their top AT preferences, and counts how many of them the system recommends back.&lt;/p&gt;

&lt;p&gt;I have two use cases that both deal with temporal dynamics.  In one case, there may be expired items that can be used for building a training model, but not a test model.  In the other, I may want to simulate the behavior of a real system by building a preference matrix on days 1-k, and testing on the ratings the user generated on the day k+1.  In this case, it&apos;s not items, but preferences(user, item, rating triplets) which may belong only to the training set.  Before we discuss appropriate design, are there any other use cases we need to keep in mind?&lt;/p&gt;</description>
                <environment></environment>
        <key id="12533600">MAHOUT-906</key>
            <summary>Allow collaborative filtering evaluators to use custom logic in splitting data set</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="srowen">Sean Owen</assignee>
                                    <reporter username="akats">Anatoliy Kats</reporter>
                        <labels>
                            <label>features</label>
                    </labels>
                <created>Fri, 2 Dec 2011 11:27:35 +0000</created>
                <updated>Thu, 9 Feb 2012 14:00:56 +0000</updated>
                            <resolved>Thu, 29 Dec 2011 20:56:57 +0000</resolved>
                                    <version>0.5</version>
                                    <fixVersion>0.6</fixVersion>
                                    <component>Collaborative Filtering</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>0</watches>
                                    <timeoriginalestimate seconds="172800">48h</timeoriginalestimate>
                            <timeestimate seconds="172800">48h</timeestimate>
                                        <comments>
                            <comment id="13161555" author="srowen" created="Fri, 2 Dec 2011 11:44:29 +0000"  >&lt;p&gt;Yes, I think a clean refactoring of this logic, so that the random selection is one of several pluggable strategies, is just fine. You can attach a patch here.&lt;/p&gt;</comment>
                            <comment id="13161570" author="akats" created="Fri, 2 Dec 2011 12:23:24 +0000"  >&lt;p&gt;OK.  I&apos;ll wait for a day to see if anyone has other use cases for me to consider, then propose a design or start working on a patch on Monday.&lt;/p&gt;</comment>
                            <comment id="13161594" author="manuel_b" created="Fri, 2 Dec 2011 12:47:18 +0000"  >&lt;p&gt;Actually it would be a good idea to implement time based splitting. Normally we want a recommender to predict ratings for items that we are going to like in the future and this should be the evaluation basis for the recommendations.&lt;/p&gt;

&lt;p&gt;In an ecommerce scenario you want the recommender to predict the item that you are going to buy next. Therefore you have to hide the newest items.&lt;/p&gt;

&lt;p&gt;The university of hildesheim (Steffen Rendle, Christoph Freudenthaler, Lars Schmidt-Thieme) wrote a paper in 2010 where they are combining SVD + HMM and are able to outperform a standard recommender:&lt;br/&gt;
&lt;a href=&quot;http://www.ismll.uni-hildesheim.de/pub/pdfs/RendleFreudenthaler2010-FPMC.pdf&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.ismll.uni-hildesheim.de/pub/pdfs/RendleFreudenthaler2010-FPMC.pdf&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13162770" author="akats" created="Mon, 5 Dec 2011 13:27:30 +0000"  >&lt;p&gt;A time-based recommender would be great!  However, I think we should take it one step at a time.  Start with the evaluation, then implement the algorithms themselves.&lt;/p&gt;

&lt;p&gt;I thought about what it takes to implement an evaluator for a time-based algorithm.  As a bare minimum, we need the FileDataModel to read the time and date information, store it in the Preference class.  In addition, there may be other considerations that determine whether an item is eligible to be in the test set.  In our case, there is the user&apos;s browsing history that landed it to an item.  We can store whatever we need in additional columns of the input data file.  FileDataModel ought to be able to read it, and be easily extendable to a class that processes this extra data.  These modification are tricky for a newcomer.  I can give it a try, but since I do not have the big picture view, I may mess up something important.  I&apos;ll start on the obvious modifications while I wait for your comments.&lt;/p&gt;</comment>
                            <comment id="13169182" author="akats" created="Wed, 14 Dec 2011 08:30:18 +0000"  >&lt;p&gt;I am beginning to write an evaluator that takes time of preference into account.  It seems that the first thing I need is to sort all the preferences by their time.  I don&apos;t quite see how to do it without tearing into Mahout&apos;s logic.  The DataModel interface only makes a contract that we can pull one time of preferece at a time, with getPreferenceTime.  I see three options.  I can pull them out one at a time into a data structure that is sortable inside the evaluator class.  Or I could add a sortByTimePreference directly to the DataModel interface.  The latter is a lot more involved, and I can only do it in close collaboration with you.  The third option is to avoid sorting altogether.  Under this option, for every test user, and for every one of their preferences, loop over all the other preferences, and add only those that were made earlier to the training set.  This option is fastest to code, but slowest to run, in O((#prefs)^2).  For the testing alone, it works for me, so that&apos;s what I&apos;ll probably do.  Here is another compromise option:  Data is normally accumulated by the servers in order of preference-making, so the input is already approximately sorted.  If Mahout can preserve the order in which it read the data off the disk, we have sorted data without ever having to sort.  In fact maybe it already does and all we have to do is contractualize it.  Is this kind of sorting necessary for a time-based algorithm itself, or only for the evaluation?  If it&apos;s the former, perhaps we can look into implementing this.&lt;/p&gt;</comment>
                            <comment id="13169241" author="srowen" created="Wed, 14 Dec 2011 09:52:08 +0000"  >&lt;p&gt;Yes, the lightest-touch approach is to pull them out into an array of Preference-plus-time objects, and sort. DataModel does not need a sort by time method. At best you could add getTime() to Preference, which returns 0 by default in current implementations, and then create a new subclass of GenericPreference with time. It is sortable then with Comparable, or you could add sortByTime() to PreferenceArray.&lt;/p&gt;

&lt;p&gt;O(pref^2) is not a problem when there are maybe 100 prefs for a user, and in an eval framework. But if the task is just to split them into objects before some time, and after some time, then you would not be doing anything with single prefs. Instead of sorting, using the TopN class to select the top, say, 5% by time is not hard, and will make O(prefs) calls to getPreferenceTime(), which is efficient. No wrapper objects and such needed.&lt;/p&gt;

&lt;p&gt;You won&apos;t necessarily know when the data is sorted, without explicit time info. For example they don&apos;t appear in time order in a file, and anyway that ordering is ignored at reading.&lt;/p&gt;</comment>
                            <comment id="13169246" author="akats" created="Wed, 14 Dec 2011 10:07:16 +0000"  >&lt;p&gt;We will be sorting preferences by time for ALL users.  The reason is that for each preference, we will need to train a recommender on all earlier preferences, like this:&lt;/p&gt;

&lt;p&gt;1.  Sort the top &quot;at&quot; preferences for a user&lt;br/&gt;
2.  for the i&apos;th pref in sorted-prefs&lt;br/&gt;
   a.  generate a training model for all users using ealier preferences&lt;br/&gt;
   b.  generate i recommendations&lt;br/&gt;
   c.  increment intersection if one of the recommendations matches the ith actual pref.&lt;br/&gt;
3.  Calculate IR statistics as before.&lt;/p&gt;

&lt;p&gt;Is this the correct logic?  Is it enough to run an O(prefs^2) sort for reasonable-size datasets, or should we pre-sort preferences by time?  If we should sort, I&apos;ll do it using Comparable at first, but really we need to call a radix sort of some kind.&lt;/p&gt;</comment>
                            <comment id="13169249" author="srowen" created="Wed, 14 Dec 2011 10:22:38 +0000"  >&lt;p&gt;Are we talking about the IR tests, estimation test or both? The IR tests operate on one user&apos;s info at a time. The estimation test does operate on all prefs at once.&lt;/p&gt;

&lt;p&gt;For the either case, I think using the TopN class to select your top fraction of test data works reasonably well. I suppose I&apos;m only wondering how you then get the training data, since you need to remove the test data from it... Hmm. Maybe just providing a sort works fine then. Your Comparable can use getPreferenceTime(), and see if that&apos;s reasonably fast.&lt;/p&gt;</comment>
                            <comment id="13169279" author="akats" created="Wed, 14 Dec 2011 12:14:59 +0000"  >&lt;p&gt;The IR tests make recommendations for one user at a time, true, but they build a model based on all other users to make a recommendation for the one.  So, as we try to recover each preference P, we build a model based on all users, and &lt;b&gt;all preferences expressed earlier than time(P)&lt;/b&gt;.&lt;/p&gt;

&lt;p&gt;You&apos;re right, sorting is not necessary, because usually it&apos;s assumed that preferences stay constant during some time period, say, a day.  Is there an existing TopN class you are referring to, or should I write my own?&lt;/p&gt;

&lt;p&gt;I am thinking I need to write a brand new evaluator and make the existing GenericRecommenderIRStatsEvaluator its subclass, rather than the other way around.  The reason is that the outer loop of a temporal evaluator class is over the time range of preferences, and only then over the users like GenericRecommenderIRStatsEvaluator.  It&apos;s natural to see the generic evaluator as a special case of the temporal one, with one pass over the outer loop.  What do you think?&lt;/p&gt;

&lt;p&gt;So, I&apos;d write a loop like this:&lt;br/&gt;
for i in 1...N:&lt;br/&gt;
  let training data be bottom (i/N * 100)% by time.&lt;br/&gt;
  let testing data be between (i/N, (i+1)/N)*100%&lt;br/&gt;
  (Alternatively, split by a time period, s.t. let days 1...i be training, and i+1 be testing)&lt;br/&gt;
  Generate the same number of preferences for each user as in the testing data&lt;br/&gt;
  Compute IR statistics on the intersection of actual and predicted preferences.&lt;/p&gt;

&lt;p&gt;How does that sound?&lt;/p&gt;</comment>
                            <comment id="13169280" author="srowen" created="Wed, 14 Dec 2011 12:24:31 +0000"  >&lt;p&gt;OK. I think we&apos;re speaking about the estimation test, not the IR tests. In the IR test there is not really a notion of training and test data; there are the relevant items and non-relevant items. The &apos;relevant&apos; items are the ones held out. You could hold out the latest prefs, I guess, though I wonder if this compromises the meaning of the result. It is not necessarily &quot;bad&quot;, for example, if the recommender doesn&apos;t consider those latest prefs the top recs. That is not what any implementation is trying to do.&lt;/p&gt;

&lt;p&gt;Sorting isn&apos;t needed, but it is probably the easiest way to split the data into training and test data. I don&apos;t know if it will be much slower than alternatives, and if it&apos;s not, fine for eval purposes. TopN is an existing class. It will be faster at picking out the &quot;most recent&quot; prefs for you but I don&apos;t know of an easy way to reuse it to also give you the rest of the older objects efficiently. So, I suppose I&apos;d start with a sort, which is probably 10 lines of code, and see if it&apos;s fast enough.&lt;/p&gt;

&lt;p&gt;I do not see a need for any new evaluator, no. The point here is to factor out the test/training split logic only, and with that pluggable, you should be able to create test/training splits based on time. No?&lt;/p&gt;</comment>
                            <comment id="13169310" author="akats" created="Wed, 14 Dec 2011 12:58:45 +0000"  >&lt;p&gt;I guess it&apos;s a hybrid of some sort between estimation and your implementation of the IR tests.  You&apos;re right, I used the term test data to what you call relevant data, and I called the rest training data.  The estimation test does not work for me because at present I am working with boolean preferences.  So, taking into account what you just said, I propose a PrefSplitIRStatsEvaluator that does the following:&lt;/p&gt;

&lt;p&gt;1.  Split the data into the training and testing set, with the splitting logic factorized into a separate class.  This class should probably ensure that only relevant data goes into test set.  I have very little experience with non-boolean data, so I&apos;ll follow your directions about where to put the relevance check.&lt;br/&gt;
2.  Build a model on the training data&lt;br/&gt;
3.  Generate the same number of preferences for each user as in the testing data&lt;br/&gt;
4.  Compute IR statistics on the intersection of actual and predicted preferences.&lt;/p&gt;

&lt;p&gt;The temporal evaluator I need will then loop over calls this class and aggregate statistics over it.  I will certainly implement it for my project, and I do not mind contributing it to Mahout.&lt;/p&gt;</comment>
                            <comment id="13169331" author="akats" created="Wed, 14 Dec 2011 13:26:05 +0000"  >&lt;p&gt;Or maybe we can somehow split the AbstractDifferenceRecommenderEvaluator into an AbstractDataSplitEvaluator and a TestSetEvaluationMetric with a public Object computeEvaluation() ...?&lt;/p&gt;</comment>
                            <comment id="13169332" author="srowen" created="Wed, 14 Dec 2011 13:26:14 +0000"  >&lt;p&gt;For the IR precision/recall evaluation, if you &lt;b&gt;do&lt;/b&gt; have preference values, then picking the &quot;relevant&quot; items by recency doesn&apos;t work. It&apos;s not true that these are the best answers that the recommendation can give. (It&apos;s not even quite true that the highest-rated items are the best recommendations!) Imagine you just rated several movies you hate. You would not want to score a recommender more highly because it recommends these.&lt;/p&gt;

&lt;p&gt;If you &lt;b&gt;don&apos;t&lt;/b&gt; have preference values, then picking the relevant items is arbitrary. Picking by time is as good as randomly picking. But, that means you&apos;re not expecting to gain anything by splitting by time. Any selection is about equivalent.&lt;/p&gt;

&lt;p&gt;So I think this doesn&apos;t help your use of the IR test, and creates a bad test for other use cases. I &lt;b&gt;do&lt;/b&gt; think it could be meaningful for the estimation test.&lt;/p&gt;

&lt;p&gt;Even if I thought it were a good test, I don&apos;t see the need for a new class. This is merely extracting how &quot;relevantIDs&quot; is computed. Yes, you can refactor that as you say, but then how does anything else change?&lt;/p&gt;</comment>
                            <comment id="13169354" author="akats" created="Wed, 14 Dec 2011 13:47:14 +0000"  >&lt;p&gt;You&apos;re right, if you have preference values, picking relevant items by recency does not work.  But, if you want to test for temporal dynamics, here is a test that works for boolean and rated data:&lt;/p&gt;

&lt;p&gt;1.  Let days 1...(i-1) be your training data&lt;br/&gt;
2.  Let day i be your test data&lt;br/&gt;
3.  Calculate an evaluation metric of the test on a recommender created by the training data.&lt;/p&gt;

&lt;p&gt;For rated data, you use the difference metric and that&apos;s already implemented.  For boolean data, you use the IR metric that I proposed earlier:  For each user, generate as many recommendations as there are in the test set.  Compute IR statistics based on the intersection of actual preferences, and generated recommendations.&lt;/p&gt;

&lt;p&gt;The easiest way to do that seems to be factoring out the data splitting from the AbstractDifferenceRecommenderEvaluator, which should be renamed, if we go ahead, to reflect its new role.&lt;/p&gt;</comment>
                            <comment id="13169361" author="srowen" created="Wed, 14 Dec 2011 13:55:03 +0000"  >&lt;p&gt;Sure, you can do that. I am not sure that gives you a better result than anything other split, but at least it is no worse. I&apos;m reluctant to add a hook that just lets someone create an invalid test (with ratings) but if that can be worked around (throw an exception if the model has no pref values?) then that&apos;s mitigated.&lt;/p&gt;

&lt;p&gt;This would really be much more useful in the estimate-based evaluation though, not here. I thought that&apos;s what this was about.&lt;/p&gt;

&lt;p&gt;I still have no understanding of why this refactoring involves adding new eval classes or moving or renaming them. It just factoring out a piece of logic that splits a set into two subsets. It ought not be more complex than that.&lt;/p&gt;</comment>
                            <comment id="13169389" author="akats" created="Wed, 14 Dec 2011 14:24:18 +0000"  >&lt;p&gt;I hope this test gives a more realistic result because sometimes preferences change with time, and this test is in support of recommendation algorithms that can take advantage of this temporal dynamics.&lt;/p&gt;

&lt;p&gt;I think all I need to do is make splitOneUsersPrefs protected instead of private, and then extend the AbstractDifferenceRecommenderEvaluator.  I&apos;m going to get started now.&lt;/p&gt;</comment>
                            <comment id="13169423" author="akats" created="Wed, 14 Dec 2011 15:11:35 +0000"  >&lt;p&gt;I have to head out, let me ask you a question before I do.  Based on what I saw so far, it seems that I need to factor out AbstractDifferenceRecommenderEvaluator::call() to check for relevant items instead of simply estimating the preference.  If that&apos;s indeed the case, how do you feel about that?&lt;/p&gt;</comment>
                            <comment id="13169485" author="srowen" created="Wed, 14 Dec 2011 16:12:15 +0000"  >&lt;p&gt;No I think it&apos;s as simple as factoring out this section of code in GenericRecommenderIRStatsEvaluator, that&apos;s all:&lt;/p&gt;


&lt;p&gt;      FastIDSet relevantItemIDs = new FastIDSet(at);&lt;/p&gt;

&lt;p&gt;      // List some most-preferred items that would count as (most) &quot;relevant&quot; results&lt;br/&gt;
      double theRelevanceThreshold = Double.isNaN(relevanceThreshold) ? computeThreshold(prefs) : relevanceThreshold;&lt;/p&gt;

&lt;p&gt;      prefs.sortByValueReversed();&lt;/p&gt;

&lt;p&gt;      for (int i = 0; i &amp;lt; size &amp;amp;&amp;amp; relevantItemIDs.size() &amp;lt; at; i++) {&lt;br/&gt;
        if (prefs.getValue&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &amp;gt;= theRelevanceThreshold) &lt;/p&gt;
{
          relevantItemIDs.add(prefs.getItemID(i));
        }
&lt;p&gt;      }&lt;/p&gt;


&lt;p&gt;I had thought you were not changing the estimated-based test? but there it is a matter of factoring out what splitOneUsersPrefs() does.&lt;/p&gt;</comment>
                            <comment id="13170003" author="akats" created="Thu, 15 Dec 2011 07:42:22 +0000"  >&lt;p&gt;You&apos;ve actually convinced me to change the estimate test.  Changing relevantItemIDs will not do what I am trying to do.  Suppose John is the current user, and I restrict his relevant items to be the preferences he expressed before Dec 5th.  I can remove them, sure, but then I&apos;ll be building a model based on all other users&apos; preferences:  I&apos;d be using Mary&apos;s preferences from Dec 10th to predict John&apos;s on Dec 5th.  That misses the entire point, that user preferences are dynamic, and change over time.  The production algorithm will not be able to predict backwards, so our test environment should not allow that either.  That&apos;s why I want to have all the data before a fixed time to be my training set, and the data for the following time period to be my test said.&lt;/p&gt;

&lt;p&gt;Which recommender do you think I should modify to make that happen?&lt;/p&gt;</comment>
                            <comment id="13170023" author="akats" created="Thu, 15 Dec 2011 08:19:13 +0000"  >&lt;p&gt;OK, I think I got it(again).  We need to factor out relevant ID computation and processOtherUser out of GenericRecommenderIRStatsEvaluator.  Then we could pass in a class that knows the split date, and split the current and other users according to that.&lt;/p&gt;</comment>
                            <comment id="13170119" author="srowen" created="Thu, 15 Dec 2011 10:53:31 +0000"  >&lt;p&gt;OK, sounds like you want to replace more logic, but that&apos;s not much harder. Even in the IR test, it&apos;s still a question of separating data into test and training data. Hopefully you can then re-use the same test-training split logic for both objects. You do not need to modify any Recommender here.&lt;/p&gt;</comment>
                            <comment id="13170126" author="akats" created="Thu, 15 Dec 2011 11:21:56 +0000"  >&lt;p&gt;Passes GenericRecommenderIRStatsEvaluatorImplTest, didn&apos;t run the entire suite.&lt;/p&gt;</comment>
                            <comment id="13170141" author="srowen" created="Thu, 15 Dec 2011 11:47:27 +0000"  >&lt;p&gt;Yes that&apos;s a good start. Do you think it&apos;s possible and desirable to use one single split method for both the estimate-based and IR test? That would be tidy.&lt;/p&gt;</comment>
                            <comment id="13170151" author="akats" created="Thu, 15 Dec 2011 12:10:46 +0000"  >&lt;p&gt;I don&apos;t quite see how.  We are not exactly splitting into training and test sets under the one-user testing paradigm.  We have a split for the current user, and we have a possibly different method for choosing which preferences will make it into the training model.  So, it&apos;s not a matter of a split on a parameter, like relevance threshold.  I kind of see how this is a possibility, but I don&apos;t fully see the purpose or motivation, so I can&apos;t propose a specific API.  What exactly do you want to see, and why?&lt;/p&gt;</comment>
                            <comment id="13170154" author="srowen" created="Thu, 15 Dec 2011 12:27:52 +0000"  >&lt;p&gt;In both cases you have data to put into a model and some held out. For IR you&apos;re holding out just a few items, from one user, but it still fits that description. The input is prefs and the output is two subsets of those prefs.&lt;/p&gt;

&lt;p&gt;The benefit would be implementing a strategy once, not twice, for the two interface methods. I would envision an interface with one method like...&lt;/p&gt;

&lt;p&gt;    split(DataModel dataModel, FastByIDMap&amp;lt;PreferenceArray&amp;gt; trainingData, FastByIDMap&amp;lt;PreferenceArray&amp;gt; testData);&lt;/p&gt;

&lt;p&gt;The two tests would do different things with those results, yes. It&apos;s going to be 10 lines of change not 1.&lt;/p&gt;

&lt;p&gt;It&apos;s not a big deal just seems easier on you.&lt;/p&gt;</comment>
                            <comment id="13170174" author="srowen" created="Thu, 15 Dec 2011 13:04:25 +0000"  >&lt;p&gt;This is a sketch of what I had in mind. It is lacking the implementation but suggests how one method might be used for both. I could be overlooking some detail of why this wouldn&apos;t work. The configuration params would re-appear in implementations.&lt;/p&gt;</comment>
                            <comment id="13170213" author="akats" created="Thu, 15 Dec 2011 14:07:15 +0000"  >&lt;p&gt;Ah, I see what you mean.  That would be neat indeed, because we could unify the split for both classes of evaluators.  My concern, though, is that it&apos;s hard for me to imagine a case where I could use the same splitting strategy for both tests &amp;#8211; IRStatsTest currently requires that we test on one user at a time, for instance.  In general my feeling is that the nature of the tests requires different splitting strategies.  I&apos;d feel more comfortable keeping things the way they are for the time being while we get some experience with the different splitting strategies that our users might need.  Afterwards, if it becomes apparent your proposal is in our users&apos; interest, we can deprecate the API in my patch, and write a wrapper class that takes my IRDataSplitter and converts it to your splitter.&lt;/p&gt;</comment>
                            <comment id="13170217" author="akats" created="Thu, 15 Dec 2011 14:09:07 +0000"  >&lt;p&gt;Also, I feel like my class names are unnecessarily long.  Feel free to edit before committing.&lt;/p&gt;</comment>
                            <comment id="13170243" author="srowen" created="Thu, 15 Dec 2011 14:29:50 +0000"  >&lt;p&gt;OK shall I wait for a complete patch?&lt;/p&gt;</comment>
                            <comment id="13170819" author="akats" created="Fri, 16 Dec 2011 08:17:49 +0000"  >&lt;p&gt;Here it is.&lt;/p&gt;</comment>
                            <comment id="13170822" author="srowen" created="Fri, 16 Dec 2011 08:22:17 +0000"  >&lt;p&gt;(The patch isn&apos;t marked for inclusion in the project &amp;#8211; I assume you intended it for commit.)&lt;/p&gt;</comment>
                            <comment id="13170835" author="srowen" created="Fri, 16 Dec 2011 08:47:18 +0000"  >&lt;p&gt;Sorry did not mean to make you re-upload, just checking for completeness.&lt;br/&gt;
This patch doesn&apos;t have any new logic for time-based splitting though? &lt;/p&gt;</comment>
                            <comment id="13170836" author="akats" created="Fri, 16 Dec 2011 08:52:22 +0000"  >&lt;p&gt;Not yet, just a refactoring so far.  Still working on it.&lt;/p&gt;</comment>
                            <comment id="13173201" author="akats" created="Tue, 20 Dec 2011 13:58:45 +0000"  >&lt;p&gt;This includes the split by time.  I haven&apos;t added unit tests.  However, I picked a date range, and separated the entries in that range from my data using awk.  Then I ran an evaluation algorithm on the entire data set, and this smaller file, both times using the date range of the smaller files.  The results were identical, after I changed GenericDataModel to sort the output of GetPreferencesFromUser.&lt;/p&gt;</comment>
                            <comment id="13173218" author="srowen" created="Tue, 20 Dec 2011 14:26:28 +0000"  >&lt;p&gt;OK the problem I&apos;m still having with it, which is a small matter of organization, is that this plucks out two unrelated pieces of code to refactor and places them into one interface with two methods. For example it&apos;s called &quot;RelevantItemsDataSplitter&quot; when that only describes 1 of the 2 roles it plays.&lt;/p&gt;

&lt;p&gt;Can this be two interfaces, since these are two different roles conceptually? For example the &quot;processOneUser()&quot; ought to be in a different interface and have a more descriptive name I&apos;d imagine. &lt;/p&gt;

&lt;p&gt;If you can please use 2 spaces instead of tab for formatting.&lt;/p&gt;

&lt;p&gt;Otherwise this is pretty uncontroversial, just pulling out some code and leaving a hook in its place, which seems just fine.&lt;/p&gt;</comment>
                            <comment id="13174016" author="akats" created="Wed, 21 Dec 2011 11:18:14 +0000"  >&lt;p&gt;Yeah, I was thinking about that too.  The reason I decided to keep them together is that they work together to create something like a training set, and something like a testing set.  Maybe we should just call it IRStatsEvaluationDataSplitter?&lt;/p&gt;</comment>
                            <comment id="13174802" author="srowen" created="Thu, 22 Dec 2011 13:34:24 +0000"  >&lt;p&gt;Do you mind if I end up splitting these interfaces? The name &quot;IRStatsEvaluationDataSplitter&quot; still just describes one thing it does and not the other.&lt;/p&gt;</comment>
                            <comment id="13175832" author="srowen" created="Sun, 25 Dec 2011 14:40:36 +0000"  >&lt;p&gt;Oh I see, this actually doesn&apos;t implement test/training splitting, which is the other thing I thought should be separated.&lt;br/&gt;
Well, why don&apos;t I have a crack at fixing up the formatting and such and adding that, and then committing the whole lot.&lt;/p&gt;</comment>
                            <comment id="13175838" author="srowen" created="Sun, 25 Dec 2011 15:37:42 +0000"  >&lt;p&gt;After looking at this more I&apos;m not sure this is the right refactoring. The time-based implementation is parameterized by four times. Shouldn&apos;t it be 1? before that time is training data, after that is test? It still splits on a relevance threshold - shouldn&apos;t it just be based on time? Splitting this logic out, so that the relevance-threshold business only exists in one implementation, will take more surgery than this. For example, evaluate() shouldn&apos;t have a relevance threshold parameter anymore, then, since that would be a property of one particular splitter strategy, specified in the constructor. The resulting time-based implementation will then not be quite so much copy and paste, which is good, as it&apos;s a symptom of this not quite splitting up the logic completely. I think that would make the change worth committing.&lt;/p&gt;

&lt;p&gt;While I can and have changed it locally, you&apos;ll want to watch the spacing, formatting, and ensure the copyright headers are in place. Also the implementations should live in .impl.eval, not .eval.&lt;/p&gt;

&lt;p&gt;It would be better still to also refactor the test/training data split as well, not just relevant items. Consider that a bonus.&lt;/p&gt;</comment>
                            <comment id="13175902" author="akats" created="Mon, 26 Dec 2011 07:24:41 +0000"  >&lt;p&gt;You&apos;re right, the copy-paste is a bad sign, but I don&apos;t quite know how to fix it.  I do want a constructor with at least three distinct times.  In a real system, preferences older than a certain age might be deleted as irrelevant.  A simple way to emulate that is to test using a sliding window:  Days 1-30 training, day 31 testing, then 2-31 training, 32 testing, etc.  So, I&apos;d need a start date, split date, and end date.  Relevance threshold is here for the same reason as it is in the generic splitter &amp;#8211; we dont&apos; want to test on negatively rated items.  I think storing it in the splitter classes is a good idea.  Perhaps we could create an abstract class that leafs through a user&apos;s preferences and returns a sorted list of those above the threshold?  Then we can use that function in our splitters.&lt;/p&gt;</comment>
                            <comment id="13175941" author="srowen" created="Mon, 26 Dec 2011 14:38:38 +0000"  >&lt;p&gt;Old data can and should just be excluded from the test corpus, full stop. Or, to put it another way: that&apos;s not specific to a time-based split, and in general the need is already met earlier in the pipeline, so to speak. Why is an end date needed... seems best to just use all the recent data you have.&lt;/p&gt;

&lt;p&gt;Understand about also including score in addition to time in the definition of &quot;relevant&quot;. I think the problem I&apos;m having is that the &quot;time+score&quot; implementation seems of nearly the same value as the current implementation, which is just &quot;score&quot;. The time bit seems secondary, and partially used as a simple filter on the input. So I&apos;m struggling a bit to like just this change; the new implementation is mostly a copy. It&apos;s on the borderline of being something you may just want to use locally for your own purpose.&lt;/p&gt;

&lt;p&gt;Any other thoughts from anyone else here?&lt;/p&gt;

&lt;p&gt;I suppose the time-based split makes a lot more sense to me for the estimation-based test and can clearly see the use in a hook and second implementation there. No question about that.&lt;/p&gt;</comment>
                            <comment id="13176111" author="akats" created="Tue, 27 Dec 2011 08:32:50 +0000"  >&lt;p&gt;You&apos;re right, old data should be excluded from the test corpus.  The point of a time-based algorithm is to treat older data differently from newer data.  So, if there is data it chooses to disregard, it should be be done at the recommender level, not the evaluator level.  However, we still need a test start and end date:  the entire idea is that we use only use preferences prior to the &quot;current&quot; time to make a recommendation.  In production, that&apos;s always the case.  In a test environment we need to approximate it using a sliding window approach:  train on days 1-20, test on day 21, then train on 1-21 test on 22, etc.&lt;/p&gt;

&lt;p&gt;I don&apos;t mind using my local modifications of the time-based splitter, so long as the trunk maintains the hook.  I see the motivation as well for putting the hook in the abstract difference recommender.  If I get around to using a preference-value recommender, I&apos;ll look into it.&lt;/p&gt;</comment>
                            <comment id="13176697" author="srowen" created="Wed, 28 Dec 2011 16:24:16 +0000"  >&lt;p&gt;OK. I&apos;m ready to commit the hook, with minor changes. The only question I have remaining about the change, is the change in definition of &apos;numItems&apos; and &apos;size&apos;. I think these were as intended, as they were. Am I missing the motivation for the change?&lt;/p&gt;</comment>
                            <comment id="13177387" author="srowen" created="Thu, 29 Dec 2011 20:56:58 +0000"  >&lt;p&gt;OK, I do understand the different computation for &quot;size&quot;. As is, it&apos;s equivalent to the original, so I used the patched version. For &quot;numItems&quot; I think the new formula will slightly overcount, as it is used. It only affects fall-out anyway, so I left it as-is.&lt;/p&gt;</comment>
                            <comment id="13177422" author="hudson" created="Thu, 29 Dec 2011 22:02:28 +0000"  >&lt;p&gt;Integrated in Mahout-Quality #1280 (See &lt;a href=&quot;https://builds.apache.org/job/Mahout-Quality/1280/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/Mahout-Quality/1280/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-906&quot; title=&quot;Allow collaborative filtering evaluators to use custom logic in splitting data set&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-906&quot;&gt;&lt;del&gt;MAHOUT-906&lt;/del&gt;&lt;/a&gt; add hook for different relevant item ID logic&lt;/p&gt;

&lt;p&gt;srowen : &lt;a href=&quot;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1225649&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1225649&lt;/a&gt;&lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/eval/RelevantItemsDataSplitter.java&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/impl/eval/GenericRecommenderIRStatsEvaluator.java&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/impl/eval/GenericRelevantItemsDataSplitter.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    </comments>
                    <attachments>
                            <attachment id="12508077" name="MAHOUT-906.patch" size="14058" author="akats" created="Tue, 20 Dec 2011 13:58:45 +0000"/>
                            <attachment id="12507667" name="MAHOUT-906.patch" size="8049" author="akats" created="Fri, 16 Dec 2011 08:31:49 +0000"/>
                            <attachment id="12507662" name="MAHOUT-906.patch" size="8049" author="akats" created="Fri, 16 Dec 2011 08:17:33 +0000"/>
                            <attachment id="12507516" name="MAHOUT-906.patch" size="14053" author="srowen" created="Thu, 15 Dec 2011 13:04:25 +0000"/>
                            <attachment id="12507505" name="MAHOUT-906.patch" size="7965" author="akats" created="Thu, 15 Dec 2011 11:21:55 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>5.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Fri, 2 Dec 2011 11:44:29 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>219328</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hxy28v:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>22515</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                </customfields>
    </item>
</channel>
</rss>