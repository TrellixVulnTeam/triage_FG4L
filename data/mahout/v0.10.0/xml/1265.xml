<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 04:15:43 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/MAHOUT-1265/MAHOUT-1265.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[MAHOUT-1265] Add Multilayer Perceptron </title>
                <link>https://issues.apache.org/jira/browse/MAHOUT-1265</link>
                <project id="12310751" key="MAHOUT">Mahout</project>
                    <description>&lt;p&gt;Design of multilayer perceptron&lt;/p&gt;


&lt;p&gt;1. Motivation&lt;br/&gt;
A multilayer perceptron (MLP) is a kind of feed forward artificial neural network, which is a mathematical model inspired by the biological neural network. The multilayer perceptron can be used for various machine learning tasks such as classification and regression. It is helpful if it can be included in mahout.&lt;/p&gt;

&lt;p&gt;2. API&lt;/p&gt;

&lt;p&gt;The design goal of API is to facilitate the usage of MLP for user, and make the implementation detail user transparent.&lt;/p&gt;

&lt;p&gt;The following is an example code of how user uses the MLP.&lt;br/&gt;
-------------------------------------&lt;br/&gt;
//  set the parameters&lt;br/&gt;
double learningRate = 0.5;&lt;br/&gt;
double momentum = 0.1;&lt;br/&gt;
int[] layerSizeArray = new int[] &lt;/p&gt;
{2, 5, 1}
&lt;p&gt;;&lt;br/&gt;
String costFuncName = &#8220;SquaredError&#8221;;&lt;br/&gt;
String squashingFuncName = &#8220;Sigmoid&#8221;;&lt;br/&gt;
//  the location to store the model, if there is already an existing model at the specified location, MLP will throw exception&lt;br/&gt;
URI modelLocation = ...&lt;br/&gt;
MultilayerPerceptron mlp = new MultiLayerPerceptron(layerSizeArray, modelLocation);&lt;br/&gt;
mlp.setLearningRate(learningRate).setMomentum(momentum).setRegularization(...).setCostFunction(...).setSquashingFunction(...);&lt;/p&gt;

&lt;p&gt;//  the user can also load an existing model with given URI and update the model with new training data, if there is no existing model at the specified location, an exception will be thrown&lt;br/&gt;
/*&lt;br/&gt;
MultilayerPerceptron mlp = new MultiLayerPerceptron(learningRate, regularization, momentum, squashingFuncName, costFuncName, modelLocation);&lt;br/&gt;
*/&lt;/p&gt;

&lt;p&gt;URI trainingDataLocation = &#8230;&lt;br/&gt;
//  the detail of training is transparent to the user, it may running in a single machine or in a distributed environment&lt;br/&gt;
mlp.train(trainingDataLocation);&lt;/p&gt;

&lt;p&gt;//  user can also train the model with one training instance in stochastic gradient descent way&lt;br/&gt;
Vector trainingInstance = ...&lt;br/&gt;
mlp.train(trainingInstance);&lt;/p&gt;

&lt;p&gt;//  prepare the input feature&lt;br/&gt;
Vector inputFeature &#8230;&lt;br/&gt;
//  the semantic meaning of the output result is defined by the user&lt;br/&gt;
//  in general case, the dimension of output vector is 1 for regression and two-class classification&lt;br/&gt;
//  the dimension of output vector is n for n-class classification (n &amp;gt; 2)&lt;br/&gt;
Vector outputVector = mlp.output(inputFeature); &lt;br/&gt;
-------------------------------------&lt;/p&gt;


&lt;p&gt;3. Methodology&lt;/p&gt;

&lt;p&gt;The output calculation can be easily implemented with feed-forward approach. Also, the single machine training is straightforward. The following will describe how to train MLP in distributed way with batch gradient descent. The workflow is illustrated as the below figure.&lt;/p&gt;


&lt;p&gt;&lt;a href=&quot;https://docs.google.com/drawings/d/1s8hiYKpdrP3epe1BzkrddIfShkxPrqSuQBH0NAawEM4/pub?w=960&amp;amp;h=720&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://docs.google.com/drawings/d/1s8hiYKpdrP3epe1BzkrddIfShkxPrqSuQBH0NAawEM4/pub?w=960&amp;amp;h=720&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For the distributed training, each training iteration is divided into two steps, the weight update calculation step and the weight update step. The distributed MLP can only be trained in batch-update approach.&lt;/p&gt;


&lt;p&gt;3.1 The partial weight update calculation step:&lt;br/&gt;
This step trains the MLP distributedly. Each task will get a copy of the MLP model, and calculate the weight update with a partition of data.&lt;/p&gt;

&lt;p&gt;Suppose the training error is E(w) = &#189; \sigma_&lt;/p&gt;
{d \in D}
&lt;p&gt; cost(t_d, y_d), where D denotes the training set, d denotes a training instance, t_d denotes the class label and y_d denotes the output of the MLP. Also, suppose sigmoid function is used as the squashing function, &lt;br/&gt;
squared error is used as the cost function, &lt;br/&gt;
t_i denotes the target value for the ith dimension of the output layer, &lt;br/&gt;
o_i denotes the actual output for the ith dimension of the output layer, &lt;br/&gt;
l denotes the learning rate,&lt;br/&gt;
w_&lt;/p&gt;
{ij} denotes the weight between the jth neuron in previous layer and the ith neuron in the next layer. &lt;br/&gt;
&lt;br/&gt;
The weight of each edge is updated as &lt;br/&gt;
&lt;br/&gt;
\Delta w_{ij}
&lt;p&gt; = l * 1 / m * \delta_j * o_i, &lt;/p&gt;

&lt;p&gt;where \delta_j = - \sigma_&lt;/p&gt;
{m} * o_j^{(m)} * (1 - o_j^{(m)}) * (t_j^{(m)} - o_j^{(m)}) for output layer, \delta = - \sigma_{m}
&lt;p&gt; * o_j^&lt;/p&gt;
{(m)} * (1 - o_j^{(m)}
&lt;p&gt;) * \sigma_k \delta_k * w_&lt;/p&gt;
{jk}
&lt;p&gt; for hidden layer. &lt;/p&gt;

&lt;p&gt;It is easy to know that \delta_j can be rewritten as &lt;/p&gt;

&lt;p&gt;\delta_j = - \sigma_&lt;/p&gt;
{i = 1}
&lt;p&gt;^k \sigma_&lt;/p&gt;
{m_i}
&lt;p&gt; * o_j^&lt;/p&gt;
{(m_i)} * (1 - o_j^{(m_i)}
&lt;p&gt;) * (t_j^&lt;/p&gt;
{(m_i)} - o_j^{(m_i)}
&lt;p&gt;)&lt;/p&gt;

&lt;p&gt;The above equation indicates that the \delta_j can be divided into k parts.&lt;/p&gt;

&lt;p&gt;So for the implementation, each mapper can calculate part of \delta_j with given partition of data, and then store the result into a specified location.&lt;/p&gt;


&lt;p&gt;3.2 The model update step:&lt;/p&gt;

&lt;p&gt;After k parts of \delta_j been calculated, a separate program can be used to merge the k parts of \delta_j into one to update the weight matrices.&lt;/p&gt;

&lt;p&gt;This program can load the results calculated in the weight update calculation step and update the weight matrices. &lt;/p&gt;</description>
                <environment></environment>
        <key id="12653426">MAHOUT-1265</key>
            <summary>Add Multilayer Perceptron </summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="smarthi">Suneel Marthi</assignee>
                                    <reporter username="yxjiang">Yexi Jiang</reporter>
                        <labels>
                            <label>machine_learning</label>
                            <label>neural_network</label>
                    </labels>
                <created>Tue, 18 Jun 2013 13:28:47 +0100</created>
                <updated>Tue, 15 Apr 2014 21:13:52 +0100</updated>
                            <resolved>Thu, 19 Dec 2013 19:31:15 +0000</resolved>
                                                    <fixVersion>0.9</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                <comments>
                            <comment id="13686833" author="tdunning" created="Tue, 18 Jun 2013 16:33:26 +0100"  >&lt;p&gt;Yexi,&lt;/p&gt;

&lt;p&gt;I would suggest that a more fluid API would be helpful to people.  For instance, &lt;br/&gt;
each layer might be an object which could be composed together to build a model which&lt;br/&gt;
is then trained.&lt;/p&gt;

&lt;p&gt;Secondly, it seems like it would be good to have different kinds of loss function and&lt;br/&gt;
regularizations.&lt;/p&gt;

&lt;p&gt;Also, regarding things like momentum, do you have an idea that this really needs to be&lt;br/&gt;
commonly adjusted?  or is there a way to set a good default?&lt;/p&gt;</comment>
                            <comment id="13686880" author="yxjiang" created="Tue, 18 Jun 2013 17:15:33 +0100"  >&lt;p&gt;Ted,&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I would suggest that a more fluid API would be helpful to people. For instance, &lt;br/&gt;
each layer might be an object which could be composed together to build a model which&lt;br/&gt;
is then trained.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It seems that you suggest a more general neural network, not just the MLP.&lt;br/&gt;
A MLP is a kind of feed-forward neural network that the topology is fixed.&lt;br/&gt;
It usually consists of several layers and every pair of neurons in adjacent layers are connected.&lt;br/&gt;
Therefore, specify the size of each layer is enough to determine the topology of a MLP.&lt;/p&gt;

&lt;p&gt;It is good if we first define a generic neural network, and then build a MLP on top of this generic neural network in the way as you said. An advantage is that the generic neural network can be reused to build other types of neural networks in the future, e.g. autoencoder for dimensional reduction, recurrent neural network for sequential mining, or possibly deep nets, etc.&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;Secondly, it seems like it would be good to have different kinds of loss function and&lt;br/&gt;
regularizations.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, the MLP would allow the user to specify different loss function, squashing functions, and regularizations.&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;Also, regarding things like momentum, do you have an idea that this really needs to be&lt;br/&gt;
commonly adjusted? or is there a way to set a good default?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;As far as I know, there is no empirical way to set a good default momentum weight. A good value is determined by the concrete problem. As for learning rate, a good way is to enable the decaying learning rate.&lt;/p&gt;

</comment>
                            <comment id="13728754" author="yxjiang" created="Sun, 4 Aug 2013 05:17:50 +0100"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tdunning&quot; class=&quot;user-hover&quot; rel=&quot;tdunning&quot;&gt;Ted Dunning&lt;/a&gt; I have finished a workable single machine version MultilayerPerceptron (based on NeuralNetwork). It supports the requirement as you mentioned above. It allow users to customize each layer including the size and the squashing function. Also, it allows users to specify different loss functions to the model. Moreover, it allow user to store the trained model and reload it for later use. Finally, it allows users to extract the weight of each layer from a trained model. This approach allows users to train and stack a deep learning neural network layer by layer. If this single machine version passes the review, I will begin to work on the map-reduce version base on it.&lt;/p&gt;</comment>
                            <comment id="13728756" author="yxjiang" created="Sun, 4 Aug 2013 05:22:04 +0100"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tdunning&quot; class=&quot;user-hover&quot; rel=&quot;tdunning&quot;&gt;Ted Dunning&lt;/a&gt; The test cases contain the test on three datasets, the simple XOR problem, the Cancer dataset (2-class classification) and the Iris dataset(3-class classification). For the later two datasets, the classification accuracy is more than 90%.&lt;/p&gt;</comment>
                            <comment id="13728764" author="tdunning" created="Sun, 4 Aug 2013 05:45:51 +0100"  >&lt;p&gt;We have several efforts that are going to be helped by a parameter server implementation.  Deep learning is one.  Other non-linear optimizations are likely to as well.&lt;/p&gt;

&lt;p&gt;Is this MLP issue a good place to start with that?&lt;/p&gt;</comment>
                            <comment id="13728769" author="yxjiang" created="Sun, 4 Aug 2013 06:06:44 +0100"  >&lt;p&gt;The MLP is implemented based on NeuralNetwork. The NeuralNetwork is more general in terms of functionality (can be used for regression, classification, dimensional reduction, etc) and architecture (Linear Regression and Logistic Regression as a two-level neural network, Autoencoder as a three-level neural network, I heard that even the SVM can be modeled as a type of neural network, but I&apos;m not sure.). &lt;/p&gt;

&lt;p&gt;In my opinion, the NeuralNetwork I implemented is a suitable start for deep learning, as one implementation of the Deep nets is based on stacking the Autoencoder.&lt;/p&gt;</comment>
                            <comment id="13732836" author="yxjiang" created="Wed, 7 Aug 2013 23:20:38 +0100"  >&lt;p&gt;Is there any who can review the code?&lt;br/&gt;
The sample code for using it can be seen in test cases.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tdunning&quot; class=&quot;user-hover&quot; rel=&quot;tdunning&quot;&gt;Ted Dunning&lt;/a&gt; Could you please give any comments?&lt;/p&gt;</comment>
                            <comment id="13733101" author="smarthi" created="Thu, 8 Aug 2013 04:07:56 +0100"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yxjiang&quot; class=&quot;user-hover&quot; rel=&quot;yxjiang&quot;&gt;Yexi Jiang&lt;/a&gt; Could you upload this to ReviewBoard? Its easier to review and comment on the code that way.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://reviews.apache.org&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.apache.org&lt;/a&gt;&lt;/p&gt;
</comment>
                            <comment id="13733110" author="yxjiang" created="Thu, 8 Aug 2013 04:36:36 +0100"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=smarthi&quot; class=&quot;user-hover&quot; rel=&quot;smarthi&quot;&gt;Suneel Marthi&lt;/a&gt; Done, please refer to &lt;a href=&quot;https://reviews.apache.org/r/13406/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.apache.org/r/13406/&lt;/a&gt;. Thank you.&lt;/p&gt;</comment>
                            <comment id="13787462" author="yxjiang" created="Sun, 6 Oct 2013 04:14:22 +0100"  >&lt;p&gt;There is no news?&lt;/p&gt;</comment>
                            <comment id="13787478" author="smarthi" created="Sun, 6 Oct 2013 04:45:41 +0100"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yxjiang&quot; class=&quot;user-hover&quot; rel=&quot;yxjiang&quot;&gt;Yexi Jiang&lt;/a&gt; I&apos;ll have time next week to review this.&lt;/p&gt;</comment>
                            <comment id="13834939" author="smarthi" created="Thu, 28 Nov 2013 15:54:16 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yxjiang&quot; class=&quot;user-hover&quot; rel=&quot;yxjiang&quot;&gt;Yexi Jiang&lt;/a&gt; Please look at my comments on Reviewboard.&lt;/p&gt;</comment>
                            <comment id="13834959" author="yxjiang" created="Thu, 28 Nov 2013 16:22:14 +0000"  >&lt;p&gt;OK, I&apos;ll revise it accordingly.&lt;/p&gt;</comment>
                            <comment id="13842393" author="yxjiang" created="Sun, 8 Dec 2013 03:48:00 +0000"  >&lt;p&gt;This is the final version of the patch. It has been reviewed by &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=smarthi&quot; class=&quot;user-hover&quot; rel=&quot;smarthi&quot;&gt;Suneel Marthi&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="13843161" author="yxjiang" created="Mon, 9 Dec 2013 13:52:05 +0000"  >&lt;p&gt;This is the final version of the patch. It has been reviewed by &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=smarthi&quot; class=&quot;user-hover&quot; rel=&quot;smarthi&quot;&gt;Suneel Marthi&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="13849767" author="yxjiang" created="Mon, 16 Dec 2013 21:50:47 +0000"  >&lt;p&gt;Updated according to latest feedback.&lt;/p&gt;</comment>
                            <comment id="13851123" author="smarthi" created="Tue, 17 Dec 2013 23:55:31 +0000"  >&lt;p&gt;Yexi, updated ur latest patch (rev 13). See attached. I formatted the code for style and tweaked some of the code. Try running this patch in ur environment and verify that its good.&lt;/p&gt;</comment>
                            <comment id="13851223" author="yxjiang" created="Wed, 18 Dec 2013 01:41:53 +0000"  >&lt;p&gt;I have applied the patch to my local code base and tested it. It works without any error.&lt;/p&gt;</comment>
                            <comment id="13851363" author="smarthi" created="Wed, 18 Dec 2013 04:56:49 +0000"  >&lt;p&gt;Updated patch, fixed styling issues.&lt;/p&gt;</comment>
                            <comment id="13852901" author="yxjiang" created="Thu, 19 Dec 2013 13:49:36 +0000"  >&lt;p&gt;The version 17.&lt;/p&gt;</comment>
                            <comment id="13853072" author="smarthi" created="Thu, 19 Dec 2013 17:54:13 +0000"  >&lt;p&gt;I&apos;ll be committing this code to trunk today. &lt;/p&gt;</comment>
                            <comment id="13853188" author="smarthi" created="Thu, 19 Dec 2013 19:30:55 +0000"  >&lt;p&gt;Patch committed to trunk, great work Yexi.&lt;/p&gt;</comment>
                            <comment id="13853303" author="yxjiang" created="Thu, 19 Dec 2013 20:54:22 +0000"  >&lt;p&gt;Great. I am thinking a mapreduce version of MLP. It may take a non-trivial amount of time.&lt;/p&gt;</comment>
                            <comment id="13853402" author="hudson" created="Thu, 19 Dec 2013 22:37:07 +0000"  >&lt;p&gt;SUCCESS: Integrated in Mahout-Quality #2376 (See &lt;a href=&quot;https://builds.apache.org/job/Mahout-Quality/2376/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/Mahout-Quality/2376/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1265&quot; title=&quot;Add Multilayer Perceptron &quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAHOUT-1265&quot;&gt;&lt;del&gt;MAHOUT-1265&lt;/del&gt;&lt;/a&gt;: Multilayer Perceptron (smarthi: rev 1552403)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/mahout/trunk/CHANGELOG&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/core/src/main/java/org/apache/mahout/classifier/mlp&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/core/src/main/java/org/apache/mahout/classifier/mlp/MultilayerPerceptron.java&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/core/src/main/java/org/apache/mahout/classifier/mlp/NeuralNetwork.java&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/core/src/main/java/org/apache/mahout/classifier/mlp/NeuralNetworkFunctions.java&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/core/src/test/java/org/apache/mahout/classifier/mlp&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/core/src/test/java/org/apache/mahout/classifier/mlp/TestMultilayerPerceptron.java&lt;/li&gt;
	&lt;li&gt;/mahout/trunk/core/src/test/java/org/apache/mahout/classifier/mlp/TestNeuralNetwork.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13853712" author="tdunning" created="Fri, 20 Dec 2013 06:08:36 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Great. I am thinking a mapreduce version of MLP. It may take a non-trivial amount of time.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Let&apos;s talk on the mailing list.  I really think that a downpour architecture will not be much harder than a map-reduce implementation and will be orders of magnitude faster.&lt;/p&gt;</comment>
                            <comment id="13969990" author="barsik" created="Tue, 15 Apr 2014 20:54:13 +0100"  >&lt;p&gt;Hi Yexi and Ted,&lt;/p&gt;

&lt;p&gt;Is there anything new on the mapreduce implementation?&lt;/p&gt;</comment>
                            <comment id="13970006" author="yxjiang" created="Tue, 15 Apr 2014 21:13:52 +0100"  >&lt;p&gt;Hi, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=barsik&quot; class=&quot;user-hover&quot; rel=&quot;barsik&quot;&gt;Mark Yakushev&lt;/a&gt;, according to &lt;a href=&quot;https://issues.apache.org/jira/browse/MAHOUT-1510&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;MAHOUT-1510&lt;/a&gt;, mahout no longer accept the proposal of MR algorithm.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310051">
                    <name>Supercedes</name>
                                            <outwardlinks description="supercedes">
                                        <issuelink>
            <issuekey id="12541603">MAHOUT-975</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12541691">MAHOUT-976</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12619253" name="MAHOUT-1265.patch" size="53672" author="smarthi" created="Wed, 18 Dec 2013 04:56:49 +0000"/>
                            <attachment id="12619559" name="Mahout-1265-17.patch" size="53558" author="yxjiang" created="Thu, 19 Dec 2013 13:49:36 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Tue, 18 Jun 2013 15:33:26 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>333704</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hzfeqf:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>334032</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                </customfields>
    </item>
</channel>
</rss>