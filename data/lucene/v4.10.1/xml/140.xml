<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 04:05:06 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/LUCENE-140/LUCENE-140.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[LUCENE-140] docs out of order</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-140</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                    <description>&lt;p&gt;Hello,&lt;br/&gt;
  I can not find out, why (and what) it is happening all the time. I got an&lt;br/&gt;
exception:&lt;br/&gt;
java.lang.IllegalStateException: docs out of order&lt;br/&gt;
        at&lt;br/&gt;
org.apache.lucene.index.SegmentMerger.appendPostings(SegmentMerger.java:219)&lt;br/&gt;
        at&lt;br/&gt;
org.apache.lucene.index.SegmentMerger.mergeTermInfo(SegmentMerger.java:191)&lt;br/&gt;
        at&lt;br/&gt;
org.apache.lucene.index.SegmentMerger.mergeTermInfos(SegmentMerger.java:172)&lt;br/&gt;
        at org.apache.lucene.index.SegmentMerger.mergeTerms(SegmentMerger.java:135)&lt;br/&gt;
        at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:88)&lt;br/&gt;
        at org.apache.lucene.index.IndexWriter.mergeSegments(IndexWriter.java:341)&lt;br/&gt;
        at org.apache.lucene.index.IndexWriter.optimize(IndexWriter.java:250)&lt;br/&gt;
        at Optimize.main(Optimize.java:29)&lt;/p&gt;

&lt;p&gt;It happens either in 1.2 and 1.3rc1 (anyway what happened to it? I can not find&lt;br/&gt;
it neither in download nor in version list in this form). Everything seems OK. I&lt;br/&gt;
can search through index, but I can not optimize it. Even worse after this&lt;br/&gt;
exception every time I add new documents and close IndexWriter new segments is&lt;br/&gt;
created! I think it has all documents added before, because of its size.&lt;/p&gt;

&lt;p&gt;My index is quite big: 500.000 docs, about 5gb of index directory.&lt;/p&gt;

&lt;p&gt;It is &lt;em&gt;repeatable&lt;/em&gt;. I drop index, reindex everything. Afterwards I add a few&lt;br/&gt;
docs, try to optimize and receive above exception.&lt;/p&gt;

&lt;p&gt;My documents&apos; structure is:&lt;br/&gt;
  static Document indexIt(String id_strony, Reader reader, String data_wydania,&lt;br/&gt;
String id_wydania, String id_gazety, String data_wstawienia)&lt;br/&gt;
{&lt;br/&gt;
    Document doc = new Document();&lt;/p&gt;

&lt;p&gt;    doc.add(Field.Keyword(&quot;id&quot;, id_strony ));&lt;br/&gt;
    doc.add(Field.Keyword(&quot;data_wydania&quot;, data_wydania));&lt;br/&gt;
    doc.add(Field.Keyword(&quot;id_wydania&quot;, id_wydania));&lt;br/&gt;
    doc.add(Field.Text(&quot;id_gazety&quot;, id_gazety));&lt;br/&gt;
    doc.add(Field.Keyword(&quot;data_wstawienia&quot;, data_wstawienia));&lt;br/&gt;
    doc.add(Field.Text(&quot;tresc&quot;, reader));&lt;/p&gt;

&lt;p&gt;    return doc;&lt;br/&gt;
}&lt;/p&gt;

&lt;p&gt;Sincerely,&lt;br/&gt;
legez&lt;/p&gt;</description>
                <environment>&lt;p&gt;Operating System: Linux&lt;br/&gt;
Platform: PC&lt;/p&gt;</environment>
        <key id="12314290">LUCENE-140</key>
            <summary>docs out of order</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="mikemccand">Michael McCandless</assignee>
                                    <reporter username="gegez@wp.pl">legez</reporter>
                        <labels>
                    </labels>
                <created>Tue, 7 Oct 2003 21:05:52 +0100</created>
                <updated>Thu, 2 Jun 2011 23:04:35 +0100</updated>
                            <resolved>Thu, 11 Jan 2007 12:14:18 +0000</resolved>
                                                    <fixVersion>2.1</fixVersion>
                                    <component>core/index</component>
                        <due></due>
                            <votes>5</votes>
                                    <watches>5</watches>
                                                                <comments>
                            <comment id="12321387" author="otis@apache.org" created="Tue, 25 Nov 2003 19:35:32 +0000"  >&lt;p&gt;Since the bug submitter hasn&apos;t followed up to this &apos;repeatable&apos; issue in almost&lt;br/&gt;
two months, I shall assume that this was not a bug in Lucene, but a misuse of&lt;br/&gt;
Lucene.&lt;/p&gt;

&lt;p&gt;There is also no self-contained code that demonstrates the bug.&lt;/p&gt;

&lt;p&gt;If this is indeed a Lucene bug, please re-open this bug entry, and provise&lt;br/&gt;
self-sufficient unit test that demonstrates this problem.&lt;/p&gt;</comment>
                            <comment id="12321388" author="lucene@ziplip.com" created="Wed, 15 Jun 2005 00:42:25 +0100"  >&lt;p&gt;More Data Integrity Issue: Docs out of Order&lt;/p&gt;

&lt;p&gt;Hi,&lt;br/&gt;
Seeing similar issue to the one reported in:&lt;br/&gt;
&lt;a href=&quot;http://nagoya.apache.org/bugzilla/show_bug.cgi?id=23650&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://nagoya.apache.org/bugzilla/show_bug.cgi?id=23650&lt;/a&gt;&lt;br/&gt;
On examining the segments, following inconsistencies were found&lt;br/&gt;
(a) The merging segments had doc number that is greater than maxDoc.&lt;br/&gt;
Don&apos;t know how it go in this state, but this occurs using standard lucene&lt;br/&gt;
code.&lt;br/&gt;
(b) Strangely, some documents had terms with zero frequency.  And when it &lt;br/&gt;
occurred,&lt;br/&gt;
the zero frequency term has several posting as (docid 0)&lt;br/&gt;
Example.. (docid freq)  &amp;#8211; MaxDoc = 7749 - NO DELETION.&lt;br/&gt;
Merging msgBody; text=it; sz=2  &amp;#8212; The field name is msgBody and term is &quot;it&quot;&lt;br/&gt;
                                    and two segments have the term.&lt;br/&gt;
(0 0)(0 0)(0 0)..........(0 0)(4 6)(5 3)(6 1)(9 1)(10 2)(12 1)......&lt;br/&gt;
...(6791 2)(6794 3)(6796 2)(6798 16)(6801 1)(6805 1)(6806 5)&lt;br/&gt;
(6808 1)(6810 1)(6815 2)(6816 3)(6817 1)(6818 1)(6821 4)(6822 1)&lt;br/&gt;
(6824 3)(6826 4)(6828 1)(6829 3)(12549 2)doc exceeds count&lt;br/&gt;
749(13570 1)doc exceeds count 7749(14896 1)doc exceeds count 7749&lt;br/&gt;
(15028 1)doc exceeds count 7749(15357 1)doc exceeds count 7749&lt;br/&gt;
(15427 1)doc exceeds count 7749(15534 1)doc exceeds count 7749&lt;br/&gt;
(15535 1)doc exceeds count 7749(15653 1)doc exceeds count 7749&lt;br/&gt;
(16530 1)doc exceeds count 7749(17108 1).......&lt;br/&gt;
(c) Also the zero frequency was not limited to the 0 document, there was&lt;br/&gt;
another instance.&lt;/p&gt;

&lt;p&gt;One work around that seemed to resolve the issue was to:&lt;br/&gt;
(a) keep the maxDoc as a member variable in SegmentMergeInfo&lt;br/&gt;
and ignore/throw exception if an inconsistent state is detected.&lt;/p&gt;

&lt;p&gt;****ADD To SegmentMerger just before &quot;docs out of order&quot; check.&lt;br/&gt;
  if (postings.freq() == 0) &lt;/p&gt;
{
            continue;
   }
&lt;p&gt;   if (doc &amp;gt;= smi.maxDoc) &lt;/p&gt;
{
      //sbLog.append(&quot;doc exceeds count \r\n &quot; + smi.maxDoc);
      continue;
   }
&lt;p&gt;****&lt;/p&gt;

&lt;p&gt;Atleast putting a check would not corrupt the segments and would&lt;br/&gt;
get us closer to the real problem as to why freq=0 and doc number exceeds&lt;br/&gt;
maxDoc. Note, the code has had the fix to the other Segment corruption issue&lt;br/&gt;
that I previously reported (namely, Using a zero length file).&lt;/p&gt;</comment>
                            <comment id="12321389" author="lucene@ziplip.com" created="Wed, 15 Jun 2005 00:46:42 +0100"  >&lt;p&gt;Created an attachment (id=15405)&lt;br/&gt;
Analysis of corrupted segments and suggestions.&lt;/p&gt;</comment>
                            <comment id="12321390" author="otis@apache.org" created="Wed, 22 Jun 2005 13:04:49 +0100"  >&lt;p&gt;Arvind, thanks for following up on this issue.  From your report I can see that&lt;br/&gt;
the index really ends up containing invalid data, but I&apos;ve never even seen this&lt;br/&gt;
happen myself.  Could you please put together sample code that gets the index in&lt;br/&gt;
this state?&lt;/p&gt;</comment>
                            <comment id="12357178" author="yseeley@gmail.com" created="Thu, 10 Nov 2005 11:59:30 +0000"  >&lt;p&gt;I&apos;ve never seen this... can anyone reproduce with Lucene 1.9?&lt;br/&gt;
CCing this to Arvind&apos;s email...&lt;/p&gt;</comment>
                            <comment id="12357180" author="otis" created="Thu, 10 Nov 2005 12:05:56 +0000"  >&lt;p&gt;2 years later, I still haven&apos;t seen this error.&lt;/p&gt;</comment>
                            <comment id="12362310" author="jcuzens" created="Tue, 10 Jan 2006 16:30:44 +0000"  >&lt;p&gt;First I think that Lucene is great and it manages to do an incredible job. However, this issue is also causing us significant problems. We originally had an AOP interceptor that would update documents in our HTTP threads however when we started to see this issue we were concerned that it was caused by multiple threads accessing the index at the same time. We put extra concurrency controls on the updates using the LuceneIndexAccess API that was posted into bugzilla by another user. This issue still remained after we added the extra concurrency control.&lt;/p&gt;

&lt;p&gt;Since then we have abandoned the AOP approach completely and moved the index rebuild to a scheduled thread which collects things that were modified from the database (using datestamp versioning) and then proceeds to update their lucene indexes. We hoped this would solve the problem because only 1 thread in 1 process would be modifying the index at any given time. Alas, we are still getting docs out of order exceptions. It is difficult for us to reproduce as it mainly happens in production and we cannot provide a testcase for it (I wish we could!). &lt;/p&gt;

&lt;p&gt;I know that this must be a difficult issue because it is difficult to reproduce. I can&apos;t think of anything strange that we are doing with the indexes (one process, one thread modifying). This probably isn&apos;t much help but I just wanted to let you know that we are also experiencing the problem. &lt;/p&gt;</comment>
                            <comment id="12362354" author="cutting" created="Wed, 11 Jan 2006 02:28:02 +0000"  >&lt;p&gt;File corruption could cause this.  Please look in your system logs to see if there are any reports of problems accessing the drive that stores this index.&lt;/p&gt;</comment>
                            <comment id="12362866" author="jcuzens" created="Tue, 17 Jan 2006 02:45:00 +0000"  >&lt;p&gt;Hi Doug,&lt;/p&gt;

&lt;p&gt;Thanks for your suggestion (and great work on Lucene!). I looked in the logs and could not find any indication of corruption. In addition we actually have the system running in a cluster where each node in the cluster has its own lucene search index. We had the issue on a different machine before we moved to the cluster and we now see it on both machines in our cluster &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;. Next time it occurs I will get the indexes and try to post them here if you think that would be helpful.&lt;/p&gt;

&lt;p&gt;Just some additional info: &lt;br/&gt;
1)  We are running SUSE 10 Linux.&lt;br/&gt;
2)  We run two quartz jobs: One job runs every 2 minutes and updates lucene with changes from the db if necessary. The other job runs at 3:30AM in the morning and does full index rebuilds. We use the LuceneIndexAccess api when working with the IndexReader/IndexWriter. Only one thread should modify the index at any time although it is possible that the full rebuild job and the update job execute concurrently they shouldn&apos;t both modify the index due to the LuceneIndexAccess API&apos;s synchronization.&lt;br/&gt;
3) Currently when doing searches we create a new IndexSearcher everytime a search is to be performed; We do not use the LuceneIndexAccess API.&lt;/p&gt;

&lt;p&gt;When I refer to the LuceneIndexAccess API I am referring to the contribution by Mark Schreiber:&lt;br/&gt;
&lt;a href=&quot;http://www.blizzy.de/lucene/lucene-indexaccess-0.1.0.zip&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.blizzy.de/lucene/lucene-indexaccess-0.1.0.zip&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Thanks for any help!&lt;/p&gt;</comment>
                            <comment id="12363156" author="rmajewski" created="Thu, 19 Jan 2006 05:57:46 +0000"  >&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I have recently experienced the same problem on relase 1.4.3. It happended in production(more than once). Filesystem index directory is used. One application is accountable for indexing, another for searching the index(it also uses IndexReader for query rewriting). Access to index directory(which is singleton) is synchronized within each application&apos;s scope. These two applications create separate directory instances and access it independently. Unfortunately,  the code doesn&apos;t lead to repeatable occurances of this exception. I assume that  these two applications having not synchronized access to the index might couse the problem, but I have received information that it had also occured with the searching application being down.&lt;/p&gt;

&lt;p&gt;Thanks in advance for any help.&lt;/p&gt;</comment>
                            <comment id="12363471" author="jcuzens" created="Sat, 21 Jan 2006 07:41:44 +0000"  >&lt;p&gt;I am posting our corrupted index (I have to do it in two  parts because it is 14.5M). I looked at it in Luke but Luke doesn&apos;t really have any tools to help really diagnose corruption issues. At this point we are considering changing our system to do builds on one machine, test them, and then have them distributed to the other machines in our cluster.&lt;/p&gt;

&lt;p&gt;If anybody could look at this it would be greatly appreciated!&lt;br/&gt;
Thanks!&lt;br/&gt;
Jarrod&lt;/p&gt;</comment>
                            <comment id="12363473" author="jcuzens" created="Sat, 21 Jan 2006 07:44:02 +0000"  >&lt;p&gt;Second part. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="12374312" author="alesj" created="Thu, 13 Apr 2006 17:13:41 +0100"  >&lt;p&gt;We used Lucene 1.4.3. for a half year now.&lt;br/&gt;
But just out of the sudden this issue appeared.&lt;/p&gt;

&lt;p&gt;We have synchronized access to index - synchronized singleton handling all write / read operations.&lt;/p&gt;

&lt;p&gt;Is there any progress on finding the cause?&lt;/p&gt;</comment>
                            <comment id="12376780" author="jlambert" created="Fri, 28 Apr 2006 02:49:21 +0100"  >&lt;p&gt;I was having this problem intermittently while indexing over multiple threads and I have found that the following steps can cause this error (with Lucene 1.3 and 1.4.x):&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Open an IndexReader (#1) over an existing index (this reader is used for searching while updating the index)&lt;/li&gt;
	&lt;li&gt;Using this reader (#1) do a search for the document(s) that you would like to update; obtain their document ID numbers&lt;/li&gt;
	&lt;li&gt;Create an IndexWriter and add several new documents to the index (for me, this writing is done in other threads) &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/star_yellow.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/li&gt;
	&lt;li&gt;Close the IndexWriter &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/star_yellow.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/li&gt;
	&lt;li&gt;Open another IndexReader (#2) over the index&lt;/li&gt;
	&lt;li&gt;Delete the previously found documents by their document ID numbers using reader #2&lt;/li&gt;
	&lt;li&gt;Close the #2 reader&lt;/li&gt;
	&lt;li&gt;Create another IndexWriter (#2) and re-add the updated documents&lt;/li&gt;
	&lt;li&gt;Close the IndexWriter #2&lt;/li&gt;
	&lt;li&gt;Close the original IndexReader (#1) and open a new reader for general searching&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;If I ensure that the steps marked with an asterisk &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/star_yellow.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; do not happen (i.e. using thread synchronization), I never get this error.  Otherwise, it will happen intermittently while closing the second IndexWriter (#2)  (sometimes I get an ArrayIndexOutOfBoundsException during the deletion).  These &apos;extra&apos; writes cause the initial &apos;segments&apos; file to be updated after which it is re-read while opening the second IndexReader (#2).&lt;/p&gt;

&lt;p&gt;This can end up deleting documents using possibly non-existent IDs, most likely causing the index corruption that this error signals.&lt;/p&gt;</comment>
                            <comment id="12458669" author="jedws" created="Fri, 15 Dec 2006 04:11:17 +0000"  >&lt;p&gt;We have seen this one as well. We don&apos;t have the same usage as above, we only ever delete documents with IndexReader.deleteDocuments(Term)&lt;/p&gt;

&lt;p&gt;We are using Lucene 1.9.1&lt;/p&gt;

&lt;p&gt;It occurs in two places, inside IndexWriter.addDocument():&lt;/p&gt;

&lt;p&gt;java.lang.IllegalStateException: docs out of order&lt;br/&gt;
	at org.apache.lucene.index.SegmentMerger.appendPostings([Lorg/apache/lucene/index/SegmentMergeInfo;I)I(Optimized Method)&lt;br/&gt;
	at org.apache.lucene.index.SegmentMerger.mergeTermInfo([Lorg/apache/lucene/index/SegmentMergeInfo;I)V(Optimized Method)&lt;br/&gt;
	at org.apache.lucene.index.SegmentMerger.mergeTermInfos()V(Optimized Method)&lt;br/&gt;
	at org.apache.lucene.index.SegmentMerger.mergeTerms()V(Optimized Method)&lt;br/&gt;
	at org.apache.lucene.index.SegmentMerger.merge()I(Optimized Method)&lt;br/&gt;
	at org.apache.lucene.index.IndexWriter.mergeSegments(II)V(IndexWriter.java:681)&lt;br/&gt;
	at org.apache.lucene.index.IndexWriter.mergeSegments(I)V(IndexWriter.java:658)&lt;br/&gt;
	at org.apache.lucene.index.IndexWriter.maybeMergeSegments()V(IndexWriter.java:646)&lt;br/&gt;
	at org.apache.lucene.index.IndexWriter.addDocument(Lorg/apache/lucene/document/Document;Lorg/apache/lucene/analysis/Analyzer;)V(IndexWriter.java:453)&lt;br/&gt;
	at org.apache.lucene.index.IndexWriter.addDocument(Lorg/apache/lucene/document/Document;)V(IndexWriter.java:436)&lt;/p&gt;

&lt;p&gt;and inside IndexWriter.close():&lt;/p&gt;

&lt;p&gt;java.lang.IllegalStateException: docs out of order&lt;br/&gt;
	at org.apache.lucene.index.SegmentMerger.appendPostings([Lorg/apache/lucene/index/SegmentMergeInfo;I)I(Optimized Method)&lt;br/&gt;
	at org.apache.lucene.index.SegmentMerger.mergeTermInfo([Lorg/apache/lucene/index/SegmentMergeInfo;I)V(Optimized Method)&lt;br/&gt;
	at org.apache.lucene.index.SegmentMerger.mergeTermInfos()V(Optimized Method)&lt;br/&gt;
	at org.apache.lucene.index.SegmentMerger.mergeTerms()V(Optimized Method)&lt;br/&gt;
	at org.apache.lucene.index.SegmentMerger.merge()I(Optimized Method)&lt;br/&gt;
	at org.apache.lucene.index.IndexWriter.mergeSegments(II)V(IndexWriter.java:681)&lt;br/&gt;
	at org.apache.lucene.index.IndexWriter.mergeSegments(I)V(IndexWriter.java:658)&lt;br/&gt;
	at org.apache.lucene.index.IndexWriter.flushRamSegments()V(IndexWriter.java:628)&lt;br/&gt;
	at org.apache.lucene.index.IndexWriter.close()V(IndexWriter.java:375)&lt;/p&gt;

&lt;p&gt;The second one exposes a problem in the close() method which is that the index write.lock is not released when exceptions are thrown in close() causing subsequent attempts to open an IndexWriter to fail.&lt;/p&gt;</comment>
                            <comment id="12459457" author="mikemccand" created="Mon, 18 Dec 2006 20:57:43 +0000"  >&lt;p&gt;I just resolved &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-603&quot; title=&quot;index optimize problem&quot; class=&quot;issue-link&quot; data-issue-key=&quot;LUCENE-603&quot;&gt;&lt;del&gt;LUCENE-603&lt;/del&gt;&lt;/a&gt; as a dup of this issue.&lt;/p&gt;

&lt;p&gt;It would be awesome if we could get a test case that shows this happening.  Enough people seem to hit it that it seems likely something is lurking out there so I&apos;d like to get it fixed!!&lt;/p&gt;</comment>
                            <comment id="12462949" author="jedws" created="Mon, 8 Jan 2007 05:42:15 +0000"  >&lt;p&gt;We have now seen this in a number of customer sites since upgrading JIRA to use Lucene 1.9.1. The JIRA report is here: &lt;a href=&quot;http://jira.atlassian.com/browse/JRA-11861&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://jira.atlassian.com/browse/JRA-11861&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We only seem to have seen it since the upgrade from 1.4.3 to 1.9.1, we hadn&apos;t seen it before then.&lt;/p&gt;

&lt;p&gt;This is now a major issue for us, it is hitting a number of our customers. I am trying to generate a repeatable test for it as a matter of urgency.&lt;/p&gt;

&lt;p&gt;As a follow-up we sometimes see the old ArrayIndexOutOfBoundsEx in BitVector.get() (BitVector.java:63)&lt;/p&gt;

&lt;p&gt;will post more if I find something worth sharing.&lt;/p&gt;</comment>
                            <comment id="12462950" author="jedws" created="Mon, 8 Jan 2007 05:44:17 +0000"  >&lt;p&gt;and we also see ArrayIndexOutOfBoundsEx in the SegmentReader.isDeleted() method:&lt;/p&gt;

&lt;p&gt;java.lang.ArrayIndexOutOfBoundsException&lt;br/&gt;
        at org.apache.lucene.index.SegmentReader.isDeleted(I)Z(Optimized Method)&lt;br/&gt;
        at org.apache.lucene.index.SegmentMerger.mergeFields()I(Optimized Method)&lt;br/&gt;
        at org.apache.lucene.index.SegmentMerger.merge()I(Optimized Method)&lt;br/&gt;
        at org.apache.lucene.index.IndexWriter.mergeSegments(II)V(IndexWriter.java:681)&lt;/p&gt;</comment>
                            <comment id="12463029" author="mikemccand" created="Mon, 8 Jan 2007 13:09:05 +0000"  >&lt;p&gt;OK: I finally found one way that this corruption can occur!  I will&lt;br/&gt;
create a unit test &amp;amp; commit a fix.&lt;/p&gt;

&lt;p&gt;If you delete by document number, and, that document number is larger&lt;br/&gt;
than maxDoc, but only by a little, then the call to&lt;br/&gt;
deletedDocs.set(num) may in fact succeed (ie, no exception), but will&lt;br/&gt;
have set bits that are &quot;out of bounds&quot; in the BitVector&apos;s bits array.&lt;/p&gt;

&lt;p&gt;This is because the bits array is an array of bytes and so you can&lt;br/&gt;
have up to 7 of these unused bits at the end.  Once this has happened,&lt;br/&gt;
any attempt to merge this segment will hit the &quot;docs out of order&quot;&lt;br/&gt;
exception because the BitVector&apos;s count() method will count these&lt;br/&gt;
&quot;illegally set&quot; bits and thus make the SegmentMerger think too many&lt;br/&gt;
docs are deleted. &lt;/p&gt;

&lt;p&gt;Unfortunately, this case only occurs if you use deleteDocument(int),&lt;br/&gt;
so I can&apos;t yet explain how this happens when using only&lt;br/&gt;
deleteDocument(Term).&lt;/p&gt;</comment>
                            <comment id="12463093" author="mikemccand" created="Mon, 8 Jan 2007 18:34:10 +0000"  >
&lt;p&gt;I&apos;ve committed a fix for this one case to the trunk.&lt;/p&gt;

&lt;p&gt;I&apos;m leaving the issue open so folks above can try the fix and confirm&lt;br/&gt;
whether or not this fixes their cases.&lt;/p&gt;

&lt;p&gt;Jed (or any other folks who have hit this above and are still&lt;br/&gt;
listening!), the fix is really trivial and would be easy to back&lt;br/&gt;
port to prior releases: just run &quot;svn diff -r 494135:494136&quot; from&lt;br/&gt;
a Lucene checkout to see them.&lt;/p&gt;

&lt;p&gt;If you are willing/able to try this in one of the environments where&lt;br/&gt;
you keep hitting this issue, that would be awesome: if this is in fact&lt;br/&gt;
your root cause, then you would see an ArrayIndexOutOfBoundsException&lt;br/&gt;
at the point that the delete of a too-large docNum occurred (rather&lt;br/&gt;
than silent corruption and the above exception much later that you now&lt;br/&gt;
see); and if it&apos;s not your root cause after testing the fix, then we&lt;br/&gt;
would know for sure to look for another cause here.&lt;/p&gt;

&lt;p&gt;Are you sure that you only ever do IndexReader.deleteDocuments(Term)&lt;br/&gt;
and not deleteDocuments(int docNum)?  I still can&apos;t explain how this&lt;br/&gt;
error could happen without using that second method.&lt;/p&gt;</comment>
                            <comment id="12463176" author="doronc" created="Tue, 9 Jan 2007 01:27:24 +0000"  >&lt;p&gt;Amazed by this long lasting bug report I was going similar routes to Mike, and I noticed 3 things - &lt;/p&gt;

&lt;p&gt;(1) the sequence of ops brought by Jason is wrong: &lt;br/&gt;
 &lt;del&gt;a&lt;/del&gt; Open an IndexReader (#1) over an existing index (this reader is used for searching while updating the index)&lt;br/&gt;
 &lt;del&gt;b&lt;/del&gt; Using this reader (#1) do a search for the document(s) that you would like to update; obtain their document ID numbers&lt;br/&gt;
 &lt;del&gt;c&lt;/del&gt; Create an IndexWriter and add several new documents to the index (for me, this writing is done in other threads) &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/star_yellow.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
 &lt;del&gt;d&lt;/del&gt; Close the IndexWriter &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/star_yellow.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
 &lt;del&gt;e&lt;/del&gt; Open another IndexReader (#2) over the index&lt;br/&gt;
 &lt;del&gt;f&lt;/del&gt; Delete the previously found documents by their document ID numbers using reader #2&lt;br/&gt;
 &lt;del&gt;g&lt;/del&gt; Close the #2 reader&lt;br/&gt;
 &lt;del&gt;h&lt;/del&gt; Create another IndexWriter (#2) and re-add the updated documents&lt;br/&gt;
 &lt;del&gt;i&lt;/del&gt; Close the IndexWriter #2&lt;br/&gt;
 &lt;del&gt;j&lt;/del&gt; Close the original IndexReader (#1) and open a new reader for general searching&lt;/p&gt;

&lt;p&gt;Problem here is that the docIDs found in (b) may be altered in step (d) and so step (f) would delete the wrong docs. In particular, it might attempt to delete ids that are out of the range. This might expose exactly the BitVector problem, and would explain the whole thing, but I too cannot see how it explains the delete-by-term case.&lt;/p&gt;

&lt;p&gt;(2) BitVectort silent ignoring of attempts to delete slightly-out-of-bound docs that fall in the higher byte - this the problem that Mike fixed. I think the fix is okay - though some applications might now get exceptions they did not get in the past - but I believe this is for their own good. &lt;br/&gt;
However when I first ran into this I didn&apos;t notice that BitVector.size() would become wrong as result of this - nice catch Mike!&lt;/p&gt;

&lt;p&gt;I think however that the test Mike added does not expose the docs out of order bug - I tried this test without the fix and it only fail on the &quot;gotException assert&quot; - if you comment this assert the test pass. &lt;/p&gt;

&lt;p&gt;The following test would expose the out-of-order bug - it would fail with out-of-order before the fix, and would succeed without it. &lt;/p&gt;

&lt;p&gt;  public void testOutOfOrder () throws IOException {&lt;br/&gt;
    String tempDir = System.getProperty(&quot;java.io.tmpdir&quot;);&lt;br/&gt;
    if (tempDir == null) &lt;/p&gt;
{
      throw new IOException(&quot;java.io.tmpdir undefined, cannot run test: &quot;+getName());
    }

&lt;p&gt;    File indexDir = new File(tempDir, &quot;lucenetestindexTemp&quot;);&lt;br/&gt;
    Directory dir = FSDirectory.getDirectory(indexDir, true);&lt;/p&gt;

&lt;p&gt;    boolean create = true;&lt;br/&gt;
    int numDocs = 0;&lt;br/&gt;
    int maxDoc = 0;&lt;br/&gt;
    while (numDocs &amp;lt; 100) {&lt;br/&gt;
      IndexWriter iw = new IndexWriter(dir,anlzr,create);&lt;br/&gt;
      create = false;&lt;br/&gt;
      iw.setUseCompoundFile(false);&lt;br/&gt;
      for (int i=0; i&amp;lt;2; i++) &lt;/p&gt;
{
        Document d = new Document();
        d.add(new Field(&quot;body&quot;,&quot;body&quot;+i,Store.NO,Index.UN_TOKENIZED));
        iw.addDocument(d);
      }
&lt;p&gt;      iw.optimize();&lt;br/&gt;
      iw.close();&lt;br/&gt;
      IndexReader ir = IndexReader.open(dir);&lt;br/&gt;
      numDocs = ir.numDocs();&lt;br/&gt;
      maxDoc = ir.maxDoc();&lt;br/&gt;
      assertEquals(numDocs,maxDoc);&lt;br/&gt;
      for (int i=7; i &amp;gt;=&lt;del&gt;1; i&lt;/del&gt;-) {&lt;br/&gt;
        try &lt;/p&gt;
{
          ir.deleteDocument(maxDoc+i);
        }
&lt;p&gt; catch (ArrayIndexOutOfBoundsException e) {  &lt;br/&gt;
        }&lt;br/&gt;
      }&lt;br/&gt;
      ir.close();&lt;br/&gt;
    }&lt;br/&gt;
  }&lt;/p&gt;

&lt;p&gt;Mike, do you agree?&lt;/p&gt;

&lt;p&gt;(3) maxDoc() computation in SegmentReader is based (on some paths) in RandomAccessFile.length(). IIRC I saw cases (in previous project) where File.length() or RAF.length() (not sure which of the two) did not always reflect real length, if the system was very busy IO wise, unless FD.sync() was called (with performance hit). &lt;/p&gt;

&lt;p&gt;This post seems relevant - RAF.length over 2GB in NFS - &lt;a href=&quot;http://forum.java.sun.com/thread.jspa?threadID=708670&amp;amp;messageID=4103657&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://forum.java.sun.com/thread.jspa?threadID=708670&amp;amp;messageID=4103657&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;Not sure if this can be the case here but at least we can discuss whether it is better to always store the length.&lt;/p&gt;

</comment>
                            <comment id="12463202" author="jedws" created="Tue, 9 Jan 2007 06:38:56 +0000"  >&lt;p&gt;Hi Michael,&lt;/p&gt;

&lt;p&gt;This is awesome, I have prepared a patched 1.9.1: &lt;a href=&quot;http://jira.atlassian.com/secure/attachment/19390/lucene-core-1.9.1-atlassian-patched-2007-01-09.jar&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://jira.atlassian.com/secure/attachment/19390/lucene-core-1.9.1-atlassian-patched-2007-01-09.jar&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Unfortunately we don&apos;t have a repeatable test for this so we will have to distribute to afflicted customers and - well, pray I guess. We have been seeing this sporadically in our main JIRA instance &lt;a href=&quot;http://jira.atlassian.com&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://jira.atlassian.com&lt;/a&gt; so we will hopefully not observe it now.&lt;/p&gt;

&lt;p&gt;We do only use the deleteDocuments(Term) method, so we are not sure whether this will truly fix our problem, but we note that that method calls deleteDocument(int) based on the TermDocs returned for the Term - and maybe they can be incorrect???&lt;/p&gt;

&lt;p&gt;Out of interest, apart from changing from 1.4.3 to 1.9.1, in the JIRA 3.7 release we changed our default merge factor to 4 from 10. We hadn&apos;t seen this problem before, and suddenly we have had a reasonable number of occurrences. &lt;/p&gt;</comment>
                            <comment id="12463203" author="jedws" created="Tue, 9 Jan 2007 06:41:40 +0000"  >&lt;p&gt;Alas, this doesn&apos;t appear to be the problem. We are still getting it, but we do at least have a little more info. We added the doc and lastDoc to the IllegalArgEx and we are getting very strange numbers:&lt;/p&gt;

&lt;p&gt;java.lang.IllegalStateException: docs out of order (-1764 &amp;lt; 0)&lt;br/&gt;
        at org.apache.lucene.index.SegmentMerger.appendPostings([Lorg/apache/lucene/index/SegmentMergeInfo;I)I(SegmentMerger.java:335)&lt;br/&gt;
        at org.apache.lucene.index.SegmentMerger.mergeTermInfo([Lorg/apache/lucene/index/SegmentMergeInfo;I)V(SegmentMerger.java:298)&lt;br/&gt;
        at org.apache.lucene.index.SegmentMerger.mergeTermInfos()V(SegmentMerger.java:272) &lt;br/&gt;
        at org.apache.lucene.index.SegmentMerger.mergeTerms()V(SegmentMerger.java:236)&lt;br/&gt;
        at org.apache.lucene.index.SegmentMerger.merge()I(SegmentMerger.java:89)&lt;br/&gt;
        at org.apache.lucene.index.IndexWriter.mergeSegments(II)V(IndexWriter.java:681)&lt;br/&gt;
        at org.apache.lucene.index.IndexWriter.mergeSegments(I)V(IndexWriter.java:658)&lt;br/&gt;
        at org.apache.lucene.index.IndexWriter.maybeMergeSegments()V(IndexWriter.java:646)&lt;br/&gt;
        at org.apache.lucene.index.IndexWriter.addDocument(Lorg/apache/lucene/document/Document;Lorg/apache/lucene/analysis/Analyzer;)V(IndexWriter.java:453) &lt;br/&gt;
        at org.apache.lucene.index.IndexWriter.addDocument(Lorg/apache/lucene/document/Document;)V(IndexWriter.java:436)&lt;/p&gt;

&lt;p&gt;where doc = -1764 and lastDoc is zero&lt;/p&gt;</comment>
                            <comment id="12463243" author="mikemccand" created="Tue, 9 Jan 2007 11:18:16 +0000"  >&lt;p&gt;Jed, thanks for testing the fix!&lt;/p&gt;

&lt;p&gt;&amp;gt; Alas, this doesn&apos;t appear to be the problem. We are still getting&lt;br/&gt;
&amp;gt; it, but we do at least have a little more info. We added the doc and&lt;br/&gt;
&amp;gt; lastDoc to the IllegalArgEx and we are getting very strange numbers:&lt;br/&gt;
&amp;gt;&lt;br/&gt;
&amp;gt; java.lang.IllegalStateException: docs out of order (-1764 &amp;lt; 0)&lt;br/&gt;
&amp;gt; ...&lt;br/&gt;
&amp;gt;&lt;br/&gt;
&amp;gt; where doc = -1764 and lastDoc is zero&lt;/p&gt;

&lt;p&gt;OK, so we&apos;ve definitely got something else at play here (bummer!). That&lt;br/&gt;
(negative doc number) is very strange.  I will keep looking a this.  I&lt;br/&gt;
will prepare a patch on 1.9.1 that adds some more instrumentation so&lt;br/&gt;
we can get more details when you hit this ...&lt;/p&gt;

&lt;p&gt;&amp;gt; We do only use the deleteDocuments(Term) method, so we are not sure&lt;br/&gt;
&amp;gt; whether this will truly fix our problem, but we note that that&lt;br/&gt;
&amp;gt; method calls deleteDocument(int) based on the TermDocs returned for&lt;br/&gt;
&amp;gt; the Term - and maybe they can be incorrect???&lt;/p&gt;

&lt;p&gt;Right, but I had thought the docNum&apos;s coming in from this path would&lt;br/&gt;
be correct.  It sounds like we have another issue at play here that&lt;br/&gt;
can somehow get even these doc numbers messed up.&lt;/p&gt;

&lt;p&gt;&amp;gt; Out of interest, apart from changing from 1.4.3 to 1.9.1, in the&lt;br/&gt;
&amp;gt; JIRA 3.7 release we changed our default merge factor to 4 from&lt;br/&gt;
&amp;gt; 10. We hadn&apos;t seen this problem before, and suddenly we have had a&lt;br/&gt;
&amp;gt; reasonable number of occurrences.&lt;/p&gt;

&lt;p&gt;Interesting.  Maybe try changing back to 4 and see if it suppresses&lt;br/&gt;
the bug?  Might give us &lt;span class=&quot;error&quot;&gt;&amp;#91;desperately needed&amp;#93;&lt;/span&gt; more data to cling to&lt;br/&gt;
here!  On the 1.4.3 -&amp;gt; 1.9.1 change, some of the cases above were even&lt;br/&gt;
pre-1.4.x (though they could have been from yet another root cause or&lt;br/&gt;
maybe filesystem) so it&apos;s hard to draw hard conclusions on this&lt;br/&gt;
front.&lt;/p&gt;
</comment>
                            <comment id="12463247" author="mikemccand" created="Tue, 9 Jan 2007 11:32:04 +0000"  >
&lt;p&gt;Doron,&lt;/p&gt;

&lt;p&gt;&amp;gt; (1) the sequence of ops brought by Jason is wrong:&lt;br/&gt;
&amp;gt;   ...&lt;br/&gt;
&amp;gt;&lt;br/&gt;
&amp;gt; Problem here is that the docIDs found in (b) may be altered in step&lt;br/&gt;
&amp;gt; (d) and so step (f) would delete the wrong docs. In particular, it&lt;br/&gt;
&amp;gt; might attempt to delete ids that are out of the range. This might&lt;br/&gt;
&amp;gt; expose exactly the BitVector problem, and would explain the whole&lt;br/&gt;
&amp;gt; thing, but I too cannot see how it explains the delete-by-term case.&lt;/p&gt;

&lt;p&gt;Right, the case I fixed only happens when the Lucene&lt;br/&gt;
deleteDocument(int docNum) is &lt;span class=&quot;error&quot;&gt;&amp;#91;slightly&amp;#93;&lt;/span&gt; mis-used.  Ie if you are&lt;br/&gt;
&quot;playing by the rules&quot; you would never have hit this bug.  And this&lt;br/&gt;
particular use case is indeed incorrect: doc numbers are only valid to&lt;br/&gt;
the one reader that you got them from.&lt;/p&gt;

&lt;p&gt;&amp;gt; I think however that the test Mike added does not expose the docs&lt;br/&gt;
&amp;gt; out of order bug - I tried this test without the fix and it only&lt;br/&gt;
&amp;gt; fail on the &quot;gotException assert&quot; - if you comment this assert the&lt;br/&gt;
&amp;gt; test pass.&lt;/p&gt;

&lt;p&gt;Huh, I see my test case (in IndexReader) indeed hitting the original&lt;br/&gt;
&quot;docs out of order&quot; exception.  If I take the current trunk and&lt;br/&gt;
comment out the (one line) bounds check in BitVector.set and run that&lt;br/&gt;
test, it hits the &quot;docs out of order&quot; exception.&lt;/p&gt;

&lt;p&gt;Are you sure you updated the change (to tighten the check to a &amp;lt;= from&lt;br/&gt;
a &amp;lt;) to index/SegmentMerger.java?  Because, I did indeed find that the&lt;br/&gt;
test failed to fail when I first wrote it (but should have).  So in&lt;br/&gt;
digging why it didn&apos;t fail as expected, I found that the check for&lt;br/&gt;
&quot;docs out of order&quot; missed the boundary case of the same doc number&lt;br/&gt;
twice in a row.  Once I fixed that, the test failed as expected.&lt;/p&gt;

&lt;p&gt;&amp;gt; (3) maxDoc() computation in SegmentReader is based (on some paths)&lt;br/&gt;
&amp;gt; in RandomAccessFile.length(). IIRC I saw cases (in previous project)&lt;br/&gt;
&amp;gt; where File.length() or RAF.length() (not sure which of the two) did&lt;br/&gt;
&amp;gt; not always reflect real length, if the system was very busy IO wise,&lt;br/&gt;
&amp;gt; unless FD.sync() was called (with performance hit).&lt;/p&gt;

&lt;p&gt;Yes I saw this too.  From the follow-on discussion it sounds like we&lt;br/&gt;
haven&apos;t found a specific known JVM bug here.  Still, it does make me&lt;br/&gt;
nervous that we rely on file length to derive maxDoc.&lt;/p&gt;

&lt;p&gt;In general I think we should rely on as little as possible from the&lt;br/&gt;
file system (there are so many cross platform issues/differences) and&lt;br/&gt;
instead explicitly store things like maxDoc into the index.  I will&lt;br/&gt;
open a separate Jira issue to track this.  Also I will record this&lt;br/&gt;
path in the instrumentation patch for 1.9.1 just to see if we are&lt;br/&gt;
actually hitting something here (I think unlikely but possible).&lt;/p&gt;</comment>
                            <comment id="12463249" author="mikemccand" created="Tue, 9 Jan 2007 11:40:58 +0000"  >&lt;p&gt;OK, I created &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-767&quot; title=&quot;maxDoc should be explicitly stored in the index, not derived from file length&quot; class=&quot;issue-link&quot; data-issue-key=&quot;LUCENE-767&quot;&gt;&lt;del&gt;LUCENE-767&lt;/del&gt;&lt;/a&gt; for the &quot;maxDoc should be explicitly stored in the index&quot; issue.&lt;/p&gt;</comment>
                            <comment id="12463294" author="mikemccand" created="Tue, 9 Jan 2007 14:50:21 +0000"  >
&lt;p&gt;Jed, one question: when you tested the fix, you fully rebuilt your&lt;br/&gt;
index from scratch, right?  Just want to verify that.  You have to&lt;br/&gt;
re-index because once the index is corrupted it will eventually hit&lt;br/&gt;
the &quot;docs out of order&quot; exception even if you fix the original cause.&lt;/p&gt;

&lt;p&gt;OK I&apos;ve prepared a patch off 1.9.1 (just attached it).  The patch&lt;br/&gt;
passes all unit tests on 1.9.1.&lt;/p&gt;

&lt;p&gt;It has the changes I committed to the trunk yesterday, plus&lt;br/&gt;
instrumentation (messages printed to a PrintStream) to catch places&lt;br/&gt;
where doc numbers are not correct.&lt;/p&gt;

&lt;p&gt;All messages I added print to a newly added infoStream static member&lt;br/&gt;
of SegmentMerger.  You can do SegmentMerger.setInfoStream(...) to&lt;br/&gt;
change it (it defaults to System.err).&lt;/p&gt;

&lt;p&gt;Jed if you could get the error to re-occur with this patch and then&lt;br/&gt;
post the resulting messages, that would be great.  Hopefully it gives&lt;br/&gt;
us enough information to find the source here or at least to have&lt;br/&gt;
another iteration with yet more instrumentation.  Thanks!&lt;/p&gt;
</comment>
                            <comment id="12463440" author="jedws" created="Wed, 10 Jan 2007 00:57:48 +0000"  >&lt;p&gt;Hi Michael,&lt;/p&gt;

&lt;p&gt;Thanks for the patch, applied and recreated. Attached is the log.&lt;/p&gt;

&lt;p&gt;To be explicit, we are recreating the index via the IndexWriter ctor with the create flag set and then completely rebuilding the index. We are not completely deleting the entire directory. There ARE old index files (_&lt;b&gt;.cfs &amp;amp; _&lt;/b&gt;.del) in the directory with updated timestamps that are months old. If I completely recreate the directory the problem does go away. This is a fairly trivial &quot;fix&quot;, but we are still investigating as we want to know if this is indeed the problem, how we have come to make it prevalent, and what the root cause is.&lt;/p&gt;

&lt;p&gt;Thanks for all the help everyone.&lt;/p&gt;</comment>
                            <comment id="12463470" author="jedws" created="Wed, 10 Jan 2007 06:07:58 +0000"  >&lt;p&gt;BTW. We have looked at all the open files referenced by the VM when the indexing errors occur, and there does not seem to be any reference to the old index segment files, so I am not sure how those files are influencing this problem.&lt;/p&gt;</comment>
                            <comment id="12463483" author="doronc" created="Wed, 10 Jan 2007 07:36:28 +0000"  >&lt;p&gt;Jed, is it possible that when re-creating the index, while IndexWriter is constructed with create=true, FSDirectory is opened with create=false?&lt;br/&gt;
I suspect so, because otherwise, old  .del files would have been deleted. &lt;br/&gt;
If indeed so, newly created segments, which have same names as segments in previous (bad) runs, when opened, would read the (bad) old .del file. &lt;br/&gt;
This would possibly expose the bug fixed by Michael. &lt;br/&gt;
I may be over speculating here, but if this is the case, it can also explain why changing the merge factor from 4 to 10 exposed the problem. &lt;/p&gt;

&lt;p&gt;In fact, let me speculate even further - if indeed when creating the index from scratch, the FSDirectory is (mistakenly) opened with create=false, as long as you always repeated the same sequencing of adding and deleting docs, you were likely to almost not suffer from this mistake, because segments created with same names as (old) .del files simply see docs as deleted before the docs are actually deleted by the program. The search behaves wrongly, not finding these docs before they are actually deleted, but no exception is thrown when adding docs. However once the merge factor was changed from 4 to 10, the matching between old .del files and new segments (with same names) was broken, and the out-of-order exception appeared. &lt;/p&gt;

&lt;p&gt;...and if this is not the case, we would need to look for something else...&lt;/p&gt;</comment>
                            <comment id="12463524" author="mikemccand" created="Wed, 10 Jan 2007 10:48:05 +0000"  >&lt;p&gt;OK from that indexing-failure.log (thanks Jed!) I can see that indeed&lt;br/&gt;
there are segments whose maxDoc() is much smaller than&lt;br/&gt;
deleteDocs.count().  This then leads to negative doc numbers on&lt;br/&gt;
merging these segments.&lt;/p&gt;

&lt;p&gt;Jed when you say &quot;there are old files (_&lt;b&gt;.cfs &amp;amp; _&lt;/b&gt;.del) in this&lt;br/&gt;
directory with updated timestamps that are months old&quot; what do you&lt;br/&gt;
mean by &quot;with updated timestamps&quot;?  Which timestamp is months old and&lt;br/&gt;
which one is updated?&lt;/p&gt;

&lt;p&gt;OK, assuming Jed you are indeed sending &quot;create=false&quot; when creating&lt;br/&gt;
the Directory and then passing that directory to IndexWriter with&lt;br/&gt;
create=true, I think we now have this case fully explained (thanks&lt;br/&gt;
Doron): your old _*.del files are being incorrectly opened &amp;amp; re-used&lt;br/&gt;
by Lucene, when they should not be.&lt;/p&gt;

&lt;p&gt;Lucene (all released versions but not the trunk version, see below)&lt;br/&gt;
does a simple fileExists(&quot;_XXX.del&quot;) call to determine if a segment&lt;br/&gt;
XXX has deletes.&lt;/p&gt;

&lt;p&gt;But when that _XXX.del is a leftover from a previous index, it very&lt;br/&gt;
likely doesn&apos;t &quot;match&quot; the newly created _XXX segment.  (Especially if&lt;br/&gt;
merge factor has changed but also if order of operations has changed,&lt;br/&gt;
which I would expect in this use case).&lt;/p&gt;

&lt;p&gt;If that file exists, Lucene assumes it&apos;s for this segment and so opens&lt;br/&gt;
it and uses it.  If it happens that this _XXX.del file has more&lt;br/&gt;
documents in it than the newly created _XXX.cfs segment, then negative&lt;br/&gt;
doc numbers will result (and then later cause the &quot;docs out of order&quot;&lt;br/&gt;
exception).  If it happens that the _XXX.del file has fewer documents&lt;br/&gt;
than the newly created _XXX.cfs segment then you&apos;ll hit&lt;br/&gt;
ArrayIndexOutOfBounds exceptions in calls to isDeleted(...).  If they&lt;br/&gt;
are exactly equal then you&apos;d randomly see some of your docs got&lt;br/&gt;
deleted.&lt;/p&gt;

&lt;p&gt;Note that the trunk version of Lucene has already fixed this bug (as&lt;br/&gt;
part of lockless commits):&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Whether a segment has deletions or not is now explictly stored in&lt;br/&gt;
    the segments file rather than relying on a &quot;fileExists(...)&quot; call.&lt;br/&gt;
    So, if an old _XXX.del existed in the filesystem, the newly&lt;br/&gt;
    created _XXX segment would not open it.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Furthermore, the trunk version of Lucene uses a new&lt;br/&gt;
    IndexFileDelter class to remove any unreferenced index files.&lt;br/&gt;
    This means it would have removed these old _&lt;b&gt;.cfs and _&lt;/b&gt;.del files&lt;br/&gt;
    even in the case where a directory was created with &quot;create=false&quot;&lt;br/&gt;
    and the IndexWriter was created with &quot;create=true&quot;.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;To summarize:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;There was one case where if you gave slightly illegal doc numbers&lt;br/&gt;
    (within 7 of the actual maxDoc) Lucene may silently accept the&lt;br/&gt;
    call but would corrupt your index only to be seen later as an&lt;br/&gt;
    &quot;docs out of order&quot; IllegalStateException when the segment is&lt;br/&gt;
    merged.  This was just a missing boundary case check.  This case&lt;br/&gt;
    is now fixed in the trunk (you get an&lt;br/&gt;
    ArrayIndexOutOfBoundsException if doc number is too large).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;There is also another case, that only happens if you have old&lt;br/&gt;
    _*.del files leftover from a previous index while re-creating a&lt;br/&gt;
    new index.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    The workaround is simple here: always open the Directory with&lt;br/&gt;
    create=true (or, remove the directory contents yourself before&lt;br/&gt;
    hand).  (IndexWriter does this if you give it a String or File&lt;br/&gt;
    with create=true).&lt;/p&gt;

&lt;p&gt;    This is really a bug in Lucene, but given that it&apos;s already fixed&lt;br/&gt;
    in the trunk, and the workaround is simple, I&apos;m inclined to not&lt;br/&gt;
    fix it in prior releases and instead publicize the issue (I will&lt;br/&gt;
    do so on java-user).&lt;/p&gt;

&lt;p&gt;    But, I will commit two additional IllegalStateException checks to&lt;br/&gt;
    the trunk when a segment is first initialized: 1) check that the&lt;br/&gt;
    two different sources of &quot;maxDoc&quot; (fieldsReader.size() and&lt;br/&gt;
    si.docCount) are the same, and 2) check that the number of pending&lt;br/&gt;
    deletions does not exceed maxDoc().  When an index has&lt;br/&gt;
    inconsistency I think the earlier it&apos;s detected the better.&lt;/p&gt;</comment>
                            <comment id="12463674" author="mikemccand" created="Wed, 10 Jan 2007 19:18:57 +0000"  >&lt;p&gt;&amp;gt; BTW. We have looked at all the open files referenced by the VM when&lt;br/&gt;
&amp;gt; the indexing errors occur, and there does not seem to be any reference&lt;br/&gt;
&amp;gt; to the old index segment files, so I am not sure how those files are&lt;br/&gt;
&amp;gt; influencing this problem.&lt;/p&gt;

&lt;p&gt;Jed just to answer this question: the _XXX.del files are opened very&lt;br/&gt;
briefly because the contents of this file are loaded / cached in&lt;br/&gt;
memory, and the the file handle is closed.  I don&apos;t think the _XXX.cfs&lt;br/&gt;
files are affecting this issue (are not opened).&lt;/p&gt;</comment>
                            <comment id="12463781" author="jedws" created="Thu, 11 Jan 2007 01:18:32 +0000"  >&lt;p&gt;Michael, Doron, you guys are legends!&lt;/p&gt;

&lt;p&gt;Indeed the problem is using only the IndexWriter with create true to recreate the directory. Creating a new Directory with create true does fix the problem. The javadoc for this constructor is fairly explicit that it should recreate the index for you (no caveat), so I would consider that a bug, but - given that head fixes it - not one that requires any action.&lt;/p&gt;

&lt;p&gt;Thanks guys for the prompt attention, excellent and thorough analysis.&lt;/p&gt;</comment>
                            <comment id="12463872" author="mikemccand" created="Thu, 11 Jan 2007 11:46:20 +0000"  >&lt;p&gt;Phew!  I&apos;m glad we finally got to the bottom of this one.&lt;/p&gt;

&lt;p&gt;Thank you for your persistent and fast testing iterations, Jed; this&lt;br/&gt;
issue has been open for far too long!&lt;/p&gt;

&lt;p&gt;I will send a summary email to java-user and resolve this issue,&lt;br/&gt;
finally.&lt;/p&gt;</comment>
                            <comment id="12463875" author="mikemccand" created="Thu, 11 Jan 2007 12:09:01 +0000"  >
&lt;p&gt;Actually, this reminds me that, as of lockless commits, there is one&lt;br/&gt;
important tradeoff on which &quot;create=true&quot; to use (the case on windows&lt;br/&gt;
where you want to re-create the index but readers are currently using&lt;br/&gt;
it).  I will call out this difference in the javadocs.&lt;/p&gt;

&lt;p&gt;Although, why do we even have a &quot;create&quot; parameter in the directory?&lt;br/&gt;
I think it&apos;s confusing (and dangerous, pre-trunk, due to this issue)&lt;br/&gt;
to have two ways of doing the same thing?&lt;/p&gt;

&lt;p&gt;Logically, I don&apos;t think a Directory should take the responsibility of&lt;br/&gt;
deleting old files (including old lock files).  It should be a clean&lt;br/&gt;
interface for doing so, but I think the IndexWriter alone should be&lt;br/&gt;
the class that deletes files from the directory.&lt;/p&gt;

&lt;p&gt;With lockless commits this has become an important difference, ie, the&lt;br/&gt;
new IndexFileDeleter class (used by IndexWriter) handles re-trying&lt;br/&gt;
files that are in-use (on Windows) whereas FSDirectory will throw an&lt;br/&gt;
exception if create=true and there are index files are in use.&lt;/p&gt;

&lt;p&gt;I think we should deprecate the &quot;create&quot; argument to&lt;br/&gt;
FSDirectory.getDirectory&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/star_yellow.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; and leave only the create argument in&lt;br/&gt;
IndexWriter&apos;s constructors.  Am I missing something?  Is there are a&lt;br/&gt;
reason not to do this?&lt;/p&gt;</comment>
                            <comment id="12463877" author="mikemccand" created="Thu, 11 Jan 2007 12:14:18 +0000"  >&lt;p&gt;Resolving this now, finally (I&apos;ll move the two &quot;create&quot; arguments&lt;br/&gt;
discussion to a separate issue if we decide to go forward with that):&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Fixed (in the trunk) to catch boundary cases of incorrect docNum&apos;s&lt;br/&gt;
    to deleteDocuments.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Fixed (in the trunk) to do earlier &quot;IllegalState&quot; checks to catch&lt;br/&gt;
    index corruption sooner.  Also fixed the existing IllegalState&lt;br/&gt;
    check to catch missing boundary cases.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;The re-using of old _XXX.del files is already fixed with lockless&lt;br/&gt;
    commits (in trunk).  This remains open for past releases, but the&lt;br/&gt;
    workaround is simple and I&apos;ve now publicized this on java-user.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12476252" author="mikemccand" created="Tue, 27 Feb 2007 18:10:32 +0000"  >&lt;p&gt;Closing all issues that were resolved for 2.1.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12312257" name="ASF.LICENSE.NOT.GRANTED--bug23650.txt" size="2143" author="lucene@ziplip.com" created="Wed, 15 Jun 2005 00:46:42 +0100"/>
                            <attachment id="12348553" name="LUCENE-140-2007-01-09-instrumentation.patch" size="6143" author="mikemccand" created="Tue, 9 Jan 2007 14:49:26 +0000"/>
                            <attachment id="12322202" name="corrupted.part1.rar" size="9437184" author="jcuzens" created="Sat, 21 Jan 2006 07:41:44 +0000"/>
                            <attachment id="12322203" name="corrupted.part2.rar" size="3509898" author="jcuzens" created="Sat, 21 Jan 2006 07:44:02 +0000"/>
                            <attachment id="12348606" name="indexing-failure.log" size="2878708" author="jedws" created="Wed, 10 Jan 2007 00:50:50 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>5.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_10010" key="com.atlassian.jira.plugin.system.customfieldtypes:importid">
                        <customfieldname>Bugzilla Id</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>23650</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Tue, 25 Nov 2003 19:35:32 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>13610</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hxyzjj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>27909</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                </customfields>
    </item>
</channel>
</rss>