<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun May 17 04:04:04 UTC 2015

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/LUCENE-1224/LUCENE-1224.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[LUCENE-1224] NGramTokenFilter creates bad TokenStream</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-1224</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                    <description>&lt;p&gt;With current trunk NGramTokenFilter(min=2,max=4) , I index &quot;abcdef&quot; string into an index, but I can&apos;t query it with &quot;abc&quot;. If I query with &quot;ab&quot;, I can get a hit result.&lt;/p&gt;

&lt;p&gt;The reason is that the NGramTokenFilter generates badly ordered TokenStream. Query is based on the Token order in the TokenStream, that how stemming or phrase should be anlayzed is based on the order (Token.positionIncrement).&lt;/p&gt;

&lt;p&gt;With current filter, query string &quot;abc&quot; is tokenized to : ab bc abc &lt;br/&gt;
meaning &quot;query a string that has ab bc abc in this order&quot;.&lt;br/&gt;
Expected filter will generate : ab abc(positionIncrement=0) bc&lt;br/&gt;
meaning &quot;query a string that has (ab|abc) bc in this order&quot;&lt;/p&gt;

&lt;p&gt;I&apos;d like to submit a patch for this issue. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</description>
                <environment></environment>
        <key id="12390825">LUCENE-1224</key>
            <summary>NGramTokenFilter creates bad TokenStream</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="kawai">Hiroaki Kawai</reporter>
                        <labels>
                    </labels>
                <created>Wed, 12 Mar 2008 09:30:16 +0000</created>
                <updated>Fri, 10 May 2013 11:34:38 +0100</updated>
                            <resolved>Fri, 26 Apr 2013 23:20:07 +0100</resolved>
                                                    <fixVersion>4.3</fixVersion>
                                    <component>modules/analysis</component>
                        <due></due>
                            <votes>2</votes>
                                    <watches>6</watches>
                                                                <comments>
                            <comment id="12578239" author="kawai" created="Thu, 13 Mar 2008 10:54:51 +0000"  >&lt;p&gt;Modified to set a right start/end offset value in Token properties.&lt;/p&gt;</comment>
                            <comment id="12579069" author="gsingers" created="Sat, 15 Mar 2008 18:08:17 +0000"  >&lt;p&gt;Please add unit tests to the patch demonstrating the issue.&lt;/p&gt;</comment>
                            <comment id="12579196" author="kawai" created="Sun, 16 Mar 2008 14:46:30 +0000"  >&lt;p&gt;Patch updated with unit test.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1225&quot; title=&quot;NGramTokenizer creates bad TokenStream&quot; class=&quot;issue-link&quot; data-issue-key=&quot;LUCENE-1225&quot;&gt;&lt;del&gt;LUCENE-1225&lt;/del&gt;&lt;/a&gt; is easier to understand this problem. This patch also includes token filter issues that is more complicated.&lt;/p&gt;</comment>
                            <comment id="12596729" author="gsingers" created="Wed, 14 May 2008 12:24:56 +0100"  >&lt;p&gt;Hi Hiroaki,&lt;/p&gt;

&lt;p&gt;I have been reviewing the tests for this and have a couple of comments.  First, I don&apos;t see why you need to bring indexing into the equation.  Second, the changes to testNGrams still don&apos;t test the issue, namely they don&apos;t examine that the output ngrams are actually in the correct position.  I think you deduce this later in testIndexAndQuery, but it is never explicitly stated.  I&apos;d drop testIndexAndQuery and just fix testNGrams such that it checks the positions appropriately.  &lt;/p&gt;

&lt;p&gt;On a more philosophical level, it is a bit curious to me that if we have the strings &quot;abcde fghi&quot; that we are fine with &quot;b&quot; being at position 1, and not at position 0, but &quot;ab&quot; needs to be at position 0.  I wonder if there is any thoughts on what the relative positions of ngrams should be.  Should they all occur at the same position?  It seems to me, that it doesn&apos;t make sense that the &quot;f&quot; ngrams don&apos;t start until some position other than 1.  This would currently prevent doing phrase queries such as &quot;ab fg&quot; with no slop.&lt;/p&gt;

&lt;p&gt;I&apos;m assuming this applies to &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1225&quot; title=&quot;NGramTokenizer creates bad TokenStream&quot; class=&quot;issue-link&quot; data-issue-key=&quot;LUCENE-1225&quot;&gt;&lt;del&gt;LUCENE-1225&lt;/del&gt;&lt;/a&gt; as well.&lt;/p&gt;

&lt;p&gt;I will link 1225 to this issue, and you can attach a single patch.&lt;/p&gt;</comment>
                            <comment id="12597069" author="kawai" created="Thu, 15 May 2008 10:29:29 +0100"  >&lt;p&gt;Q: Why it is necessary to index&lt;br/&gt;
A: Because it was necessary to show how the query is performed. &lt;br/&gt;
That is the point I wanted to address. &lt;/p&gt;

&lt;p&gt;Q: testNGrams don&apos;t test the issue&lt;br/&gt;
A: Exactlly. it don&apos;t test the issue.&lt;br/&gt;
I modified the test because it failed with my patch, that&lt;br/&gt;
Token.toString() prints additional incrementPosition information.&lt;br/&gt;
I read the existing test program, and found that current test&lt;br/&gt;
program depends on Token.toString() method.&lt;br/&gt;
I thought we&apos;d better test it without Token.toString().&lt;br/&gt;
Current test program tests that the Token have NO positionIncrement.&lt;/p&gt;

&lt;p&gt;testIndexAndQuery is the very test that address the issue.&lt;br/&gt;
Please don&apos;t drop it. Think the case, we want to search a word &lt;br/&gt;
that contain &quot;abcd&quot; with 2-gram index.&lt;br/&gt;
The test does searching &quot;abcd&quot; with 2,3-gram.&lt;/p&gt;

&lt;p&gt;We have the 2gram of abcde; &apos;ab&apos;, &apos;bc&apos;, &apos;cd&apos;, &apos;de&apos;.&lt;br/&gt;
Reffering the current lucene implementation, the position gap &lt;br/&gt;
of &apos;ab&apos; and &apos;bc&apos; must be 1.&lt;/p&gt;</comment>
                            <comment id="12597079" author="gsingers" created="Thu, 15 May 2008 11:25:55 +0100"  >&lt;p&gt;OK, let me change the comment.  You can test this problem without indexing and querying.  All of the information is available on the token.  I would suggest you revert the test to it&apos;s original and then modify testNGrams()  by adding asserts that check that the positionIncrement value is set properly.   By going the indexing/querying route, you are not only testing the token filters, but pretty much all of Lucene and are thus subject to any problems there.  In other words, it ain&apos;t a unit test.  If you set the posiitionIncrement properly and test for it, it will work in Lucene for the queries, etc.  If it doesn&apos;t, we have much bigger problems than ngrams.  That being said, if you want to fix testNgrams, and leave the query case in, that is fine by me.&lt;/p&gt;
</comment>
                            <comment id="12597080" author="gsingers" created="Thu, 15 May 2008 11:27:25 +0100"  >&lt;p&gt;FWIW, I also think we should address the more philosophical question of what the intermediate positions should be of the tokens.  The more I think about it, the more I think all &quot;grams&quot; of a given word should be at the same position, but I would like to hear from others on this before deciding.&lt;/p&gt;</comment>
                            <comment id="12597094" author="kawai" created="Thu, 15 May 2008 12:36:58 +0100"  >&lt;p&gt;Umm..., if you don&apos;t like indexing and querying in the unit test, where should I place the join test that use NGramTokenizer? It might be nice if we could place that join test in a proper place.&lt;/p&gt;

&lt;p&gt;I placed the testIndexAndQuery in the code because the other code like KeywordAnalyzer (in the core) test code has index&amp;amp;query test code in its unit tests.&lt;/p&gt;

&lt;p&gt;I&apos;m fine with separating the codes into different files.&lt;/p&gt;</comment>
                            <comment id="12597098" author="kawai" created="Thu, 15 May 2008 12:45:14 +0100"  >&lt;p&gt;In my understanding,&lt;br/&gt;
----------&lt;br/&gt;
The sequence we have &quot;This is an example&quot;&lt;/p&gt;

&lt;p&gt;If we want to tokenize with white space tokenizer, the tokens are&lt;br/&gt;
&quot;This&quot;, &quot;is&quot;, &quot;an&quot;, &quot;example&quot;&lt;br/&gt;
positions are 0,1,2,3&lt;/p&gt;

&lt;p&gt;If we want to tokenize with 2-gram, the tokens are&lt;br/&gt;
&quot;Th&quot; &quot;hi&quot; &quot;is&quot; &quot;s &quot; &quot; i&quot; &quot;is&quot; &quot;s &quot; &quot; a&quot; &quot;an&quot; &quot;n &quot; &quot; e&quot; &quot;ex&quot; &quot;xa&quot; &quot;am&quot; &quot;mp&quot; &quot;pl&quot; &quot;le&quot;&lt;br/&gt;
positions are 0,1,2,3,4,...&lt;/p&gt;</comment>
                            <comment id="12597107" author="gsingers" created="Thu, 15 May 2008 13:03:20 +0100"  >&lt;blockquote&gt;&lt;p&gt;Umm..., if you don&apos;t like indexing and querying in the unit test, where should I place the join test that use NGramTokenizer? It might be nice if we could place that join test in a proper place.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;My point is, I don&apos;t think the test needs to do any indexing/querying at all to satisfy the change.  It adds absolutely nothing to the test and only complicates the matter.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I placed the testIndexAndQuery in the code because the other code like KeywordAnalyzer (in the core) test code has index&amp;amp;query test code in its unit tests.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Just because another does it doesn&apos;t make it right.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If we want to tokenize with white space tokenizer, the tokens are&lt;br/&gt;
&quot;This&quot;, &quot;is&quot;, &quot;an&quot;, &quot;example&quot;&lt;br/&gt;
positions are 0,1,2,3&lt;/p&gt;

&lt;p&gt;If we want to tokenize with 2-gram, the tokens are&lt;br/&gt;
&quot;Th&quot; &quot;hi&quot; &quot;is&quot; &quot;s &quot; &quot; i&quot; &quot;is&quot; &quot;s &quot; &quot; a&quot; &quot;an&quot; &quot;n &quot; &quot; e&quot; &quot;ex&quot; &quot;xa&quot; &quot;am&quot; &quot;mp&quot; &quot;pl&quot; &quot;le&quot;&lt;br/&gt;
positions are 0,1,2,3,4,...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, I understand how it currently works.  My question is more along the lines of is this the right way of doing it?  I don&apos;t know that it is, but it is a bigger question than you and me.  I mean, if we are willing to accept that this issue is a bug, then it presents plenty of other problems in terms of position related queries.  For example, I think it makes sense to search for &quot;th ex&quot; as a phrase query, but that is not possible do to the positions (at least not w/o a lot of slop)&lt;/p&gt;

</comment>
                            <comment id="12597112" author="dmsmith" created="Thu, 15 May 2008 13:32:58 +0100"  >&lt;p&gt;My take as a user:&lt;/p&gt;

&lt;p&gt;Maybe, I don&apos;t understand the application of an n-gram filter, but my expectation is that words from the input that are indexed are positioned. Isn&apos;t that required to be able to do &quot;near&quot; searches?&lt;/p&gt;

&lt;p&gt;It would not matter to me if the n-grams have sub-positions to distinguish them (e.g. 1.a, 1.b, 1.c, 1,d, 2.a, ... for the example above. note: not implying any representation in this notation)&lt;/p&gt;</comment>
                            <comment id="12597156" author="kawai" created="Thu, 15 May 2008 16:15:30 +0100"  >&lt;p&gt;About test code: I&apos;m not going to say that &quot;I&apos;m right&quot;. I just wanted to address the issue and share what we should solve. If you don&apos;t like the code, please just tell me how I should do (the better way). I initially put the code there because I thought it was reasonable and proper, but I&apos;m fine with changing it.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;For example, I think it makes sense to search for &quot;th ex&quot; as a phrase query&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;For example, I think it makes sense to search for &quot;example&quot; as a phrase query instead.&lt;/p&gt;

&lt;p&gt;I want to address that NGramTokenizer is very useful for non-white-space-separated languages, for example Japanese. In that case, we won&apos;t search &quot;th ex&quot;, because it assumes sentences are separated by whte space. I want to search by a fragment of a text sequence.&lt;/p&gt;

&lt;p&gt;I agree that this might be a big problem. IMHO, the issues comes from concept mismatch of TokenFilter and TermPosition. The discussion should moved to mailing-list?&lt;/p&gt;</comment>
                            <comment id="12597161" author="gsingers" created="Thu, 15 May 2008 16:32:56 +0100"  >



&lt;p&gt;I think the right way is simply to change the existing test to check  &lt;br/&gt;
that the term positions are correct per the changes.  Right now, it  &lt;br/&gt;
doesn&apos;t check the position increment and it should.  This can be done  &lt;br/&gt;
by looking at the positionIncrement on the Token that is produced by  &lt;br/&gt;
the TokenStream and doesn&apos;t require indexing. &lt;/p&gt;
</comment>
                            <comment id="12597174" author="otis" created="Thu, 15 May 2008 17:05:08 +0100"  >&lt;p&gt;Hiroaki:&lt;br/&gt;
I agree with Grant about unit tests.  I looked at the unit tests and thought the same thing as Grant - why is Hiroaki adding indexing/searching into the mix?  Your change is about modifying the positions of n-grams, and you don&apos;t need to index or search for that.  The test will be a lot simpler if you just test for positions, like Grant suggested.&lt;/p&gt;

&lt;p&gt;Also, once you change the unit test this way, it will be a lot easier to play with positions and figure out what the &quot;right&quot; way to handle positions is.&lt;/p&gt;

&lt;p&gt;Finally, it might turn out that people have different needs or different expectations for n-gram positions.  Thus, when making changes, perhaps you can think of a mechanism that allows the caller to instruct the n-gram tokenizer which token positioning approach to take (e.g. the &quot;incremental&quot; one, or the one based on the position of the originating token, or...)&lt;/p&gt;</comment>
                            <comment id="12597489" author="kawai" created="Fri, 16 May 2008 15:32:04 +0100"  >&lt;p&gt;After all, where should I place the testIndexAndQuery? Does anybody have a suggestion?&lt;/p&gt;</comment>
                            <comment id="12638423" author="tfeak" created="Fri, 10 Oct 2008 00:04:15 +0100"  >&lt;p&gt;This bug caused me &lt;b&gt;major&lt;/b&gt; headaches trying to figure out why substring matching with an NGramTokenFilter wasn&apos;t working for anything other then when setting min and max to the same values. &lt;/p&gt;

&lt;p&gt;The patch seems to fix the issue when applied locally, however it also has a bug in it. It will stop parsing a token stream if a token comes through that is less then the minGramSize, even if there are tokens yet in the stream that are greater then minGramSize.&lt;/p&gt;</comment>
                            <comment id="12741628" author="markrmiller@gmail.com" created="Tue, 11 Aug 2009 00:56:14 +0100"  >&lt;p&gt;Sounds like this should really be addressed ... along with &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1225&quot; title=&quot;NGramTokenizer creates bad TokenStream&quot; class=&quot;issue-link&quot; data-issue-key=&quot;LUCENE-1225&quot;&gt;&lt;del&gt;LUCENE-1225&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="12834471" author="rcmuir" created="Tue, 16 Feb 2010 21:11:42 +0000"  >&lt;p&gt;I too think its really important we fix this. I have to agree with Hiroaki&apos;s analysis of the situation, and the problems can be seen by looking at the code in both the filter/tokenizers and the tests themselves.&lt;/p&gt;

&lt;p&gt;Currently the tokenizers are limited to 1024 characters (&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1227&quot; title=&quot;NGramTokenizer to handle more than 1024 chars&quot; class=&quot;issue-link&quot; data-issue-key=&quot;LUCENE-1227&quot;&gt;&lt;del&gt;LUCENE-1227&lt;/del&gt;&lt;/a&gt;), this is very related to this issue.&lt;br/&gt;
Look at the test for 1,3 ngrams of &quot;abcde&quot;:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void testNgrams() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; Exception {
        NGramTokenizer tokenizer = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; NGramTokenizer(input, 1, 3);
        assertTokenStreamContents(tokenizer,
          &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;[]{&lt;span class=&quot;code-quote&quot;&gt;&quot;a&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;b&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;c&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;d&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;e&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;ab&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;bc&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;cd&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;de&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;abc&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;bcd&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;cde&quot;&lt;/span&gt;}, 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;in my opinion the output should instead be: a, ab, ...&lt;br/&gt;
Otherwise the tokenizer will either always be limited to 1024 chars or must read the entire document into RAM.&lt;br/&gt;
This same problem exists for the EdgeNgram variants.&lt;/p&gt;

&lt;p&gt;I agree with Grant&apos;s comment about the philosophical discussion about positions of the tokens, perhaps we need an option for this (where they are all posInc=1, or the posInc=0 is generated based on whitespace). I guess I think we could accomodate both needs by having tokenizer/filter variants too, but I&apos;m not sure.&lt;/p&gt;

&lt;p&gt;The general problem i have with trying to determine a fix is that it will break backwards compatibility, and I also know that EdgeNGram is being used for some purposes such as &quot;auto-suggest&quot;. So I don&apos;t really have any idea beyond making new filters/tokenizers, as I think there is another use case where the old behavior fits?&lt;/p&gt;</comment>
                            <comment id="13643307" author="jpountz" created="Fri, 26 Apr 2013 23:20:07 +0100"  >&lt;p&gt;All n-grams now have the same position and offsets as the original token (&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-4955&quot; title=&quot;NGramTokenFilter increments positions for each gram&quot; class=&quot;issue-link&quot; data-issue-key=&quot;LUCENE-4955&quot;&gt;&lt;del&gt;LUCENE-4955&lt;/del&gt;&lt;/a&gt;).&lt;/p&gt;</comment>
                            <comment id="13654280" author="thetaphi" created="Fri, 10 May 2013 11:34:38 +0100"  >&lt;p&gt;Closed after release.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12390830">LUCENE-1225</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12377997" name="LUCENE-1224.patch" size="9483" author="kawai" created="Sun, 16 Mar 2008 14:46:30 +0000"/>
                            <attachment id="12377766" name="NGramTokenFilter.patch" size="1370" author="kawai" created="Thu, 13 Mar 2008 10:54:51 +0000"/>
                            <attachment id="12377679" name="NGramTokenFilter.patch" size="1250" author="kawai" created="Wed, 12 Mar 2008 09:30:49 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Sat, 15 Mar 2008 18:08:17 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>12522</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Lucene Fields</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2|hxysu7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>26823</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                </customfields>
    </item>
</channel>
</rss>