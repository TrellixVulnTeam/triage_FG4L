org.apache.lucene.analysis.BaseCharFilter.addOffCorrectMap(int,int)
org.apache.lucene.analysis.BaseCharFilter.BaseCharFilter(CharStream)
org.apache.lucene.analysis.BaseCharFilter.getLastCumulativeDiff()
org.apache.lucene.analysis.BaseCharFilter.OffCorrectMap.OffCorrectMap(int,int)
org.apache.lucene.analysis.BaseCharFilter.OffCorrectMap.toString()
org.apache.lucene.analysis.BaseTokenTestCase.assertTokEq(List,List,boolean)
org.apache.lucene.analysis.BaseTokenTestCase.assertTokEqual(List,List)
org.apache.lucene.analysis.BaseTokenTestCase.assertTokEqualOff(List,List)
org.apache.lucene.analysis.BaseTokenTestCase.getTokens(TokenStream)
org.apache.lucene.analysis.BaseTokenTestCase.tokAt(List,String,int,int,int)
org.apache.lucene.analysis.BaseTokenTestCase.tokens(String)
org.apache.lucene.analysis.BaseTokenTestCase.tsToString(TokenStream)
org.apache.lucene.analysis.CharFilter.CharFilter(CharStream)
org.apache.lucene.analysis.CharFilter.close()
org.apache.lucene.analysis.CharFilter.correctOffset(int)
org.apache.lucene.analysis.CharFilter.read(char[],int,int)
org.apache.lucene.analysis.CharReader.CharReader(Reader)
org.apache.lucene.analysis.CharReader.get(Reader)
org.apache.lucene.analysis.CharTokenizer.incrementToken()
org.apache.lucene.analysis.cjk.CJKTokenizer.CJKTokenizer(Reader)
org.apache.lucene.analysis.cjk.CJKTokenizer.next(Token)
org.apache.lucene.analysis.cn.ChineseTokenizer.ChineseTokenizer(Reader)
org.apache.lucene.analysis.cn.ChineseTokenizer.flush(Token)
org.apache.lucene.analysis.cn.smart.SentenceTokenizer.next()
org.apache.lucene.analysis.cn.smart.SentenceTokenizer.SentenceTokenizer(Reader)
org.apache.lucene.analysis.MappingCharFilter.MappingCharFilter(NormalizeCharMap,CharStream)
org.apache.lucene.analysis.MappingCharFilter.mark(int)
org.apache.lucene.analysis.MappingCharFilter.markSupported()
org.apache.lucene.analysis.MappingCharFilter.match(NormalizeCharMap)
org.apache.lucene.analysis.MappingCharFilter.nextChar()
org.apache.lucene.analysis.MappingCharFilter.pushChar(int)
org.apache.lucene.analysis.MappingCharFilter.pushLastChar(int)
org.apache.lucene.analysis.MappingCharFilter.read()
org.apache.lucene.analysis.NormalizeCharMap.add(String,String)
org.apache.lucene.analysis.standard.StandardTokenizer.StandardTokenizer(Reader,boolean)
org.apache.lucene.analysis.TestCharFilter.CharFilter1.CharFilter1(CharStream)
org.apache.lucene.analysis.TestCharFilter.CharFilter1.correct(int)
org.apache.lucene.analysis.TestCharFilter.CharFilter2.CharFilter2(CharStream)
org.apache.lucene.analysis.TestCharFilter.testCharFilter1()
org.apache.lucene.analysis.TestCharFilter.testCharFilter11()
org.apache.lucene.analysis.TestCharFilter.testCharFilter12()
org.apache.lucene.analysis.TestCharFilter.testCharFilter2()
org.apache.lucene.analysis.TestMappingCharFilter.setUp()
org.apache.lucene.analysis.TestMappingCharFilter.test1to1()
org.apache.lucene.analysis.TestMappingCharFilter.test1to2()
org.apache.lucene.analysis.TestMappingCharFilter.test1to3()
org.apache.lucene.analysis.TestMappingCharFilter.test2to1()
org.apache.lucene.analysis.TestMappingCharFilter.test2to4()
org.apache.lucene.analysis.TestMappingCharFilter.test3to1()
org.apache.lucene.analysis.TestMappingCharFilter.test4to2()
org.apache.lucene.analysis.TestMappingCharFilter.test5to0()
org.apache.lucene.analysis.TestMappingCharFilter.testChained()
org.apache.lucene.analysis.TestMappingCharFilter.testNothingChange()
org.apache.lucene.analysis.TestMappingCharFilter.testTokenStream()
org.apache.lucene.analysis.Tokenizer.reset(CharStream)
org.apache.lucene.analysis.Tokenizer.Tokenizer()
org.apache.lucene.analysis.Tokenizer.Tokenizer(CharStream)
org.apache.lucene.analysis.Tokenizer.Tokenizer(Reader)
org.apache.lucene.wikipedia.analysis.WikipediaTokenizer.collapseAndSaveTokens(Token,int,String)
org.apache.lucene.wikipedia.analysis.WikipediaTokenizer.collapseTokens(Token,int)
org.apache.lucene.wikipedia.analysis.WikipediaTokenizer.reset()
org.apache.lucene.wikipedia.analysis.WikipediaTokenizer.reset(Reader)
org.apache.lucene.wikipedia.analysis.WikipediaTokenizer.setInput(Reader)
org.apache.lucene.wikipedia.analysis.WikipediaTokenizer.setupToken(Token)
