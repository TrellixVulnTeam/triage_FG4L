lucli.LuceneMethods.invertDocument(Document)
org.apache.lucene.analysis.ar.ArabicNormalizationFilter.ArabicNormalizationFilter(TokenStream)
org.apache.lucene.analysis.ar.ArabicStemFilter.ArabicStemFilter(TokenStream)
org.apache.lucene.analysis.ar.TestArabicNormalizationFilter.check(String,String)
org.apache.lucene.analysis.br.BrazilianStemFilter.BrazilianStemFilter(TokenStream)
org.apache.lucene.analysis.br.BrazilianStemFilter.BrazilianStemFilter(TokenStream,Set)
org.apache.lucene.analysis.cjk.CJKTokenizer.CJKTokenizer(Reader)
org.apache.lucene.analysis.cjk.TestCJKTokenizer.checkCJKToken(String,TestToken[])
org.apache.lucene.analysis.cjk.TestCJKTokenizer.checkCJKToken(String,Token[])
org.apache.lucene.analysis.cjk.TestCJKTokenizer.newToken(String,int,int,int)
org.apache.lucene.analysis.cjk.TestCJKTokenizer.testC()
org.apache.lucene.analysis.cjk.TestCJKTokenizer.testJa1()
org.apache.lucene.analysis.cjk.TestCJKTokenizer.testJa2()
org.apache.lucene.analysis.cjk.TestCJKTokenizer.testMix()
org.apache.lucene.analysis.cjk.TestCJKTokenizer.testMix2()
org.apache.lucene.analysis.cjk.TestCJKTokenizer.testSingleChar()
org.apache.lucene.analysis.cn.ChineseFilter.ChineseFilter(TokenStream)
org.apache.lucene.analysis.cn.ChineseTokenizer.ChineseTokenizer(Reader)
org.apache.lucene.analysis.cn.ChineseTokenizer.flush()
org.apache.lucene.analysis.cn.ChineseTokenizer.flush(Token)
org.apache.lucene.analysis.cn.ChineseTokenizer.push(char)
org.apache.lucene.analysis.cn.smart.SentenceTokenizer.SentenceTokenizer(Reader)
org.apache.lucene.analysis.cn.smart.WordSegmenter.convertSegToken(SegToken,String,int)
org.apache.lucene.analysis.cn.smart.WordSegmenter.convertSegToken(SegToken,String,int,String)
org.apache.lucene.analysis.cn.smart.WordSegmenter.segmentSentence(String,int)
org.apache.lucene.analysis.cn.smart.WordSegmenter.segmentSentence(Token)
org.apache.lucene.analysis.cn.smart.WordTokenFilter.processNextSentence(Token)
org.apache.lucene.analysis.cn.TestChineseTokenizer.testOtherLetterOffset()
org.apache.lucene.analysis.compound.CompoundWordTokenFilterBase.addAllLowerCase(Set,Collection)
org.apache.lucene.analysis.compound.CompoundWordTokenFilterBase.CompoundWordTokenFilterBase(TokenStream,Set,int,int,int,boolean)
org.apache.lucene.analysis.compound.CompoundWordTokenFilterBase.CompoundWordTokenFilterBase(TokenStream,String[],int,int,int,boolean)
org.apache.lucene.analysis.compound.CompoundWordTokenFilterBase.makeDictionary(String[])
org.apache.lucene.analysis.compound.CompoundWordTokenFilterBase.next()
org.apache.lucene.analysis.compound.CompoundWordTokenFilterBase.setToken(Token)
org.apache.lucene.analysis.compound.TestCompoundWordTokenFilter.assertFiltersTo(TokenFilter,String[],int[],int[],int[])
org.apache.lucene.analysis.compound.TestCompoundWordTokenFilter.getHyphenationPatternFileContents()
org.apache.lucene.analysis.cz.TestCzechAnalyzer.assertAnalyzesTo(Analyzer,String,String[])
org.apache.lucene.analysis.de.GermanStemFilter.GermanStemFilter(TokenStream)
org.apache.lucene.analysis.de.GermanStemFilter.GermanStemFilter(TokenStream,Set)
org.apache.lucene.analysis.el.GreekLowerCaseFilter.GreekLowerCaseFilter(TokenStream,char[])
org.apache.lucene.analysis.fr.ElisionFilter.ElisionFilter(TokenStream)
org.apache.lucene.analysis.fr.ElisionFilter.ElisionFilter(TokenStream,Set)
org.apache.lucene.analysis.fr.ElisionFilter.ElisionFilter(TokenStream,String[])
org.apache.lucene.analysis.fr.ElisionFilter.setArticles(Set)
org.apache.lucene.analysis.fr.FrenchStemFilter.FrenchStemFilter(TokenStream)
org.apache.lucene.analysis.fr.FrenchStemFilter.FrenchStemFilter(TokenStream,Set)
org.apache.lucene.analysis.fr.TestElision.filtre(TokenFilter)
org.apache.lucene.analysis.miscellaneous.SingleTokenTokenStream.getToken()
org.apache.lucene.analysis.miscellaneous.SingleTokenTokenStream.SingleTokenTokenStream(Token)
org.apache.lucene.analysis.ngram.EdgeNGramTokenFilter.EdgeNGramTokenFilter(TokenStream)
org.apache.lucene.analysis.ngram.EdgeNGramTokenFilter.EdgeNGramTokenFilter(TokenStream,Side,int,int)
org.apache.lucene.analysis.ngram.EdgeNGramTokenFilter.EdgeNGramTokenFilter(TokenStream,String,int,int)
org.apache.lucene.analysis.ngram.EdgeNGramTokenFilter.ngram(Token)
org.apache.lucene.analysis.ngram.EdgeNGramTokenFilterTest.testBackRangeOfNgrams()
org.apache.lucene.analysis.ngram.EdgeNGramTokenFilterTest.testBackUnigram()
org.apache.lucene.analysis.ngram.EdgeNGramTokenFilterTest.testFrontRangeOfNgrams()
org.apache.lucene.analysis.ngram.EdgeNGramTokenFilterTest.testFrontUnigram()
org.apache.lucene.analysis.ngram.EdgeNGramTokenFilterTest.testOversizedNgrams()
org.apache.lucene.analysis.ngram.EdgeNGramTokenFilterTest.testSmallTokenInStream()
org.apache.lucene.analysis.ngram.EdgeNGramTokenizer.EdgeNGramTokenizer(Reader,Side,int,int)
org.apache.lucene.analysis.ngram.EdgeNGramTokenizer.EdgeNGramTokenizer(Reader,String,int,int)
org.apache.lucene.analysis.ngram.NGramTokenFilter.NGramTokenFilter(TokenStream)
org.apache.lucene.analysis.ngram.NGramTokenFilter.NGramTokenFilter(TokenStream,int,int)
org.apache.lucene.analysis.ngram.NGramTokenFilterTest.checkStream(TokenStream,String[])
org.apache.lucene.analysis.ngram.NGramTokenFilterTest.setUp()
org.apache.lucene.analysis.ngram.NGramTokenFilterTest.testBigrams()
org.apache.lucene.analysis.ngram.NGramTokenFilterTest.testInvalidInput2()
org.apache.lucene.analysis.ngram.NGramTokenFilterTest.testNgrams()
org.apache.lucene.analysis.ngram.NGramTokenFilterTest.testUnigrams()
org.apache.lucene.analysis.ngram.NGramTokenizer.NGramTokenizer(Reader)
org.apache.lucene.analysis.ngram.NGramTokenizer.NGramTokenizer(Reader,int,int)
org.apache.lucene.analysis.nl.DutchStemFilter.DutchStemFilter(TokenStream)
org.apache.lucene.analysis.nl.DutchStemFilter.DutchStemFilter(TokenStream,Set,Map)
org.apache.lucene.analysis.payloads.DelimitedPayloadTokenFilterTest.assertTermEquals(String,TokenStream,byte[])
org.apache.lucene.analysis.payloads.DelimitedPayloadTokenFilterTest.testIntEncoding()
org.apache.lucene.analysis.payloads.DelimitedPayloadTokenFilterTest.testNext()
org.apache.lucene.analysis.payloads.NumericPayloadTokenFilter.NumericPayloadTokenFilter(TokenStream,float,String)
org.apache.lucene.analysis.payloads.TokenOffsetPayloadTokenFilter.TokenOffsetPayloadTokenFilter(TokenStream)
org.apache.lucene.analysis.payloads.TypeAsPayloadTokenFilter.TypeAsPayloadTokenFilter(TokenStream)
org.apache.lucene.analysis.position.PositionFilter.PositionFilter(TokenStream)
org.apache.lucene.analysis.position.PositionFilter.PositionFilter(TokenStream,int)
org.apache.lucene.analysis.position.PositionFilterTest.createToken(String)
org.apache.lucene.analysis.position.PositionFilterTest.filterTest(TokenStream,String[],int[])
org.apache.lucene.analysis.position.PositionFilterTest.filterTest(TokenStream,Token[],int[])
org.apache.lucene.analysis.position.PositionFilterTest.main(String[])
org.apache.lucene.analysis.position.PositionFilterTest.test6GramFilterNoPositions()
org.apache.lucene.analysis.position.PositionFilterTest.TestTokenStream.incrementToken()
org.apache.lucene.analysis.position.PositionFilterTest.TestTokenStream.next(Token)
org.apache.lucene.analysis.position.PositionFilterTest.TestTokenStream.reset()
org.apache.lucene.analysis.position.PositionFilterTest.TestTokenStream.TestTokenStream(String[])
org.apache.lucene.analysis.position.PositionFilterTest.TestTokenStream.TestTokenStream(Token[])
org.apache.lucene.analysis.reverse.ReverseStringFilter.reverse(String)
org.apache.lucene.analysis.reverse.ReverseStringFilter.ReverseStringFilter(TokenStream)
org.apache.lucene.analysis.reverse.TestReverseStringFilter.testFilter()
org.apache.lucene.analysis.reverse.TestReverseStringFilter.testReverseString()
org.apache.lucene.analysis.ru.RussianLowerCaseFilter.RussianLowerCaseFilter(TokenStream,char[])
org.apache.lucene.analysis.ru.RussianStemFilter.RussianStemFilter(TokenStream,char[])
org.apache.lucene.analysis.ru.TestRussianAnalyzer.test1251()
org.apache.lucene.analysis.ru.TestRussianAnalyzer.testDigitsInRussianCharset()
org.apache.lucene.analysis.ru.TestRussianAnalyzer.testKOI8()
org.apache.lucene.analysis.ru.TestRussianAnalyzer.testUnicode()
org.apache.lucene.analysis.sinks.TokenTypeSinkTokenizerTest.test()
org.apache.lucene.analysis.sinks.TokenTypeSinkTokenizerTest.WordTokenFilter.WordTokenFilter(TokenStream)
org.apache.lucene.analysis.snowball.SnowballFilter.SnowballFilter(TokenStream,SnowballProgram)
org.apache.lucene.analysis.snowball.SnowballFilter.SnowballFilter(TokenStream,String)
org.apache.lucene.analysis.snowball.TestSnowball.testFilterTokens()
org.apache.lucene.analysis.snowball.TestSnowball.TestTokenStream.TestTokenStream()
org.apache.lucene.analysis.th.TestThaiAnalyzer.assertAnalyzesTo(Analyzer,String,String[],int,int,String)
org.apache.lucene.analysis.th.ThaiWordFilter.ThaiWordFilter(TokenStream)
org.apache.lucene.collation.CollationKeyFilter.CollationKeyFilter(TokenStream,Collator)
org.apache.lucene.collation.ICUCollationKeyFilter.ICUCollationKeyFilter(TokenStream,Collator)
org.apache.lucene.index.memory.AnalyzerUtil.getLoggingAnalyzer(Analyzer,PrintStream,String)
org.apache.lucene.index.memory.AnalyzerUtil.getLoggingAnalyzer.tokenStream.toString(boolean)
org.apache.lucene.index.memory.AnalyzerUtil.getLoggingAnalyzer.tokenStream.toString(Token)
org.apache.lucene.index.memory.AnalyzerUtil.getMaxTokenAnalyzer(Analyzer,int)
org.apache.lucene.index.memory.AnalyzerUtil.getMostFrequentTerms(Analyzer,String,int)
org.apache.lucene.index.memory.AnalyzerUtil.getTokenCachingAnalyzer(Analyzer)
org.apache.lucene.index.memory.MemoryIndex.addField(String,TokenStream,float)
org.apache.lucene.index.memory.MemoryIndex.keywordTokenStream(Collection)
org.apache.lucene.index.memory.PatternAnalyzer.FastStringTokenizer.FastStringTokenizer(String,boolean,boolean,Set)
org.apache.lucene.index.memory.PatternAnalyzer.FastStringTokenizer.isTokenChar(char,boolean)
org.apache.lucene.index.memory.PatternAnalyzer.PatternTokenizer.PatternTokenizer(String,Pattern,boolean)
org.apache.lucene.index.memory.SynonymTokenFilter.createToken(String,AttributeSource.State)
org.apache.lucene.index.memory.SynonymTokenFilter.createToken(String,Token,Token)
org.apache.lucene.index.memory.SynonymTokenFilter.SynonymTokenFilter(TokenStream,SynonymMap,int)
org.apache.lucene.queryParser.analyzing.AnalyzingQueryParser.getFuzzyQuery(String,String,float)
org.apache.lucene.queryParser.analyzing.AnalyzingQueryParser.getPrefixQuery(String,String)
org.apache.lucene.queryParser.analyzing.AnalyzingQueryParser.getRangeQuery(String,String,String,boolean)
org.apache.lucene.queryParser.analyzing.AnalyzingQueryParser.getWildcardQuery(String,String)
org.apache.lucene.search.FuzzyLikeThisQuery.addTerms(IndexReader,FieldVals)
org.apache.lucene.search.similar.MoreLikeThis.addTermFrequencies(Reader,Map,String)
org.apache.lucene.search.similar.SimilarityQueries.formSimilarQuery(String,Analyzer,String,Set)
org.apache.lucene.search.vectorhighlight.AbstractTestCase.BasicNGramTokenizer.BasicNGramTokenizer(Reader,int,String)
org.apache.lucene.search.vectorhighlight.AbstractTestCase.BasicNGramTokenizer.getFinalOffset()
org.apache.lucene.search.vectorhighlight.IndexTimeSynonymTest.TokenArrayAnalyzer.TokenArrayAnalyzer(Token)
org.apache.lucene.search.vectorhighlight.IndexTimeSynonymTest.TokenArrayAnalyzer.tokenStream(String,Reader)
org.apache.lucene.store.instantiated.TestIndicesEquals.assembleDocument(Document,int)
org.apache.lucene.wikipedia.analysis.WikipediaTokenizer.collapseAndSaveTokens(int,String)
org.apache.lucene.wikipedia.analysis.WikipediaTokenizer.collapseAndSaveTokens(Token,int,String)
org.apache.lucene.wikipedia.analysis.WikipediaTokenizer.collapseTokens(int)
org.apache.lucene.wikipedia.analysis.WikipediaTokenizer.collapseTokens(Token,int)
org.apache.lucene.wikipedia.analysis.WikipediaTokenizerImpl.getText(TermAttribute)
org.apache.lucene.wikipedia.analysis.WikipediaTokenizerImpl.getText(Token)
org.apache.lucene.wikipedia.analysis.WikipediaTokenizer.setInput(Reader)
org.apache.lucene.wikipedia.analysis.WikipediaTokenizer.setupSavedToken(int,String)
org.apache.lucene.wikipedia.analysis.WikipediaTokenizer.setupSavedToken(Token,int,String)
org.apache.lucene.wikipedia.analysis.WikipediaTokenizer.setupToken()
org.apache.lucene.wikipedia.analysis.WikipediaTokenizer.setupToken(Token)
org.apache.lucene.wikipedia.analysis.WikipediaTokenizerTest.checkLinkPhrases(WikipediaTokenizer)
org.apache.lucene.wikipedia.analysis.WikipediaTokenizerTest.testBoth()
org.apache.lucene.wikipedia.analysis.WikipediaTokenizerTest.testHandwritten()
org.apache.lucene.wikipedia.analysis.WikipediaTokenizerTest.testLinkPhrases()
org.apache.lucene.wikipedia.analysis.WikipediaTokenizerTest.testLinks()
org.apache.lucene.wikipedia.analysis.WikipediaTokenizerTest.testLucene1133()
org.apache.lucene.wikipedia.analysis.WikipediaTokenizer.WikipediaTokenizer(Reader,int,Set)
org.apache.lucene.wordnet.SynExpand.expand(String,Searcher,Analyzer,String,float)
org.apache.lucene.xmlparser.builders.LikeThisQueryBuilder.getQuery(Element)
org.apache.lucene.xmlparser.builders.SpanOrTermsBuilder.getSpanQuery(Element)
org.apache.lucene.xmlparser.builders.TermsFilterBuilder.getFilter(Element)
