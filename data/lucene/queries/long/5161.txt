Today there is a loop in SimpleFS/NIOFS:


try {
          do {
            final int readLength;
            if (total + chunkSize > len) {
              readLength = len - total;
            } else {
              // LUCENE-1566 - work around JVM Bug by breaking very large reads into chunks
              readLength = chunkSize;
            }
            final int i = file.read(b, offset + total, readLength);
            total += i;
          } while (total < len);
        } catch (OutOfMemoryError e) {



I bet if you look at the clover report its untested, because its fixed at 100MB for 32-bit users and 2GB for 64-bit users (are these defaults even good?!).

Also if you call the setter on a 64-bit machine to change the size, it just totally ignores it. We should remove that, the setter should always work.

And we should set it to small values in tests so this loop is actually executed.